# vLLM Kernel å¼€å‘æŒ‡å—

## ä¸€ã€æ ¸å¿ƒé—®é¢˜è§£ç­”

### Q: vllm/vllm-openai é•œåƒé‡Œé¢æ˜¯ä»€ä¹ˆï¼Ÿ

```
/usr/local/lib/python3.12/dist-packages/vllm/
â”œâ”€â”€ _C.abi3.so                      # ç¼–è¯‘å¥½çš„ C++/CUDA æ‰©å±•
â”œâ”€â”€ model_executor/
â”‚   â””â”€â”€ layers/
â”‚       â”œâ”€â”€ linear.py               # ğŸ‘ˆ çº¿æ€§å±‚å®ç°ï¼ˆä½ è¦åŠ«æŒçš„åœ°æ–¹ï¼‰
â”‚       â”œâ”€â”€ utils.py                # ğŸ‘ˆ GEMM å‡½æ•°æ´¾å‘
â”‚       â””â”€â”€ quantization/           # ğŸ‘ˆ é‡åŒ–æ–¹æ³•
â””â”€â”€ ...
```

**å…³é”®ç‚¹**ï¼š
- vLLM æ˜¯é€šè¿‡ `pip install` å®‰è£…çš„ï¼Œä¸æ˜¯æºç 
- `.so` æ–‡ä»¶æ˜¯ç¼–è¯‘å¥½çš„ C++/CUDA kernel
- `.py` æ–‡ä»¶å¯ä»¥é€šè¿‡ `pip install -e .` è¦†ç›–

### Q: å¦‚ä½•è®©ä¿®æ”¹çš„ä»£ç ç”Ÿæ•ˆï¼Ÿ

1. `pip uninstall vllm` - ç§»é™¤é¢„è£…çš„ vLLM
2. æŠŠä½ çš„ vLLM æºç æŒ‚è½½è¿›å®¹å™¨
3. `pip install -e .` - å¯ç¼–è¾‘æ¨¡å¼å®‰è£…ï¼ˆæ”¹ä»£ç ç«‹å³ç”Ÿæ•ˆï¼‰

---

## äºŒã€å®Œæ•´å¼€å‘æµç¨‹

### Step 1: æ„å»ºå¼€å‘é•œåƒ

```bash
cd /home/v-hanshao/vllmbench
docker build -t vllm-dev:v0.13.0 -f Dockerfile.dev .
```

### Step 2: å¯åŠ¨å¼€å‘å®¹å™¨

```bash
docker run --gpus all -it --rm --ipc=host \
    --name vllm-kernel-dev \
    -v /home/v-hanshao/vllmbench:/root/vllmbench \
    -v /home/v-hanshao/GPU:/root/GPU \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -e HF_TOKEN=${HF_TOKEN} \
    vllm-dev:v0.13.0 /bin/bash
```

### Step 3: å®‰è£… vLLM æºç ï¼ˆè¿›å…¥å®¹å™¨åæ‰§è¡Œä¸€æ¬¡ï¼‰

```bash
cd /root/vllmbench

# æ–¹æ¡ˆA: ä»…ä¿®æ”¹ Python ä»£ç ï¼ˆæ¨èï¼Œæœ€å¿«ï¼‰
VLLM_USE_PRECOMPILED=1 pip install -e .

# æ–¹æ¡ˆB: éœ€è¦ä¿®æ”¹ C++/CUDA ä»£ç ï¼ˆé¦–æ¬¡çº¦ 15-30 åˆ†é’Ÿï¼‰
pip install -e .
```

### Step 4: éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥ vLLM ç‰ˆæœ¬
python -c "import vllm; print(vllm.__version__)"

# è¿è¡Œç®€å•æµ‹è¯•
vllm bench throughput --model Qwen/Qwen2.5-0.5B --input-len 128 --output-len 64 --num-prompts 10
```

---

## ä¸‰ã€ä½ è¦åŠ«æŒçš„ä»£ç ä½ç½®

### 1. GEMM æ›¿æ¢ç‚¹ï¼ˆæœ€ç›´æ¥ï¼‰

**æ–‡ä»¶**: `vllm/model_executor/layers/utils.py`

```python
# ç¬¬ 96-103 è¡Œ
def default_unquantized_gemm(
    layer: torch.nn.Module,
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor | None = None,
):
    return torch.nn.functional.linear(x, weight, bias)  # ğŸ‘ˆ æ›¿æ¢è¿™é‡Œï¼
```

### 2. Linear å±‚ apply æ–¹æ³•

**æ–‡ä»¶**: `vllm/model_executor/layers/linear.py`

```python
# UnquantizedLinearMethod.apply() ç¬¬ 237 è¡Œ
def apply(
    self,
    layer: torch.nn.Module,
    x: torch.Tensor,
    bias: torch.Tensor | None = None,
) -> torch.Tensor:
    return dispatch_unquantized_gemm()(layer, x, layer.weight, bias)
```

### 3. è‡ªå®šä¹‰é‡åŒ–æ–¹æ³•

**ç›®å½•**: `vllm/model_executor/layers/quantization/`

å‚è€ƒ `fp8.py` çš„å®ç°ï¼Œåˆ›å»ºä½ è‡ªå·±çš„é‡åŒ–ç±»ã€‚

---

## å››ã€ä»£ç ä¿®æ”¹ç¤ºä¾‹

### ç¤ºä¾‹1: åœ¨ utils.py ä¸­æ·»åŠ ä½ çš„ Kernel å¼€å…³

```python
# vllm/model_executor/layers/utils.py

import os

# ä½ çš„è‡ªå®šä¹‰ Kernel å¼€å…³
USE_CUSTOM_GEMM = os.environ.get("VLLM_USE_CUSTOM_GEMM", "0") == "1"

def default_unquantized_gemm(
    layer: torch.nn.Module,
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor | None = None,
):
    if USE_CUSTOM_GEMM:
        # è°ƒç”¨ä½ çš„è‡ªå®šä¹‰ GEMM
        from your_custom_kernels import custom_gemm
        return custom_gemm(x, weight, bias)
    
    return torch.nn.functional.linear(x, weight, bias)
```

### ç¤ºä¾‹2: åˆ›å»ºå®Œæ•´çš„è‡ªå®šä¹‰é‡åŒ–æ–¹æ³•

è§ `custom_kernels/README.md` ä¸­çš„å®Œæ•´ç¤ºä¾‹ã€‚

---

## äº”ã€è¿è¡Œ Benchmark

```bash
# åŸºç¡€ååé‡æµ‹è¯•
vllm bench throughput \
    --model Qwen/Qwen2.5-0.5B \
    --input-len 512 \
    --output-len 128 \
    --num-prompts 100

# å»¶è¿Ÿæµ‹è¯•
vllm bench latency \
    --model Qwen/Qwen2.5-0.5B \
    --input-len 512 \
    --output-len 128

# ä½¿ç”¨ä½ çš„è‡ªå®šä¹‰ Kernel
VLLM_USE_CUSTOM_GEMM=1 vllm bench throughput \
    --model Qwen/Qwen2.5-0.5B \
    --input-len 512 \
    --output-len 128
```

---

## å…­ã€å¼€å‘æŠ€å·§

### 1. å¿«é€Ÿè°ƒè¯•ï¼ˆä¸é‡å¯å®¹å™¨ï¼‰

å› ä¸ºä½¿ç”¨äº† `-e` å¯ç¼–è¾‘æ¨¡å¼ï¼Œä¿®æ”¹ Python ä»£ç åç›´æ¥è¿è¡Œæ–°å‘½ä»¤å³å¯ç”Ÿæ•ˆã€‚

### 2. ç¼–è¯‘è‡ªå®šä¹‰ CUDA Kernel

```bash
cd /root/vllmbench/custom_kernels
nvcc -shared -o libcustom_gemm.so \
    -lcublas -lcusparselt \
    -I/usr/local/cuda/include \
    custom_gemm.cu
```

### 3. ä¿å­˜å¼€å‘ç¯å¢ƒï¼ˆé¿å…é‡å¤ç¼–è¯‘ï¼‰

```bash
# é¦–æ¬¡ pip install -e . å®Œæˆå
docker commit vllm-kernel-dev vllm-dev:v0.13.0-compiled
```

---

## ä¸ƒã€ç›®å½•ç»“æ„å»ºè®®

```
/root/vllmbench/              # vLLM æºç  (pip install -e .)
â”œâ”€â”€ vllm/
â”‚   â””â”€â”€ model_executor/
â”‚       â””â”€â”€ layers/
â”‚           â”œâ”€â”€ linear.py     # ä¿®æ”¹è¿™é‡ŒåŠ«æŒ Linear å±‚
â”‚           â””â”€â”€ utils.py      # ä¿®æ”¹è¿™é‡Œæ›¿æ¢ GEMM
â”‚
â””â”€â”€ custom_kernels/           # ä½ çš„è‡ªå®šä¹‰ Kernel
    â”œâ”€â”€ triton/
    â”‚   â””â”€â”€ quant_expand.py   # Triton Quant+Expand å®ç°
    â”œâ”€â”€ cuda/
    â”‚   â””â”€â”€ custom_gemm.cu    # CUDA GEMM å®ç°
    â””â”€â”€ __init__.py           # Python å°è£…
```
