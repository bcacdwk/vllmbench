# vLLM æ¡†æ¶æ¦‚è¿° (Framework Overview)

æœ¬æ–‡æ¡£æ—¨åœ¨å¸®åŠ©ä½ å…¨é¢äº†è§£ vLLM é¡¹ç›®çš„æ•´ä½“ç›®å½•ç»“æ„ã€å„ä¸ªæ–‡ä»¶å¤¹çš„ç”¨é€”ã€æ ¸å¿ƒè®¾è®¡ç†å¿µï¼Œä»¥åŠå¦‚ä½•è¿è¡Œå’Œæµ‹è¯•è¿™ä¸ªé¡¹ç›®ã€‚æ— è®ºä½ æ˜¯åˆå­¦è€…è¿˜æ˜¯æœ‰ç»éªŒçš„å¼€å‘è€…ï¼Œæœ¬æ–‡æ¡£éƒ½å°†ä¸ºä½ æä¾›æ·±å…¥ç†è§£ vLLM çš„åŸºç¡€ã€‚

---

## 0. vLLM ç®€ä»‹ä¸æ ¸å¿ƒè®¾è®¡ç†å¿µ

### 0.1 ä»€ä¹ˆæ˜¯ vLLMï¼Ÿ

vLLMï¼ˆè™šæ‹ŸåŒ–å¤§è¯­è¨€æ¨¡å‹ï¼‰æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’ŒæœåŠ¡å¼•æ“ã€‚å®ƒç”±åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡çš„ç ”ç©¶äººå‘˜å¼€å‘ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„å†…å­˜ç®¡ç†å’Œååé‡é—®é¢˜ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- **PagedAttention**: é©å‘½æ€§çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°† KV Cache åˆ†é¡µç®¡ç†ï¼Œç±»ä¼¼æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜åˆ†é¡µ
- **è¿ç»­æ‰¹å¤„ç† (Continuous Batching)**: åŠ¨æ€åœ°å°†ä¸åŒè¯·æ±‚æ‰¹å¤„ç†åœ¨ä¸€èµ·ï¼Œæœ€å¤§åŒ–GPUåˆ©ç”¨ç‡
- **é«˜æ•ˆå†…å­˜ç®¡ç†**: é€šè¿‡åˆ†é¡µæœºåˆ¶å‡å°‘å†…å­˜ç¢ç‰‡å’Œæµªè´¹
- **CUDA Graph æ”¯æŒ**: å‡å°‘ Python/CUDA è°ƒç”¨å¼€é”€
- **å¼ é‡å¹¶è¡Œ (Tensor Parallelism)**: æ”¯æŒå¤šGPUåˆ†å¸ƒå¼æ¨ç†
- **å¤šæ¨¡æ€æ”¯æŒ**: æ”¯æŒè§†è§‰-è¯­è¨€æ¨¡å‹ã€éŸ³é¢‘æ¨¡å‹ç­‰
- **é‡åŒ–æ¨ç†**: æ”¯æŒ FP8ã€AWQã€GPTQ ç­‰å¤šç§é‡åŒ–æ–¹æ¡ˆ

### 0.2 vLLM çš„æ ¸å¿ƒæŠ€æœ¯åŸç†

#### PagedAttention æœºåˆ¶

ä¼ ç»Ÿ LLM æ¨ç†éœ€è¦ä¸ºæ¯ä¸ªè¯·æ±‚é¢„åˆ†é…å›ºå®šå¤§å°çš„ KV Cache å†…å­˜ï¼Œå¯¼è‡´å¤§é‡å†…å­˜æµªè´¹ã€‚PagedAttention é€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³è¿™ä¸ªé—®é¢˜ï¼š

```
ä¼ ç»Ÿæ–¹å¼:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request 1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  (50% waste) â”‚
â”‚ Request 2: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘]  (37% waste) â”‚
â”‚ Request 3: [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  (75% waste) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PagedAttention:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Block Pool: [â–ˆâ–ˆ][â–ˆâ–ˆ][â–ˆâ–ˆ][â–ˆâ–ˆ][â–ˆâ–ˆ][â–‘â–‘]       â”‚
â”‚ Request 1:  Block 0 -> Block 2             â”‚
â”‚ Request 2:  Block 1 -> Block 3 -> Block 4  â”‚
â”‚ Request 3:  Block 5                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### è¿ç»­æ‰¹å¤„ç† (Continuous Batching)

ä¸ä¼ ç»Ÿçš„é™æ€æ‰¹å¤„ç†ä¸åŒï¼ŒvLLM é‡‡ç”¨è¿ç»­æ‰¹å¤„ç†ï¼š
- **åŠ¨æ€åŠ å…¥**: æ–°è¯·æ±‚å¯ä»¥åœ¨ä»»æ„æ—¶åˆ»åŠ å…¥æ‰¹å¤„ç†
- **åŠ¨æ€é€€å‡º**: å®Œæˆçš„è¯·æ±‚ç«‹å³é‡Šæ”¾èµ„æº
- **æ··åˆé˜¶æ®µ**: åŒä¸€æ‰¹æ¬¡å¯ä»¥åŒæ—¶åŒ…å« prefill å’Œ decode é˜¶æ®µçš„è¯·æ±‚

```
æ—¶é—´çº¿:
t1: [Req1-prefill] [Req2-prefill]
t2: [Req1-decode]  [Req2-decode]  [Req3-prefill]  <- Req3 åŠ¨æ€åŠ å…¥
t3: [Req1-done]    [Req2-decode]  [Req3-decode]   <- Req1 å®Œæˆé€€å‡º
t4:                [Req2-done]    [Req3-decode]  [Req4-prefill]
```

### 0.3 æ¶æ„æ¼”è¿›ï¼šV0 åˆ° V1

vLLM ç»å†äº†é‡å¤§æ¶æ„å‡çº§ï¼š

| ç‰¹æ€§ | V0 (Legacy) | V1 (å½“å‰æ¨è) |
|------|-------------|---------------|
| è°ƒåº¦å™¨ | åŒæ­¥è°ƒåº¦ | å¼‚æ­¥è°ƒåº¦ |
| KV Cache ç®¡ç† | åŸºæœ¬åˆ†é¡µ | ä¼˜åŒ–çš„ Prefix Caching |
| å¹¶è¡Œæ”¯æŒ | åŸºç¡€ TP/PP | å¢å¼ºçš„ DP/TP/PP |
| æŠ•æœºè§£ç  | åŸºç¡€æ”¯æŒ | Eagle/Medusa/NGram |
| ä»£ç ä½ç½® | `vllm/engine/` | `vllm/v1/` |

**å½“å‰çŠ¶æ€**: V1 æ˜¯é»˜è®¤æ¶æ„ï¼Œ`vllm/engine/llm_engine.py` ç°åœ¨æŒ‡å‘ V1 å®ç°ã€‚

---

## 1. é¡¹ç›®ç›®å½•ç»“æ„æ¦‚è§ˆ

```
vllmbench/
â”œâ”€â”€ vllm/                   # ğŸ”¥ æ ¸å¿ƒæ¨ç†æ¡†æ¶ï¼ˆæœ€é‡è¦ï¼‰
â”‚   â”œâ”€â”€ entrypoints/        # ç”¨æˆ·æ¥å£å…¥å£
â”‚   â”œâ”€â”€ engine/             # æ¨ç†å¼•æ“ï¼ˆæŒ‡å‘V1ï¼‰
â”‚   â”œâ”€â”€ v1/                 # V1 æ–°æ¶æ„ï¼ˆæ ¸å¿ƒå®ç°ï¼‰
â”‚   â”œâ”€â”€ model_executor/     # æ¨¡å‹æ‰§è¡Œå™¨ï¼ˆæ¨¡å‹å®šä¹‰ã€é‡åŒ–å±‚ï¼‰
â”‚   â”œâ”€â”€ attention/          # æ³¨æ„åŠ›æœºåˆ¶å®ç°
â”‚   â”œâ”€â”€ distributed/        # åˆ†å¸ƒå¼ç›¸å…³ä»£ç 
â”‚   â”œâ”€â”€ config/             # é…ç½®ç±»å®šä¹‰
â”‚   â”œâ”€â”€ compilation/        # ç¼–è¯‘ä¼˜åŒ–ï¼ˆCUDA Graphç­‰ï¼‰
â”‚   â””â”€â”€ ...                 # å…¶ä»–å­æ¨¡å—
â”œâ”€â”€ benchmarks/             # æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
â”œâ”€â”€ tests/                  # æµ‹è¯•ç”¨ä¾‹ï¼ˆéå¸¸å…¨é¢ï¼‰
â”œâ”€â”€ examples/               # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ csrc/                   # C++/CUDA æºä»£ç 
â”œâ”€â”€ docs/                   # æ–‡æ¡£
â”œâ”€â”€ tools/                  # è¾…åŠ©å·¥å…·
â”œâ”€â”€ custom_kernels/         # è‡ªå®šä¹‰kernelç¤ºä¾‹
â”œâ”€â”€ requirements/           # ä¾èµ–é…ç½®
â”œâ”€â”€ cmake/                  # CMake æ„å»ºé…ç½®
â”œâ”€â”€ .buildkite/             # CI/CD é…ç½®
â”œâ”€â”€ .github/                # GitHub Actions é…ç½®
â””â”€â”€ ...                     # å…¶ä»–é…ç½®æ–‡ä»¶
```

---

## 2. å„ç›®å½•è¯¦ç»†è¯´æ˜

### 2.1 `vllm/` - æ ¸å¿ƒæ¨ç†æ¡†æ¶ â­â­â­

è¿™æ˜¯æ•´ä¸ª vLLM é¡¹ç›®çš„æ ¸å¿ƒï¼ŒåŒ…å«äº†æ‰€æœ‰æ¨ç†ç›¸å…³çš„ä»£ç ã€‚å†…éƒ¨ç»„ç»‡éå¸¸å¤æ‚ï¼Œè¯¦ç»†ä»‹ç»è¯·å‚è€ƒ [framework_vllmcore.md](./framework_vllmcore.md)ã€‚

#### vllm ç›®å½•å†…éƒ¨ç»“æ„è¯¦è§£ï¼š

```
vllm/
â”œâ”€â”€ __init__.py              # æ¨¡å—åˆå§‹åŒ–ï¼Œå¯¼å‡ºå…¬å…± API
â”œâ”€â”€ entrypoints/             # ğŸ”µ ç”¨æˆ·æ¥å£å…¥å£ç‚¹
â”‚   â”œâ”€â”€ llm.py               # LLM ç±» - ç¦»çº¿æ¨ç†ä¸»å…¥å£
â”‚   â”œâ”€â”€ api_server.py        # FastAPI æœåŠ¡å™¨
â”‚   â”œâ”€â”€ openai/              # OpenAI å…¼å®¹ API
â”‚   â”‚   â”œâ”€â”€ api_server.py    # OpenAI API æœåŠ¡å™¨
â”‚   â”‚   â”œâ”€â”€ serving_chat.py  # Chat Completion å¤„ç†
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ cli/                 # å‘½ä»¤è¡Œæ¥å£
â”‚       â”œâ”€â”€ main.py          # CLI ä¸»å…¥å£
â”‚       â”œâ”€â”€ serve.py         # serve å‘½ä»¤
â”‚       â”œâ”€â”€ openai.py        # OpenAI å…¼å®¹å‘½ä»¤
â”‚       â””â”€â”€ benchmark/       # benchmark å­å‘½ä»¤
â”‚
â”œâ”€â”€ engine/                  # ğŸ”µ æ¨ç†å¼•æ“ (Legacyï¼Œç°æŒ‡å‘ V1)
â”‚   â”œâ”€â”€ llm_engine.py        # ç°åœ¨å¯¼å…¥è‡ª v1
â”‚   â”œâ”€â”€ async_llm_engine.py  # å¼‚æ­¥å¼•æ“
â”‚   â””â”€â”€ arg_utils.py         # å‚æ•°è§£æ
â”‚
â”œâ”€â”€ v1/                      # ğŸ”¥ V1 æ–°æ¶æ„ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”œâ”€â”€ engine/              # V1 å¼•æ“
â”‚   â”‚   â”œâ”€â”€ llm_engine.py    # LLMEngine ä¸»ç±»
â”‚   â”‚   â”œâ”€â”€ core_client.py   # å¼•æ“æ ¸å¿ƒå®¢æˆ·ç«¯
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ worker/              # Worker å®ç°
â”‚   â”‚   â”œâ”€â”€ gpu_model_runner.py  # GPU æ¨¡å‹è¿è¡Œå™¨
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ core/                # æ ¸å¿ƒè°ƒåº¦é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ sched/           # è°ƒåº¦å™¨
â”‚   â”‚   â”œâ”€â”€ kv_cache_manager.py  # KV Cache ç®¡ç†
â”‚   â”‚   â””â”€â”€ block_pool.py    # å—æ± ç®¡ç†
â”‚   â”œâ”€â”€ attention/           # V1 æ³¨æ„åŠ›
â”‚   â”œâ”€â”€ sample/              # é‡‡æ ·å™¨
â”‚   â””â”€â”€ spec_decode/         # æŠ•æœºè§£ç 
â”‚
â”œâ”€â”€ model_executor/          # ğŸ”´ æ¨¡å‹æ‰§è¡Œå™¨ï¼ˆéå¸¸é‡è¦ï¼‰
â”‚   â”œâ”€â”€ models/              # 200+ æ¨¡å‹å®ç°
â”‚   â”‚   â”œâ”€â”€ llama.py         # Llama ç³»åˆ—
â”‚   â”‚   â”œâ”€â”€ qwen2.py         # Qwen ç³»åˆ—
â”‚   â”‚   â”œâ”€â”€ mixtral.py       # MoE æ¨¡å‹
â”‚   â”‚   â””â”€â”€ registry.py      # æ¨¡å‹æ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ layers/              # æ¨¡å‹å±‚å®ç°
â”‚   â”‚   â”œâ”€â”€ linear.py        # çº¿æ€§å±‚ï¼ˆå«é‡åŒ–ï¼‰
â”‚   â”‚   â”œâ”€â”€ activation.py    # æ¿€æ´»å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ layernorm.py     # LayerNorm
â”‚   â”‚   â”œâ”€â”€ rotary_embedding/  # RoPE ä½ç½®ç¼–ç 
â”‚   â”‚   â”œâ”€â”€ fused_moe/       # èåˆ MoE å±‚
â”‚   â”‚   â””â”€â”€ quantization/    # é‡åŒ–å®ç°
â”‚   â”‚       â”œâ”€â”€ fp8.py       # FP8 é‡åŒ–
â”‚   â”‚       â”œâ”€â”€ awq.py       # AWQ é‡åŒ–
â”‚   â”‚       â””â”€â”€ gptq.py      # GPTQ é‡åŒ–
â”‚   â””â”€â”€ model_loader/        # æ¨¡å‹åŠ è½½å™¨
â”‚
â”œâ”€â”€ attention/               # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ layer.py             # æ³¨æ„åŠ›å±‚å°è£…
â”‚   â”œâ”€â”€ selector.py          # åç«¯é€‰æ‹©å™¨
â”‚   â””â”€â”€ backends/            # æ³¨æ„åŠ›åç«¯
â”‚       â”œâ”€â”€ abstract.py      # æŠ½è±¡åŸºç±»
â”‚       â”œâ”€â”€ registry.py      # åç«¯æ³¨å†Œè¡¨
â”‚       â””â”€â”€ utils.py         # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ distributed/             # åˆ†å¸ƒå¼æ”¯æŒ
â”‚   â”œâ”€â”€ parallel_state.py    # å¹¶è¡ŒçŠ¶æ€ç®¡ç†
â”‚   â””â”€â”€ kv_transfer/         # KV Cache ä¼ è¾“
â”‚
â”œâ”€â”€ config/                  # é…ç½®ç±»
â”‚   â”œâ”€â”€ model.py             # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ cache.py             # KV Cache é…ç½®
â”‚   â”œâ”€â”€ vllm.py              # ä¸»é…ç½® VllmConfig
â”‚   â”œâ”€â”€ parallel.py          # å¹¶è¡Œé…ç½®
â”‚   â””â”€â”€ scheduler.py         # è°ƒåº¦å™¨é…ç½®
â”‚
â”œâ”€â”€ compilation/             # ç¼–è¯‘ä¼˜åŒ–
â”‚   â”œâ”€â”€ cuda_graph.py        # CUDA Graph æ”¯æŒ
â”‚   â”œâ”€â”€ counter.py           # ç¼–è¯‘è®¡æ•°å™¨
â”‚   â”œâ”€â”€ fusion.py            # ç®—å­èåˆ
â”‚   â””â”€â”€ backends.py          # ç¼–è¯‘åç«¯
â”‚
â”œâ”€â”€ platforms/               # å¹³å°é€‚é…
â”‚   â”œâ”€â”€ cuda.py              # CUDA æ”¯æŒ
â”‚   â”œâ”€â”€ rocm.py              # ROCm/AMD æ”¯æŒ
â”‚   â”œâ”€â”€ cpu.py               # CPU æ”¯æŒ
â”‚   â”œâ”€â”€ tpu.py               # TPU æ”¯æŒ
â”‚   â””â”€â”€ xpu.py               # XPU/Intel æ”¯æŒ
â”‚
â”œâ”€â”€ lora/                    # LoRA æ”¯æŒ
â”œâ”€â”€ multimodal/              # å¤šæ¨¡æ€æ”¯æŒ
â”œâ”€â”€ tokenizers/              # åˆ†è¯å™¨
â”œâ”€â”€ transformers_utils/      # Transformers å·¥å…·
â”œâ”€â”€ triton_utils/            # Triton å·¥å…·
â”œâ”€â”€ plugins/                 # æ’ä»¶ç³»ç»Ÿ
â”œâ”€â”€ utils/                   # é€šç”¨å·¥å…·
â”œâ”€â”€ _custom_ops.py           # è‡ªå®šä¹‰ç®—å­ç»‘å®š
â”œâ”€â”€ sampling_params.py       # é‡‡æ ·å‚æ•°
â”œâ”€â”€ outputs.py               # è¾“å‡ºå®šä¹‰
â””â”€â”€ sequence.py              # åºåˆ—å®šä¹‰
```

#### æ ¸å¿ƒæ¨¡å—åŠŸèƒ½æ¦‚è¿°ï¼š

| æ¨¡å— | åŠŸèƒ½ | é‡è¦æ€§ |
|------|------|--------|
| `entrypoints/` | ç”¨æˆ·äº¤äº’å…¥å£ï¼ŒAPI æœåŠ¡å™¨ | â­â­â­ |
| `v1/engine/` | V1 æ¨ç†å¼•æ“æ ¸å¿ƒ | â­â­â­ |
| `v1/worker/` | GPU æ¨¡å‹è¿è¡Œå™¨ | â­â­â­ |
| `model_executor/models/` | æ¨¡å‹å®šä¹‰ | â­â­â­ |
| `model_executor/layers/` | æ¨¡å‹å±‚å®ç° | â­â­â­ |
| `attention/` | æ³¨æ„åŠ›æœºåˆ¶ | â­â­â­ |
| `distributed/` | åˆ†å¸ƒå¼æ”¯æŒ | â­â­ |
| `compilation/` | ç¼–è¯‘ä¼˜åŒ– | â­â­ |
| `config/` | é…ç½®ç®¡ç† | â­â­ |

### 2.2 `benchmarks/` - æ€§èƒ½åŸºå‡†æµ‹è¯• â­â­

ç”¨äºæ€§èƒ½æµ‹è¯•å’Œè¯„ä¼°çš„è„šæœ¬é›†åˆã€‚vLLM æä¾›äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œå¸®åŠ©ç”¨æˆ·è¯„ä¼°ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

```
benchmarks/
â”œâ”€â”€ benchmark_throughput.py       # ååé‡æµ‹è¯•
â”œâ”€â”€ benchmark_serving.py          # åœ¨çº¿æœåŠ¡æµ‹è¯•
â”œâ”€â”€ benchmark_latency.py          # å»¶è¿Ÿæµ‹è¯•
â”œâ”€â”€ benchmark_prefix_caching.py   # å‰ç¼€ç¼“å­˜æµ‹è¯•
â”œâ”€â”€ backend_request_func.py       # è¯·æ±‚åç«¯å‡½æ•°
â”œâ”€â”€ benchmark_utils.py            # åŸºå‡†æµ‹è¯•å·¥å…·
â”œâ”€â”€ kernels/                      # kernel çº§åˆ«çš„ benchmark
â”‚   â”œâ”€â”€ benchmark_paged_attention.py  # PagedAttention æµ‹è¯•
â”‚   â”œâ”€â”€ benchmark_layernorm.py    # LayerNorm æµ‹è¯•
â”‚   â”œâ”€â”€ benchmark_rope.py         # RoPE æµ‹è¯•
â”‚   â”œâ”€â”€ benchmark_moe.py          # MoE æµ‹è¯•
â”‚   â””â”€â”€ benchmark_fp8_gemm.py     # FP8 GEMM æµ‹è¯•
â”œâ”€â”€ cutlass_benchmarks/          # CUTLASS benchmark
â””â”€â”€ ...
```

#### ä½¿ç”¨ vLLM CLI è¿è¡Œ benchmarkï¼ˆæ¨èï¼‰ï¼š

```bash
# ååé‡æµ‹è¯• - æµ‹é‡æœ€å¤§ååé‡
vllm bench throughput \
    --model meta-llama/Llama-3.2-1B-Instruct \
    --input-len 128 \
    --output-len 128 \
    --num-prompts 100 \
    --dtype auto \
    --tensor-parallel-size 1

# æœåŠ¡æµ‹è¯• - æ¨¡æ‹ŸçœŸå®æœåŠ¡åœºæ™¯
vllm bench serve \
    --model meta-llama/Llama-3.2-1B-Instruct \
    --dataset-name sharegpt \
    --request-rate 10 \
    --num-prompts 500

# å»¶è¿Ÿæµ‹è¯• - æµ‹é‡é¦–tokenå»¶è¿Ÿå’Œç”Ÿæˆå»¶è¿Ÿ
vllm bench latency \
    --model meta-llama/Llama-3.2-1B-Instruct \
    --batch-size 1 \
    --input-len 32 \
    --output-len 128
```

#### æ€§èƒ½æŒ‡æ ‡è§£è¯»ï¼š

| æŒ‡æ ‡ | å«ä¹‰ | å…¸å‹å€¼ |
|------|------|--------|
| Throughput (tok/s) | æ¯ç§’ç”Ÿæˆçš„ token æ•° | 100-10000+ |
| TTFT (ms) | Time To First Tokenï¼Œé¦– token å»¶è¿Ÿ | 10-500ms |
| TPOT (ms) | Time Per Output Tokenï¼Œæ¯ token ç”Ÿæˆæ—¶é—´ | 5-50ms |
| ITL (ms) | Inter-Token Latencyï¼Œtoken é—´å»¶è¿Ÿ | 5-50ms |

### 2.3 `tests/` - æµ‹è¯•ç”¨ä¾‹ â­â­

vLLM æ‹¥æœ‰éå¸¸å…¨é¢çš„æµ‹è¯•å¥—ä»¶ï¼Œæ¶µç›–äº†å‡ ä¹æ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼š

```
tests/
â”œâ”€â”€ basic_correctness/          # åŸºç¡€æ­£ç¡®æ€§æµ‹è¯•
â”œâ”€â”€ models/                     # æ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ language/               # è¯­è¨€æ¨¡å‹æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ generation/         # ç”Ÿæˆæ¨¡å‹æµ‹è¯•
â”‚   â”‚   â””â”€â”€ pooling/            # æ± åŒ–æ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ multimodal/             # å¤šæ¨¡æ€æ¨¡å‹æµ‹è¯•
â”‚   â””â”€â”€ quantization/           # é‡åŒ–æ¨¡å‹æµ‹è¯•
â”œâ”€â”€ kernels/                    # Kernel æµ‹è¯•
â”‚   â”œâ”€â”€ attention/              # æ³¨æ„åŠ› kernel æµ‹è¯•
â”‚   â”œâ”€â”€ moe/                    # MoE kernel æµ‹è¯•
â”‚   â””â”€â”€ quantization/           # é‡åŒ– kernel æµ‹è¯•
â”œâ”€â”€ quantization/               # é‡åŒ–æµ‹è¯•
â”‚   â”œâ”€â”€ test_fp8.py             # FP8 é‡åŒ–
â”‚   â”œâ”€â”€ test_compressed_tensors.py  # CompressedTensors
â”‚   â””â”€â”€ test_modelopt.py        # ModelOpt é‡åŒ–
â”œâ”€â”€ distributed/                # åˆ†å¸ƒå¼æµ‹è¯•
â”œâ”€â”€ entrypoints/                # å…¥å£ç‚¹æµ‹è¯•
â”œâ”€â”€ engine/                     # å¼•æ“æµ‹è¯•
â”œâ”€â”€ lora/                       # LoRA æµ‹è¯•
â”œâ”€â”€ multimodal/                 # å¤šæ¨¡æ€æµ‹è¯•
â”œâ”€â”€ v1/                         # V1 æ¶æ„æµ‹è¯•
â””â”€â”€ conftest.py                 # pytest é…ç½®
```

**è¿è¡Œæµ‹è¯•ç¤ºä¾‹**ï¼š
```bash
# è¿è¡Œè¯­è¨€æ¨¡å‹ç”Ÿæˆæµ‹è¯•
pytest tests/models/language/generation/ -v

# è¿è¡Œæ‰€æœ‰ kernel æµ‹è¯•
pytest tests/kernels/ -v

# è¿è¡Œé‡åŒ–ç›¸å…³æµ‹è¯•
pytest tests/quantization/ -v

# å¹¶è¡Œè¿è¡Œæµ‹è¯•
pytest tests/kernels/ -n 4 -v  # ä½¿ç”¨ 4 ä¸ªè¿›ç¨‹
```

### 2.4 `examples/` - ä½¿ç”¨ç¤ºä¾‹ â­â­

åŒ…å«å„ç§ä½¿ç”¨åœºæ™¯çš„ç¤ºä¾‹ä»£ç ï¼Œæ˜¯å­¦ä¹  vLLM çš„æœ€ä½³èµ„æºï¼š

```
examples/
â”œâ”€â”€ offline_inference/              # ç¦»çº¿æ¨ç†ç¤ºä¾‹
â”‚   â”œâ”€â”€ basic/                      # åŸºç¡€ç¤ºä¾‹
â”‚   â”‚   â”œâ”€â”€ generate.py             # æ–‡æœ¬ç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ chat.py                 # å¯¹è¯
â”‚   â”‚   â”œâ”€â”€ embed.py                # åµŒå…¥ç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ classify.py             # åˆ†ç±»
â”‚   â”‚   â”œâ”€â”€ score.py                # è¯„åˆ†
â”‚   â”‚   â””â”€â”€ reward.py               # å¥–åŠ±æ¨¡å‹
â”‚   â”œâ”€â”€ vision_language.py          # è§†è§‰è¯­è¨€æ¨¡å‹
â”‚   â”œâ”€â”€ spec_decode.py              # æŠ•æœºè§£ç 
â”‚   â”œâ”€â”€ lora_with_quantization_inference.py  # LoRA + é‡åŒ–
â”‚   â”œâ”€â”€ structured_outputs.py       # ç»“æ„åŒ–è¾“å‡º
â”‚   â””â”€â”€ data_parallel.py            # æ•°æ®å¹¶è¡Œ
â”œâ”€â”€ online_serving/                 # åœ¨çº¿æœåŠ¡ç¤ºä¾‹
â”œâ”€â”€ pooling/                        # æ± åŒ–ç¤ºä¾‹
â”œâ”€â”€ others/                         # å…¶ä»–ç¤ºä¾‹
â”œâ”€â”€ template_*.jinja                # èŠå¤©æ¨¡æ¿
â””â”€â”€ tool_chat_template_*.jinja      # å·¥å…·è°ƒç”¨æ¨¡æ¿
```

### 2.5 `docs/` - æ–‡æ¡£

å®˜æ–¹æ–‡æ¡£çš„æºæ–‡ä»¶ï¼Œä½¿ç”¨ MkDocs æ„å»ºï¼š

```
docs/
â”œâ”€â”€ getting_started/           # å…¥é—¨æŒ‡å—
â”œâ”€â”€ usage/                     # ä½¿ç”¨è¯´æ˜
â”œâ”€â”€ serving/                   # åœ¨çº¿æœåŠ¡
â”œâ”€â”€ models/                    # æ”¯æŒçš„æ¨¡å‹
â”œâ”€â”€ configuration/             # é…ç½®è¯´æ˜
â”œâ”€â”€ deployment/                # éƒ¨ç½²æŒ‡å—
â”œâ”€â”€ benchmarking/              # æ€§èƒ½æµ‹è¯•æ–‡æ¡£
â”œâ”€â”€ contributing/              # è´¡çŒ®æŒ‡å—
â”œâ”€â”€ features/                  # ç‰¹æ€§è¯´æ˜
â”œâ”€â”€ design/                    # è®¾è®¡æ–‡æ¡£
â””â”€â”€ api/                       # API æ–‡æ¡£
```

**å®˜æ–¹æ–‡æ¡£ç½‘ç«™**: https://docs.vllm.ai/en/stable/usage/

### 2.6 `csrc/` - C++/CUDA æºä»£ç  â­â­â­

åº•å±‚é«˜æ€§èƒ½ kernel çš„å®ç°ï¼Œè¿™æ˜¯ vLLM æ€§èƒ½ä¼˜åŠ¿çš„æ ¸å¿ƒæ¥æºï¼š

```
csrc/
â”œâ”€â”€ attention/                     # æ³¨æ„åŠ› kernel
â”‚   â”œâ”€â”€ attention_kernels.cuh      # FlashAttention å˜ä½“
â”‚   â”œâ”€â”€ paged_attention_v1.cu      # PagedAttention V1
â”‚   â””â”€â”€ paged_attention_v2.cu      # PagedAttention V2
â”œâ”€â”€ quantization/                  # é‡åŒ– kernel
â”‚   â”œâ”€â”€ w8a8/                      # W8A8 é‡åŒ–
â”‚   â”‚   â”œâ”€â”€ fp8/                   # FP8 é‡åŒ–
â”‚   â”‚   â””â”€â”€ int8/                  # INT8 é‡åŒ–
â”‚   â”œâ”€â”€ awq/                       # AWQ é‡åŒ–
â”‚   â”œâ”€â”€ gptq/                      # GPTQ é‡åŒ–
â”‚   â”œâ”€â”€ gptq_marlin/               # GPTQ Marlin æ ¼å¼
â”‚   â”œâ”€â”€ marlin/                    # Marlin é‡åŒ–æ ¼å¼
â”‚   â””â”€â”€ fp4/                       # FP4 é‡åŒ–
â”œâ”€â”€ moe/                           # MoE (Mixture of Experts)
â”œâ”€â”€ cutlass_extensions/            # CUTLASS æ‰©å±•
â”œâ”€â”€ mamba/                         # Mamba æ¨¡å‹ kernel
â”œâ”€â”€ sparse/                        # ç¨€ç–è®¡ç®— kernel
â”œâ”€â”€ activation_kernels.cu          # æ¿€æ´»å‡½æ•° kernel
â”œâ”€â”€ layernorm_kernels.cu           # LayerNorm kernel
â”œâ”€â”€ pos_encoding_kernels.cu        # ä½ç½®ç¼–ç  kernel
â”œâ”€â”€ cache_kernels.cu               # KV Cache æ“ä½œ
â””â”€â”€ torch_bindings.cpp             # PyTorch ç»‘å®šå…¥å£
```

#### å…³é”® Kernel è¯´æ˜ï¼š

| Kernel | æ–‡ä»¶ | åŠŸèƒ½ |
|--------|------|------|
| PagedAttention | `attention/paged_attention_*.cu` | åˆ†é¡µæ³¨æ„åŠ›è®¡ç®— |
| Rotary Embedding | `pos_encoding_kernels.cu` | RoPE ä½ç½®ç¼–ç  |
| RMSNorm | `layernorm_kernels.cu` | Root Mean Square LayerNorm |
| SiLU/GELU | `activation_kernels.cu` | æ¿€æ´»å‡½æ•° |
| FP8 Quant | `quantization/w8a8/fp8/` | FP8 é‡åŒ–/åé‡åŒ– |
| CUTLASS GEMM | `cutlass_extensions/` | é«˜æ•ˆçŸ©é˜µä¹˜æ³• |

### 2.7 `tools/` - è¾…åŠ©å·¥å…·

å¼€å‘å’Œè¿ç»´ç›¸å…³çš„å·¥å…·ï¼š

```
tools/
â”œâ”€â”€ profiler/                  # æ€§èƒ½åˆ†æå·¥å…·
â”‚   â”œâ”€â”€ visualize_layerwise_profile.py  # å±‚çº§åˆ†æå¯è§†åŒ–
â”‚   â””â”€â”€ nvtx_profile.py        # NVTX æ ‡è®°
â”œâ”€â”€ ep_kernels/                # Expert Parallelism kernels
â”œâ”€â”€ pre_commit/                # ä»£ç æ£€æŸ¥é’©å­
â”œâ”€â”€ flashinfer-build.sh        # FlashInfer æ„å»ºè„šæœ¬
â”œâ”€â”€ install_deepgemm.sh        # DeepGEMM å®‰è£…è„šæœ¬
â”œâ”€â”€ install_gdrcopy.sh         # GDRCopy å®‰è£…è„šæœ¬
â””â”€â”€ check_repo.sh              # ä»“åº“æ£€æŸ¥è„šæœ¬
```

### 2.8 `requirements/` - ä¾èµ–é…ç½®

åˆ†å±‚çš„ä¾èµ–ç®¡ç†ï¼š

```
requirements/
â”œâ”€â”€ common.txt             # åŸºç¡€å…¬å…±ä¾èµ–
â”œâ”€â”€ dev.txt                # å¼€å‘ä¾èµ–
â”œâ”€â”€ test.txt               # æµ‹è¯•ä¾èµ–
â”œâ”€â”€ cuda.txt               # CUDA ç‰¹å®šä¾èµ–
â”œâ”€â”€ rocm.txt               # ROCm/AMD ä¾èµ–
â”œâ”€â”€ cpu.txt                # CPU ä¾èµ–
â”œâ”€â”€ tpu.txt                # TPU ä¾èµ–
â”œâ”€â”€ xpu.txt                # XPU/Intel ä¾èµ–
â”œâ”€â”€ build.txt              # æ„å»ºä¾èµ–
â””â”€â”€ docs.txt               # æ–‡æ¡£ä¾èµ–
```

---

## 3. å¦‚ä½•è¿è¡Œ vLLM

### 3.1 å®‰è£…

#### æ–¹å¼ä¸€ï¼šä» PyPI å®‰è£…ï¼ˆæ¨èï¼‰
```bash
# åŸºç¡€å®‰è£…
pip install vllm

# æŒ‡å®š CUDA ç‰ˆæœ¬
pip install vllm --extra-index-url https://download.pytorch.org/whl/cu121
```

#### æ–¹å¼äºŒï¼šä»æºç å®‰è£…
```bash
# å…‹éš†ä»“åº“
git clone https://github.com/vllm-project/vllm.git
cd vllm

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¼€å‘æ¨¡å¼å®‰è£…
pip install -e .
```

#### æ–¹å¼ä¸‰ï¼šä½¿ç”¨ Docker
```bash
# æ‹‰å–å®˜æ–¹é•œåƒ
docker pull vllm/vllm-openai:latest

# è¿è¡Œå®¹å™¨
docker run --gpus all -p 8000:8000 \
    vllm/vllm-openai:latest \
    --model meta-llama/Llama-3.2-1B-Instruct
```

### 3.2 åŸºæœ¬æ¨ç†ç¤ºä¾‹

#### ç¦»çº¿æ‰¹é‡æ¨ç†
```python
from vllm import LLM, SamplingParams

# åˆ›å»º LLM å®ä¾‹
llm = LLM(
    model="meta-llama/Llama-3.2-1B-Instruct",
    dtype="auto",                    # è‡ªåŠ¨é€‰æ‹©æ•°æ®ç±»å‹
    tensor_parallel_size=1,          # GPU æ•°é‡
    gpu_memory_utilization=0.9,      # GPU å†…å­˜åˆ©ç”¨ç‡
)

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=256,
    stop=["<|end|>", "<|eot_id|>"]
)

# æ‰¹é‡ç”Ÿæˆ
prompts = [
    "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
    "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚",
    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ã€‚"
]
outputs = llm.generate(prompts, sampling_params)

# æ‰“å°ç»“æœ
for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}")
    print("-" * 50)
```

#### å¯¹è¯æ¨ç†
```python
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-3.2-1B-Instruct")

# ä½¿ç”¨ chat æ–¹æ³•
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"}
]

outputs = llm.chat(
    messages,
    sampling_params=SamplingParams(temperature=0.7, max_tokens=512)
)

print(outputs[0].outputs[0].text)
```

### 3.3 è¿è¡Œ benchmark

```bash
# ååé‡æµ‹è¯• - æµ‹é‡æœ€å¤§å¤„ç†èƒ½åŠ›
vllm bench throughput \
    --model meta-llama/Llama-3.2-1B-Instruct \
    --input-len 128 \
    --output-len 128 \
    --num-prompts 100 \
    --dtype auto

# åœ¨çº¿æœåŠ¡æµ‹è¯• - æ¨¡æ‹ŸçœŸå®è¯·æ±‚
vllm bench serve \
    --model meta-llama/Llama-3.2-1B-Instruct \
    --dataset-name sharegpt \
    --request-rate 10 \
    --num-prompts 500

# å»¶è¿Ÿæµ‹è¯• - å•è¯·æ±‚å»¶è¿Ÿåˆ†æ
vllm bench latency \
    --model meta-llama/Llama-3.2-1B-Instruct \
    --batch-size 1 \
    --input-len 32 \
    --output-len 128
```

### 3.4 å¯åŠ¨ API æœåŠ¡å™¨

#### åŸºç¡€æœåŠ¡å™¨
```bash
# å¯åŠ¨ OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨
vllm serve meta-llama/Llama-3.2-1B-Instruct \
    --host 0.0.0.0 \
    --port 8000
```

#### é«˜çº§é…ç½®
```bash
vllm serve meta-llama/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 2 \
    --dtype auto \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.9 \
    --enable-prefix-caching \
    --quantization fp8
```

#### ä½¿ç”¨ API
```python
from openai import OpenAI

# è¿æ¥åˆ° vLLM æœåŠ¡å™¨
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="your-api-key"
)

# Chat Completion
response = client.chat.completions.create(
    model="meta-llama/Llama-3.2-1B-Instruct",
    messages=[{"role": "user", "content": "Hello!"}],
    temperature=0.7,
    max_tokens=256
)
print(response.choices[0].message.content)
```

---

## 4. æ¨¡å‹ä¸‹è½½ä¸é…ç½®

### 4.1 ä» HuggingFace ä¸‹è½½æ¨¡å‹

vLLM ç›´æ¥æ”¯æŒ HuggingFace æ¨¡å‹æ ¼å¼ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½å’Œç¼“å­˜æ¨¡å‹ã€‚

#### æ–¹å¼ä¸€ï¼šè‡ªåŠ¨ä¸‹è½½
```python
# vLLM ä¼šè‡ªåŠ¨ä» HuggingFace ä¸‹è½½æ¨¡å‹åˆ° ~/.cache/huggingface
llm = LLM(model="meta-llama/Llama-3.2-1B-Instruct")
```

#### æ–¹å¼äºŒï¼šæ‰‹åŠ¨ä¸‹è½½
```bash
# ä½¿ç”¨ huggingface-cli
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct

# æŒ‡å®šä¸‹è½½è·¯å¾„
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct \
    --local-dir ./models/llama-3.2
```

#### æ–¹å¼ä¸‰ï¼šä½¿ç”¨æœ¬åœ°è·¯å¾„
```python
llm = LLM(model="/path/to/your/model")
```

### 4.2 å¸¸ç”¨æ¨¡å‹æ¨è

| æ¨¡å‹ç³»åˆ— | HuggingFace è·¯å¾„ | å‚æ•°é‡ | ç‰¹ç‚¹ |
|---------|-----------------|--------|------|
| **Llama 3.2** | `meta-llama/Llama-3.2-1B-Instruct` | 1B | Meta æœ€æ–°è½»é‡æ¨¡å‹ |
| **Llama 3.2** | `meta-llama/Llama-3.2-3B-Instruct` | 3B | å¹³è¡¡æ€§èƒ½ä¸é€Ÿåº¦ |
| **Llama 3.1** | `meta-llama/Meta-Llama-3.1-8B-Instruct` | 8B | ä¸»æµå¼€æºæ¨¡å‹ |
| **Qwen 2.5** | `Qwen/Qwen2.5-7B-Instruct` | 7B | é˜¿é‡Œåƒé—®ï¼Œä¸­æ–‡ä¼˜ç§€ |
| **DeepSeek** | `deepseek-ai/deepseek-llm-7b-chat` | 7B | æ€§ä»·æ¯”é«˜ |
| **Mistral** | `mistralai/Mistral-7B-Instruct-v0.2` | 7B | Mistral AI |
| **Mixtral** | `mistralai/Mixtral-8x7B-Instruct-v0.1` | 8x7B | MoE æ¨¡å‹ |

### 4.3 é‡åŒ–æ¨¡å‹è¯¦è§£

#### FP8 é‡åŒ–ï¼ˆæ¨èï¼ŒH100/Ada GPUï¼‰
```python
llm = LLM(
    model="meta-llama/Llama-3.1-8B-Instruct",
    quantization="fp8",
    dtype="float16"
)
```

#### AWQ é‡åŒ–ï¼ˆ4-bitï¼Œä»»æ„ GPUï¼‰
```python
llm = LLM(
    model="TheBloke/Llama-2-7B-Chat-AWQ",
    quantization="awq"
)
```

#### GPTQ é‡åŒ–ï¼ˆ4-bitï¼Œç»å…¸æ–¹æ³•ï¼‰
```python
llm = LLM(
    model="TheBloke/Llama-2-7B-Chat-GPTQ",
    quantization="gptq"
)
```

#### é‡åŒ–æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç²¾åº¦ | å†…å­˜èŠ‚çœ | é€Ÿåº¦ | GPU è¦æ±‚ |
|------|------|---------|------|---------|
| FP8 | é«˜ | 50% | å¿« | H100/Ada |
| AWQ | ä¸­ | 75% | ä¸­ | ä»»æ„ |
| GPTQ | ä¸­ | 75% | ä¸­ | ä»»æ„ |

---

## 5. å…³é”®é…ç½®å‚æ•°è¯¦è§£

### 5.1 LLM åˆå§‹åŒ–å‚æ•°

```python
from vllm import LLM

llm = LLM(
    # ============ æ¨¡å‹é…ç½® ============
    model="meta-llama/Llama-3.2-1B-Instruct",  # æ¨¡å‹è·¯å¾„
    tokenizer=None,                  # è‡ªå®šä¹‰ tokenizer è·¯å¾„
    tokenizer_mode="auto",           # tokenizer æ¨¡å¼
    trust_remote_code=False,         # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
    
    # ============ æ•°æ®ç±»å‹ ============
    dtype="auto",                    # æ•°æ®ç±»å‹: auto, float16, bfloat16
    
    # ============ é‡åŒ–é…ç½® ============
    quantization=None,               # é‡åŒ–æ–¹æ³•: None, "fp8", "awq", "gptq"
    
    # ============ å¹¶è¡Œé…ç½® ============
    tensor_parallel_size=1,          # å¼ é‡å¹¶è¡Œ GPU æ•°é‡
    pipeline_parallel_size=1,        # æµæ°´çº¿å¹¶è¡Œé˜¶æ®µæ•°
    
    # ============ å†…å­˜é…ç½® ============
    gpu_memory_utilization=0.9,      # GPU å†…å­˜åˆ©ç”¨ç‡ (0-1)
    max_model_len=None,              # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    cpu_offload_gb=0,                # CPU å¸è½½å¤§å° (GB)
    swap_space=4,                    # äº¤æ¢ç©ºé—´å¤§å° (GB)
    
    # ============ ä¼˜åŒ–é…ç½® ============
    enforce_eager=False,             # ç¦ç”¨ CUDA Graph
    enable_prefix_caching=False,     # å¯ç”¨å‰ç¼€ç¼“å­˜
)
```

### 5.2 é‡‡æ ·å‚æ•°è¯¦è§£

```python
from vllm import SamplingParams

sampling_params = SamplingParams(
    # ============ åŸºç¡€ç”Ÿæˆæ§åˆ¶ ============
    max_tokens=256,              # æœ€å¤§ç”Ÿæˆ token æ•°
    min_tokens=0,                # æœ€å°ç”Ÿæˆ token æ•°
    
    # ============ é‡‡æ ·ç­–ç•¥ ============
    temperature=1.0,             # æ¸©åº¦ï¼Œè¶Šé«˜è¶Šéšæœº (0-2)
    top_p=1.0,                   # nucleus sampling (0-1)
    top_k=-1,                    # top-k samplingï¼Œ-1 ç¦ç”¨
    
    # ============ æƒ©ç½šé¡¹ ============
    presence_penalty=0.0,        # å­˜åœ¨æƒ©ç½š (-2 åˆ° 2)
    frequency_penalty=0.0,       # é¢‘ç‡æƒ©ç½š (-2 åˆ° 2)
    repetition_penalty=1.0,      # é‡å¤æƒ©ç½š
    
    # ============ åœæ­¢æ¡ä»¶ ============
    stop=None,                   # åœæ­¢è¯åˆ—è¡¨
    stop_token_ids=None,         # åœæ­¢ token ID åˆ—è¡¨
    ignore_eos=False,            # æ˜¯å¦å¿½ç•¥ EOS token
    
    # ============ è¾“å‡ºæ§åˆ¶ ============
    n=1,                         # æ¯ä¸ª prompt ç”Ÿæˆçš„ç»“æœæ•°
    best_of=None,                # ä» best_of ä¸ªç»“æœä¸­é€‰æœ€ä½³
    logprobs=None,               # è¿”å› top-k logprobs æ•°é‡
)
```

---

## 6. æ¨ç†å…¥å£ä¸è°ƒç”¨é“¾è¯¦è§£

vLLM çš„æ¨ç†å…¥å£ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç§ï¼Œæ¯ç§éƒ½æœ‰å…¶é€‚ç”¨åœºæ™¯ï¼š

### 6.1 ç¦»çº¿æ‰¹é‡æ¨ç†ï¼ˆOffline Inferenceï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ç”¨æˆ·ä»£ç å…¥å£                                      â”‚
â”‚  llm = LLM(model="...")                                                 â”‚
â”‚  outputs = llm.generate(prompts, sampling_params)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM ç±» (vllm/entrypoints/llm.py)                                       â”‚
â”‚  â”œâ”€â”€ __init__(): åˆ›å»º LLMEngine                                         â”‚
â”‚  â”œâ”€â”€ generate(): æ·»åŠ è¯·æ±‚å¹¶å¾ªç¯è°ƒç”¨ engine.step()                        â”‚
â”‚  â””â”€â”€ chat(): åº”ç”¨èŠå¤©æ¨¡æ¿åè°ƒç”¨ generate()                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLMEngine (vllm/v1/engine/llm_engine.py)                               â”‚
â”‚  â”œâ”€â”€ __init__(): åˆå§‹åŒ–å¤„ç†å™¨å’Œ EngineCore                               â”‚
â”‚  â”œâ”€â”€ add_request(): æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—                                       â”‚
â”‚  â””â”€â”€ step(): è·å–è¾“å‡ºå¹¶å¤„ç†                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPUModelRunner (vllm/v1/worker/gpu_model_runner.py)                    â”‚
â”‚  â”œâ”€â”€ execute_model(): æ‰§è¡Œæ¨¡å‹æ¨ç†                                       â”‚
â”‚  â”‚   â”œâ”€â”€ _prepare_inputs(): å‡†å¤‡è¾“å…¥å¼ é‡                                 â”‚
â”‚  â”‚   â”œâ”€â”€ model.forward(): è°ƒç”¨æ¨¡å‹å‰å‘ä¼ æ’­                               â”‚
â”‚  â”‚   â””â”€â”€ sampler(): é‡‡æ ·ç”Ÿæˆ token                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Forward (ä»¥ Qwen2ForCausalLM ä¸ºä¾‹)                                â”‚
â”‚  vllm/model_executor/models/qwen2.py                                    â”‚
â”‚  â”œâ”€â”€ Qwen2ForCausalLM.forward(): é¡¶å±‚å‰å‘                                â”‚
â”‚  â”œâ”€â”€ Qwen2Model.forward(): ä¸»æ¨¡å‹å‰å‘                                    â”‚
â”‚  â”‚   â”œâ”€â”€ embed_tokens(): è¯åµŒå…¥                                         â”‚
â”‚  â”‚   â”œâ”€â”€ layers[i].forward(): N ä¸ª Decoder Layer                        â”‚
â”‚  â”‚   â””â”€â”€ norm(): æœ€ç»ˆ LayerNorm                                         â”‚
â”‚  â””â”€â”€ compute_logits(): è®¡ç®— logits                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 åœ¨çº¿æœåŠ¡ï¼ˆOnline Servingï¼‰

```
HTTP è¯·æ±‚ â†’ OpenAI API Server (vllm/entrypoints/openai/api_server.py)
    â”‚
    â–¼
AsyncLLMEngine (å¼‚æ­¥ç‰ˆæœ¬çš„å¼•æ“)
    â”‚
    â–¼
... (åç»­æµç¨‹ä¸ç¦»çº¿æ¨ç†ç›¸åŒ)
```

### 6.3 CLI å…¥å£

```bash
# ä¸»è¦çš„ CLI å‘½ä»¤
vllm serve        # å¯åŠ¨ API æœåŠ¡å™¨
vllm bench        # è¿è¡Œæ€§èƒ½æµ‹è¯•
vllm chat         # äº¤äº’å¼å¯¹è¯
```

---

## 7. åˆ†å¸ƒå¼æ¨ç†é…ç½®

### 7.1 å¼ é‡å¹¶è¡Œ (Tensor Parallelism)

å°†æ¨¡å‹çš„æ¯ä¸€å±‚æ‹†åˆ†åˆ°å¤šä¸ª GPU ä¸Šï¼š

```python
llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",
    tensor_parallel_size=4  # 4 GPU å¼ é‡å¹¶è¡Œ
)
```

### 7.2 æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism)

å°†æ¨¡å‹çš„ä¸åŒå±‚åˆ†é…åˆ°ä¸åŒ GPUï¼š

```python
llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",
    pipeline_parallel_size=2  # 2 ä¸ªæµæ°´çº¿é˜¶æ®µ
)
```

### 7.3 æ··åˆå¹¶è¡Œ

```python
# 4 GPU å¼ é‡å¹¶è¡Œ Ã— 2 æµæ°´çº¿é˜¶æ®µ = 8 GPU
llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",
    tensor_parallel_size=4,
    pipeline_parallel_size=2
)
```

---

## 8. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 8.1 å†…å­˜ä¸è¶³ (OOM)

```python
# è§£å†³æ–¹æ¡ˆ1ï¼šé™ä½å†…å­˜åˆ©ç”¨ç‡
llm = LLM(model="...", gpu_memory_utilization=0.8)

# è§£å†³æ–¹æ¡ˆ2ï¼šå‡å°æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
llm = LLM(model="...", max_model_len=4096)

# è§£å†³æ–¹æ¡ˆ3ï¼šä½¿ç”¨é‡åŒ–
llm = LLM(model="...", quantization="fp8")

# è§£å†³æ–¹æ¡ˆ4ï¼šå¤š GPU å¹¶è¡Œ
llm = LLM(model="...", tensor_parallel_size=2)
```

### 8.2 æ¨ç†é€Ÿåº¦æ…¢

```python
# å¯ç”¨ CUDA Graph
llm = LLM(model="...", enforce_eager=False)

# å¯ç”¨å‰ç¼€ç¼“å­˜
llm = LLM(model="...", enable_prefix_caching=True)
```

---

## 9. å°ç»“

æœ¬æ–‡æ¡£ä»‹ç»äº† vLLM é¡¹ç›®çš„æ•´ä½“ç»“æ„å’Œä½¿ç”¨æ–¹æ³•ã€‚å¦‚éœ€æ·±å…¥äº†è§£ï¼š

- **æ ¸å¿ƒæ¡†æ¶ç»†èŠ‚** â†’ è¯·å‚è€ƒ [framework_vllmcore.md](./framework_vllmcore.md)
- **çº¿æ€§å±‚ä¸ GEMM** â†’ è¯·å‚è€ƒ [framework_lineargemm.md](./framework_lineargemm.md)

vLLM çš„è®¾è®¡ç†å¿µæ˜¯é€šè¿‡ PagedAttentionã€è¿ç»­æ‰¹å¤„ç†å’Œ CUDA Graph ç­‰æŠ€æœ¯ï¼Œå®ç°é«˜ååã€ä½å»¶è¿Ÿçš„å¤§æ¨¡å‹æ¨ç†ã€‚æ•´ä¸ªé¡¹ç›®ç»“æ„æ¸…æ™°ï¼Œæ¨¡å—åŒ–ç¨‹åº¦é«˜ï¼Œä¾¿äºäºŒæ¬¡å¼€å‘å’Œå®šåˆ¶ã€‚

### å…³é”®æ–‡ä»¶é€ŸæŸ¥è¡¨

| ç›®çš„ | å…³é”®æ–‡ä»¶ |
|------|---------|
| ç¦»çº¿æ¨ç†å…¥å£ | `vllm/entrypoints/llm.py` |
| åœ¨çº¿æœåŠ¡å…¥å£ | `vllm/entrypoints/openai/api_server.py` |
| V1 å¼•æ“ | `vllm/v1/engine/llm_engine.py` |
| GPU æ‰§è¡Œå™¨ | `vllm/v1/worker/gpu_model_runner.py` |
| æ¨¡å‹å®šä¹‰ | `vllm/model_executor/models/*.py` |
| çº¿æ€§å±‚ | `vllm/model_executor/layers/linear.py` |
| é‡åŒ–æ–¹æ³• | `vllm/model_executor/layers/quantization/*.py` |
| æ³¨æ„åŠ›å±‚ | `vllm/attention/layer.py` |
| é‡‡æ ·å‚æ•° | `vllm/sampling_params.py` |
| é…ç½®ç±» | `vllm/config/` |
