# vLLM æ¡†æ¶æ¦‚è¿° (Framework Overview)

æœ¬æ–‡æ¡£æ—¨åœ¨å¸®åŠ©ä½ å¿«é€Ÿäº†è§£ vLLM é¡¹ç›®çš„æ•´ä½“ç›®å½•ç»“æ„ã€å„ä¸ªæ–‡ä»¶å¤¹çš„ç”¨é€”ï¼Œä»¥åŠå¦‚ä½•è¿è¡Œå’Œæµ‹è¯•è¿™ä¸ªé¡¹ç›®ã€‚

---

## 1. é¡¹ç›®ç›®å½•ç»“æ„æ¦‚è§ˆ

```
vllmbench/
â”œâ”€â”€ vllm/                   # ğŸ”¥ æ ¸å¿ƒæ¨ç†æ¡†æ¶ï¼ˆæœ€é‡è¦ï¼‰
â”œâ”€â”€ benchmarks/             # æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
â”œâ”€â”€ tests/                  # æµ‹è¯•ç”¨ä¾‹ï¼ˆéå¸¸å…¨é¢ï¼‰
â”œâ”€â”€ examples/               # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ csrc/                   # C++/CUDA æºä»£ç 
â”œâ”€â”€ docs/                   # æ–‡æ¡£
â”œâ”€â”€ tools/                  # è¾…åŠ©å·¥å…·
â”œâ”€â”€ custom_kernels/         # è‡ªå®šä¹‰kernelç¤ºä¾‹
â”œâ”€â”€ requirements/           # ä¾èµ–é…ç½®
â”œâ”€â”€ cmake/                  # CMake æ„å»ºé…ç½®
â”œâ”€â”€ .buildkite/             # CI/CD é…ç½®
â”œâ”€â”€ .github/                # GitHub Actions é…ç½®
â””â”€â”€ ...                     # å…¶ä»–é…ç½®æ–‡ä»¶
```

---

## 2. å„ç›®å½•è¯¦ç»†è¯´æ˜

### 2.1 `vllm/` - æ ¸å¿ƒæ¨ç†æ¡†æ¶ â­â­â­

è¿™æ˜¯æ•´ä¸ª vLLM é¡¹ç›®çš„æ ¸å¿ƒï¼ŒåŒ…å«äº†æ‰€æœ‰æ¨ç†ç›¸å…³çš„ä»£ç ã€‚å†…éƒ¨ç»„ç»‡éå¸¸å¤æ‚ï¼Œè¯¦ç»†ä»‹ç»è¯·å‚è€ƒ [framework_vllmcore.md](./framework_vllmcore.md)ã€‚

ç®€è¦æ¦‚è¿°ï¼š
- `vllm/entrypoints/` - å…¥å£ç‚¹ï¼ŒåŒ…å« LLM ç±»å’Œ API æœåŠ¡å™¨
- `vllm/engine/` - LLM å¼•æ“å®ç°
- `vllm/model_executor/` - æ¨¡å‹æ‰§è¡Œå™¨ï¼ŒåŒ…å«æ¨¡å‹å®šä¹‰å’Œé‡åŒ–å±‚
- `vllm/attention/` - æ³¨æ„åŠ›æœºåˆ¶å®ç°
- `vllm/v1/` - V1 ç‰ˆæœ¬çš„æ–°æ¶æ„å®ç°

### 2.2 `benchmarks/` - æ€§èƒ½åŸºå‡†æµ‹è¯• â­â­

ç”¨äºæ€§èƒ½æµ‹è¯•å’Œè¯„ä¼°çš„è„šæœ¬é›†åˆã€‚

```
benchmarks/
â”œâ”€â”€ benchmark_throughput.py       # ååé‡æµ‹è¯•ï¼ˆå·²ç§»è‡³ CLIï¼‰
â”œâ”€â”€ benchmark_serving.py          # åœ¨çº¿æœåŠ¡æµ‹è¯•
â”œâ”€â”€ benchmark_latency.py          # å»¶è¿Ÿæµ‹è¯•
â”œâ”€â”€ benchmark_prefix_caching.py   # å‰ç¼€ç¼“å­˜æµ‹è¯•
â”œâ”€â”€ backend_request_func.py       # è¯·æ±‚åç«¯å‡½æ•°
â”œâ”€â”€ benchmark_utils.py            # åŸºå‡†æµ‹è¯•å·¥å…·
â”œâ”€â”€ kernels/                      # kernel çº§åˆ«çš„ benchmark
â”œâ”€â”€ fused_kernels/               # èåˆ kernel benchmark
â”œâ”€â”€ cutlass_benchmarks/          # CUTLASS benchmark
â””â”€â”€ ...
```

**æ³¨æ„**: ç°åœ¨æ¨èä½¿ç”¨ vLLM CLI æ¥è¿è¡Œ benchmarkï¼š
```bash
# ååé‡æµ‹è¯•
vllm bench throughput --help

# æœåŠ¡æµ‹è¯•  
vllm bench serve --help

# å»¶è¿Ÿæµ‹è¯•
vllm bench latency --help
```

### 2.3 `tests/` - æµ‹è¯•ç”¨ä¾‹ â­â­

éå¸¸å…¨é¢çš„æµ‹è¯•é›†åˆï¼Œæ¶µç›–äº†å‡ ä¹æ‰€æœ‰åŠŸèƒ½ï¼š

```
tests/
â”œâ”€â”€ basic_correctness/      # åŸºç¡€æ­£ç¡®æ€§æµ‹è¯•
â”œâ”€â”€ models/                 # æ¨¡å‹æµ‹è¯•
â”œâ”€â”€ kernels/               # kernel æµ‹è¯•
â”œâ”€â”€ quantization/          # é‡åŒ–æµ‹è¯•
â”œâ”€â”€ distributed/           # åˆ†å¸ƒå¼æµ‹è¯•
â”œâ”€â”€ entrypoints/           # å…¥å£ç‚¹æµ‹è¯•
â”œâ”€â”€ engine/                # å¼•æ“æµ‹è¯•
â”œâ”€â”€ samplers/              # é‡‡æ ·å™¨æµ‹è¯•
â”œâ”€â”€ lora/                  # LoRA æµ‹è¯•
â”œâ”€â”€ multimodal/            # å¤šæ¨¡æ€æµ‹è¯•
â”œâ”€â”€ v1/                    # V1 æ¶æ„æµ‹è¯•
â”œâ”€â”€ conftest.py            # pytest é…ç½®
â””â”€â”€ ...
```

**è¿è¡Œæµ‹è¯•ç¤ºä¾‹**ï¼š
```bash
# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/models/test_llama.py -v

# è¿è¡Œæ‰€æœ‰ kernel æµ‹è¯•
pytest tests/kernels/ -v

# è¿è¡Œé‡åŒ–ç›¸å…³æµ‹è¯•
pytest tests/quantization/ -v
```

### 2.4 `examples/` - ä½¿ç”¨ç¤ºä¾‹ â­â­

åŒ…å«å„ç§ä½¿ç”¨åœºæ™¯çš„ç¤ºä¾‹ä»£ç ï¼š

```
examples/
â”œâ”€â”€ offline_inference/           # ç¦»çº¿æ¨ç†ç¤ºä¾‹
â”‚   â”œâ”€â”€ basic/                   # åŸºç¡€ç¤ºä¾‹
â”‚   â”‚   â”œâ”€â”€ generate.py          # æ–‡æœ¬ç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ chat.py              # å¯¹è¯
â”‚   â”‚   â”œâ”€â”€ embed.py             # åµŒå…¥
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ vision_language.py       # è§†è§‰è¯­è¨€æ¨¡å‹
â”‚   â”œâ”€â”€ spec_decode.py           # æŠ•æœºè§£ç 
â”‚   â”œâ”€â”€ lora_with_quantization_inference.py  # LoRA + é‡åŒ–
â”‚   â””â”€â”€ ...
â”œâ”€â”€ online_serving/              # åœ¨çº¿æœåŠ¡ç¤ºä¾‹
â”œâ”€â”€ pooling/                     # æ± åŒ–ç¤ºä¾‹
â”œâ”€â”€ template_*.jinja             # èŠå¤©æ¨¡æ¿
â””â”€â”€ tool_chat_template_*.jinja   # å·¥å…·è°ƒç”¨æ¨¡æ¿
```

### 2.5 `docs/` - æ–‡æ¡£

å®˜æ–¹æ–‡æ¡£çš„æºæ–‡ä»¶ï¼Œä½¿ç”¨ MkDocs æ„å»ºï¼š

```
docs/
â”œâ”€â”€ getting_started/    # å…¥é—¨æŒ‡å—
â”œâ”€â”€ usage/              # ä½¿ç”¨è¯´æ˜
â”œâ”€â”€ models/             # æ”¯æŒçš„æ¨¡å‹
â”œâ”€â”€ configuration/      # é…ç½®è¯´æ˜
â”œâ”€â”€ deployment/         # éƒ¨ç½²æŒ‡å—
â”œâ”€â”€ benchmarking/       # æ€§èƒ½æµ‹è¯•æ–‡æ¡£
â”œâ”€â”€ contributing/       # è´¡çŒ®æŒ‡å—
â””â”€â”€ ...
```

**å®˜æ–¹æ–‡æ¡£ç½‘ç«™**: https://docs.vllm.ai/en/stable/usage/

### 2.6 `csrc/` - C++/CUDA æºä»£ç 

åº•å±‚é«˜æ€§èƒ½ kernel çš„å®ç°ï¼š

```
csrc/
â”œâ”€â”€ attention/              # æ³¨æ„åŠ› kernel
â”œâ”€â”€ quantization/           # é‡åŒ– kernel
â”œâ”€â”€ moe/                    # MoE kernel
â”œâ”€â”€ cutlass_extensions/     # CUTLASS æ‰©å±•
â”œâ”€â”€ mamba/                  # Mamba æ¨¡å‹ kernel
â”œâ”€â”€ sparse/                 # ç¨€ç–è®¡ç®— kernel
â”œâ”€â”€ activation_kernels.cu   # æ¿€æ´»å‡½æ•° kernel
â”œâ”€â”€ layernorm_kernels.cu    # LayerNorm kernel
â”œâ”€â”€ pos_encoding_kernels.cu # ä½ç½®ç¼–ç  kernel
â”œâ”€â”€ torch_bindings.cpp      # PyTorch ç»‘å®š
â””â”€â”€ ...
```

**æ³¨æ„**: è¿™äº›æ˜¯ç¼–è¯‘åä¾› Python è°ƒç”¨çš„ CUDA/C++ å®ç°ï¼Œé€šå¸¸ä¸éœ€è¦ç›´æ¥ä¿®æ”¹ã€‚

### 2.7 `tools/` - è¾…åŠ©å·¥å…·

å¼€å‘å’Œè¿ç»´ç›¸å…³çš„å·¥å…·ï¼š

```
tools/
â”œâ”€â”€ profiler/              # æ€§èƒ½åˆ†æå·¥å…·
â”œâ”€â”€ ep_kernels/            # Expert Parallelism kernels
â”œâ”€â”€ pre_commit/            # ä»£ç æ£€æŸ¥é’©å­
â”œâ”€â”€ flashinfer-build.sh    # FlashInfer æ„å»ºè„šæœ¬
â”œâ”€â”€ install_deepgemm.sh    # DeepGEMM å®‰è£…è„šæœ¬
â”œâ”€â”€ install_gdrcopy.sh     # GDRCopy å®‰è£…è„šæœ¬
â”œâ”€â”€ check_repo.sh          # ä»“åº“æ£€æŸ¥è„šæœ¬
â””â”€â”€ ...
```

### 2.8 `custom_kernels/` - è‡ªå®šä¹‰ Kernel ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰ kernel çš„ç¤ºä¾‹ç›®å½•ï¼ˆé¡¹ç›®ä½œè€…æ·»åŠ ï¼‰ï¼š

```
custom_kernels/
â”œâ”€â”€ cuda/           # CUDA kernel ç¤ºä¾‹
â”œâ”€â”€ triton/         # Triton kernel ç¤ºä¾‹
â”œâ”€â”€ compile.sh      # ç¼–è¯‘è„šæœ¬
â””â”€â”€ patch_example.py # è¡¥ä¸ç¤ºä¾‹
```

---

## 3. å¦‚ä½•è¿è¡Œ vLLM

### 3.1 å®‰è£…

**æ–¹å¼ä¸€ï¼šä» PyPI å®‰è£…**
```bash
pip install vllm
```

**æ–¹å¼äºŒï¼šä»æºç å®‰è£…**
```bash
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

### 3.2 åŸºæœ¬æ¨ç†ç¤ºä¾‹

```python
from vllm import LLM, SamplingParams

# åˆ›å»º LLM å®ä¾‹
llm = LLM(model="meta-llama/Llama-3.2-1B-Instruct")

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=256
)

# ç”Ÿæˆ
prompts = ["ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"]
outputs = llm.generate(prompts, sampling_params)

# æ‰“å°ç»“æœ
for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}")
```

### 3.3 è¿è¡Œ benchmark

```bash
# ä½¿ç”¨ vLLM CLI è¿è¡Œååé‡æµ‹è¯•
vllm bench throughput \
    --model meta-llama/Llama-3.2-1B-Instruct \
    --input-len 128 \
    --output-len 128 \
    --num-prompts 100

# æœåŠ¡åŸºå‡†æµ‹è¯•
vllm bench serve \
    --model meta-llama/Llama-3.2-1B-Instruct \
    --dataset-name sharegpt \
    --request-rate 10
```

### 3.4 å¯åŠ¨ API æœåŠ¡å™¨

```bash
# å¯åŠ¨ OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨
vllm serve meta-llama/Llama-3.2-1B-Instruct \
    --host 0.0.0.0 \
    --port 8000

# æˆ–è€…ä½¿ç”¨ Python
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.2-1B-Instruct
```

---

## 4. æ¨¡å‹ä¸‹è½½ä¸é…ç½®

### 4.1 ä» HuggingFace ä¸‹è½½æ¨¡å‹

vLLM ç›´æ¥æ”¯æŒ HuggingFace æ¨¡å‹æ ¼å¼ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–æ¨¡å‹ï¼š

**æ–¹å¼ä¸€ï¼šè‡ªåŠ¨ä¸‹è½½**
```python
# vLLM ä¼šè‡ªåŠ¨ä» HuggingFace ä¸‹è½½æ¨¡å‹
llm = LLM(model="meta-llama/Llama-3.2-1B-Instruct")
```

**æ–¹å¼äºŒï¼šæ‰‹åŠ¨ä¸‹è½½**
```bash
# ä½¿ç”¨ huggingface-cli
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct

# æˆ–ä½¿ç”¨ Python
from huggingface_hub import snapshot_download
snapshot_download("meta-llama/Llama-3.2-1B-Instruct", local_dir="./models/llama-3.2")
```

**æ–¹å¼ä¸‰ï¼šä½¿ç”¨æœ¬åœ°è·¯å¾„**
```python
llm = LLM(model="/path/to/your/model")
```

### 4.2 å¸¸ç”¨æ¨¡å‹æ¨è

| æ¨¡å‹ç³»åˆ— | HuggingFace è·¯å¾„ | è¯´æ˜ |
|---------|-----------------|------|
| Llama 3.2 | `meta-llama/Llama-3.2-1B-Instruct` | Meta æœ€æ–°è½»é‡æ¨¡å‹ |
| Llama 3.1 | `meta-llama/Meta-Llama-3.1-8B-Instruct` | ä¸»æµå¼€æºæ¨¡å‹ |
| Qwen 2.5 | `Qwen/Qwen2.5-7B-Instruct` | é˜¿é‡Œåƒé—®æ¨¡å‹ |
| DeepSeek | `deepseek-ai/deepseek-llm-7b-chat` | DeepSeek æ¨¡å‹ |
| Mistral | `mistralai/Mistral-7B-Instruct-v0.2` | Mistral AI æ¨¡å‹ |

### 4.3 å¤„ç†ä¸åŒæ ¼å¼çš„æ¨¡å‹

**SafeTensors æ ¼å¼ï¼ˆæ¨èï¼‰**
```python
# ç›´æ¥ä½¿ç”¨ï¼ŒvLLM åŸç”Ÿæ”¯æŒ
llm = LLM(model="Qwen/Qwen2.5-7B-Instruct")
```

**PyTorch æ ¼å¼ (.pt/.bin)**
```python
# åŒæ ·ç›´æ¥æ”¯æŒ
llm = LLM(model="/path/to/pytorch/model")
```

**GGUF æ ¼å¼**
```python
# vLLM æ”¯æŒ GGUF é‡åŒ–æ¨¡å‹
llm = LLM(model="/path/to/model.gguf")
```

### 4.4 é‡åŒ–æ¨¡å‹

**FP8 é‡åŒ–**
```python
llm = LLM(
    model="meta-llama/Llama-3.1-8B-Instruct",
    quantization="fp8"
)
```

**AWQ é‡åŒ–**
```python
llm = LLM(
    model="TheBloke/Llama-2-7B-Chat-AWQ",
    quantization="awq"
)
```

**GPTQ é‡åŒ–**
```python
llm = LLM(
    model="TheBloke/Llama-2-7B-Chat-GPTQ",
    quantization="gptq"
)
```

---

## 5. å…³é”®é…ç½®å‚æ•°

### 5.1 æ¨¡å‹é…ç½®

```python
llm = LLM(
    model="meta-llama/Llama-3.2-1B-Instruct",
    
    # æ•°æ®ç±»å‹
    dtype="auto",  # auto, float16, bfloat16, float32
    
    # é‡åŒ–æ–¹æ³•
    quantization=None,  # None, "fp8", "awq", "gptq", "squeezellm"
    
    # å¼ é‡å¹¶è¡Œ
    tensor_parallel_size=1,  # GPU æ•°é‡
    
    # ä¿¡ä»»è¿œç¨‹ä»£ç 
    trust_remote_code=False,
    
    # GPU å†…å­˜åˆ©ç”¨ç‡
    gpu_memory_utilization=0.9,
    
    # æœ€å¤§æ¨¡å‹é•¿åº¦ï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰
    max_model_len=None,  # None è¡¨ç¤ºä½¿ç”¨æ¨¡å‹é»˜è®¤å€¼
    
    # æ˜¯å¦å¯ç”¨å‰ç¼€ç¼“å­˜
    enable_prefix_caching=False,
)
```

### 5.2 é‡‡æ ·å‚æ•°

```python
from vllm import SamplingParams

sampling_params = SamplingParams(
    # ç”Ÿæˆæ§åˆ¶
    max_tokens=256,           # æœ€å¤§ç”Ÿæˆ token æ•°
    temperature=0.8,          # æ¸©åº¦ï¼Œè¶Šé«˜è¶Šéšæœº
    top_p=0.95,               # nucleus sampling
    top_k=50,                 # top-k sampling
    
    # æƒ©ç½šé¡¹
    presence_penalty=0.0,     # å­˜åœ¨æƒ©ç½š
    frequency_penalty=0.0,    # é¢‘ç‡æƒ©ç½š
    repetition_penalty=1.0,   # é‡å¤æƒ©ç½š
    
    # åœæ­¢æ¡ä»¶
    stop=None,                # åœæ­¢è¯åˆ—è¡¨
    stop_token_ids=None,      # åœæ­¢ token ID åˆ—è¡¨
    ignore_eos=False,         # æ˜¯å¦å¿½ç•¥ EOS
    
    # è¾“å‡ºæ§åˆ¶
    n=1,                      # æ¯ä¸ª prompt ç”Ÿæˆå‡ ä¸ªç»“æœ
    best_of=None,             # é‡‡æ ·æœ€ä½³
    logprobs=None,            # è¿”å› logprobs æ•°é‡
)
```

---

## 6. æ¨ç†å…¥å£ä¸è°ƒç”¨é“¾

vLLM çš„æ¨ç†å…¥å£ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç§ï¼š

### 6.1 ç¦»çº¿æ‰¹é‡æ¨ç†ï¼ˆOffline Inferenceï¼‰

```
ç”¨æˆ·ä»£ç 
  â”‚
  â–¼
LLM.generate()                    # vllm/entrypoints/llm.py
  â”‚
  â–¼
LLMEngine                         # vllm/v1/engine/llm_engine.py
  â”‚
  â–¼
EngineCoreClient                  # å¼•æ“æ ¸å¿ƒå®¢æˆ·ç«¯
  â”‚
  â–¼
GPUModelRunner.execute_model()    # vllm/v1/worker/gpu_model_runner.py
  â”‚
  â–¼
Model.forward()                   # vllm/model_executor/models/*.py
```

### 6.2 åœ¨çº¿æœåŠ¡ï¼ˆOnline Servingï¼‰

```
HTTP è¯·æ±‚
  â”‚
  â–¼
OpenAI API Server                 # vllm/entrypoints/openai/api_server.py
  â”‚
  â–¼
AsyncLLMEngine                    # å¼‚æ­¥å¼•æ“
  â”‚
  â–¼
... (åŒä¸Š)
```

### 6.3 CLI å…¥å£

```bash
# ä¸»è¦çš„ CLI å‘½ä»¤
vllm serve        # å¯åŠ¨æœåŠ¡å™¨
vllm bench        # è¿è¡Œ benchmark
vllm chat         # äº¤äº’å¼å¯¹è¯
```

---

## 7. å°ç»“

æœ¬æ–‡æ¡£ä»‹ç»äº† vLLM é¡¹ç›®çš„æ•´ä½“ç»“æ„ã€‚å¦‚éœ€æ·±å…¥äº†è§£ï¼š

- **æ ¸å¿ƒæ¡†æ¶ç»†èŠ‚** â†’ è¯·å‚è€ƒ [framework_vllmcore.md](./framework_vllmcore.md)
- **çº¿æ€§å±‚ä¸ GEMM** â†’ è¯·å‚è€ƒ [framework_lineargemm.md](./framework_lineargemm.md)

vLLM çš„è®¾è®¡ç†å¿µæ˜¯é€šè¿‡ PagedAttentionã€è¿ç»­æ‰¹å¤„ç†å’Œ CUDA Graph ç­‰æŠ€æœ¯ï¼Œå®ç°é«˜ååã€ä½å»¶è¿Ÿçš„å¤§æ¨¡å‹æ¨ç†ã€‚æ•´ä¸ªé¡¹ç›®ç»“æ„æ¸…æ™°ï¼Œæ¨¡å—åŒ–ç¨‹åº¦é«˜ï¼Œä¾¿äºäºŒæ¬¡å¼€å‘å’Œå®šåˆ¶ã€‚
