{
  "meta": {
    "gpu_name": "NVIDIA GeForce RTX 5080",
    "gpu_short_name": "RTX5080",
    "compute_capability": "cc120",
    "model_name": "SQUARE",
    "mode": "square",
    "backend": "cuBLASLt",
    "dtype": "fp8e4m3",
    "alg_count": 8,
    "warmup": 25,
    "repeat": 50,
    "torch_version": "2.9.0+cu129",
    "time": "2026-01-27T11:43:25.601692",
    "M_list": [
      64,
      128,
      256,
      512,
      1024,
      2048,
      4096,
      8192,
      16384,
      32768,
      65536
    ],
    "NK_list": [
      [
        64,
        64
      ],
      [
        128,
        128
      ],
      [
        256,
        256
      ],
      [
        512,
        512
      ],
      [
        1024,
        1024
      ],
      [
        2048,
        2048
      ],
      [
        4096,
        4096
      ],
      [
        8192,
        8192
      ],
      [
        16384,
        16384
      ],
      [
        32768,
        32768
      ],
      [
        65536,
        65536
      ]
    ],
    "search_stats": {
      "total": 11,
      "success": 10,
      "failed": 0,
      "errors": 1
    },
    "skipped_nk": [
      {
        "N": 65536,
        "K": 65536,
        "reason": "CUDA OOM: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.15 GiB is free. Including non-PyTorch memory, this process has 10.29 GiB memory in use. Of the allocated memory 10.00 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
      }
    ]
  },
  "nk_entries": {
    "(64,64)": {
      "m_thresholds": [
        64
      ],
      "alg_by_m": {
        "64": {
          "workspace": 0,
          "tops": 0.15714560449123383,
          "lat_us": 3.336319923400879,
          "algo_data": "IwAAAA0AAAARAAAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAUARdgBABwAAAAcAAAADgAcAEQAAAAAAAAAAAAAAA==",
          "alg_id": 35
        }
      }
    },
    "(128,128)": {
      "m_thresholds": [
        128
      ],
      "alg_by_m": {
        "128": {
          "workspace": 0,
          "tops": 1.2627360820770264,
          "lat_us": 3.3215999603271484,
          "algo_data": "IwAAAA0AAAARAAAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAAARdgBABwAAAAcAAAADgAcAEQAAAAAAAAAAAAAAA==",
          "alg_id": 35
        }
      }
    },
    "(256,256)": {
      "m_thresholds": [
        256
      ],
      "alg_by_m": {
        "256": {
          "workspace": 0,
          "tops": 9.963664054870605,
          "lat_us": 3.367680072784424,
          "algo_data": "IwAAAA0AAAARAAAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAAARdgBABwAAAAcAAAADgAcAEQAAAAAAAAAAAAAAA==",
          "alg_id": 35
        }
      }
    },
    "(512,512)": {
      "m_thresholds": [
        512
      ],
      "alg_by_m": {
        "512": {
          "workspace": 0,
          "tops": 64.10368347167969,
          "lat_us": 4.1875200271606445,
          "algo_data": "IwAAAA0AAAARAAAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAAARdgBABwAAAAcAAAADgAcAEQAAAAAAAAAAAAAAA==",
          "alg_id": 35
        }
      }
    },
    "(1024,1024)": {
      "m_thresholds": [
        1024
      ],
      "alg_by_m": {
        "1024": {
          "workspace": 0,
          "tops": 148.8265380859375,
          "lat_us": 14.42944049835205,
          "algo_data": "IwAAAA8AAAAQAAAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAAARdgBABwAAAAcAAAADgAcAEQAAAAAAAAAAAAAAA==",
          "alg_id": 35
        }
      }
    },
    "(2048,2048)": {
      "m_thresholds": [
        2048
      ],
      "alg_by_m": {
        "2048": {
          "workspace": 0,
          "tops": 214.88417053222656,
          "lat_us": 79.9494400024414,
          "algo_data": "IwAAAA8AAAAQAAAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAAARdgBABwAAAAcAAAADgAcAEQAAAAAAAAAAAAAAA==",
          "alg_id": 35
        }
      }
    },
    "(4096,4096)": {
      "m_thresholds": [
        4096
      ],
      "alg_by_m": {
        "4096": {
          "workspace": 0,
          "tops": 232.3302001953125,
          "lat_us": 591.5673217773438,
          "algo_data": "IwAAABIAAAAQAAAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAAARdgBABwAAAAcAAAADgAcAEQAAAAAAAAAAAAAAA==",
          "alg_id": 35
        }
      }
    },
    "(8192,8192)": {
      "m_thresholds": [
        8192
      ],
      "alg_by_m": {
        "8192": {
          "workspace": 0,
          "tops": 240.92913818359375,
          "lat_us": 4563.630859375,
          "algo_data": "IwAAABQAAAAPAAAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAAARdgBABwAAAAcAAAADgAcAEQAAAAAAAAAAAAAAA==",
          "alg_id": 35
        }
      }
    },
    "(16384,16384)": {
      "m_thresholds": [
        16384
      ],
      "alg_by_m": {
        "16384": {
          "workspace": 0,
          "tops": 241.80068969726562,
          "lat_us": 36377.453125,
          "algo_data": "IwAAABQAAAAPAAAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAAARdgBABwAAAAcAAAADgAcAEQAAAAAAAAAAAAAAA==",
          "alg_id": 35
        }
      }
    },
    "(32768,32768)": {
      "m_thresholds": [
        32768
      ],
      "alg_by_m": {
        "32768": {
          "workspace": 0,
          "tops": 240.86988830566406,
          "lat_us": 292144.21875,
          "algo_data": "IwAAABQAAAAPAAAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAAARdgBABwAAAAcAAAADgAcAEQAAAAAAAAAAAAAAA==",
          "alg_id": 35
        }
      }
    },
    "(65536,65536)": {
      "m_thresholds": [],
      "alg_by_m": {},
      "errors_by_m": {
        "65536": "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.15 GiB is free. Including non-PyTorch memory, this process has 10.29 GiB memory in use. Of the allocated memory 10.00 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
      }
    }
  }
}