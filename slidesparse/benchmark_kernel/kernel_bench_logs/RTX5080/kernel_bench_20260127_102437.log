======================================================================
SlideSparse Kernel Benchmark Log
Started: 2026-01-27 10:24:37
======================================================================

Hardware:
  GPU: NVIDIA GeForce RTX 5080 (cc120)
  Python: py312
  CUDA: cu129
  Arch: x86_64

[INFO] æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/benchmark_kernel/kernel_bench_logs/kernel_bench_20260127_102437.log

======================================================================
TASK 1: cuBLASLt Model æµ‹è¯•
Started: 2026-01-27 10:24:37
======================================================================


------------------------------------------------------------
  cuBLASLt Model: Llama3.2-1B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cublaslt --model Llama3.2-1B-INT8

============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
ğŸ”¨ Building cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64...
Command: /usr/local/cuda/bin/nvcc -std=c++17 -O3 -Xcompiler -fPIC --shared -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=sm_121 -I /usr/local/cuda/include /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/cublaslt_gemm.cu -L/usr/lib/x86_64-linux-gnu -lcublasLt -lcublas -lcuda -o /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/build/cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
âœ“ Built: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 10:26:27.776818402 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 1, æœ‰æ•ˆ: 1
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 10:27:02.230811386 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 10:27:19.647691140 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 10:28:06.455876716 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 6, æœ‰æ•ˆ: 6
Traceback (most recent call last):
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 641, in <module>
    main()
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 616, in main
    ret = run_search(
          ^^^^^^^^^^^
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 407, in run_search
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 224, in empty_cache
    torch._C._cuda_emptyCache()
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[W127 10:28:10.955011322 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536
[ERROR] cuBLASLt search failed with code 1

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[W127 10:28:10.489369014 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-1B-INT8 å®Œæˆ (213.2s)

------------------------------------------------------------
  cuBLASLt Model: Llama3.2-1B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cublaslt --model Llama3.2-1B-FP8

============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 10:29:56.769120305 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 1, æœ‰æ•ˆ: 1
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 10:30:31.278600424 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 10:30:48.660564576 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 10:31:35.403508021 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 6, æœ‰æ•ˆ: 6
Traceback (most recent call last):
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 641, in <module>
    main()
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 616, in main
    ret = run_search(
          ^^^^^^^^^^^
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 407, in run_search
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 224, in empty_cache
    torch._C._cuda_emptyCache()
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[W127 10:31:39.837061701 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536
[ERROR] cuBLASLt search failed with code 1

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[W127 10:31:39.418782037 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-1B-FP8 å®Œæˆ (208.9s)

------------------------------------------------------------
  cuBLASLt Model: Llama3.2-3B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cublaslt --model Llama3.2-3B-INT8

============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 10:34:10.874182487 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 1, æœ‰æ•ˆ: 1
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 10:35:04.030665412 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 10:35:30.624372572 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 10:36:49.357381416 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 4
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 4
Traceback (most recent call last):
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 641, in <module>
    main()
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 616, in main
    ret = run_search(
          ^^^^^^^^^^^
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 407, in run_search
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 224, in empty_cache
    torch._C._cuda_emptyCache()
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[W127 10:36:54.126058324 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536
[ERROR] cuBLASLt search failed with code 1

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[W127 10:36:54.664660066 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-3B-INT8 å®Œæˆ (315.2s)

------------------------------------------------------------
  cuBLASLt Model: Llama3.2-3B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cublaslt --model Llama3.2-3B-FP8

============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 10:39:25.118761012 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 1, æœ‰æ•ˆ: 1
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 10:40:19.400138110 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 10:40:46.040218285 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 10:42:05.878609165 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 4
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 4
Traceback (most recent call last):
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 641, in <module>
    main()
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 616, in main
    ret = run_search(
          ^^^^^^^^^^^
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 407, in run_search
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 224, in empty_cache
    torch._C._cuda_emptyCache()
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[W127 10:42:09.701954239 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536
[ERROR] cuBLASLt search failed with code 1

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[W127 10:42:10.224818824 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-3B-FP8 å®Œæˆ (315.6s)

------------------------------------------------------------
  cuBLASLt Model: Qwen2.5-7B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cublaslt --model Qwen2.5-7B-INT8

============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 10:47:44.747212229 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 1, æœ‰æ•ˆ: 1
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 10:49:44.092330244 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 10:50:39.708146526 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 7, æœ‰æ•ˆ: 7

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 10:53:33.914697560 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 4
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 4
Traceback (most recent call last):
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 641, in <module>
    main()
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 616, in main
    ret = run_search(
          ^^^^^^^^^^^
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 407, in run_search
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 224, in empty_cache
    torch._C._cuda_emptyCache()
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[W127 10:53:38.117480487 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536
[ERROR] cuBLASLt search failed with code 1

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[W127 10:53:38.642133905 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Qwen2.5-7B-INT8 å®Œæˆ (688.4s)

------------------------------------------------------------
  cuBLASLt Model: Qwen2.5-7B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cublaslt --model Qwen2.5-7B-FP8

============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 10:59:13.276334168 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 1, æœ‰æ•ˆ: 1
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:01:12.661636370 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:02:08.129958623 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 7, æœ‰æ•ˆ: 7

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 11:05:01.373166339 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 4
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 4
Traceback (most recent call last):
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 641, in <module>
    main()
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 616, in main
    ret = run_search(
          ^^^^^^^^^^^
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 407, in run_search
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 224, in empty_cache
    torch._C._cuda_emptyCache()
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[W127 11:05:06.356999888 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536
[ERROR] cuBLASLt search failed with code 1

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[W127 11:05:07.893281215 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Qwen2.5-7B-FP8 å®Œæˆ (688.2s)

------------------------------------------------------------
  cuBLASLt Model: Qwen2.5-14B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cublaslt --model Qwen2.5-14B-INT8

============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:11:36.010318404 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 1, æœ‰æ•ˆ: 1
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:13:55.724869970 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:15:04.378608336 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 7, æœ‰æ•ˆ: 7

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 11:17:17.523402128 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 5, æœ‰æ•ˆ: 5
Traceback (most recent call last):
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 641, in <module>
    main()
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 616, in main
    ret = run_search(
          ^^^^^^^^^^^
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 407, in run_search
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 224, in empty_cache
    torch._C._cuda_emptyCache()
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[W127 11:17:21.081050727 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536
[ERROR] cuBLASLt search failed with code 1

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[W127 11:17:21.621455936 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Qwen2.5-14B-INT8 å®Œæˆ (734.7s)

------------------------------------------------------------
  cuBLASLt Model: Qwen2.5-14B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cublaslt --model Qwen2.5-14B-FP8

============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:23:50.715104481 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 1, æœ‰æ•ˆ: 1
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:26:10.613841799 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:27:19.291839883 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 7, æœ‰æ•ˆ: 7

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-FP8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-FP8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 11:29:32.531234051 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 5, æœ‰æ•ˆ: 5
Traceback (most recent call last):
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 641, in <module>
    main()
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 616, in main
    ret = run_search(
          ^^^^^^^^^^^
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 407, in run_search
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 224, in empty_cache
    torch._C._cuda_emptyCache()
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[W127 11:29:36.103007869 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536
[ERROR] cuBLASLt search failed with code 1

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[W127 11:29:36.642767214 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Qwen2.5-14B-FP8 å®Œæˆ (735.0s)

[INFO] cuBLASLt Model ç»Ÿè®¡: æˆåŠŸ 8, å¤±è´¥ 0

----------------------------------------------------------------------
TASK 1: cuBLASLt Model æµ‹è¯• - SUCCESS
Duration: 3899.3 seconds (65.0 minutes)
----------------------------------------------------------------------


======================================================================
TASK 2: cuBLASLt Square æµ‹è¯•
Started: 2026-01-27 11:29:36
======================================================================


------------------------------------------------------------
  cuBLASLt Square æµ‹è¯•
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cublaslt --model square

============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 64)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/11: (128, 128)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/11: (256, 256)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/11: (512, 512)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 5/11: (1024, 1024)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 6/11: (2048, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 7/11: (4096, 4096)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 8/11: (8192, 8192)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 9/11: (16384, 16384)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 10/11: (32768, 32768)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 11/11: (65536, 65536)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 65536)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_SQUARE.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_SQUARE.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:36:34.018911174 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 64)
      â†’ ç®—æ³•æ•°: 6, æœ‰æ•ˆ: 6
    NK 2/11: (128, 128)
      â†’ ç®—æ³•æ•°: 6, æœ‰æ•ˆ: 6
    NK 3/11: (256, 256)
      â†’ ç®—æ³•æ•°: 6, æœ‰æ•ˆ: 6
    NK 4/11: (512, 512)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 5/11: (1024, 1024)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 6/11: (2048, 2048)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 7/11: (4096, 4096)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 8/11: (8192, 8192)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 9/11: (16384, 16384)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 10/11: (32768, 32768)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 11/11: (65536, 65536)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 65536)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_SQUARE.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_SQUARE.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:39:09.066071971 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 64)
      â†’ ç®—æ³•æ•°: 1, æœ‰æ•ˆ: 1
    NK 2/11: (128, 128)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 3/11: (256, 256)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 4/11: (512, 512)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 5/11: (1024, 1024)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 6/11: (2048, 2048)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 7/11: (4096, 4096)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 8/11: (8192, 8192)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 9/11: (16384, 16384)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 10/11: (32768, 32768)
      â†’ ç®—æ³•æ•°: 3, æœ‰æ•ˆ: 3
    NK 11/11: (65536, 65536)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 65536)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_SQUARE.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_SQUARE.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:40:19.045826953 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 64)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/11: (128, 128)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/11: (256, 256)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 4/11: (512, 512)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 5/11: (1024, 1024)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 6/11: (2048, 2048)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 7/11: (4096, 4096)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 8/11: (8192, 8192)
      â†’ ç®—æ³•æ•°: 7, æœ‰æ•ˆ: 7
    NK 9/11: (16384, 16384)
      â†’ ç®—æ³•æ•°: 7, æœ‰æ•ˆ: 7
    NK 10/11: (32768, 32768)
      â†’ ç®—æ³•æ•°: 7, æœ‰æ•ˆ: 7
    NK 11/11: (65536, 65536)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 65536)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_SQUARE.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_SQUARE.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 11:43:26.933750708 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
============================================================
cuBLASLt Dense GEMM ç®—æ³•æœç´¢
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuBLASLt å¯ç”¨

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 64)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 2/11: (128, 128)
      â†’ ç®—æ³•æ•°: 8, æœ‰æ•ˆ: 8
    NK 3/11: (256, 256)
      â†’ ç®—æ³•æ•°: 6, æœ‰æ•ˆ: 6
    NK 4/11: (512, 512)
      â†’ ç®—æ³•æ•°: 5, æœ‰æ•ˆ: 5
    NK 5/11: (1024, 1024)
      â†’ ç®—æ³•æ•°: 6, æœ‰æ•ˆ: 6
    NK 6/11: (2048, 2048)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0
Traceback (most recent call last):
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 641, in <module>
    main()
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 616, in main
    ret = run_search(
          ^^^^^^^^^^^
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 407, in run_search
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 224, in empty_cache
    torch._C._cuda_emptyCache()
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[W127 11:43:29.366753668 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
M=N=K: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp4e2m1 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536
[ERROR] cuBLASLt search failed with code 1

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[W127 11:43:30.978730257 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Square æµ‹è¯•å®Œæˆ (833.3s)

----------------------------------------------------------------------
TASK 2: cuBLASLt Square æµ‹è¯• - SUCCESS
Duration: 833.3 seconds (13.9 minutes)
----------------------------------------------------------------------


======================================================================
TASK 3: cuSPARSELt Model é«˜ç¨€ç– (2_4~2_10)
Started: 2026-01-27 11:43:30
======================================================================


------------------------------------------------------------
  cuSPARSELt Model é«˜ç¨€ç–: Llama3.2-1B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Llama3.2-1B-INT8 --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3072, 2048), (2048, 2048), (16384, 2048), (2048, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
ğŸ”¨ Building cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64...
Command: /usr/local/cuda/bin/nvcc -std=c++17 -O3 -Xcompiler -fPIC --shared -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=sm_121 -I /usr/local/cuda/include /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/cusparselt_gemm.cu -L/usr/lib/x86_64-linux-gnu -lcusparseLt -lcusparse -lcublas -lcuda -o /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/build/cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
âœ“ Built: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:43:56.055875989 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3072, 2752), (2048, 2752), (16384, 2752), (2048, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:44:20.826131977 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3072, 3072), (2048, 3072), (16384, 3072), (2048, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:44:46.016296593 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3072, 3296), (2048, 3296), (16384, 3296), (2048, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:45:13.786989780 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3072, 2048), (2048, 2048), (16384, 2048), (2048, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:45:33.608861782 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3072, 2752), (2048, 2752), (16384, 2752), (2048, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:45:57.560271738 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3072, 3072), (2048, 3072), (16384, 3072), (2048, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:46:23.507087450 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3072, 3296), (2048, 3296), (16384, 3296), (2048, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:46:51.546779211 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3072, 2048), (2048, 2048), (16384, 2048), (2048, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:47:02.057047129 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3072, 2752), (2048, 2752), (16384, 2752), (2048, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:47:14.855964965 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3072, 3072), (2048, 3072), (16384, 3072), (2048, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:47:26.625246527 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3072, 3296), (2048, 3296), (16384, 3296), (2048, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:47:40.645472133 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3072, 2048), (2048, 2048), (16384, 2048), (2048, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 21

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 11:48:27.170398656 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3072, 2752), (2048, 2752), (16384, 2752), (2048, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2752)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 2/4: (2048, 2752)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/4: (16384, 2752)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (2048, 10944)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 11:49:31.263729620 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3072, 3072), (2048, 3072), (16384, 3072), (2048, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 2/4: (2048, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (2048, 12288)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 11:50:37.347928483 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3072, 3296), (2048, 3296), (16384, 3296), (2048, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3296)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 2/4: (2048, 3296)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 3/4: (16384, 3296)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (2048, 13120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 11:51:55.765178615 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3072, 2048), (2048, 2048), (16384, 2048), (2048, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 11:52:08.575298536 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3072, 2752), (2048, 2752), (16384, 2752), (2048, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 3/4: (16384, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (2048, 10944)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 11:52:23.292077526 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3072, 3072), (2048, 3072), (16384, 3072), (2048, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (2048, 12288)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 534.94 MiB is free. Including non-PyTorch memory, this process has 14.92 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 218.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 11:52:36.684981922 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3072, 3328), (2048, 3328), (16384, 3328), (2048, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3328)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 2/4: (2048, 3328)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 3328)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (2048, 13120)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.41 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.93 GiB is free. Including non-PyTorch memory, this process has 9.51 GiB memory in use. Of the allocated memory 8.99 GiB is allocated by PyTorch, and 237.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 11:52:50.830770690 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 11:52:50.363869078 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-1B-INT8 é«˜ç¨€ç–å®Œæˆ (560.4s)

------------------------------------------------------------
  cuSPARSELt Model é«˜ç¨€ç–: Llama3.2-1B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Llama3.2-1B-FP8 --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3072, 2048), (2048, 2048), (16384, 2048), (2048, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:53:12.530657176 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3072, 2752), (2048, 2752), (16384, 2752), (2048, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:53:36.574255192 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3072, 3072), (2048, 3072), (16384, 3072), (2048, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:54:02.435548735 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3072, 3296), (2048, 3296), (16384, 3296), (2048, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 11:54:30.364856637 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3072, 2048), (2048, 2048), (16384, 2048), (2048, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:54:50.031357629 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3072, 2752), (2048, 2752), (16384, 2752), (2048, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:55:14.912821471 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3072, 3072), (2048, 3072), (16384, 3072), (2048, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:55:40.146217302 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3072, 3296), (2048, 3296), (16384, 3296), (2048, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 11:56:08.135433134 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3072, 2048), (2048, 2048), (16384, 2048), (2048, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:56:18.544371378 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3072, 2752), (2048, 2752), (16384, 2752), (2048, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:56:30.541272460 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3072, 3072), (2048, 3072), (16384, 3072), (2048, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:56:43.058130243 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3072, 3296), (2048, 3296), (16384, 3296), (2048, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 11:56:57.003184835 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3072, 2048), (2048, 2048), (16384, 2048), (2048, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 11:57:44.512252411 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3072, 2752), (2048, 2752), (16384, 2752), (2048, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2752)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 20
    NK 2/4: (2048, 2752)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/4: (16384, 2752)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (2048, 10944)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 11:58:51.536681303 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3072, 3072), (2048, 3072), (16384, 3072), (2048, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (2048, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 20
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 4/4: (2048, 12288)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:00:07.192385812 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3072, 3296), (2048, 3296), (16384, 3296), (2048, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3296)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 2/4: (2048, 3296)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 3/4: (16384, 3296)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (2048, 13120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:01:25.290399540 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3072, 2048), (2048, 2048), (16384, 2048), (2048, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 4/4: (2048, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:01:37.768424223 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3072, 2752), (2048, 2752), (16384, 2752), (2048, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 4/4: (2048, 10944)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:01:52.193543645 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3072, 3072), (2048, 3072), (16384, 3072), (2048, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (2048, 12288)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 534.94 MiB is free. Including non-PyTorch memory, this process has 14.92 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 218.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:02:05.224143233 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3072, 3328), (2048, 3328), (16384, 3328), (2048, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3328)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 2/4: (2048, 3328)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 3328)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (2048, 13120)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.41 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.93 GiB is free. Including non-PyTorch memory, this process has 9.51 GiB memory in use. Of the allocated memory 8.99 GiB is allocated by PyTorch, and 237.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:02:18.683801389 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 12:02:19.202226070 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-1B-FP8 é«˜ç¨€ç–å®Œæˆ (568.8s)

------------------------------------------------------------
  cuSPARSELt Model é«˜ç¨€ç–: Llama3.2-3B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Llama3.2-3B-INT8 --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(5120, 3072), (3072, 3072), (16384, 3072), (3072, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:02:48.701350504 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(5120, 4096), (3072, 4096), (16384, 4096), (3072, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:03:24.183636856 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(5120, 4608), (3072, 4608), (16384, 4608), (3072, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:04:05.622020531 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(5120, 4928), (3072, 4928), (16384, 4928), (3072, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:04:51.693960700 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(5120, 3072), (3072, 3072), (16384, 3072), (3072, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:05:18.733922891 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(5120, 4096), (3072, 4096), (16384, 4096), (3072, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:05:56.723378905 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(5120, 4608), (3072, 4608), (16384, 4608), (3072, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:06:38.141421853 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(5120, 4928), (3072, 4928), (16384, 4928), (3072, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:07:23.773626623 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(5120, 3072), (3072, 3072), (16384, 3072), (3072, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:07:37.054307781 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(5120, 4096), (3072, 4096), (16384, 4096), (3072, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:07:53.855344218 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(5120, 4608), (3072, 4608), (16384, 4608), (3072, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:08:10.033466836 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(5120, 4928), (3072, 4928), (16384, 4928), (3072, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:08:28.966210739 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(5120, 3072), (3072, 3072), (16384, 3072), (3072, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:09:42.619362864 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(5120, 4096), (3072, 4096), (16384, 4096), (3072, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 4/4: (3072, 10944)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:11:25.900762698 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(5120, 4608), (3072, 4608), (16384, 4608), (3072, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4608)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 2/4: (3072, 4608)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 3/4: (16384, 4608)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (3072, 12288)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:13:18.756177963 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(5120, 4928), (3072, 4928), (16384, 4928), (3072, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4928)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 20
    NK 2/4: (3072, 4928)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 3/4: (16384, 4928)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 4/4: (3072, 13120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:15:19.414639337 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(5120, 3072), (3072, 3072), (16384, 3072), (3072, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 11

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:15:38.823498037 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(5120, 4096), (3072, 4096), (16384, 4096), (3072, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 2/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 4/4: (3072, 10944)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:15:54.587293513 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(5120, 4608), (3072, 4608), (16384, 4608), (3072, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (3072, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (3072, 12288)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 486.94 MiB is free. Including non-PyTorch memory, this process has 14.97 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 182.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:16:13.520773771 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(5120, 4928), (3072, 4928), (16384, 4928), (3072, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 2/4: (3072, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 3/4: (16384, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (3072, 13120)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.41 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.89 GiB is free. Including non-PyTorch memory, this process has 9.56 GiB memory in use. Of the allocated memory 9.07 GiB is allocated by PyTorch, and 194.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:16:39.807968934 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 12:16:39.371842263 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-3B-INT8 é«˜ç¨€ç–å®Œæˆ (860.2s)

------------------------------------------------------------
  cuSPARSELt Model é«˜ç¨€ç–: Llama3.2-3B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Llama3.2-3B-FP8 --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(5120, 3072), (3072, 3072), (16384, 3072), (3072, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:17:09.118666094 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(5120, 4096), (3072, 4096), (16384, 4096), (3072, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:17:46.840976017 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(5120, 4608), (3072, 4608), (16384, 4608), (3072, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:18:27.040145882 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(5120, 4928), (3072, 4928), (16384, 4928), (3072, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:19:13.249036700 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(5120, 3072), (3072, 3072), (16384, 3072), (3072, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:19:40.291110452 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(5120, 4096), (3072, 4096), (16384, 4096), (3072, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:20:16.519241953 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(5120, 4608), (3072, 4608), (16384, 4608), (3072, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:20:58.907140817 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(5120, 4928), (3072, 4928), (16384, 4928), (3072, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:21:45.710739765 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(5120, 3072), (3072, 3072), (16384, 3072), (3072, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:21:59.922329513 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(5120, 4096), (3072, 4096), (16384, 4096), (3072, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:22:14.789530270 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(5120, 4608), (3072, 4608), (16384, 4608), (3072, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:22:32.147251969 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(5120, 4928), (3072, 4928), (16384, 4928), (3072, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:22:50.905768260 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(5120, 3072), (3072, 3072), (16384, 3072), (3072, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 20
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:24:09.603743953 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(5120, 4096), (3072, 4096), (16384, 4096), (3072, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (3072, 10944)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:25:44.649119653 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(5120, 4608), (3072, 4608), (16384, 4608), (3072, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4608)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (3072, 4608)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 20
    NK 3/4: (16384, 4608)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (3072, 12288)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:27:36.909904087 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(5120, 4928), (3072, 4928), (16384, 4928), (3072, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4928)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 22
    NK 2/4: (3072, 4928)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 3/4: (16384, 4928)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 4/4: (3072, 13120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:29:40.027327149 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(5120, 3072), (3072, 3072), (16384, 3072), (3072, 8192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 2/4: (3072, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (3072, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:29:58.694728315 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(5120, 4096), (3072, 4096), (16384, 4096), (3072, 10944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 2/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 12
    NK 4/4: (3072, 10944)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:30:15.679994504 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(5120, 4608), (3072, 4608), (16384, 4608), (3072, 12288)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3072, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 4608)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (3072, 12288)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 486.94 MiB is free. Including non-PyTorch memory, this process has 14.97 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 182.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:30:35.839476205 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(5120, 4928), (3072, 4928), (16384, 4928), (3072, 13120)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (3072, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 4928)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (3072, 13120)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.41 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.89 GiB is free. Including non-PyTorch memory, this process has 9.56 GiB memory in use. Of the allocated memory 9.07 GiB is allocated by PyTorch, and 194.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 12:31:00.561915976 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 12:31:01.109240477 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-3B-FP8 é«˜ç¨€ç–å®Œæˆ (861.7s)

------------------------------------------------------------
  cuSPARSELt Model é«˜ç¨€ç–: Qwen2.5-7B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Qwen2.5-7B-INT8 --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(4608, 3584), (3584, 3584), (37888, 3584), (3584, 18944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:32:16.473705588 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(4608, 4800), (3584, 4800), (37888, 4800), (3584, 25280)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 25280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:33:52.153798077 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(4608, 5376), (3584, 5376), (37888, 5376), (3584, 28416)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 28416)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:35:39.528018712 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(4608, 5760), (3584, 5760), (37888, 5760), (3584, 30336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 30336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 12:37:34.560830246 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(4608, 3584), (3584, 3584), (37888, 3584), (3584, 18944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:38:47.567549642 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(4608, 4800), (3584, 4800), (37888, 4800), (3584, 25280)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 25280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:40:23.194500573 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(4608, 5376), (3584, 5376), (37888, 5376), (3584, 28416)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 28416)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:42:11.906548246 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(4608, 5760), (3584, 5760), (37888, 5760), (3584, 30336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 30336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 12:44:05.767998808 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(4608, 3584), (3584, 3584), (37888, 3584), (3584, 18944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:44:30.395222657 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(4608, 4800), (3584, 4800), (37888, 4800), (3584, 25280)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 25280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:45:01.864809698 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(4608, 5376), (3584, 5376), (37888, 5376), (3584, 28416)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 28416)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:45:34.502476080 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(4608, 5760), (3584, 5760), (37888, 5760), (3584, 30336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 30336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 12:46:10.240011077 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(4608, 3584), (3584, 3584), (37888, 3584), (3584, 18944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:49:35.620716347 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(4608, 4800), (3584, 4800), (37888, 4800), (3584, 25280)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 4800)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 2/4: (3584, 4800)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/4: (37888, 4800)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (3584, 25280)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:54:02.271123407 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(4608, 5376), (3584, 5376), (37888, 5376), (3584, 28416)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 2/4: (3584, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 3/4: (37888, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (3584, 28416)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 12:59:10.472707938 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(4608, 5760), (3584, 5760), (37888, 5760), (3584, 30336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5760)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (3584, 5760)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 3/4: (37888, 5760)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (3584, 30336)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 13:05:08.801349139 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(4608, 3584), (3584, 3584), (37888, 3584), (3584, 18944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (3584, 18944)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.92 GiB is free. Including non-PyTorch memory, this process has 13.52 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 213.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 13:05:35.294365386 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(4608, 4800), (3584, 4800), (37888, 4800), (3584, 25280)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (3584, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 3/4: (37888, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (3584, 25280)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.17 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.67 GiB is free. Including non-PyTorch memory, this process has 11.78 GiB memory in use. Of the allocated memory 11.20 GiB is allocated by PyTorch, and 284.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 13:06:20.625610530 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(4608, 5376), (3584, 5376), (37888, 5376), (3584, 28416)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3584, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 3/4: (37888, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (3584, 28416)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.94 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.25 GiB is free. Including non-PyTorch memory, this process has 13.20 GiB memory in use. Of the allocated memory 12.59 GiB is allocated by PyTorch, and 322.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 13:07:07.521014739 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(4608, 5760), (3584, 5760), (37888, 5760), (3584, 30336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3584, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (37888, 5760)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0
    NK 4/4: (3584, 30336)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.37 GiB is free. Including non-PyTorch memory, this process has 14.07 GiB memory in use. Of the allocated memory 13.44 GiB is allocated by PyTorch, and 342.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=26, å¤±è´¥=0, é”™è¯¯=18
    æˆåŠŸç‡: 59.1%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 13:07:58.508261694 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 13:07:59.036795412 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Qwen2.5-7B-INT8 é«˜ç¨€ç–å®Œæˆ (2217.9s)

------------------------------------------------------------
  cuSPARSELt Model é«˜ç¨€ç–: Qwen2.5-7B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Qwen2.5-7B-FP8 --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(4608, 3584), (3584, 3584), (37888, 3584), (3584, 18944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 13:09:14.325678414 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(4608, 4800), (3584, 4800), (37888, 4800), (3584, 25280)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 25280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 13:10:50.972665883 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(4608, 5376), (3584, 5376), (37888, 5376), (3584, 28416)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 28416)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 13:12:37.471917791 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(4608, 5760), (3584, 5760), (37888, 5760), (3584, 30336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 30336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 13:14:31.785103343 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(4608, 3584), (3584, 3584), (37888, 3584), (3584, 18944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 13:15:44.729343605 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(4608, 4800), (3584, 4800), (37888, 4800), (3584, 25280)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 25280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 13:17:20.730839739 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(4608, 5376), (3584, 5376), (37888, 5376), (3584, 28416)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 28416)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 13:19:07.783192346 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(4608, 5760), (3584, 5760), (37888, 5760), (3584, 30336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 30336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 13:21:02.746457132 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(4608, 3584), (3584, 3584), (37888, 3584), (3584, 18944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 13:21:27.514067137 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(4608, 4800), (3584, 4800), (37888, 4800), (3584, 25280)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 25280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 13:21:58.901172466 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(4608, 5376), (3584, 5376), (37888, 5376), (3584, 28416)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 28416)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 13:22:31.671402129 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(4608, 5760), (3584, 5760), (37888, 5760), (3584, 30336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 30336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 13:23:07.647416860 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(4608, 3584), (3584, 3584), (37888, 3584), (3584, 18944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (3584, 18944)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 13:26:34.705202751 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(4608, 4800), (3584, 4800), (37888, 4800), (3584, 25280)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 4800)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (3584, 4800)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 3/4: (37888, 4800)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (3584, 25280)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 13:31:12.339289241 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(4608, 5376), (3584, 5376), (37888, 5376), (3584, 28416)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (3584, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/4: (37888, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (3584, 28416)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 13:36:12.393689535 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(4608, 5760), (3584, 5760), (37888, 5760), (3584, 30336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5760)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 2/4: (3584, 5760)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/4: (37888, 5760)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 4/4: (3584, 30336)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 13:42:15.420937389 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(4608, 3584), (3584, 3584), (37888, 3584), (3584, 18944)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3584, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 3/4: (37888, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (3584, 18944)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.92 GiB is free. Including non-PyTorch memory, this process has 13.52 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 213.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 13:42:46.601938809 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(4608, 4800), (3584, 4800), (37888, 4800), (3584, 25280)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3584, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (37888, 4800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (3584, 25280)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.17 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.67 GiB is free. Including non-PyTorch memory, this process has 11.78 GiB memory in use. Of the allocated memory 11.20 GiB is allocated by PyTorch, and 284.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 13:43:34.929183448 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(4608, 5376), (3584, 5376), (37888, 5376), (3584, 28416)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 2/4: (3584, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (37888, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (3584, 28416)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.94 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.25 GiB is free. Including non-PyTorch memory, this process has 13.20 GiB memory in use. Of the allocated memory 12.59 GiB is allocated by PyTorch, and 322.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 13:44:24.551704125 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(4608, 5760), (3584, 5760), (37888, 5760), (3584, 30336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 2/4: (3584, 5760)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (37888, 5760)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0
    NK 4/4: (3584, 30336)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.37 GiB is free. Including non-PyTorch memory, this process has 14.07 GiB memory in use. Of the allocated memory 13.44 GiB is allocated by PyTorch, and 342.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=26, å¤±è´¥=0, é”™è¯¯=18
    æˆåŠŸç‡: 59.1%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 13:45:15.232938853 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 13:45:15.774522160 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Qwen2.5-7B-FP8 é«˜ç¨€ç–å®Œæˆ (2236.7s)

------------------------------------------------------------
  cuSPARSELt Model é«˜ç¨€ç–: Qwen2.5-14B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Qwen2.5-14B-INT8 --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(7168, 5120), (5120, 5120), (27648, 5120), (5120, 13824)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 13:46:40.343974418 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(7168, 6848), (5120, 6848), (27648, 6848), (5120, 18432)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 18432)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 13:48:31.293980225 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(7168, 7680), (5120, 7680), (27648, 7680), (5120, 20736)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 20736)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 13:50:39.648619698 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(7168, 8192), (5120, 8192), (27648, 8192), (5120, 22144)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 22144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 13:52:55.079507667 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(7168, 5120), (5120, 5120), (27648, 5120), (5120, 13824)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 13:54:16.644298439 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(7168, 6848), (5120, 6848), (27648, 6848), (5120, 18432)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 18432)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 13:56:08.072062933 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(7168, 7680), (5120, 7680), (27648, 7680), (5120, 20736)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 20736)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 13:58:16.913880194 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(7168, 8192), (5120, 8192), (27648, 8192), (5120, 22144)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 22144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 14:00:31.036301022 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(7168, 5120), (5120, 5120), (27648, 5120), (5120, 13824)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 14:00:58.354230852 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(7168, 6848), (5120, 6848), (27648, 6848), (5120, 18432)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 18432)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 14:01:33.077196936 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(7168, 7680), (5120, 7680), (27648, 7680), (5120, 20736)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 20736)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 14:02:12.031658383 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(7168, 8192), (5120, 8192), (27648, 8192), (5120, 22144)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 22144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 14:02:52.276897659 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(7168, 5120), (5120, 5120), (27648, 5120), (5120, 13824)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 14:06:52.873918368 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(7168, 6848), (5120, 6848), (27648, 6848), (5120, 18432)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 6848)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 2/4: (5120, 6848)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 3/4: (27648, 6848)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (5120, 18432)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 14:11:49.733916054 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(7168, 7680), (5120, 7680), (27648, 7680), (5120, 20736)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 7680)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 2/4: (5120, 7680)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 3/4: (27648, 7680)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (5120, 20736)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 14:17:54.574522152 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(7168, 8192), (5120, 8192), (27648, 8192), (5120, 22144)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (5120, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 3/4: (27648, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (5120, 22144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 14:24:25.806917400 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(7168, 5120), (5120, 5120), (27648, 5120), (5120, 13824)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (5120, 13824)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.29 GiB is free. Including non-PyTorch memory, this process has 10.16 GiB memory in use. Of the allocated memory 9.64 GiB is allocated by PyTorch, and 229.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=32, å¤±è´¥=0, é”™è¯¯=12
    æˆåŠŸç‡: 72.7%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-INT8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 14:24:51.850252230 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(7168, 6848), (5120, 6848), (27648, 6848), (5120, 18432)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (5120, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (27648, 6848)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0
    NK 4/4: (5120, 18432)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.00 GiB is free. Including non-PyTorch memory, this process has 13.44 GiB memory in use. Of the allocated memory 12.85 GiB is allocated by PyTorch, and 304.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=25, å¤±è´¥=0, é”™è¯¯=19
    æˆåŠŸç‡: 56.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-INT8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 14:25:35.524723226 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(7168, 7680), (5120, 7680), (27648, 7680), (5120, 20736)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (5120, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (27648, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (5120, 20736)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 5.06 GiB. GPU 0 has a total capacity of 15.47 GiB of which 370.94 MiB is free. Including non-PyTorch memory, this process has 15.08 GiB memory in use. Of the allocated memory 14.45 GiB is allocated by PyTorch, and 342.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-INT8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 14:26:28.773570585 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(7168, 8192), (5120, 8192), (27648, 8192), (5120, 22144)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (5120, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (27648, 8192)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0
    NK 4/4: (5120, 22144)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 5.41 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.80 GiB is free. Including non-PyTorch memory, this process has 10.64 GiB memory in use. Of the allocated memory 10.03 GiB is allocated by PyTorch, and 327.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=22, å¤±è´¥=0, é”™è¯¯=22
    æˆåŠŸç‡: 50.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-INT8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 14:27:04.740943172 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-14B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 14:27:05.277413373 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Qwen2.5-14B-INT8 é«˜ç¨€ç–å®Œæˆ (2509.5s)

------------------------------------------------------------
  cuSPARSELt Model é«˜ç¨€ç–: Qwen2.5-14B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Qwen2.5-14B-FP8 --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(7168, 5120), (5120, 5120), (27648, 5120), (5120, 13824)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 14:28:29.543092292 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(7168, 6848), (5120, 6848), (27648, 6848), (5120, 18432)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 18432)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 14:30:20.259708756 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(7168, 7680), (5120, 7680), (27648, 7680), (5120, 20736)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 20736)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 14:32:28.899196496 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(7168, 8192), (5120, 8192), (27648, 8192), (5120, 22144)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 22144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 14:34:43.360328627 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(7168, 5120), (5120, 5120), (27648, 5120), (5120, 13824)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 14:36:05.975803372 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(7168, 6848), (5120, 6848), (27648, 6848), (5120, 18432)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 18432)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 14:37:56.217597131 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(7168, 7680), (5120, 7680), (27648, 7680), (5120, 20736)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 20736)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 14:40:04.029120000 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(7168, 8192), (5120, 8192), (27648, 8192), (5120, 22144)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 22144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 14:42:19.097585921 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(7168, 5120), (5120, 5120), (27648, 5120), (5120, 13824)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 14:42:46.549372999 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(7168, 6848), (5120, 6848), (27648, 6848), (5120, 18432)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 18432)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 14:43:21.615966982 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(7168, 7680), (5120, 7680), (27648, 7680), (5120, 20736)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 20736)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 14:44:00.621597545 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(7168, 8192), (5120, 8192), (27648, 8192), (5120, 22144)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 22144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 14:44:41.875616024 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(7168, 5120), (5120, 5120), (27648, 5120), (5120, 13824)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 4/4: (5120, 13824)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 14:48:32.025812357 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(7168, 6848), (5120, 6848), (27648, 6848), (5120, 18432)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 6848)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 2/4: (5120, 6848)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 3/4: (27648, 6848)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (5120, 18432)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 14:53:47.871701694 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(7168, 7680), (5120, 7680), (27648, 7680), (5120, 20736)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 7680)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (5120, 7680)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 3/4: (27648, 7680)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (5120, 20736)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 14:59:37.477084133 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(7168, 8192), (5120, 8192), (27648, 8192), (5120, 22144)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 2/4: (5120, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 3/4: (27648, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (5120, 22144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 15:05:46.470973270 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(7168, 5120), (5120, 5120), (27648, 5120), (5120, 13824)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (27648, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 4/4: (5120, 13824)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.29 GiB is free. Including non-PyTorch memory, this process has 10.16 GiB memory in use. Of the allocated memory 9.64 GiB is allocated by PyTorch, and 229.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=32, å¤±è´¥=0, é”™è¯¯=12
    æˆåŠŸç‡: 72.7%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-FP8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-FP8_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 15:06:12.123066469 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(7168, 6848), (5120, 6848), (27648, 6848), (5120, 18432)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (5120, 6848)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (27648, 6848)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0
    NK 4/4: (5120, 18432)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.00 GiB is free. Including non-PyTorch memory, this process has 13.44 GiB memory in use. Of the allocated memory 12.85 GiB is allocated by PyTorch, and 304.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=25, å¤±è´¥=0, é”™è¯¯=19
    æˆåŠŸç‡: 56.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-FP8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-FP8_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 15:07:00.628273987 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(7168, 7680), (5120, 7680), (27648, 7680), (5120, 20736)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (5120, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 12
    NK 3/4: (27648, 7680)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (5120, 20736)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 5.06 GiB. GPU 0 has a total capacity of 15.47 GiB of which 370.94 MiB is free. Including non-PyTorch memory, this process has 15.08 GiB memory in use. Of the allocated memory 14.45 GiB is allocated by PyTorch, and 342.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-FP8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-FP8_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 15:07:55.367310478 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(7168, 8192), (5120, 8192), (27648, 8192), (5120, 22144)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 2/4: (5120, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 3/4: (27648, 8192)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0
    NK 4/4: (5120, 22144)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 5.41 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.80 GiB is free. Including non-PyTorch memory, this process has 10.64 GiB memory in use. Of the allocated memory 10.03 GiB is allocated by PyTorch, and 327.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=22, å¤±è´¥=0, é”™è¯¯=22
    æˆåŠŸç‡: 50.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-FP8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-14B-FP8_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 15:08:31.222146923 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-14B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 15:08:31.763259535 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Qwen2.5-14B-FP8 é«˜ç¨€ç–å®Œæˆ (2486.5s)

[INFO] cuSPARSELt Model é«˜ç¨€ç–ç»Ÿè®¡: æˆåŠŸ 8, å¤±è´¥ 0

----------------------------------------------------------------------
TASK 3: cuSPARSELt Model é«˜ç¨€ç– (2_4~2_10) - SUCCESS
Duration: 12301.8 seconds (205.0 minutes)
----------------------------------------------------------------------


======================================================================
TASK 4: cuSPARSELt Square é«˜ç¨€ç– (2_4~2_10)
Started: 2026-01-27 15:08:32
======================================================================


------------------------------------------------------------
  cuSPARSELt Square é«˜ç¨€ç–æµ‹è¯•
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model square --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(64, 64), (128, 128), (256, 256), (512, 512), (1024, 1024)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 64)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 128)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 256)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 512)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1024)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 32768)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 11/11: (65536, 65536)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 65536)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_SQUARE_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_SQUARE_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 15:10:23.436477852 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(64, 96), (128, 192), (256, 352), (512, 704), (1024, 1376)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 96)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 352)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 704)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 5472)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 21856)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 43712)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0
    NK 11/11: (65536, 87392)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 87392)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=9, å¤±è´¥=0, é”™è¯¯=2
    æˆåŠŸç‡: 81.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_SQUARE_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_SQUARE_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 15:10:46.208509001 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(64, 96), (128, 192), (256, 384), (512, 768), (1024, 1536)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 96)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 768)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1536)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 24576)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 49152)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.40 GiB is free. Including non-PyTorch memory, this process has 13.04 GiB memory in use. Of the allocated memory 12.75 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    NK 11/11: (65536, 98304)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 98304)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=9, å¤±è´¥=0, é”™è¯¯=2
    æˆåŠŸç‡: 81.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_SQUARE_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_SQUARE_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 15:11:11.420289293 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(64, 128), (128, 224), (256, 416), (512, 832), (1024, 1664)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 128)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 224)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 416)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 832)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 6560)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 26240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 52448)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 3.20 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 13.90 GiB memory in use. Of the allocated memory 13.61 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    NK 11/11: (65536, 104864)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 104864)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=9, å¤±è´¥=0, é”™è¯¯=2
    æˆåŠŸç‡: 81.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_SQUARE_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_SQUARE_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 15:11:38.916659454 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(64, 64), (128, 128), (256, 256), (512, 512), (1024, 1024)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 64)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 128)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 256)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 512)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1024)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 32768)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 11/11: (65536, 65536)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 65536)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_SQUARE_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_SQUARE_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 15:13:27.132611767 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(64, 96), (128, 192), (256, 352), (512, 704), (1024, 1376)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 96)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 352)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 704)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 5472)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 21856)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 43712)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 11/11: (65536, 87392)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 87392)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_SQUARE_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_SQUARE_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 15:15:50.620871039 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(64, 96), (128, 192), (256, 384), (512, 768), (1024, 1536)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 96)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 768)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1536)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 24576)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 49152)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 11/11: (65536, 98304)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 98304)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_SQUARE_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_SQUARE_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 15:18:32.307848588 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(64, 128), (128, 224), (256, 416), (512, 832), (1024, 1664)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 128)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 224)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 416)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 832)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 6560)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 26240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 52448)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 11/11: (65536, 104864)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 104864)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_SQUARE_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_SQUARE_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 15:21:19.316323323 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(64, 64), (128, 128), (256, 256), (512, 512), (1024, 1024)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 64)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 128)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 256)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 512)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1024)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 32768)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 11/11: (65536, 65536)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 65536)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_SQUARE_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_SQUARE_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 15:21:48.768429139 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(64, 96), (128, 192), (256, 352), (512, 704), (1024, 1376)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 96)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 352)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 704)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 5472)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 21856)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 43712)
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(64, 96), (128, 192), (256, 384), (512, 768), (1024, 1536)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 96)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 768)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1536)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 24576)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 49152)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.78 GiB is free. Including non-PyTorch memory, this process has 12.67 GiB memory in use. Of the allocated memory 12.38 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    NK 11/11: (65536, 98304)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 98304)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=9, å¤±è´¥=0, é”™è¯¯=2
    æˆåŠŸç‡: 81.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_SQUARE_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_SQUARE_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 15:22:09.729072213 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(64, 128), (128, 224), (256, 416), (512, 832), (1024, 1664)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 128)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/11: (128, 224)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/11: (256, 416)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/11: (512, 832)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 5/11: (1024, 1664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 6/11: (2048, 3296)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 7/11: (4096, 6560)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 8/11: (8192, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 9/11: (16384, 26240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 10/11: (32768, 52448)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 3.20 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.21 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    NK 11/11: (65536, 104864)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 104864)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=9, å¤±è´¥=0, é”™è¯¯=2
    æˆåŠŸç‡: 81.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_SQUARE_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_SQUARE_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 15:22:20.678354659 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(64, 64), (128, 128), (256, 256), (512, 512), (1024, 1024)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 64)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 2/11: (128, 128)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 10
    NK 3/11: (256, 256)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 11
    NK 4/11: (512, 512)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 5/11: (1024, 1024)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 6/11: (2048, 2048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 7/11: (4096, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 8/11: (8192, 8192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 12
    NK 9/11: (16384, 16384)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 10/11: (32768, 32768)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 11/11: (65536, 65536)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 65536)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_SQUARE_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_SQUARE_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 15:27:39.878184119 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(64, 96), (128, 192), (256, 352), (512, 704), (1024, 1376)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 96)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 10
    NK 2/11: (128, 192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 10
    NK 3/11: (256, 352)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 12
    NK 4/11: (512, 704)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 5/11: (1024, 1376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 6/11: (2048, 2752)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 7/11: (4096, 5472)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 8/11: (8192, 10944)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 12
    NK 9/11: (16384, 21856)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 10/11: (32768, 43712)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 12
    NK 11/11: (65536, 87392)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 87392)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_SQUARE_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_SQUARE_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 15:34:00.915769999 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(64, 96), (128, 192), (256, 384), (512, 768), (1024, 1536)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 96)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 11
    NK 2/11: (128, 192)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/11: (256, 384)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 4/11: (512, 768)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 5/11: (1024, 1536)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 6/11: (2048, 3072)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 7/11: (4096, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 8/11: (8192, 12288)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 9/11: (16384, 24576)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 10/11: (32768, 49152)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 11/11: (65536, 98304)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 98304)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_SQUARE_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_SQUARE_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 15:41:35.515003708 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(64, 128), (128, 224), (256, 416), (512, 832), (1024, 1664)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 128)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 2/11: (128, 224)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 10
    NK 3/11: (256, 416)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 10
    NK 4/11: (512, 832)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 5/11: (1024, 1664)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 6/11: (2048, 3296)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 7/11: (4096, 6560)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 8/11: (8192, 13120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 9/11: (16384, 26240)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 10/11: (32768, 52448)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 11/11: (65536, 104864)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 104864)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=10, å¤±è´¥=0, é”™è¯¯=1
    æˆåŠŸç‡: 90.9%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_SQUARE_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_SQUARE_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 15:50:14.116392500 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
M=N=K: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6
[ERROR] cuSPARSELt search (sparsity=2_6) failed

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_4[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(64, 64), (128, 128), (256, 256), (512, 512), (1024, 1024)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_4
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 64)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/11: (128, 128)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/11: (256, 256)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 5
    NK 4/11: (512, 512)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 5/11: (1024, 1024)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 6/11: (2048, 2048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 7/11: (4096, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 8/11: (8192, 8192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 9/11: (16384, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 10/11: (32768, 32768)
      âš  æƒé‡å‰ªæå¤±è´¥: CUDA OOM during prune: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.12 GiB is allocated by PyTorch, and 128.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    NK 11/11: (65536, 65536)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 65536)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=9, å¤±è´¥=0, é”™è¯¯=2
    æˆåŠŸç‡: 81.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_SQUARE_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_SQUARE_2_4.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 15:50:25.159838091 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(64, 128), (128, 192), (256, 384), (512, 704), (1024, 1408)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_6
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 128)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 2/11: (128, 192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 5
    NK 3/11: (256, 384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 5
    NK 4/11: (512, 704)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 5/11: (1024, 1408)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 6/11: (2048, 2752)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 7/11: (4096, 5504)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 8/11: (8192, 10944)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 9/11: (16384, 21888)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 10/11: (32768, 43712)
      âš  æƒé‡å‰ªæå¤±è´¥: CUDA OOM during prune: CUDA out of memory. Tried to allocate 5.34 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.81 GiB is free. Including non-PyTorch memory, this process has 12.64 GiB memory in use. Of the allocated memory 12.17 GiB is allocated by PyTorch, and 171.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    NK 11/11: (65536, 87424)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 87424)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=9, å¤±è´¥=0, é”™è¯¯=2
    æˆåŠŸç‡: 81.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_SQUARE_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_SQUARE_2_6.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 15:50:39.772762083 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(64, 128), (128, 192), (256, 384), (512, 768), (1024, 1536)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_8
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 128)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 2/11: (128, 192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 5
    NK 3/11: (256, 384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/11: (512, 768)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 5/11: (1024, 1536)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 6/11: (2048, 3072)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 7/11: (4096, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 8/11: (8192, 12288)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 9/11: (16384, 24576)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 10/11: (32768, 49152)
      âš  æƒé‡å‰ªæå¤±è´¥: CUDA OOM during prune: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.27 GiB is free. Including non-PyTorch memory, this process has 14.17 GiB memory in use. Of the allocated memory 13.69 GiB is allocated by PyTorch, and 192.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    NK 11/11: (65536, 98304)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 98304)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=9, å¤±è´¥=0, é”™è¯¯=2
    æˆåŠŸç‡: 81.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_SQUARE_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_SQUARE_2_8.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 15:50:54.645668836 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(64, 128), (128, 256), (256, 448), (512, 832), (1024, 1664)]...
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: SQUARE
Model: SQUARE
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_10
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      M=N=K åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/11: (64, 128)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 2/11: (128, 256)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 5
    NK 3/11: (256, 448)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 4/11: (512, 832)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 5/11: (1024, 1664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 6/11: (2048, 3328)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 7/11: (4096, 6592)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 8/11: (8192, 13120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 9/11: (16384, 26240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 10/11: (32768, 52480)
      âš  æƒé‡å‰ªæå¤±è´¥: CUDA OOM during prune: CUDA out of memory. Tried to allocate 1.60 GiB. GPU 0 has a total capacity of 15.47 GiB of which 342.94 MiB is free. Including non-PyTorch memory, this process has 15.11 GiB memory in use. Of the allocated memory 14.61 GiB is allocated by PyTorch, and 205.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    NK 11/11: (65536, 104896)
      âš  CUDA OOM ç”Ÿæˆæ•°æ®ï¼Œè·³è¿‡ NK=(65536, 104896)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=11, æˆåŠŸ=9, å¤±è´¥=0, é”™è¯¯=2
    æˆåŠŸç‡: 81.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_SQUARE_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_SQUARE_2_10.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 15:51:13.064176817 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_6

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_8

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model SQUARE --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 15:51:13.669664536 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Square é«˜ç¨€ç–æµ‹è¯•å®Œæˆ (2561.9s)

----------------------------------------------------------------------
TASK 4: cuSPARSELt Square é«˜ç¨€ç– (2_4~2_10) - SUCCESS
Duration: 2561.9 seconds (42.7 minutes)
----------------------------------------------------------------------


======================================================================
TASK 5: cuSPARSELt Model ä½ç¨€ç– (2_12~2_inf)
Started: 2026-01-27 15:51:13
======================================================================


------------------------------------------------------------
  cuSPARSELt Model ä½ç¨€ç–: Llama3.2-1B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Llama3.2-1B-INT8 --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3072, 3424), (2048, 3424), (16384, 3424), (2048, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 15:51:45.980786068 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3072, 3520), (2048, 3520), (16384, 3520), (2048, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 15:52:14.653535123 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3072, 3584), (2048, 3584), (16384, 3584), (2048, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 15:52:44.639974725 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3072, 4096), (2048, 4096), (16384, 4096), (2048, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 15:53:19.237411776 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3072, 3424), (2048, 3424), (16384, 3424), (2048, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 15:53:48.117839898 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3072, 3520), (2048, 3520), (16384, 3520), (2048, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 15:54:18.064384311 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3072, 3584), (2048, 3584), (16384, 3584), (2048, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 15:54:48.564208889 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3072, 4096), (2048, 4096), (16384, 4096), (2048, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 15:55:24.626366671 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3072, 3424), (2048, 3424), (16384, 3424), (2048, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 15:55:39.699491505 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3072, 3520), (2048, 3520), (16384, 3520), (2048, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 15:55:54.944891456 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3072, 3584), (2048, 3584), (16384, 3584), (2048, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 15:56:08.958344370 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3072, 4096), (2048, 4096), (16384, 4096), (2048, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 15:56:22.734689299 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3072, 3424), (2048, 3424), (16384, 3424), (2048, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3424)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 2/4: (2048, 3424)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 3/4: (16384, 3424)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (2048, 13664)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 15:57:51.065958097 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3072, 3520), (2048, 3520), (16384, 3520), (2048, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3520)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 2/4: (2048, 3520)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 3/4: (16384, 3520)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (2048, 14048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 15:59:11.066874913 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3072, 3584), (2048, 3584), (16384, 3584), (2048, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 2/4: (2048, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 3/4: (16384, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 4/4: (2048, 14336)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:00:33.884792998 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3072, 4096), (2048, 4096), (16384, 4096), (2048, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (2048, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (2048, 16384)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:02:08.600305267 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3072, 3456), (2048, 3456), (16384, 3456), (2048, 13696)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3456)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 2/4: (2048, 3456)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 3456)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 4/4: (2048, 13696)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.69 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.53 GiB is free. Including non-PyTorch memory, this process has 9.92 GiB memory in use. Of the allocated memory 9.38 GiB is allocated by PyTorch, and 245.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:02:23.641277904 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3072, 3520), (2048, 3520), (16384, 3520), (2048, 14080)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (2048, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 4/4: (2048, 14080)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.88 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.26 GiB is free. Including non-PyTorch memory, this process has 10.18 GiB memory in use. Of the allocated memory 9.64 GiB is allocated by PyTorch, and 249.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:02:41.130935328 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3072, 3584), (2048, 3584), (16384, 3584), (2048, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (2048, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 4/4: (2048, 14336)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 7.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.08 GiB is free. Including non-PyTorch memory, this process has 10.36 GiB memory in use. Of the allocated memory 9.82 GiB is allocated by PyTorch, and 254.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:02:55.331409204 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3072, 4096), (2048, 4096), (16384, 4096), (2048, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (2048, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 4/4: (2048, 16384)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.77 GiB is free. Including non-PyTorch memory, this process has 11.67 GiB memory in use. Of the allocated memory 11.22 GiB is allocated by PyTorch, and 160.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:03:10.412642629 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 16:03:11.943507975 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-1B-INT8 ä½ç¨€ç–å®Œæˆ (717.3s)

------------------------------------------------------------
  cuSPARSELt Model ä½ç¨€ç–: Llama3.2-1B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Llama3.2-1B-FP8 --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3072, 3424), (2048, 3424), (16384, 3424), (2048, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:03:42.362868631 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3072, 3520), (2048, 3520), (16384, 3520), (2048, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:04:12.017172038 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3072, 3584), (2048, 3584), (16384, 3584), (2048, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:04:42.208979009 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3072, 4096), (2048, 4096), (16384, 4096), (2048, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-1B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:05:17.984688480 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3072, 3424), (2048, 3424), (16384, 3424), (2048, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:05:46.769749937 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3072, 3520), (2048, 3520), (16384, 3520), (2048, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:06:16.450027636 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3072, 3584), (2048, 3584), (16384, 3584), (2048, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:06:46.663617566 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3072, 4096), (2048, 4096), (16384, 4096), (2048, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (2048, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-1B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:07:22.936598944 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3072, 3424), (2048, 3424), (16384, 3424), (2048, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3424)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:07:36.709801589 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3072, 3520), (2048, 3520), (16384, 3520), (2048, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:07:51.993997119 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3072, 3584), (2048, 3584), (16384, 3584), (2048, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:08:05.836090213 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3072, 4096), (2048, 4096), (16384, 4096), (2048, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (2048, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (2048, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-1B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:08:19.661061961 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3072, 3424), (2048, 3424), (16384, 3424), (2048, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3424)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (2048, 3424)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 3/4: (16384, 3424)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (2048, 13664)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:09:40.311788262 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3072, 3520), (2048, 3520), (16384, 3520), (2048, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3520)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 2/4: (2048, 3520)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/4: (16384, 3520)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (2048, 14048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:10:58.459798014 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3072, 3584), (2048, 3584), (16384, 3584), (2048, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 2/4: (2048, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 3/4: (16384, 3584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (2048, 14336)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:12:17.359718782 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3072, 4096), (2048, 4096), (16384, 4096), (2048, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (2048, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (2048, 16384)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-1B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:13:43.600014807 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3072, 3456), (2048, 3456), (16384, 3456), (2048, 13696)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3456)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (2048, 3456)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (16384, 3456)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 4/4: (2048, 13696)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.69 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.53 GiB is free. Including non-PyTorch memory, this process has 9.92 GiB memory in use. Of the allocated memory 9.38 GiB is allocated by PyTorch, and 245.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:13:58.181601813 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3072, 3520), (2048, 3520), (16384, 3520), (2048, 14080)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 2/4: (2048, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 3520)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 4/4: (2048, 14080)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.88 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.26 GiB is free. Including non-PyTorch memory, this process has 10.18 GiB memory in use. Of the allocated memory 9.64 GiB is allocated by PyTorch, and 249.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:14:15.586030546 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3072, 3584), (2048, 3584), (16384, 3584), (2048, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (2048, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 3/4: (16384, 3584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (2048, 14336)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 7.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.08 GiB is free. Including non-PyTorch memory, this process has 10.36 GiB memory in use. Of the allocated memory 9.82 GiB is allocated by PyTorch, and 254.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:14:29.609097725 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3072, 4096), (2048, 4096), (16384, 4096), (2048, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-1B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (3072, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (2048, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 4096)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (2048, 16384)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.77 GiB is free. Including non-PyTorch memory, this process has 11.67 GiB memory in use. Of the allocated memory 11.22 GiB is allocated by PyTorch, and 160.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-1B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:14:45.122645037 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-1B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 16:14:45.655247506 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-1B-FP8 ä½ç¨€ç–å®Œæˆ (694.7s)

------------------------------------------------------------
  cuSPARSELt Model ä½ç¨€ç–: Llama3.2-3B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Llama3.2-3B-INT8 --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(5120, 5120), (3072, 5120), (16384, 5120), (3072, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:15:39.632794484 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(5120, 5280), (3072, 5280), (16384, 5280), (3072, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:16:31.759429482 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(5120, 5376), (3072, 5376), (16384, 5376), (3072, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:17:25.732511171 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(5120, 6144), (3072, 6144), (16384, 6144), (3072, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 4/4: (3072, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:18:28.101807725 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(5120, 5120), (3072, 5120), (16384, 5120), (3072, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:19:19.454292601 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(5120, 5280), (3072, 5280), (16384, 5280), (3072, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:20:13.250608955 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(5120, 5376), (3072, 5376), (16384, 5376), (3072, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:21:07.132261298 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(5120, 6144), (3072, 6144), (16384, 6144), (3072, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:22:09.681681857 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(5120, 5120), (3072, 5120), (16384, 5120), (3072, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:22:29.852205605 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(5120, 5280), (3072, 5280), (16384, 5280), (3072, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:22:51.913290775 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(5120, 5376), (3072, 5376), (16384, 5376), (3072, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:23:10.302988136 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(5120, 6144), (3072, 6144), (16384, 6144), (3072, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:23:31.602372853 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(5120, 5120), (3072, 5120), (16384, 5120), (3072, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (3072, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 3/4: (16384, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (3072, 13664)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:25:42.847244725 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(5120, 5280), (3072, 5280), (16384, 5280), (3072, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5280)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (3072, 5280)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 3/4: (16384, 5280)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (3072, 14048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:28:05.865664915 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(5120, 5376), (3072, 5376), (16384, 5376), (3072, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 2/4: (3072, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 20
    NK 3/4: (16384, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (3072, 14336)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:30:12.547613215 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(5120, 6144), (3072, 6144), (16384, 6144), (3072, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 21
    NK 2/4: (3072, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 3/4: (16384, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 20
    NK 4/4: (3072, 16384)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:32:45.021985894 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(5120, 5120), (3072, 5120), (16384, 5120), (3072, 13696)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (3072, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 3/4: (16384, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 11
    NK 4/4: (3072, 13696)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.69 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.44 GiB is free. Including non-PyTorch memory, this process has 10.01 GiB memory in use. Of the allocated memory 9.47 GiB is allocated by PyTorch, and 245.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:33:05.141315765 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(5120, 5312), (3072, 5312), (16384, 5312), (3072, 14080)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5312)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3072, 5312)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 5312)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 4/4: (3072, 14080)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.88 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.21 GiB is free. Including non-PyTorch memory, this process has 10.24 GiB memory in use. Of the allocated memory 9.74 GiB is allocated by PyTorch, and 210.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:33:31.570447771 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(5120, 5376), (3072, 5376), (16384, 5376), (3072, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3072, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 3/4: (16384, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 4/4: (3072, 14336)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 7.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.03 GiB is free. Including non-PyTorch memory, this process has 10.42 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 212.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:33:56.262660081 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(5120, 6144), (3072, 6144), (16384, 6144), (3072, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3072, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 3/4: (16384, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (3072, 16384)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.59 GiB is free. Including non-PyTorch memory, this process has 11.86 GiB memory in use. Of the allocated memory 11.33 GiB is allocated by PyTorch, and 240.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=32, å¤±è´¥=0, é”™è¯¯=12
    æˆåŠŸç‡: 72.7%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:34:13.206185184 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 16:34:13.727439441 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-3B-INT8 ä½ç¨€ç–å®Œæˆ (1168.1s)

------------------------------------------------------------
  cuSPARSELt Model ä½ç¨€ç–: Llama3.2-3B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Llama3.2-3B-FP8 --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(5120, 5120), (3072, 5120), (16384, 5120), (3072, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:35:07.764291606 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(5120, 5280), (3072, 5280), (16384, 5280), (3072, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:36:00.005119859 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(5120, 5376), (3072, 5376), (16384, 5376), (3072, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:36:53.774049395 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(5120, 6144), (3072, 6144), (16384, 6144), (3072, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Llama3.2-3B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:37:56.113030872 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(5120, 5120), (3072, 5120), (16384, 5120), (3072, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:38:47.282832778 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(5120, 5280), (3072, 5280), (16384, 5280), (3072, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:39:41.031548137 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(5120, 5376), (3072, 5376), (16384, 5376), (3072, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:40:35.070333889 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(5120, 6144), (3072, 6144), (16384, 6144), (3072, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3072, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (16384, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Llama3.2-3B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 16:41:37.245488048 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(5120, 5120), (3072, 5120), (16384, 5120), (3072, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 13664)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:41:56.460219671 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(5120, 5280), (3072, 5280), (16384, 5280), (3072, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 5280)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14048)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:42:18.112469735 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(5120, 5376), (3072, 5376), (16384, 5376), (3072, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 14336)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:42:37.486839528 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(5120, 6144), (3072, 6144), (16384, 6144), (3072, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3072, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (16384, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3072, 16384)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Llama3.2-3B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 16:42:58.711887239 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(5120, 5120), (3072, 5120), (16384, 5120), (3072, 13664)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 2/4: (3072, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 3/4: (16384, 5120)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (3072, 13664)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:45:02.181671627 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(5120, 5280), (3072, 5280), (16384, 5280), (3072, 14048)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5280)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (3072, 5280)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 3/4: (16384, 5280)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 4/4: (3072, 14048)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:47:29.030019384 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(5120, 5376), (3072, 5376), (16384, 5376), (3072, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (3072, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 21
    NK 3/4: (16384, 5376)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (3072, 14336)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:49:39.134914488 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(5120, 6144), (3072, 6144), (16384, 6144), (3072, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (3072, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13
    NK 3/4: (16384, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (3072, 16384)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Llama3.2-3B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 16:52:12.354639389 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(5120, 5120), (3072, 5120), (16384, 5120), (3072, 13696)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 2/4: (3072, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (16384, 5120)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (3072, 13696)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.69 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.44 GiB is free. Including non-PyTorch memory, this process has 10.01 GiB memory in use. Of the allocated memory 9.47 GiB is allocated by PyTorch, and 245.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:52:33.550860652 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(5120, 5312), (3072, 5312), (16384, 5312), (3072, 14080)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5312)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3072, 5312)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 3/4: (16384, 5312)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 12
    NK 4/4: (3072, 14080)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 6.88 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.21 GiB is free. Including non-PyTorch memory, this process has 10.24 GiB memory in use. Of the allocated memory 9.74 GiB is allocated by PyTorch, and 210.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:52:59.732168304 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(5120, 5376), (3072, 5376), (16384, 5376), (3072, 14336)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (3072, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 11
    NK 3/4: (16384, 5376)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (3072, 14336)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 7.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 5.03 GiB is free. Including non-PyTorch memory, this process has 10.42 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 212.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:53:22.225757112 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(5120, 6144), (3072, 6144), (16384, 6144), (3072, 16384)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Llama3.2-3B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (5120, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 2/4: (3072, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 11
    NK 3/4: (16384, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 4/4: (3072, 16384)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.59 GiB is free. Including non-PyTorch memory, this process has 11.86 GiB memory in use. Of the allocated memory 11.33 GiB is allocated by PyTorch, and 240.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=32, å¤±è´¥=0, é”™è¯¯=12
    æˆåŠŸç‡: 72.7%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Llama3.2-3B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 16:53:39.843963939 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Llama3.2-3B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 16:53:39.382577153 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Llama3.2-3B-FP8 ä½ç¨€ç–å®Œæˆ (1165.7s)

------------------------------------------------------------
  cuSPARSELt Model ä½ç¨€ç–: Qwen2.5-7B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Qwen2.5-7B-INT8 --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(4608, 5984), (3584, 5984), (37888, 5984), (3584, 31584)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 31584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:55:43.742062882 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(4608, 6144), (3584, 6144), (37888, 6144), (3584, 32480)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 32480)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:57:48.174606242 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(4608, 6272), (3584, 6272), (37888, 6272), (3584, 33152)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 33152)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 16:59:57.564081697 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(4608, 7168), (3584, 7168), (37888, 7168), (3584, 37888)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 37888)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 17:02:26.898623456 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(4608, 5984), (3584, 5984), (37888, 5984), (3584, 31584)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 31584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 17:04:28.257101813 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(4608, 6144), (3584, 6144), (37888, 6144), (3584, 32480)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 32480)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 17:06:33.585205812 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(4608, 6272), (3584, 6272), (37888, 6272), (3584, 33152)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 33152)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 17:08:41.741738345 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(4608, 7168), (3584, 7168), (37888, 7168), (3584, 37888)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 37888)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 17:11:10.343781921 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(4608, 5984), (3584, 5984), (37888, 5984), (3584, 31584)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 31584)
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(4608, 6144), (3584, 6144), (37888, 6144), (3584, 32480)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 32480)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 17:12:09.762255433 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(4608, 6272), (3584, 6272), (37888, 6272), (3584, 33152)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 33152)
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(4608, 7168), (3584, 7168), (37888, 7168), (3584, 37888)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 37888)
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(4608, 5984), (3584, 5984), (37888, 5984), (3584, 31584)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5984)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 22
    NK 2/4: (3584, 5984)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 3/4: (37888, 5984)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (3584, 31584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 17:20:23.719086459 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(4608, 6144), (3584, 6144), (37888, 6144), (3584, 32480)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 2/4: (3584, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 3/4: (37888, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (3584, 32480)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 17:27:00.632309430 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(4608, 6272), (3584, 6272), (37888, 6272), (3584, 33152)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6272)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 2/4: (3584, 6272)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 3/4: (37888, 6272)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (3584, 33152)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 17:33:32.151010090 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(4608, 7168), (3584, 7168), (37888, 7168), (3584, 37888)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 7168)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (3584, 7168)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 3/4: (37888, 7168)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 4/4: (3584, 37888)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 17:40:30.651632791 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12
[ERROR] cuSPARSELt search (sparsity=2_12) failed

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16
[ERROR] cuSPARSELt search (sparsity=2_16) failed

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf
[ERROR] cuSPARSELt search (sparsity=2_inf) failed

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(4608, 6016), (3584, 6016), (37888, 6016), (3584, 31616)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6016)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 2/4: (3584, 6016)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 3/4: (37888, 6016)
 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 0
    NK 4/4: (3584, 31616)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 1.93 GiB. GPU 0 has a total capacity of 15.47 GiB of which 810.94 MiB is free. Including non-PyTorch memory, this process has 14.65 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 357.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=26, å¤±è´¥=7, é”™è¯¯=11
    æˆåŠŸç‡: 59.1%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 17:41:21.544238442 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(4608, 6144), (3584, 6144), (37888, 6144), (3584, 32512)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (3584, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 3/4: (37888, 6144)
 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0
    NK 4/4: (3584, 32512)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 15.47 GiB of which 396.94 MiB is free. Including non-PyTorch memory, this process has 15.06 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 366.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=25, å¤±è´¥=1, é”™è¯¯=18
    æˆåŠŸç‡: 56.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 17:42:10.781793255 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(4608, 6272), (3584, 6272), (37888, 6272), (3584, 33152)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3584, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 10
    NK 3/4: (37888, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 4/4: (3584, 33152)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 2.02 GiB. GPU 0 has a total capacity of 15.47 GiB of which 98.94 MiB is free. Including non-PyTorch memory, this process has 15.35 GiB memory in use. Of the allocated memory 14.69 GiB is allocated by PyTorch, and 374.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 17:43:13.117546426 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(4608, 7168), (3584, 7168), (37888, 7168), (3584, 37888)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (3584, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 11
    NK 3/4: (37888, 7168)
 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 0
    NK 4/4: (3584, 37888)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 654.94 MiB is free. Including non-PyTorch memory, this process has 14.81 GiB memory in use. Of the allocated memory 14.47 GiB is allocated by PyTorch, and 36.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=26, å¤±è´¥=7, é”™è¯¯=11
    æˆåŠŸç‡: 59.1%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 17:44:12.211844656 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())

[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 17:44:12.735520580 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Qwen2.5-7B-INT8 ä½ç¨€ç–å®Œæˆ (3033.3s)

------------------------------------------------------------
  cuSPARSELt Model ä½ç¨€ç–: Qwen2.5-7B-FP8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Qwen2.5-7B-FP8 --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(4608, 5984), (3584, 5984), (37888, 5984), (3584, 31584)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 31584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 17:46:17.984942591 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(4608, 6144), (3584, 6144), (37888, 6144), (3584, 32480)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 32480)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 17:48:21.625173701 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(4608, 6272), (3584, 6272), (37888, 6272), (3584, 33152)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 33152)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 17:50:30.178661143 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(4608, 7168), (3584, 7168), (37888, 7168), (3584, 37888)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 37888)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-7B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 17:52:59.055436071 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(4608, 5984), (3584, 5984), (37888, 5984), (3584, 31584)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 31584)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 17:55:01.262215995 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(4608, 6144), (3584, 6144), (37888, 6144), (3584, 32480)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 32480)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 17:57:06.534793210 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(4608, 6272), (3584, 6272), (37888, 6272), (3584, 33152)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 33152)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 17:59:15.878908512 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(4608, 7168), (3584, 7168), (37888, 7168), (3584, 37888)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (3584, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (37888, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 37888)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-7B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 18:01:44.967459322 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(4608, 5984), (3584, 5984), (37888, 5984), (3584, 31584)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 5984)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 31584)
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(4608, 6144), (3584, 6144), (37888, 6144), (3584, 32480)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 32480)
      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-7B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 18:02:43.116471307 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(4608, 6272), (3584, 6272), (37888, 6272), (3584, 33152)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 33152)
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(4608, 7168), (3584, 7168), (37888, 7168), (3584, 37888)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (3584, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (37888, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (3584, 37888)
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(4608, 5984), (3584, 5984), (37888, 5984), (3584, 31584)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 5984)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 2/4: (3584, 5984)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 3/4: (37888, 5984)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (3584, 31584)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 18:10:42.152115640 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(4608, 6144), (3584, 6144), (37888, 6144), (3584, 32480)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (3584, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 22
    NK 3/4: (37888, 6144)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (3584, 32480)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 18:16:42.172511745 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(4608, 6272), (3584, 6272), (37888, 6272), (3584, 33152)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6272)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 14
    NK 2/4: (3584, 6272)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 19
    NK 3/4: (37888, 6272)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (3584, 33152)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 18:23:04.047624633 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(4608, 7168), (3584, 7168), (37888, 7168), (3584, 37888)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 7168)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 2/4: (3584, 7168)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 20
    NK 3/4: (37888, 7168)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 4/4: (3584, 37888)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 13

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-7B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 18:29:58.145385692 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] dtype=all, å°†æµ‹è¯•: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12
[ERROR] cuSPARSELt search (sparsity=2_12) failed

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16
[ERROR] cuSPARSELt search (sparsity=2_16) failed

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf
[ERROR] cuSPARSELt search (sparsity=2_inf) failed

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(4608, 6016), (3584, 6016), (37888, 6016), (3584, 31616)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6016)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (3584, 6016)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 6
    NK 3/4: (37888, 6016)
 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 0
    NK 4/4: (3584, 31616)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 1.93 GiB. GPU 0 has a total capacity of 15.47 GiB of which 810.94 MiB is free. Including non-PyTorch memory, this process has 14.65 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 357.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=26, å¤±è´¥=7, é”™è¯¯=11
    æˆåŠŸç‡: 59.1%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 18:30:54.213259254 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(4608, 6144), (3584, 6144), (37888, 6144), (3584, 32512)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3584, 6144)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 3/4: (37888, 6144)
 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

      â†’ ç®—æ³•æ•°: 0, æœ‰æ•ˆ: 0
    NK 4/4: (3584, 32512)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 15.47 GiB of which 396.94 MiB is free. Including non-PyTorch memory, this process has 15.06 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 366.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=25, å¤±è´¥=1, é”™è¯¯=18
    æˆåŠŸç‡: 56.8%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 18:31:45.733893862 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(4608, 6272), (3584, 6272), (37888, 6272), (3584, 33152)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 2/4: (3584, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 3/4: (37888, 6272)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 7
    NK 4/4: (3584, 33152)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 2.02 GiB. GPU 0 has a total capacity of 15.47 GiB of which 98.94 MiB is free. Including non-PyTorch memory, this process has 15.35 GiB memory in use. Of the allocated memory 14.69 GiB is allocated by PyTorch, and 374.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=33, å¤±è´¥=0, é”™è¯¯=11
    æˆåŠŸç‡: 75.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 18:32:42.688227710 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(4608, 7168), (3584, 7168), (37888, 7168), (3584, 37888)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-7B-FP8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (4608, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 8
    NK 2/4: (3584, 7168)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 9
    NK 3/4: (37888, 7168)
 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

 ** On entry to cusparseLtSpMMACompress() parameter number 4 (d_compressed) had an illegal value: NULL pointer

      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 0
    NK 4/4: (3584, 37888)
      âš  æ¿€æ´»å‡†å¤‡å¤±è´¥: CUDA OOM during activation prep: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 654.94 MiB is free. Including non-PyTorch memory, this process has 14.81 GiB memory in use. Of the allocated memory 14.47 GiB is allocated by PyTorch, and 36.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=26, å¤±è´¥=7, é”™è¯¯=11
    æˆåŠŸç‡: 59.1%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_Qwen2.5-7B-FP8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[W127 18:33:41.956283344 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())

[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_12

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_14

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_16

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model Qwen2.5-7B-FP8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[W127 18:33:41.507038460 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[SUCCESS] Qwen2.5-7B-FP8 ä½ç¨€ç–å®Œæˆ (2968.8s)

------------------------------------------------------------
  cuSPARSELt Model ä½ç¨€ç–: Qwen2.5-14B-INT8
------------------------------------------------------------
[INFO] æ‰§è¡Œ: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --backend cusparselt --model Qwen2.5-14B-INT8 --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(7168, 8544), (5120, 8544), (27648, 8544), (5120, 23040)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8544)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 8544)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 8544)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 23040)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 18:36:03.766080621 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(7168, 8800), (5120, 8800), (27648, 8800), (5120, 23712)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 8800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 8800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 23712)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 18:38:30.619985699 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(7168, 8960), (5120, 8960), (27648, 8960), (5120, 24192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8960)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 8960)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 8960)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 24192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 18:41:01.442008816 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(7168, 10240), (5120, 10240), (27648, 10240), (5120, 27648)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 10240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 10240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 10240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 27648)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_Qwen2.5-14B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[W127 18:43:54.883986859 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(7168, 8544), (5120, 8544), (27648, 8544), (5120, 23040)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8544)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 8544)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 8544)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 23040)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 18:46:14.823188678 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(7168, 8800), (5120, 8800), (27648, 8800), (5120, 23712)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 8800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 8800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 23712)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 18:48:41.427892879 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(7168, 8960), (5120, 8960), (27648, 8960), (5120, 24192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8960)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 8960)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 8960)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 24192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 18:51:12.360999820 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(7168, 10240), (5120, 10240), (27648, 10240), (5120, 27648)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 10240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 2/4: (5120, 10240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3
    NK 3/4: (27648, 10240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 27648)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 3

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_Qwen2.5-14B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[W127 18:54:05.571249842 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(7168, 8544), (5120, 8544), (27648, 8544), (5120, 23040)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8544)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 8544)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 8544)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 23040)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 18:54:52.792927093 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(7168, 8800), (5120, 8800), (27648, 8800), (5120, 23712)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 8800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 8800)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 23712)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_14.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 18:55:42.736308480 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(7168, 8960), (5120, 8960), (27648, 8960), (5120, 24192)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8960)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 8960)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 8960)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 24192)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_16.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 18:56:27.539569308 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(7168, 10240), (5120, 10240), (27648, 10240), (5120, 27648)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 10240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 2/4: (5120, 10240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 3/4: (27648, 10240)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2
    NK 4/4: (5120, 27648)
      â†’ ç®—æ³•æ•°: 2, æœ‰æ•ˆ: 2

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_Qwen2.5-14B-INT8_2_inf.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[W127 18:57:17.749260796 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(7168, 8544), (5120, 8544), (27648, 8544), (5120, 23040)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_12
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8544)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 16
    NK 2/4: (5120, 8544)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/4: (27648, 8544)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 17
    NK 4/4: (5120, 23040)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15

    æœç´¢ç»Ÿè®¡: æ€»è®¡=44, æˆåŠŸ=44, å¤±è´¥=0, é”™è¯¯=0
    æˆåŠŸç‡: 100.0%

[4/4] ä¿å­˜ç»“æœ...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_Qwen2.5-14B-INT8_2_12.json

âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[W127 19:03:46.274137880 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(7168, 8800), (5120, 8800), (27648, 8800), (5120, 23712)]
============================================================
cuSPARSELt Sparse GEMM ç®—æ³•æœç´¢ (2:4 ç¨€ç–)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: Qwen2.5-14B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_14
Segment-K æµ‹è¯•: å¼€å¯
API æœç´¢å¯¹æ¯”: å¼€å¯
warmup=25, repeat=50

[1/4] ç¼–è¯‘ CUDA æ‰©å±•...
âœ“ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] åŠ è½½ CUDA æ‰©å±•...
âœ“ cuSPARSELt å¯ç”¨
âœ“ Segment-K æ”¯æŒ: æ˜¯

[3/4] å¼€å§‹ç®—æ³•æœç´¢...
      NK ç»„åˆ: 4 ä¸ª, M åˆ—è¡¨: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]

    NK 1/4: (7168, 8800)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 18
    NK 2/4: (5120, 8800)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 3/4: (27648, 8800)
      â†’ ç®—æ³•æ•°: 4, æœ‰æ•ˆ: 15
    NK 4/4: (5120, 23712)

======================================================================
æ”¶åˆ°ä¸­æ–­ä¿¡å· (signal 2)
======================================================================
[INFO] çŠ¶æ€å·²ä¿å­˜: /root/vllmbench/slidesparse/benchmark_kernel/kernel_bench_logs/kernel_bench_20260127_102437_status.json
