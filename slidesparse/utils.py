#!/usr/bin/env python3
# SPDX-License-Identifier: Apache-2.0
"""
SlideSparse æ–‡ä»¶åç»Ÿä¸€å·¥å…·åº“

æä¾›ç»Ÿä¸€çš„ç¡¬ä»¶ä¿¡æ¯è·å–ã€æ–‡ä»¶å‘½åå’Œæ¨¡å—åŠ è½½åŠŸèƒ½ã€‚

å‘½åè§„èŒƒ
========
æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶åéµå¾ªç»Ÿä¸€æ ¼å¼ï¼š
    {prefix}_{GPU}_{CC}[_{dtype}]_{PyVer}_{CUDAVer}_{Arch}.{ext}

dtype éƒ¨åˆ†æ˜¯å¯é€‰çš„ï¼Œæ”¯æŒä¸‰ç§æƒ…å†µï¼š
1. å•ä¸ª dtype:   cublaslt_gemm_H100_cc90_FP8E4M3_py312_cu124_x86_64.so
2. å¤šä¸ª dtype:   cublaslt_gemm_H100_cc90_FP8_INT8_py312_cu124_x86_64.so
3. æ—  dtype:     cublaslt_gemm_H100_cc90_py312_cu124_x86_64.so

ç¤ºä¾‹ï¼š
    cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so       # æ”¯æŒå¤šç§ç±»å‹çš„ GEMM
    dequant_bias_tuned_H100_cc90_BF16_py312_cu124_x86_64.py  # ç‰¹å®šç±»å‹
    alg_id_LUT_A100_cc80_INT8_py311_cu121_x86_64.json   # ç‰¹å®šç±»å‹

ç»„ä»¶è¯´æ˜ï¼š
    - prefix:    ç”¨é€”å‰ç¼€ï¼ˆcublaslt_gemm, cusparselt_gemm, dequant_bias_tuned ç­‰ï¼‰
    - GPU:       GPU ç®€ç§°ï¼ˆH100, A100, B200, GB10 ç­‰ï¼‰
    - CC:        Compute Capabilityï¼ˆcc90, cc100, cc121 ç­‰ï¼‰
    - dtype:     æ•°æ®ç±»å‹ï¼ˆå¯é€‰ï¼Œå•ä¸ªæˆ–å¤šä¸ªï¼šFP8E4M3, INT8, BF16, FP32 ç­‰ï¼‰
    - PyVer:     Python ç‰ˆæœ¬ï¼ˆpy312, py311 ç­‰ï¼‰
    - CUDAVer:   CUDA ç‰ˆæœ¬ï¼ˆcu129, cu124 ç­‰ï¼‰
    - Arch:      ç³»ç»Ÿæ¶æ„ï¼ˆx86_64, aarch64 ç­‰ï¼‰

ä¸»è¦åŠŸèƒ½
========
1. HardwareInfo: ç¡¬ä»¶ä¿¡æ¯å•ä¾‹ç±»ï¼Œç¼“å­˜æ‰€æœ‰ç¡¬ä»¶ä¿¡æ¯
2. FileNameBuilder: æ–‡ä»¶åæ„å»ºå™¨
3. FileFinder: æ–‡ä»¶æŸ¥æ‰¾å™¨ï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…
4. ModuleLoader: æ¨¡å—åŠ è½½å™¨ï¼Œæ”¯æŒ .py å’Œ .so

ä½¿ç”¨ç¤ºä¾‹
========
>>> from slidesparse.utils import hw_info, build_filename, find_file, load_module
>>>
>>> # è·å–ç¡¬ä»¶ä¿¡æ¯
>>> print(hw_info.gpu_name)  # "H100"
>>> print(hw_info.cc_tag)    # "cc90"
>>>
>>> # æ„å»ºæ–‡ä»¶åï¼ˆæ—  dtypeï¼Œç”¨äºæ”¯æŒå¤šç±»å‹çš„æ‰©å±•ï¼‰
>>> name = build_filename("cublaslt_gemm", ext=".so")
>>> # -> "cublaslt_gemm_H100_cc90_py312_cu124_x86_64.so"
>>>
>>> # æ„å»ºæ–‡ä»¶åï¼ˆå¸¦å•ä¸ª dtypeï¼‰
>>> name = build_filename("dequant_bias_tuned", dtype="BF16", ext=".py")
>>> # -> "dequant_bias_tuned_H100_cc90_BF16_py312_cu124_x86_64.py"
>>>
>>> # æ„å»ºæ–‡ä»¶åï¼ˆå¸¦å¤šä¸ª dtypeï¼‰
>>> name = build_filename("gemm_kernel", dtype=["FP8", "INT8"], ext=".so")
>>> # -> "gemm_kernel_H100_cc90_FP8_INT8_py312_cu124_x86_64.so"
>>>
>>> # æŸ¥æ‰¾æ–‡ä»¶
>>> path = find_file("cublaslt_gemm", search_dir=build_dir)
>>>
>>> # åŠ è½½æ¨¡å—
>>> module = load_module("cublaslt_gemm", search_dir=build_dir)
"""

import base64
import ctypes
import ctypes.util
import importlib
import importlib.util
import json
import os
import platform
import re
import shutil
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from functools import cached_property

# å»¶è¿Ÿå¯¼å…¥ torch
_torch = None


def _get_torch():
    """å»¶è¿Ÿå¯¼å…¥ torchï¼Œé¿å…åœ¨ä¸éœ€è¦æ—¶åŠ è½½"""
    global _torch
    if _torch is None:
        try:
            import torch
            _torch = torch
        except ImportError:
            raise ImportError("PyTorch is required but not installed")
    return _torch


# =============================================================================
# æ•°æ®ç±»å‹æ ‡å‡†åŒ–
# =============================================================================

# æ•°æ®ç±»å‹åˆ«åæ˜ å°„ï¼ˆè¾“å…¥ -> æ ‡å‡†åç§°ï¼‰
DTYPE_ALIASES = {
    # FP8 å˜ä½“
    "fp8": "FP8E4M3",
    "fp8e4m3": "FP8E4M3",
    "fp8_e4m3": "FP8E4M3",
    "FP8": "FP8E4M3",
    "FP8E4M3": "FP8E4M3",
    "e4m3": "FP8E4M3",
    "fp8e5m2": "FP8E5M2",
    "fp8_e5m2": "FP8E5M2",
    "FP8E5M2": "FP8E5M2",
    "e5m2": "FP8E5M2",
    # INT8
    "int8": "INT8",
    "INT8": "INT8",
    "i8": "INT8",
    # INT32
    "int32": "INT32",
    "INT32": "INT32",
    "i32": "INT32",
    # BF16
    "bf16": "BF16",
    "BF16": "BF16",
    "bfloat16": "BF16",
    # FP16
    "fp16": "FP16",
    "FP16": "FP16",
    "float16": "FP16",
    "half": "FP16",
    # FP32
    "fp32": "FP32",
    "FP32": "FP32",
    "float32": "FP32",
    "float": "FP32",
}


def normalize_dtype(dtype: str) -> str:
    """
    æ ‡å‡†åŒ–æ•°æ®ç±»å‹åç§°
    
    Args:
        dtype: è¾“å…¥çš„æ•°æ®ç±»å‹åç§°ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
        
    Returns:
        æ ‡å‡†åŒ–çš„æ•°æ®ç±»å‹åç§°
        
    Raises:
        ValueError: æœªçŸ¥çš„æ•°æ®ç±»å‹
        
    Examples:
        >>> normalize_dtype("fp8")
        'FP8E4M3'
        >>> normalize_dtype("int8")
        'INT8'
    """
    key = dtype.lower().replace("-", "_").replace(" ", "")
    if key in DTYPE_ALIASES:
        return DTYPE_ALIASES[key]
    # å°è¯•åŸå§‹è¾“å…¥
    if dtype in DTYPE_ALIASES.values():
        return dtype
    raise ValueError(f"æœªçŸ¥çš„æ•°æ®ç±»å‹: {dtype}. æ”¯æŒçš„ç±»å‹: {set(DTYPE_ALIASES.values())}")


# #############################################################################
#
#  PART 1: CUDA ç¼–è¯‘ã€é“¾æ¥ã€åº“åŠ è½½å·¥å…·
#
#  æœ¬éƒ¨åˆ†æä¾›ç»Ÿä¸€çš„ CUDA æ‰©å±•ç¼–è¯‘å’Œè¿è¡Œæ—¶åº“åŠ è½½åŠŸèƒ½ã€‚
#
#  è§„èŒƒæµç¨‹ï¼š
#  =========
#  ã€ç¼–è¯‘æ—¶ã€‘
#   1. ä¼˜å…ˆæŒ‡å®šç³»ç»Ÿåº“è·¯å¾„ (-L/usr/lib/x86_64-linux-gnu)
#   2. ç„¶åé“¾æ¥åº“å (-lcusparseLt ç­‰)
#   3. ç¡®ä¿é“¾æ¥åˆ°ç³»ç»Ÿå®‰è£…çš„æ–°ç‰ˆæœ¬åº“ï¼Œè€Œé pip åŒ…çš„æ—§ç‰ˆæœ¬
#
#  ã€è¿è¡Œæ—¶ã€‘
#   1. è®¾ç½®ç¯å¢ƒå˜é‡ (CUSPARSELT_PATH ç­‰) æŒ‡å‘ç³»ç»Ÿåº“
#   2. é¢„åŠ è½½ç³»ç»Ÿåº“ (RTLD_GLOBAL æ¨¡å¼ï¼Œç¡®ä¿ç¬¦å·å…¨å±€å¯è§)
#   3. åŠ è½½è‡ªå®šä¹‰ .so æ–‡ä»¶
#
#  æ”¯æŒçš„ç¼–è¯‘æ–¹å¼ï¼š
#  ===============
#  - build_cuda_extension():       ä½¿ç”¨ torch.utils.cpp_extension.load (PyTorch æ‰©å±•)
#  - build_cuda_extension_direct(): ç›´æ¥ä½¿ç”¨ nvcc ç¼–è¯‘ (çº¯ C åº“ï¼Œç”¨ ctypes åŠ è½½)
#
# #############################################################################


# =============================================================================
# ç³»ç»Ÿåº“è·¯å¾„é…ç½®
# =============================================================================

# ç³»ç»Ÿåº“æœç´¢è·¯å¾„ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
SYSTEM_LIB_PATHS = {
    "x86_64": "/usr/lib/x86_64-linux-gnu",
    "aarch64": "/usr/lib/aarch64-linux-gnu",
    "default": "/usr/local/cuda/lib64",
}

def get_system_lib_path() -> str:
    """è·å–å½“å‰æ¶æ„çš„ç³»ç»Ÿåº“è·¯å¾„"""
    import platform
    arch = platform.machine()
    return SYSTEM_LIB_PATHS.get(arch, SYSTEM_LIB_PATHS["default"])


# =============================================================================
# NVCC æ¶æ„æ ‡å¿—
# =============================================================================

# æ”¯æŒçš„ GPU æ¶æ„åˆ—è¡¨
SUPPORTED_ARCHITECTURES = [
    ("80", "sm_80"),   # Ampere (A100, A10, A30)
    ("86", "sm_86"),   # Ampere (RTX 30xx)
    ("89", "sm_89"),   # Ada Lovelace (RTX 40xx)
    ("90", "sm_90"),   # Hopper (H100, H200)
    ("100", "sm_100"), # Blackwell (B100, B200)
    ("120", "sm_120"), # Blackwell (RTX 50xx)
    ("121", "sm_121"), # Blackwell (GB10)
]


def get_nvcc_arch_flags(
    min_compute: int = 80,
    max_compute: int = 121,
) -> List[str]:
    """
    ç”Ÿæˆ nvcc æ¶æ„ç¼–è¯‘é€‰é¡¹
    
    æ”¯æŒä» SM 80 (Ampere) åˆ° SM 121 (Blackwell)
    
    Args:
        min_compute: æœ€å°æ”¯æŒçš„ compute capability
        max_compute: æœ€å¤§æ”¯æŒçš„ compute capability
        
    Returns:
        nvcc -gencode æ ‡å¿—åˆ—è¡¨
    """
    flags = []
    for compute, sm in SUPPORTED_ARCHITECTURES:
        cc = int(compute)
        if min_compute <= cc <= max_compute:
            flags.append(f"-gencode=arch=compute_{compute},code={sm}")
    return flags


def get_current_arch_flag() -> str:
    """
    è·å–å½“å‰ GPU æ¶æ„çš„ nvcc ç¼–è¯‘æ ‡å¿—
    
    Returns:
        å•ä¸ª -gencode æ ‡å¿—ï¼Œé’ˆå¯¹å½“å‰ GPU
    """
    torch = _get_torch()
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA not available")
    prop = torch.cuda.get_device_properties(0)
    compute = f"{prop.major}{prop.minor}"
    return f"-gencode=arch=compute_{compute},code=sm_{compute}"


# =============================================================================
# é“¾æ¥åº“é…ç½®
# =============================================================================

# æ”¯æŒçš„åç«¯ç±»å‹
SUPPORTED_BACKENDS = ["cublaslt", "cusparselt"]


def get_backend_ldflags(backend: str, with_lib_path: bool = True) -> List[str]:
    """
    è·å–åç«¯æ‰€éœ€çš„é“¾æ¥æ ‡å¿—
    
    Args:
        backend: åç«¯åç§° ("cublaslt" æˆ– "cusparselt")
        with_lib_path: æ˜¯å¦åŒ…å« -L åº“è·¯å¾„ï¼ˆç¼–è¯‘æ—¶éœ€è¦ï¼Œæ¨è Trueï¼‰
        
    Returns:
        é“¾æ¥æ ‡å¿—åˆ—è¡¨
    """
    lib_path = get_system_lib_path()
    
    if backend.lower() == "cublaslt":
        flags = ["-lcublasLt", "-lcublas", "-lcuda"]
    elif backend.lower() == "cusparselt":
        flags = ["-lcusparseLt", "-lcusparse", "-lcuda"]
    else:
        raise ValueError(f"æœªçŸ¥çš„åç«¯: {backend}ï¼Œæ”¯æŒ: {SUPPORTED_BACKENDS}")
    
    if with_lib_path:
        return [f"-L{lib_path}"] + flags
    return flags


# å…¼å®¹æ€§åˆ«åï¼ˆåç«¯é“¾æ¥åº“é…ç½®ï¼‰
BACKEND_LDFLAGS = {
    "cublaslt": get_backend_ldflags("cublaslt", with_lib_path=True),
    "cusparselt": get_backend_ldflags("cusparselt", with_lib_path=True),
}

# ç®€åŒ–ç‰ˆé“¾æ¥åº“ï¼ˆä¸å« -L è·¯å¾„ï¼Œç”¨äº torch.utils.cpp_extensionï¼‰
CUBLASLT_LDFLAGS = get_backend_ldflags("cublaslt", with_lib_path=True)
CUSPARSELT_LDFLAGS = get_backend_ldflags("cusparselt", with_lib_path=True)


# =============================================================================
# è¿è¡Œæ—¶åº“åŠ è½½
# =============================================================================

# åº“åŠ è½½çŠ¶æ€
_CUBLASLT_LOADED = False
_CUSPARSELT_LOADED = False


def ensure_cublaslt_loaded() -> None:
    """
    é¢„åŠ è½½ç³»ç»Ÿ cuBLASLt åº“ï¼Œé¿å…ç¬¦å·å†²çªã€‚
    
    å¿…é¡»åœ¨åŠ è½½è‡ªå®šä¹‰ .so ä¹‹å‰å®Œæˆã€‚ä½¿ç”¨ RTLD_GLOBAL ç¡®ä¿ç¬¦å·å…¨å±€å¯è§ã€‚
    
    ç¯å¢ƒå˜é‡:
        CUBLASLT_PATH: æŒ‡å®š libcublasLt.so çš„å®Œæ•´è·¯å¾„ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        
    Raises:
        OSError: æ— æ³•æ‰¾åˆ°å…¼å®¹çš„ libcublasLt
    """
    global _CUBLASLT_LOADED
    if _CUBLASLT_LOADED:
        return

    # æ„å»ºæœç´¢è·¯å¾„ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
    preferred_paths = []
    
    # 1. ç¯å¢ƒå˜é‡ä¼˜å…ˆ
    env_path = os.environ.get("CUBLASLT_PATH")
    if env_path:
        preferred_paths.append(env_path)

    # 2. ç³»ç»Ÿåº“è·¯å¾„
    preferred_paths.extend([
        "/usr/lib/x86_64-linux-gnu/libcublasLt.so",
        "/usr/lib/aarch64-linux-gnu/libcublasLt.so",
        "/usr/local/cuda/lib64/libcublasLt.so",
    ])
    
    # 3. ctypes é»˜è®¤æœç´¢
    found = ctypes.util.find_library("cublasLt")
    if found:
        preferred_paths.append(found)

    # å°è¯•åŠ è½½
    for path in dict.fromkeys(preferred_paths):  # å»é‡ä½†ä¿æŒä¼˜å…ˆçº§
        if not path:
            continue
        try:
            lib = ctypes.CDLL(path, mode=ctypes.RTLD_GLOBAL)
            getattr(lib, "cublasLtCreate")  # éªŒè¯åº“å¯ç”¨
            _CUBLASLT_LOADED = True
            return
        except (OSError, AttributeError):
            continue

    raise OSError(
        "æ— æ³•æ‰¾åˆ°å…¼å®¹çš„ libcublasLtã€‚\n"
        "è¯·è®¾ç½® CUBLASLT_PATH ç¯å¢ƒå˜é‡ï¼Œæˆ–ç¡®ä¿ CUDA å·²æ­£ç¡®å®‰è£…ã€‚"
    )


def ensure_cusparselt_loaded() -> None:
    """
    é¢„åŠ è½½ç³»ç»Ÿ cuSPARSELt åº“ (0.8.1+)ï¼Œé¿å…ä¸ PyTorch pip åŒ… (0.7.x) å†²çªã€‚
    
    å¿…é¡»åœ¨åŠ è½½è‡ªå®šä¹‰ .so ä¹‹å‰å®Œæˆã€‚ä½¿ç”¨ RTLD_GLOBAL ç¡®ä¿ç¬¦å·å…¨å±€å¯è§ã€‚
    
    ç¯å¢ƒå˜é‡:
        CUSPARSELT_PATH: æŒ‡å®š libcusparseLt.so.0 çš„å®Œæ•´è·¯å¾„ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        
    Raises:
        OSError: æ— æ³•æ‰¾åˆ°å…¼å®¹çš„ libcusparseLt (éœ€è¦ 0.8+)
    """
    global _CUSPARSELT_LOADED
    if _CUSPARSELT_LOADED:
        return

    # æ„å»ºæœç´¢è·¯å¾„ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
    preferred_paths = []
    
    # 1. ç¯å¢ƒå˜é‡ä¼˜å…ˆ
    env_path = os.environ.get("CUSPARSELT_PATH")
    if env_path:
        preferred_paths.append(env_path)

    # 2. ç³»ç»Ÿåº“è·¯å¾„ï¼ˆä¼˜å…ˆæ–°ç‰ˆæœ¬ç›®å½•ï¼‰
    preferred_paths.extend([
        # x86_64 ç³»ç»Ÿåº“
        "/usr/lib/x86_64-linux-gnu/libcusparseLt.so.0",
        "/usr/lib/x86_64-linux-gnu/libcusparseLt/12/libcusparseLt.so.0",
        "/usr/lib/x86_64-linux-gnu/libcusparseLt/13/libcusparseLt.so.0",
        # aarch64 ç³»ç»Ÿåº“
        "/usr/lib/aarch64-linux-gnu/libcusparseLt.so.0",
        "/usr/lib/aarch64-linux-gnu/libcusparseLt/12/libcusparseLt.so.0",
        "/usr/lib/aarch64-linux-gnu/libcusparseLt/13/libcusparseLt.so.0",
        # CUDA é»˜è®¤è·¯å¾„
        "/usr/local/cuda/lib64/libcusparseLt.so.0",
    ])
    
    # 3. ctypes é»˜è®¤æœç´¢ï¼ˆå¯èƒ½æ‰¾åˆ° pip åŒ…çš„æ—§ç‰ˆæœ¬ï¼Œä¼˜å…ˆçº§æœ€ä½ï¼‰
    found = ctypes.util.find_library("cusparseLt")
    if found:
        preferred_paths.append(found)

    # å°è¯•åŠ è½½
    for path in dict.fromkeys(preferred_paths):  # å»é‡ä½†ä¿æŒä¼˜å…ˆçº§
        if not path:
            continue
        try:
            lib = ctypes.CDLL(path, mode=ctypes.RTLD_GLOBAL)
            # éªŒè¯æ˜¯ 0.8+ ç‰ˆæœ¬ï¼ˆæ­¤ API åœ¨ 0.7 ä¸­ä¸å­˜åœ¨ï¼‰
            getattr(lib, "cusparseLtMatmulAlgSelectionDestroy")
            _CUSPARSELT_LOADED = True
            return
        except (OSError, AttributeError):
            continue

    raise OSError(
        "æ— æ³•æ‰¾åˆ°å…¼å®¹çš„ libcusparseLt (éœ€è¦ 0.8+)ã€‚\n"
        "ç³»ç»Ÿå®‰è£…: apt install libcusparselt0 libcusparselt-dev\n"
        "æˆ–è®¾ç½® CUSPARSELT_PATH ç¯å¢ƒå˜é‡æŒ‡å‘ç³»ç»Ÿåº“è·¯å¾„ã€‚\n"
        "æ³¨æ„: PyTorch pip åŒ…è‡ªå¸¦çš„ 0.7.x ç‰ˆæœ¬ä¸å…¼å®¹ã€‚"
    )


# åç«¯å¯¹åº”çš„åº“åŠ è½½å‡½æ•°
BACKEND_LOADERS = {
    "cublaslt": ensure_cublaslt_loaded,
    "cusparselt": ensure_cusparselt_loaded,
}


# =============================================================================
# ç¼–è¯‘è¾…åŠ©å‡½æ•°
# =============================================================================

# é»˜è®¤ç¼–è¯‘é€‰é¡¹
DEFAULT_CFLAGS = ['-O3', '-std=c++17']

DEFAULT_CUDA_CFLAGS = [
    '-O3',
    '-std=c++17',
    '--expt-relaxed-constexpr',
    '--expt-extended-lambda',
    '-U__CUDA_NO_HALF_OPERATORS__',
    '-U__CUDA_NO_HALF_CONVERSIONS__',
    '-U__CUDA_NO_BFLOAT16_CONVERSIONS__',
]


def should_rebuild(so_path: Path, source_paths: List[Path]) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°ç¼–è¯‘
    
    Args:
        so_path: .so æ–‡ä»¶è·¯å¾„
        source_paths: æºæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Returns:
        å¦‚æœ .so ä¸å­˜åœ¨æˆ–æ¯”ä»»ä¸€æºæ–‡ä»¶æ—§ï¼Œè¿”å› True
    """
    if not so_path.exists():
        return True
    
    so_mtime = so_path.stat().st_mtime
    for src in source_paths:
        if src.exists() and src.stat().st_mtime > so_mtime:
            return True
    return False


def clean_build_artifacts(build_dir: Path, keep_extensions: Optional[List[str]] = None):
    """
    æ¸…ç†ç¼–è¯‘ä¸­é—´æ–‡ä»¶
    
    Args:
        build_dir: æ„å»ºç›®å½•
        keep_extensions: è¦ä¿ç•™çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼ˆé»˜è®¤ ['.so', '.py']ï¼‰
    """
    if keep_extensions is None:
        keep_extensions = ['.so', '.py']
    
    if not build_dir.exists():
        return
    
    for item in build_dir.iterdir():
        if item.suffix in keep_extensions:
            continue
        if item.is_dir():
            shutil.rmtree(item)
        else:
            item.unlink()


# =============================================================================
# PyTorch æ‰©å±•ç¼–è¯‘ (torch.utils.cpp_extension)
# =============================================================================

def build_cuda_extension(
    name: str,
    source_file: Path,
    build_dir: Path,
    *,
    extra_cflags: Optional[List[str]] = None,
    extra_cuda_cflags: Optional[List[str]] = None,
    extra_ldflags: Optional[List[str]] = None,
    extra_include_paths: Optional[List[str]] = None,
    force: bool = False,
    verbose: bool = True,
    clean_after_build: bool = True,
) -> Path:
    """
    ä½¿ç”¨ torch.utils.cpp_extension.load ç¼–è¯‘ CUDA æ‰©å±•
    
    ç”Ÿæˆçš„ .so æ–‡ä»¶å¯ä»¥ä½œä¸º Python æ¨¡å—å¯¼å…¥ï¼Œæ”¯æŒ pybind11 ç»‘å®šã€‚
    é€‚ç”¨äºéœ€è¦ä¸ PyTorch Tensor äº¤äº’çš„ CUDA ä»£ç ã€‚
    
    Args:
        name: æ‰©å±•åç§°ï¼ˆä¸å« .so åç¼€ï¼‰
        source_file: æºæ–‡ä»¶è·¯å¾„ (.cu)
        build_dir: æ„å»ºç›®å½•
        extra_cflags: é¢å¤–çš„ C++ ç¼–è¯‘æ ‡å¿—
        extra_cuda_cflags: é¢å¤–çš„ CUDA ç¼–è¯‘æ ‡å¿—
        extra_ldflags: é¢å¤–çš„é“¾æ¥æ ‡å¿—ï¼ˆå¦‚ ["-lcublasLt"]ï¼‰
        extra_include_paths: é¢å¤–çš„å¤´æ–‡ä»¶æœç´¢è·¯å¾„
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç¼–è¯‘
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        clean_after_build: ç¼–è¯‘åæ˜¯å¦æ¸…ç†ä¸­é—´æ–‡ä»¶
        
    Returns:
        ç¼–è¯‘ç”Ÿæˆçš„ .so æ–‡ä»¶è·¯å¾„
    """
    from torch.utils.cpp_extension import load
    
    source_file = Path(source_file)
    build_dir = Path(build_dir)
    
    if not source_file.exists():
        raise FileNotFoundError(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
    
    build_dir.mkdir(parents=True, exist_ok=True)
    
    # æŸ¥æ‰¾å·²å­˜åœ¨çš„ .so
    so_pattern = f"{name}*.so"
    existing_sos = list(build_dir.glob(so_pattern))
    
    if existing_sos and not force:
        so_path = existing_sos[0]
        if not should_rebuild(so_path, [source_file]):
            if verbose:
                print(f"âœ“ Using existing: {so_path.name}")
            return so_path
        elif verbose:
            print(f"âš  Source changed, rebuilding...")
    
    if verbose:
        print(f"ğŸ”¨ Building {name}...")
    
    # CUDA è·¯å¾„
    cuda_home = os.environ.get('CUDA_HOME', '/usr/local/cuda')
    
    # åˆå¹¶ç¼–è¯‘é€‰é¡¹
    cflags = DEFAULT_CFLAGS + (extra_cflags or [])
    cuda_cflags = DEFAULT_CUDA_CFLAGS + get_nvcc_arch_flags() + (extra_cuda_cflags or [])
    ldflags = extra_ldflags or []
    include_paths = [os.path.join(cuda_home, 'include')] + (extra_include_paths or [])
    
    # ç¼–è¯‘
    try:
        load(
            name=name,
            sources=[str(source_file)],
            extra_cflags=cflags,
            extra_cuda_cflags=cuda_cflags,
            extra_ldflags=ldflags,
            extra_include_paths=include_paths,
            build_directory=str(build_dir),
            verbose=verbose,
        )
    except Exception as e:
        raise RuntimeError(f"ç¼–è¯‘å¤±è´¥: {e}") from e
    
    # æŸ¥æ‰¾ç”Ÿæˆçš„ .so
    new_sos = list(build_dir.glob(so_pattern))
    if not new_sos:
        raise RuntimeError(f"ç¼–è¯‘å®Œæˆä½†æœªæ‰¾åˆ° .so æ–‡ä»¶: {so_pattern}")
    
    so_path = new_sos[0]
    
    if verbose:
        print(f"âœ“ Built: {so_path.name}")
    
    if clean_after_build:
        if verbose:
            print(f"ğŸ§¹ Cleaning build artifacts...")
        clean_build_artifacts(build_dir)
    
    return so_path


# =============================================================================
# ç›´æ¥ NVCC ç¼–è¯‘ (çº¯ C åº“ï¼Œç”¨ ctypes åŠ è½½)
# =============================================================================

def build_cuda_extension_direct(
    name: str,
    source_file: Path,
    build_dir: Path,
    *,
    extra_cuda_cflags: Optional[List[str]] = None,
    extra_ldflags: Optional[List[str]] = None,
    extra_include_paths: Optional[List[str]] = None,
    force: bool = False,
    verbose: bool = True,
) -> Path:
    """
    ç›´æ¥ä½¿ç”¨ nvcc ç¼–è¯‘ CUDA æ‰©å±•ï¼ˆä¸ä¾èµ– PyTorchï¼‰
    
    ç”Ÿæˆçš„ .so æ˜¯çº¯ C åº“ï¼Œé€šè¿‡ ctypes.CDLL åŠ è½½ã€‚
    é€‚ç”¨äºä¸ä¾èµ– PyTorch çš„çº¯ CUDA ä»£ç ï¼Œç¼–è¯‘é€Ÿåº¦å¿«ã€‚
    
    Args:
        name: æ‰©å±•åç§°ï¼ˆä¸å« .so åç¼€ï¼‰
        source_file: æºæ–‡ä»¶è·¯å¾„ (.cu)
        build_dir: æ„å»ºç›®å½•
        extra_cuda_cflags: é¢å¤–çš„ CUDA ç¼–è¯‘æ ‡å¿—
        extra_ldflags: é¢å¤–çš„é“¾æ¥æ ‡å¿—ï¼ˆå¦‚ ["-L/usr/lib", "-lcusparseLt"]ï¼‰
        extra_include_paths: é¢å¤–çš„å¤´æ–‡ä»¶æœç´¢è·¯å¾„
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç¼–è¯‘
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        
    Returns:
        ç¼–è¯‘ç”Ÿæˆçš„ .so æ–‡ä»¶è·¯å¾„
    """
    source_file = Path(source_file)
    build_dir = Path(build_dir)
    
    if not source_file.exists():
        raise FileNotFoundError(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
    
    build_dir.mkdir(parents=True, exist_ok=True)
    
    # è¾“å‡ºæ–‡ä»¶
    so_path = build_dir / f"{name}.so"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç¼–è¯‘
    if so_path.exists() and not force:
        if not should_rebuild(so_path, [source_file]):
            if verbose:
                print(f"âœ“ Using existing: {so_path.name}")
            return so_path
        elif verbose:
            print(f"âš  Source changed, rebuilding...")
    
    if verbose:
        print(f"ğŸ”¨ Building {name}...")
    
    # CUDA è·¯å¾„
    cuda_home = os.environ.get('CUDA_HOME', '/usr/local/cuda')
    nvcc = os.path.join(cuda_home, 'bin', 'nvcc')
    
    # æ„å»ºç¼–è¯‘å‘½ä»¤
    cmd = [nvcc]
    cmd.extend(['-std=c++17', '-O3', '-Xcompiler', '-fPIC', '--shared'])
    cmd.extend(get_nvcc_arch_flags())
    
    if extra_cuda_cflags:
        cmd.extend(extra_cuda_cflags)
    
    # å¤´æ–‡ä»¶è·¯å¾„
    cmd.extend(['-I', os.path.join(cuda_home, 'include')])
    if extra_include_paths:
        for inc in extra_include_paths:
            cmd.extend(['-I', inc])
    
    # æºæ–‡ä»¶
    cmd.append(str(source_file))
    
    # é“¾æ¥æ ‡å¿—
    if extra_ldflags:
        cmd.extend(extra_ldflags)
    
    # è¾“å‡º
    cmd.extend(['-o', str(so_path)])
    
    if verbose:
        print(f"Command: {' '.join(cmd)}")
    
    # æ‰§è¡Œç¼–è¯‘
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        error_msg = result.stderr or result.stdout
        raise RuntimeError(f"ç¼–è¯‘å¤±è´¥:\n{error_msg}")
    
    if not so_path.exists():
        raise RuntimeError(f"ç¼–è¯‘å®Œæˆä½†æœªæ‰¾åˆ° .so æ–‡ä»¶: {so_path}")
    
    if verbose:
        print(f"âœ“ Built: {so_path.name}")
    
    return so_path


# =============================================================================
# é«˜çº§åŠ è½½æ¥å£ (è‡ªåŠ¨ç¼–è¯‘ + åŠ è½½)
# =============================================================================

def load_cuda_extension(
    script_type: str,
    backend: str,
    source_file: "Path",
    build_dir: "Optional[Path]" = None,
    *,
    verbose: bool = True,
    force_compile: bool = False,
) -> object:
    """
    åŠ è½½æˆ–ç¼–è¯‘ PyTorch CUDA æ‰©å±•ï¼ˆé«˜çº§æ¥å£ï¼‰
    
    è‡ªåŠ¨å¤„ç†ï¼š
    1. é¢„åŠ è½½ç³»ç»Ÿ CUDA åº“ï¼ˆé¿å…ç‰ˆæœ¬å†²çªï¼‰
    2. æ£€æŸ¥å·²æœ‰ .so æ˜¯å¦å¯ç”¨
    3. å¿…è¦æ—¶ç¼–è¯‘æ–°çš„ .so
    4. åŠ è½½å¹¶è¿”å›æ¨¡å—
    
    å‘½åè§„èŒƒ:
        {script_type}_{backend}_{GPU}_{CC}_{PyVer}_{CUDAVer}_{Arch}.so
        ä¾‹å¦‚: alg_search_cublaslt_H100_cc90_py312_cu129_x86_64.so
    
    Args:
        script_type: è„šæœ¬ç±»å‹ï¼ˆå¦‚ "alg_search", "layout_search"ï¼‰
        backend: åç«¯ç±»å‹ï¼ˆ"cublaslt" æˆ– "cusparselt"ï¼‰
        source_file: CUDA æºæ–‡ä»¶è·¯å¾„ (.cu)
        build_dir: æ„å»ºç›®å½•ï¼Œé»˜è®¤ä¸º source_file åŒçº§çš„ build/
        verbose: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
        force_compile: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç¼–è¯‘
    
    Returns:
        ç¼–è¯‘å¥½çš„æ‰©å±•æ¨¡å—ï¼ˆå¯è°ƒç”¨å…¶å¯¼å‡ºçš„å‡½æ•°ï¼‰
    """
    torch = _get_torch()
    from torch.utils.cpp_extension import load
    
    if backend not in SUPPORTED_BACKENDS:
        raise ValueError(f"æ— æ•ˆçš„åç«¯ç±»å‹: {backend}ï¼Œæ”¯æŒ: {SUPPORTED_BACKENDS}")
    
    source_file = Path(source_file)
    if not source_file.exists():
        raise FileNotFoundError(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
    
    # Step 1: é¢„åŠ è½½ç³»ç»Ÿåº“
    if verbose:
        lib_name = "cuBLASLt" if backend == "cublaslt" else "cuSPARSELt"
        print(f"[1/4] åŠ è½½ {lib_name} åº“...", end=" ", flush=True)
    
    BACKEND_LOADERS[backend]()
    
    if verbose:
        print("âœ“", flush=True)
    
    # è·å–ç¡¬ä»¶ä¿¡æ¯
    hw = hw_info
    
    # æ„å»ºæ‰©å±•åç§°
    ext_name = build_stem(f"{script_type}_{backend}")
    so_pattern = f"{ext_name}*.so"
    
    # ç¡®å®šæ„å»ºç›®å½•
    if build_dir is None:
        build_dir = source_file.parent / "build"
    else:
        build_dir = Path(build_dir)
    build_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 2: æ£€æŸ¥å·²æœ‰çš„ .so
    existing_so = list(build_dir.glob(so_pattern))
    need_compile = force_compile
    
    if not need_compile:
        if not existing_so:
            need_compile = True
        else:
            need_compile = source_file.stat().st_mtime > existing_so[0].stat().st_mtime
    
    if not need_compile and existing_so:
        if verbose:
            print(f"[2/4] åŠ è½½ {hw.gpu_name} æ‰©å±•...", end=" ", flush=True)
        
        spec = importlib.util.spec_from_file_location(ext_name, str(existing_so[0]))
        ext = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(ext)
        
        if verbose:
            print(f"âœ“ ({existing_so[0].name})", flush=True)
        return ext
    else:
        if verbose:
            reason = "å¼ºåˆ¶" if force_compile else ("é¦–æ¬¡" if not existing_so else "æºæ–‡ä»¶å·²æ›´æ–°")
            print(f"[2/4] ç¼–è¯‘ {hw.gpu_name} æ‰©å±•ï¼ˆ{reason}ï¼‰...", end=" ", flush=True)
        
        ext = load(
            name=ext_name,
            sources=[str(source_file)],
            extra_cuda_cflags=["-O3", f"-arch={hw.sm_code}"],
            extra_ldflags=BACKEND_LDFLAGS[backend],
            verbose=False,
            build_directory=str(build_dir),
            with_cuda=True,
        )
        
        # æ¸…ç†ä¸­é—´æ–‡ä»¶
        for pattern in [".ninja_deps", ".ninja_log", "build.ninja", "*.o"]:
            for f in build_dir.glob(pattern):
                f.unlink(missing_ok=True)
        
        if verbose:
            print("âœ“", flush=True)
        return ext


# #############################################################################
#
#  PART 2: ç¡¬ä»¶ä¿¡æ¯
#
#  æœ¬éƒ¨åˆ†æä¾›ç»Ÿä¸€çš„ç¡¬ä»¶ä¿¡æ¯è·å–åŠŸèƒ½ã€‚
#
#  ä¸»è¦å†…å®¹ï¼š
#  =========
#  - HardwareInfo: ç¡¬ä»¶ä¿¡æ¯å•ä¾‹ç±»ï¼Œç¼“å­˜æ‰€æœ‰ç¡¬ä»¶ç›¸å…³ä¿¡æ¯
#  - hw_info: å…¨å±€å•ä¾‹å®ä¾‹
#  - ä¾¿æ·å‡½æ•°: get_gpu_name, get_gpu_cc, get_sm_code ç­‰
#
#  ä½¿ç”¨ç¤ºä¾‹ï¼š
#  =========
#  >>> from slidesparse.utils import hw_info
#  >>> print(hw_info.gpu_name)     # "H100"
#  >>> print(hw_info.cc_tag)       # "cc90"
#  >>> print(hw_info.supports_fp8) # True
#
# #############################################################################


# =============================================================================
# ç¡¬ä»¶ä¿¡æ¯ç±»
# =============================================================================

@dataclass
class HardwareInfo:
    """
    ç¡¬ä»¶ä¿¡æ¯å•ä¾‹ç±»
    
    ç¼“å­˜æ‰€æœ‰ç¡¬ä»¶ç›¸å…³ä¿¡æ¯ï¼Œé¿å…é‡å¤æŸ¥è¯¢ã€‚
    æ‰€æœ‰å±æ€§ä½¿ç”¨ cached_property å®ç°æ‡’åŠ è½½ã€‚
    
    Attributes:
        gpu_name: GPU ç®€ç§°ï¼ˆH100, A100 ç­‰ï¼‰
        gpu_full_name: GPU å®Œæ•´åç§°
        cc_major: Compute Capability ä¸»ç‰ˆæœ¬
        cc_minor: Compute Capability æ¬¡ç‰ˆæœ¬
        cc_tag: CC æ ‡ç­¾ï¼ˆcc90, cc100 ç­‰ï¼‰
        python_tag: Python ç‰ˆæœ¬æ ‡ç­¾ï¼ˆpy312 ç­‰ï¼‰
        cuda_tag: CUDA ç‰ˆæœ¬æ ‡ç­¾ï¼ˆcu129 ç­‰ï¼‰
        arch_tag: ç³»ç»Ÿæ¶æ„æ ‡ç­¾ï¼ˆx86_64 ç­‰ï¼‰
    """
    
    _instance: Optional['HardwareInfo'] = field(default=None, repr=False, init=False)
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    # -------------------------------------------------------------------------
    # GPU ä¿¡æ¯
    # -------------------------------------------------------------------------
    
    @cached_property
    def gpu_full_name(self) -> str:
        """GPU å®Œæ•´åç§°"""
        torch = _get_torch()
        if torch.cuda.is_available():
            prop = torch.cuda.get_device_properties(0)
            return prop.name
        # å¤‡é€‰ï¼šnvidia-smi
        try:
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                return result.stdout.strip().split('\n')[0]
        except Exception:
            pass
        return "unknown"
    
    @cached_property
    def gpu_name(self) -> str:
        """
        GPU ç®€ç§°ï¼ˆH100, A100, B200, RTX5080 ç­‰ï¼‰
        
        å¤„ç†å¸¸è§æ ¼å¼:
        - "NVIDIA A100-SXM4-40GB" -> "A100"
        - "NVIDIA H100 PCIe" -> "H100"
        - "NVIDIA GeForce RTX 5080" -> "RTX5080"
        - "NVIDIA GeForce RTX 4090" -> "RTX4090"
        - "NVIDIA GeForce GTX 1080 Ti" -> "GTX1080Ti"
        - "NVIDIA TITAN RTX" -> "TitanRTX"
        """
        full_name = self.gpu_full_name
        if full_name == "unknown":
            return "unknown"
        
        # ç§»é™¤ "NVIDIA " å‰ç¼€
        name = full_name
        if name.startswith("NVIDIA "):
            name = name[7:]
        
        # å¤„ç† GeForce RTX/GTX ç³»åˆ—: "GeForce RTX 5080" -> "RTX5080"
        if name.startswith("GeForce "):
            name = name[8:]  # ç§»é™¤ "GeForce "
            # ç°åœ¨ name å¯èƒ½æ˜¯ "RTX 5080" æˆ– "GTX 1080 Ti"
            # æå– RTX/GTX å‰ç¼€å’Œå‹å·
            parts = name.split()
            if len(parts) >= 2 and parts[0] in ("RTX", "GTX"):
                prefix = parts[0]  # RTX æˆ– GTX
                model = "".join(parts[1:])  # 5080 æˆ– 1080Ti
                return f"{prefix}{model}"
            # å…¶ä»– GeForce æƒ…å†µ
            return "".join(parts)
        
        # å¤„ç† TITAN ç³»åˆ—: "TITAN RTX" -> "TitanRTX"
        if name.startswith("TITAN "):
            return "Titan" + name[6:].replace(" ", "")
        
        # æ•°æ®ä¸­å¿ƒå¡: "A100-SXM4-40GB" -> "A100", "H100 PCIe" -> "H100"
        # æå–ç¬¬ä¸€ä¸ªç©ºæ ¼æˆ–è¿å­—ç¬¦ä¹‹å‰çš„éƒ¨åˆ†
        for sep in [" ", "-"]:
            end_pos = name.find(sep)
            if end_pos != -1:
                name = name[:end_pos]
                break
        
        # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        if not name:
            name = full_name
            for c in [" ", "-", "/"]:
                name = name.replace(c, "_")
        
        return name
    
    @cached_property
    def cc_major(self) -> int:
        """Compute Capability ä¸»ç‰ˆæœ¬"""
        torch = _get_torch()
        if torch.cuda.is_available():
            prop = torch.cuda.get_device_properties(0)
            return prop.major
        return 0
    
    @cached_property
    def cc_minor(self) -> int:
        """Compute Capability æ¬¡ç‰ˆæœ¬"""
        torch = _get_torch()
        if torch.cuda.is_available():
            prop = torch.cuda.get_device_properties(0)
            return prop.minor
        return 0
    
    @cached_property
    def cc_tag(self) -> str:
        """CC æ ‡ç­¾ï¼ˆcc90, cc100, cc121 ç­‰ï¼‰"""
        return f"cc{self.cc_major}{self.cc_minor}"
    
    @cached_property
    def sm_code(self) -> str:
        """SM ä»£ç ï¼ˆsm_90, sm_100 ç­‰ï¼‰"""
        return f"sm_{self.cc_major}{self.cc_minor}"
    
    @cached_property
    def gpu_memory_gb(self) -> float:
        """GPU æ˜¾å­˜å¤§å° (GB)"""
        torch = _get_torch()
        if torch.cuda.is_available():
            prop = torch.cuda.get_device_properties(0)
            return prop.total_memory / (1024 ** 3)
        return 0.0
    
    # -------------------------------------------------------------------------
    # Python ä¿¡æ¯
    # -------------------------------------------------------------------------
    
    @cached_property
    def python_version(self) -> Tuple[int, int]:
        """Python ç‰ˆæœ¬ (major, minor)"""
        return (sys.version_info.major, sys.version_info.minor)
    
    @cached_property
    def python_tag(self) -> str:
        """Python ç‰ˆæœ¬æ ‡ç­¾ï¼ˆpy312, py311 ç­‰ï¼‰"""
        return f"py{self.python_version[0]}{self.python_version[1]}"
    
    # -------------------------------------------------------------------------
    # CUDA ä¿¡æ¯
    # -------------------------------------------------------------------------
    
    @cached_property
    def cuda_runtime_version(self) -> str:
        """CUDA Runtime ç‰ˆæœ¬ï¼ˆPyTorch ç¼–è¯‘æ—¶ä½¿ç”¨çš„ç‰ˆæœ¬ï¼‰"""
        torch = _get_torch()
        try:
            return torch.version.cuda or "unknown"
        except Exception:
            return "unknown"
    
    @cached_property
    def cuda_driver_version(self) -> str:
        """CUDA Driver ç‰ˆæœ¬ï¼ˆnvidia-smi æ˜¾ç¤ºçš„ç‰ˆæœ¬ï¼‰"""
        try:
            result = subprocess.run(
                ["nvidia-smi"],
                capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                for line in result.stdout.split('\n'):
                    if "CUDA Version" in line:
                        match = re.search(r'CUDA Version:\s*(\d+\.\d+)', line)
                        if match:
                            return match.group(1)
        except Exception:
            pass
        return "unknown"
    
    @cached_property
    def cuda_tag(self) -> str:
        """
        CUDA ç‰ˆæœ¬æ ‡ç­¾ï¼ˆcu129, cu124 ç­‰ï¼‰
        
        ä¼˜å…ˆä½¿ç”¨ Runtime ç‰ˆæœ¬ï¼Œå› ä¸ºè¿™æ˜¯å®é™…ç¼–è¯‘æ—¶ä½¿ç”¨çš„ç‰ˆæœ¬ã€‚
        """
        version = self.cuda_runtime_version
        if version == "unknown":
            version = self.cuda_driver_version
        if version == "unknown":
            return "cu000"
        # "12.9" -> "cu129", "12.4" -> "cu124"
        parts = version.split(".")
        if len(parts) >= 2:
            major = parts[0]
            minor = parts[1].split(".")[0]  # å¤„ç† "12.4.1" è¿™ç§æƒ…å†µ
            return f"cu{major}{minor}"
        return f"cu{version.replace('.', '')}"
    
    # -------------------------------------------------------------------------
    # ç³»ç»Ÿæ¶æ„
    # -------------------------------------------------------------------------
    
    @cached_property
    def arch_raw(self) -> str:
        """åŸå§‹ç³»ç»Ÿæ¶æ„"""
        return platform.machine()
    
    @cached_property
    def arch_tag(self) -> str:
        """ç³»ç»Ÿæ¶æ„æ ‡ç­¾ï¼ˆx86_64, aarch64 ç­‰ï¼‰"""
        machine = self.arch_raw
        if machine in ("x86_64", "AMD64"):
            return "x86_64"
        elif machine in ("aarch64", "arm64"):
            return "aarch64"
        return machine.lower()
    
    # -------------------------------------------------------------------------
    # é©±åŠ¨ä¿¡æ¯
    # -------------------------------------------------------------------------
    
    @cached_property
    def driver_version(self) -> str:
        """NVIDIA é©±åŠ¨ç‰ˆæœ¬"""
        try:
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=driver_version", "--format=csv,noheader"],
                capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                return result.stdout.strip().split('\n')[0]
        except Exception:
            pass
        return "unknown"
    
    # -------------------------------------------------------------------------
    # PyTorch ä¿¡æ¯
    # -------------------------------------------------------------------------
    
    @cached_property
    def pytorch_version(self) -> str:
        """PyTorch ç‰ˆæœ¬"""
        torch = _get_torch()
        return torch.__version__
    
    # -------------------------------------------------------------------------
    # æ¶æ„æ£€æµ‹
    # -------------------------------------------------------------------------
    
    # æ¶æ„åç§°æ˜ å°„
    ARCH_INFO = {
        7: ("Volta", "volta"),         # V100 ç­‰
        8: ("Ampere", "ampere"),       # A100, A10, A30 ç­‰
        9: ("Hopper", "hopper"),       # H100, H200 ç­‰
        10: ("Blackwell", "blackwell"), # B100, B200 ç­‰
        12: ("Blackwell", "blackwell"), # GB10 ç­‰ (CC 12.x ä¹Ÿæ˜¯ Blackwell å®¶æ—)
    }
    
    @cached_property
    def arch_name(self) -> str:
        """æ¶æ„åç§°ï¼ˆAmpere, Hopper, Blackwell ç­‰ï¼‰"""
        if self.cc_major in self.ARCH_INFO:
            return self.ARCH_INFO[self.cc_major][0]
        return f"SM{self.cc_major}{self.cc_minor}"
    
    @cached_property
    def arch_suffix(self) -> str:
        """æ¶æ„åç¼€ï¼ˆampere, hopper, blackwell ç­‰ï¼‰"""
        if self.cc_major in self.ARCH_INFO:
            return self.ARCH_INFO[self.cc_major][1]
        return f"sm{self.cc_major}{self.cc_minor}"
    
    # -------------------------------------------------------------------------
    # åŠŸèƒ½æ£€æµ‹
    # -------------------------------------------------------------------------
    
    @cached_property
    def supports_fp8(self) -> bool:
        """æ˜¯å¦æ”¯æŒåŸç”Ÿ FP8ï¼ˆCC >= 8.9ï¼ŒAda/Hopper+ï¼‰"""
        return (self.cc_major, self.cc_minor) >= (8, 9)
    
    @cached_property
    def supports_int8(self) -> bool:
        """æ˜¯å¦æ”¯æŒåŸç”Ÿ INT8ï¼ˆCC >= 8.0ï¼ŒAmpere+ï¼‰"""
        return self.cc_major >= 8
    
    @cached_property
    def triton_supported(self) -> Tuple[bool, str]:
        """
        æ£€æŸ¥ Triton æ˜¯å¦æ”¯æŒå½“å‰æ¶æ„
        
        Returns:
            (supported, reason)
        """
        # å·²çŸ¥ä¸è¢«æ”¯æŒçš„æ¶æ„
        UNSUPPORTED = {
            (12, 1): "GB10 (sm_121a) is not yet supported by Triton/ptxas",
        }
        
        if (self.cc_major, self.cc_minor) in UNSUPPORTED:
            return False, UNSUPPORTED[(self.cc_major, self.cc_minor)]
        
        return True, "Architecture is supported"
    
    @cached_property
    def needs_eager_mode(self) -> bool:
        """æ˜¯å¦éœ€è¦ä½¿ç”¨ eager modeï¼ˆç¦ç”¨ torch.compileï¼‰"""
        return not self.triton_supported[0]
    
    # -------------------------------------------------------------------------
    # æ±‡æ€»ä¿¡æ¯
    # -------------------------------------------------------------------------
    
    def to_dict(self) -> Dict[str, Any]:
        """å¯¼å‡ºä¸ºå­—å…¸"""
        return {
            "gpu": {
                "name": self.gpu_name,
                "full_name": self.gpu_full_name,
                "memory_gb": round(self.gpu_memory_gb, 1),
            },
            "compute_capability": {
                "major": self.cc_major,
                "minor": self.cc_minor,
                "tag": self.cc_tag,
                "sm_code": self.sm_code,
            },
            "cuda": {
                "runtime_version": self.cuda_runtime_version,
                "driver_version": self.cuda_driver_version,
                "tag": self.cuda_tag,
            },
            "python": {
                "version": f"{self.python_version[0]}.{self.python_version[1]}",
                "tag": self.python_tag,
            },
            "system": {
                "arch": self.arch_tag,
                "driver_version": self.driver_version,
            },
            "architecture": {
                "name": self.arch_name,
                "suffix": self.arch_suffix,
            },
            "capabilities": {
                "supports_fp8": self.supports_fp8,
                "supports_int8": self.supports_int8,
                "triton_supported": self.triton_supported[0],
            },
            "pytorch_version": self.pytorch_version,
        }
    
    def print_info(self):
        """æ‰“å°ç¡¬ä»¶ä¿¡æ¯"""
        print("=" * 60)
        print("SlideSparse Hardware Info")
        print("=" * 60)
        print(f"GPU:           {self.gpu_full_name}")
        print(f"GPU (short):   {self.gpu_name}")
        print(f"Memory:        {self.gpu_memory_gb:.1f} GB")
        print(f"CC:            {self.cc_tag} ({self.sm_code})")
        print(f"Architecture:  {self.arch_name}")
        print(f"Python:        {self.python_tag}")
        print(f"CUDA Runtime:  {self.cuda_runtime_version}")
        print(f"CUDA Driver:   {self.cuda_driver_version}")
        print(f"CUDA Tag:      {self.cuda_tag}")
        print(f"System Arch:   {self.arch_tag}")
        print(f"Driver:        {self.driver_version}")
        print(f"PyTorch:       {self.pytorch_version}")
        print("-" * 60)
        print(f"FP8 Support:   {self.supports_fp8}")
        print(f"INT8 Support:  {self.supports_int8}")
        print(f"Triton:        {'âœ“' if self.triton_supported[0] else 'âœ— ' + self.triton_supported[1]}")
        print("=" * 60)


# å…¨å±€å•ä¾‹
hw_info = HardwareInfo()


# ä¾¿æ·å‡½æ•°ï¼ˆhw_info å±æ€§çš„å¿«æ·è®¿é—®ï¼‰
def get_gpu_name() -> str:
    """è·å– GPU ç®€ç§°"""
    return hw_info.gpu_name


def get_gpu_cc() -> str:
    """è·å– CC æ ‡ç­¾"""
    return hw_info.cc_tag


def get_python_version_tag() -> str:
    """è·å– Python ç‰ˆæœ¬æ ‡ç­¾"""
    return hw_info.python_tag


def get_cuda_ver() -> str:
    """è·å– CUDA ç‰ˆæœ¬æ ‡ç­¾"""
    return hw_info.cuda_tag


def get_arch_tag() -> str:
    """è·å–ç³»ç»Ÿæ¶æ„æ ‡ç­¾"""
    return hw_info.arch_tag


def get_sm_code() -> str:
    """è·å– SM ä»£ç """
    return hw_info.sm_code


def print_system_info():
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
    hw_info.print_info()


# #############################################################################
#
#  PART 3: æ–‡ä»¶åä¸ IO
#
#  æœ¬éƒ¨åˆ†æä¾›ç»Ÿä¸€çš„æ–‡ä»¶å‘½åã€æŸ¥æ‰¾ã€ä¿å­˜å’Œæ¨¡å—åŠ è½½åŠŸèƒ½ã€‚
#
#  å‘½åè§„èŒƒï¼š
#  =========
#  æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶åéµå¾ªç»Ÿä¸€æ ¼å¼ï¼š
#      {prefix}_{GPU}_{CC}[_{dtype}]_{PyVer}_{CUDAVer}_{Arch}.{ext}
#
#  ä¸»è¦åŠŸèƒ½ï¼š
#  =========
#  - build_filename:  æ„å»ºæ ‡å‡†åŒ–æ–‡ä»¶å
#  - find_file:       æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
#  - load_module:     åŠ è½½ Python æ¨¡å— (.py/.so)
#  - save_json/csv:   ä¿å­˜æ•°æ®æ–‡ä»¶
#  - ensure_result_dir: åˆ›å»ºç»“æœç›®å½•
#
# #############################################################################


# =============================================================================
# æ–‡ä»¶åæ„å»º
# =============================================================================

def build_filename(
    prefix: str,
    dtype: Optional[Union[str, List[str]]] = None,
    ext: str = "",
    *,
    gpu_name: Optional[str] = None,
    cc_tag: Optional[str] = None,
    python_tag: Optional[str] = None,
    cuda_tag: Optional[str] = None,
    arch_tag: Optional[str] = None,
) -> str:
    """
    æ„å»ºæ ‡å‡†åŒ–æ–‡ä»¶å
    
    æ ¼å¼: {prefix}_{GPU}_{CC}[_{dtype}]_{PyVer}_{CUDAVer}_{Arch}.{ext}
    
    dtype éƒ¨åˆ†æ˜¯å¯é€‰çš„ï¼Œæ”¯æŒä¸‰ç§æƒ…å†µï¼š
    - None: ä¸åŒ…å« dtypeï¼Œç”¨äºæ”¯æŒå¤šç§ç±»å‹çš„æ‰©å±•
    - str: å•ä¸ª dtype
    - List[str]: å¤šä¸ª dtypeï¼ŒæŒ‰é¡ºåºè¿æ¥
    
    Args:
        prefix: ç”¨é€”å‰ç¼€ï¼ˆcublaslt_gemm, cusparselt_gemm, dequant_bias_tuned ç­‰ï¼‰
        dtype: æ•°æ®ç±»å‹ï¼ˆå•ä¸ªå­—ç¬¦ä¸²ã€å­—ç¬¦ä¸²åˆ—è¡¨ã€æˆ– Noneï¼‰
        ext: æ–‡ä»¶æ‰©å±•åï¼ˆ.so, .py, .json ç­‰ï¼‰ï¼Œä¸åŒ…å«ç‚¹æ—¶è‡ªåŠ¨æ·»åŠ 
        gpu_name: GPU åç§°ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
        cc_tag: CC æ ‡ç­¾ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
        python_tag: Python ç‰ˆæœ¬æ ‡ç­¾ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
        cuda_tag: CUDA ç‰ˆæœ¬æ ‡ç­¾ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
        arch_tag: ç³»ç»Ÿæ¶æ„æ ‡ç­¾ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
        
    Returns:
        æ ‡å‡†åŒ–çš„æ–‡ä»¶å
        
    Examples:
        # æ—  dtypeï¼ˆæ”¯æŒå¤šç§ç±»å‹çš„ GEMM æ‰©å±•ï¼‰
        >>> build_filename("cublaslt_gemm", ext=".so")
        'cublaslt_gemm_H100_cc90_py312_cu124_x86_64.so'
        
        # å•ä¸ª dtype
        >>> build_filename("dequant_bias_tuned", dtype="BF16", ext=".py")
        'dequant_bias_tuned_H100_cc90_BF16_py312_cu124_x86_64.py'
        
        # å¤šä¸ª dtype
        >>> build_filename("gemm_kernel", dtype=["FP8", "INT8"], ext=".so")
        'gemm_kernel_H100_cc90_FP8_INT8_py312_cu124_x86_64.so'
    """
    # ä½¿ç”¨æä¾›çš„å€¼æˆ–ä»ç¡¬ä»¶ä¿¡æ¯è·å–
    _gpu = gpu_name or hw_info.gpu_name
    _cc = cc_tag or hw_info.cc_tag
    _py = python_tag or hw_info.python_tag
    _cuda = cuda_tag or hw_info.cuda_tag
    _arch = arch_tag or hw_info.arch_tag
    
    # æ„å»ºç»„ä»¶åˆ—è¡¨
    components = [prefix, _gpu, _cc]
    
    # æ·»åŠ æ•°æ®ç±»å‹ï¼ˆå¦‚æœæä¾›ï¼‰
    if dtype:
        if isinstance(dtype, str):
            # å•ä¸ª dtype
            components.append(normalize_dtype(dtype))
        elif isinstance(dtype, (list, tuple)):
            # å¤šä¸ª dtypeï¼Œé€ä¸ªæ ‡å‡†åŒ–åæ·»åŠ 
            for d in dtype:
                components.append(normalize_dtype(d))
    
    # æ·»åŠ å…¶ä½™ç»„ä»¶
    components.extend([_py, _cuda, _arch])
    
    # è¿æ¥ç»„ä»¶
    name = "_".join(components)
    
    # å¤„ç†æ‰©å±•å
    if ext:
        if not ext.startswith("."):
            ext = "." + ext
        name += ext
    
    return name


def build_stem(
    prefix: str,
    dtype: Optional[Union[str, List[str]]] = None,
    **kwargs
) -> str:
    """
    æ„å»ºæ–‡ä»¶åä¸»å¹²ï¼ˆä¸å«æ‰©å±•åï¼‰
    
    ç­‰åŒäº build_filename(..., ext="")
    """
    return build_filename(prefix, dtype=dtype, ext="", **kwargs)


def build_dir_name(
    prefix: Optional[str] = None,
    dtype: Optional[str] = None,
    *,
    gpu_name: Optional[str] = None,
    cc_tag: Optional[str] = None,
) -> str:
    """
    æ„å»ºç›®å½•åï¼ˆç”¨äºæŒ‰ GPU+CC+dtype åˆ†ç±»çš„åœºæ™¯ï¼‰
    
    æ ¼å¼: {GPU}_{CC}_{dtype} æˆ–å¸¦ prefix æ—¶ {prefix}_{GPU}_{CC}_{dtype}
    
    Args:
        prefix: å¯é€‰å‰ç¼€
        dtype: æ•°æ®ç±»å‹ï¼ˆå¿…éœ€ï¼‰
        gpu_name: GPU åç§°ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
        cc_tag: CC æ ‡ç­¾ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
        
    Examples:
        >>> build_dir_name(dtype="FP8E4M3")
        'H100_cc90_FP8E4M3'
        
        >>> build_dir_name(prefix="results", dtype="INT8")
        'results_H100_cc90_INT8'
    """
    _gpu = gpu_name or hw_info.gpu_name
    _cc = cc_tag or hw_info.cc_tag
    
    components = []
    if prefix:
        components.append(prefix)
    components.extend([_gpu, _cc])
    
    if dtype:
        components.append(normalize_dtype(dtype))
    
    return "_".join(components)


# =============================================================================
# æ–‡ä»¶æŸ¥æ‰¾
# =============================================================================

def find_file(
    prefix: str,
    dtype: Optional[Union[str, List[str]]] = None,
    search_dir: Union[str, Path] = ".",
    ext: Optional[str] = None,
    *,
    exact: bool = True,
    gpu_name: Optional[str] = None,
    cc_tag: Optional[str] = None,
    python_tag: Optional[str] = None,
    cuda_tag: Optional[str] = None,
    arch_tag: Optional[str] = None,
) -> Optional[Path]:
    """
    æŸ¥æ‰¾ç¬¦åˆå‘½åè§„èŒƒçš„æ–‡ä»¶
    
    Args:
        prefix: ç”¨é€”å‰ç¼€
        dtype: æ•°æ®ç±»å‹ï¼ˆå•ä¸ªå­—ç¬¦ä¸²ã€å­—ç¬¦ä¸²åˆ—è¡¨ã€æˆ– Noneï¼‰
        search_dir: æœç´¢ç›®å½•
        ext: æ–‡ä»¶æ‰©å±•åï¼ˆNone è¡¨ç¤ºä»»æ„æ‰©å±•åï¼‰
        exact: True è¡¨ç¤ºç²¾ç¡®åŒ¹é…ï¼ŒFalse è¡¨ç¤ºæ¨¡ç³ŠåŒ¹é…ï¼ˆå¿½ç•¥æŸäº›ç»„ä»¶ï¼‰
        gpu_name: GPU åç§°è¦†ç›–
        cc_tag: CC æ ‡ç­¾è¦†ç›–
        python_tag: Python ç‰ˆæœ¬æ ‡ç­¾è¦†ç›–
        cuda_tag: CUDA ç‰ˆæœ¬æ ‡ç­¾è¦†ç›–
        arch_tag: ç³»ç»Ÿæ¶æ„æ ‡ç­¾è¦†ç›–
        
    Returns:
        æ‰¾åˆ°çš„æ–‡ä»¶è·¯å¾„ï¼Œæœªæ‰¾åˆ°è¿”å› None
        
    Examples:
        >>> find_file("cublaslt_gemm", search_dir="build", ext=".so")
        PosixPath('build/cublaslt_gemm_H100_cc90_py312_cu124_x86_64.so')
        
        >>> find_file("dequant_bias_tuned", dtype="BF16", search_dir="build", ext=".py")
        PosixPath('build/dequant_bias_tuned_H100_cc90_BF16_py312_cu124_x86_64.py')
    """
    search_dir = Path(search_dir)
    
    if not search_dir.exists():
        return None
    
    if exact:
        # ç²¾ç¡®åŒ¹é…ï¼šæ„å»ºå®Œæ•´æ–‡ä»¶å
        if ext:
            filename = build_filename(
                prefix, dtype=dtype, ext=ext,
                gpu_name=gpu_name, cc_tag=cc_tag,
                python_tag=python_tag, cuda_tag=cuda_tag, arch_tag=arch_tag
            )
            target = search_dir / filename
            return target if target.exists() else None
        else:
            # å°è¯•å¸¸è§æ‰©å±•å
            stem = build_stem(
                prefix, dtype=dtype,
                gpu_name=gpu_name, cc_tag=cc_tag,
                python_tag=python_tag, cuda_tag=cuda_tag, arch_tag=arch_tag
            )
            for ext_try in [".so", ".py", ".json", ".csv", ""]:
                target = search_dir / (stem + ext_try)
                if target.exists():
                    return target
            return None
    else:
        # æ¨¡ç³ŠåŒ¹é…ï¼šä½¿ç”¨ glob æ¨¡å¼
        _gpu = gpu_name or hw_info.gpu_name
        _cc = cc_tag or hw_info.cc_tag
        
        # æ„å»º dtype æ¨¡å¼
        if dtype is None:
            dtype_pattern = "*"
        elif isinstance(dtype, str):
            dtype_pattern = normalize_dtype(dtype)
        else:
            # å¤šä¸ª dtype è¿æ¥
            dtype_pattern = "_".join(normalize_dtype(d) for d in dtype)
        
        # æ¨¡ç³ŠåŒ¹é…æ¨¡å¼ï¼šprefix_GPU_CC_[dtype_]*_py*_cu*_arch
        pattern = f"{prefix}_{_gpu}_{_cc}_{dtype_pattern}_*" if dtype else f"{prefix}_{_gpu}_{_cc}_*"
        if ext:
            if not ext.startswith("."):
                ext = "." + ext
            pattern += ext
        
        matches = list(search_dir.glob(pattern))
        return matches[0] if matches else None


def find_files(
    prefix: str,
    dtype: Optional[Union[str, List[str]]] = None,
    search_dir: Union[str, Path] = ".",
    ext: Optional[str] = None,
    **kwargs
) -> List[Path]:
    """
    æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶
    
    å‚æ•°åŒ find_fileï¼Œä½†è¿”å›æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶åˆ—è¡¨ã€‚
    """
    search_dir = Path(search_dir)
    
    if not search_dir.exists():
        return []
    
    _gpu = kwargs.get("gpu_name") or hw_info.gpu_name
    _cc = kwargs.get("cc_tag") or hw_info.cc_tag
    
    # æ„å»º dtype æ¨¡å¼
    if dtype is None:
        dtype_pattern = "*"
    elif isinstance(dtype, str):
        dtype_pattern = normalize_dtype(dtype)
    else:
        dtype_pattern = "_".join(normalize_dtype(d) for d in dtype)
    
    # æ¨¡ç³ŠåŒ¹é…æ¨¡å¼
    pattern = f"{prefix}_{_gpu}_{_cc}_{dtype_pattern}_*" if dtype else f"{prefix}_{_gpu}_{_cc}_*"
    if ext:
        if not ext.startswith("."):
            ext = "." + ext
        pattern += ext
    
    return sorted(search_dir.glob(pattern))


def find_dir(
    dtype: Optional[str] = None,
    search_dir: Union[str, Path] = ".",
    *,
    prefix: Optional[str] = None,
    gpu_name: Optional[str] = None,
    cc_tag: Optional[str] = None,
) -> Optional[Path]:
    """
    æŸ¥æ‰¾ç¬¦åˆå‘½åè§„èŒƒçš„ç›®å½•
    
    æ ¼å¼: {GPU}_{CC}_{dtype} æˆ– {prefix}_{GPU}_{CC}_{dtype}
    
    Args:
        dtype: æ•°æ®ç±»å‹
        search_dir: æœç´¢ç›®å½•
        prefix: å¯é€‰å‰ç¼€
        gpu_name: GPU åç§°è¦†ç›–
        cc_tag: CC æ ‡ç­¾è¦†ç›–
        
    Returns:
        æ‰¾åˆ°çš„ç›®å½•è·¯å¾„ï¼Œæœªæ‰¾åˆ°è¿”å› None
    """
    search_dir = Path(search_dir)
    
    if not search_dir.exists():
        return None
    
    dir_name = build_dir_name(
        prefix=prefix, dtype=dtype,
        gpu_name=gpu_name, cc_tag=cc_tag
    )
    
    target = search_dir / dir_name
    return target if target.is_dir() else None


# =============================================================================
# æ¨¡å—åŠ è½½
# =============================================================================

# æ¨¡å—ç¼“å­˜
_module_cache: Dict[str, Any] = {}


def load_module(
    prefix: str,
    dtype: Optional[Union[str, List[str]]] = None,
    search_dir: Union[str, Path] = ".",
    *,
    ext: Optional[str] = None,
    cache: bool = True,
    gpu_name: Optional[str] = None,
    cc_tag: Optional[str] = None,
    python_tag: Optional[str] = None,
    cuda_tag: Optional[str] = None,
    arch_tag: Optional[str] = None,
) -> Any:
    """
    åŠ è½½ Python æ¨¡å—ï¼ˆ.py æˆ– .soï¼‰
    
    è‡ªåŠ¨æ ¹æ®å½“å‰ç¡¬ä»¶ä¿¡æ¯æ„å»ºæ¨¡å—åå¹¶åŠ è½½ã€‚
    
    Args:
        prefix: æ¨¡å—å‰ç¼€
        dtype: æ•°æ®ç±»å‹ï¼ˆå•ä¸ªå­—ç¬¦ä¸²ã€å­—ç¬¦ä¸²åˆ—è¡¨ã€æˆ– Noneï¼‰
        search_dir: æœç´¢ç›®å½•
        ext: æ–‡ä»¶æ‰©å±•åï¼ˆNone è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ .so æˆ– .pyï¼‰
        cache: æ˜¯å¦ç¼“å­˜æ¨¡å—
        gpu_name: GPU åç§°è¦†ç›–
        cc_tag: CC æ ‡ç­¾è¦†ç›–
        python_tag: Python ç‰ˆæœ¬æ ‡ç­¾è¦†ç›–
        cuda_tag: CUDA ç‰ˆæœ¬æ ‡ç­¾è¦†ç›–
        arch_tag: ç³»ç»Ÿæ¶æ„æ ‡ç­¾è¦†ç›–
        
    Returns:
        åŠ è½½çš„ Python æ¨¡å—
        
    Raises:
        FileNotFoundError: æ¨¡å—æ–‡ä»¶ä¸å­˜åœ¨
        ImportError: æ¨¡å—åŠ è½½å¤±è´¥
        
    Examples:
        # æ—  dtypeï¼ˆæ”¯æŒå¤šç±»å‹çš„ GEMM æ‰©å±•ï¼‰
        >>> module = load_module("cublaslt_gemm", search_dir="build")
        >>> module.gemm(...)
        
        # å¸¦ dtype
        >>> module = load_module("dequant_bias_tuned", dtype="BF16", search_dir="build")
    """
    search_dir = Path(search_dir)
    
    # æ„å»ºç¼“å­˜é”®
    dtype_key = str(dtype) if dtype else "None"
    cache_key = f"{prefix}_{dtype_key}_{search_dir}_{gpu_name}_{cc_tag}_{python_tag}_{cuda_tag}_{arch_tag}"
    
    if cache and cache_key in _module_cache:
        return _module_cache[cache_key]
    
    # æŸ¥æ‰¾æ¨¡å—æ–‡ä»¶
    module_path = None
    
    if ext:
        module_path = find_file(
            prefix, dtype=dtype, search_dir=search_dir, ext=ext,
            gpu_name=gpu_name, cc_tag=cc_tag,
            python_tag=python_tag, cuda_tag=cuda_tag, arch_tag=arch_tag
        )
    else:
        # ä¼˜å…ˆ .soï¼Œç„¶å .py
        for try_ext in [".so", ".py"]:
            module_path = find_file(
                prefix, dtype=dtype, search_dir=search_dir, ext=try_ext,
                gpu_name=gpu_name, cc_tag=cc_tag,
                python_tag=python_tag, cuda_tag=cuda_tag, arch_tag=arch_tag
            )
            if module_path:
                break
    
    if not module_path:
        expected_name = build_filename(
            prefix, dtype=dtype, ext=ext or ".so/.py",
            gpu_name=gpu_name, cc_tag=cc_tag,
            python_tag=python_tag, cuda_tag=cuda_tag, arch_tag=arch_tag
        )
        raise FileNotFoundError(
            f"æ¨¡å—ä¸å­˜åœ¨: {expected_name}\n"
            f"æœç´¢è·¯å¾„: {search_dir.absolute()}\n"
        )
    
    # æ·»åŠ ç›®å½•åˆ° sys.path
    if str(search_dir.absolute()) not in sys.path:
        sys.path.insert(0, str(search_dir.absolute()))
    
    # åŠ è½½æ¨¡å—
    module_name = module_path.stem
    
    if module_path.suffix == ".py":
        spec = importlib.util.spec_from_file_location(module_name, module_path)
        if spec is None or spec.loader is None:
            raise ImportError(f"æ— æ³•åŠ è½½æ¨¡å—: {module_path}")
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
    else:
        # .so æ–‡ä»¶
        module = importlib.import_module(module_name)
    
    if cache:
        _module_cache[cache_key] = module
    
    return module


def clear_module_cache():
    """æ¸…é™¤æ¨¡å—ç¼“å­˜"""
    global _module_cache
    _module_cache.clear()


# =============================================================================
# ç®—æ³•æŸ¥è¡¨ï¼ˆè¿è¡Œæ—¶ä½¿ç”¨ï¼‰
# =============================================================================

def lookup_best_cublaslt_alg(json_data: Dict, N: int, K: int, M: int) -> Optional[str]:
    """
    ä» JSON æ•°æ®ä¸­æŸ¥è¯¢ cuBLASLt æœ€ä½³ç®—æ³•é…ç½®ã€‚
    
    æŸ¥è¯¢é€»è¾‘ï¼š
    1. ç”¨ (N, K) åœ¨ nk_entries ä¸­æ‰¾åˆ°å¯¹åº”æ¡ç›®
    2. åœ¨ m_thresholds ä¸­æ‰¾åˆ° <= query_M çš„æœ€å¤§å€¼
    3. è¿”å›è¯¥ M å¯¹åº”çš„ alg_by_m[m][0]ï¼ˆæœ€ä½³é…ç½®çš„ base64 ç¼–ç ï¼‰
    
    Args:
        json_data: åŠ è½½çš„ JSON æ•°æ®ï¼ˆç”± alg_search ç”Ÿæˆï¼‰
        N: çŸ©é˜µ W çš„è¡Œæ•°
        K: å…±äº«ç»´åº¦
        M: çŸ©é˜µ A çš„è¡Œæ•°ï¼ˆbatch sizeï¼‰
    
    Returns:
        æœ€ä½³ç®—æ³•çš„ base64 ç¼–ç å­—ç¬¦ä¸²ï¼ˆ64B cublasLtMatmulAlgo_t æ•°æ®ï¼‰ï¼Œ
        å¦‚æœæ‰¾ä¸åˆ°è¿”å› None
    
    Example:
        >>> with open("alg_id_LUT_BitNet-2B4T.json") as f:
        ...     lut = json.load(f)
        >>> algo_b64 = lookup_best_cublaslt_alg(lut, 3840, 2560, 128)
        >>> algo_bytes = decode_cublaslt_algo_data(algo_b64)
    """
    nk_key = f"({N},{K})"
    nk_entries = json_data.get("nk_entries", {})
    
    if nk_key not in nk_entries:
        return None
    
    entry = nk_entries[nk_key]
    m_thresholds = entry.get("m_thresholds", [])
    alg_by_m = entry.get("alg_by_m", {})
    
    if not m_thresholds:
        return None
    
    # æ‰¾åˆ° <= M çš„æœ€å¤§é˜ˆå€¼
    selected_m = None
    for threshold in m_thresholds:
        if threshold <= M:
            selected_m = threshold
        else:
            break
    
    if selected_m is None:
        # M æ¯”æ‰€æœ‰é˜ˆå€¼éƒ½å°ï¼Œä½¿ç”¨æœ€å°çš„é˜ˆå€¼
        selected_m = m_thresholds[0]
    
    m_key = str(selected_m)
    if m_key in alg_by_m:
        # æ ¼å¼: alg_by_m[m_key] = [best_b64, 2nd_b64, 3rd_b64]
        alg_list = alg_by_m[m_key]
        if isinstance(alg_list, list) and len(alg_list) > 0:
            return alg_list[0]
    
    return None


def decode_cublaslt_algo_data(algo_data_b64: str) -> bytes:
    """
    è§£ç  base64 ç¼–ç çš„ cuBLASLt algo_dataï¼Œè¿”å› 64 å­—èŠ‚çš„åŸå§‹æ•°æ®ã€‚
    
    è¿è¡Œæ—¶ä½¿ç”¨ï¼šå°†è¿”å›çš„ bytes ç›´æ¥ memcpy åˆ° cublasLtMatmulAlgo_t ç»“æ„ä½“ã€‚
    
    Args:
        algo_data_b64: base64 ç¼–ç çš„ç®—æ³•æ•°æ®
        
    Returns:
        64 å­—èŠ‚çš„åŸå§‹ç®—æ³•æ•°æ®
    """
    return base64.b64decode(algo_data_b64)


def lookup_best_cusparselt_alg(json_data: Dict, N: int, K: int, M: int) -> Optional[Dict]:
    """
    ä» JSON æ•°æ®ä¸­æŸ¥è¯¢ cuSPARSELt æœ€ä½³ç®—æ³•é…ç½®ã€‚
    
    æŸ¥è¯¢é€»è¾‘ï¼š
    1. ç”¨ (N, K) åœ¨ nk_entries ä¸­æ‰¾åˆ°å¯¹åº”æ¡ç›®
    2. åœ¨ m_thresholds ä¸­æ‰¾åˆ° <= query_M çš„æœ€å¤§å€¼
    3. è¿”å›è¯¥ M å¯¹åº”çš„ alg_by_m[m][0]ï¼ˆæœ€ä½³é…ç½®ï¼‰
    
    Args:
        json_data: åŠ è½½çš„ JSON æ•°æ®ï¼ˆç”± alg_search ç”Ÿæˆï¼‰
        N: ç¨€ç–çŸ©é˜µ W çš„è¡Œæ•°
        K: å…±äº«ç»´åº¦
        M: ç¨ å¯†çŸ©é˜µ A çš„è¡Œæ•°ï¼ˆbatch sizeï¼‰
    
    Returns:
        æœ€ä½³é…ç½®å­—å…¸ {"alg_id": int, "split_k": int, "workspace": int}ï¼Œ
        å¦‚æœæ‰¾ä¸åˆ°è¿”å› None
    
    Example:
        >>> with open("alg_search_cusparselt_BitNet-2B4T.json") as f:
        ...     lut = json.load(f)
        >>> config = lookup_best_cusparselt_alg(lut, 3840, 2560, 128)
        >>> print(config)  # {"alg_id": 0, "split_k": 1, "workspace": 0}
    """
    nk_key = f"({N},{K})"
    nk_entries = json_data.get("nk_entries", {})
    
    if nk_key not in nk_entries:
        return None
    
    entry = nk_entries[nk_key]
    m_thresholds = entry.get("m_thresholds", [])
    alg_by_m = entry.get("alg_by_m", {})
    
    if not m_thresholds:
        return None
    
    # æ‰¾åˆ° <= M çš„æœ€å¤§é˜ˆå€¼
    selected_m = None
    for threshold in m_thresholds:
        if threshold <= M:
            selected_m = threshold
        else:
            break
    
    if selected_m is None:
        # M æ¯”æ‰€æœ‰é˜ˆå€¼éƒ½å°ï¼Œä½¿ç”¨æœ€å°çš„é˜ˆå€¼
        selected_m = m_thresholds[0]
    
    m_key = str(selected_m)
    if m_key in alg_by_m:
        alg_list = alg_by_m[m_key]
        if isinstance(alg_list, list) and len(alg_list) > 0:
            first_entry = alg_list[0]
            # æ”¯æŒæ–°æ ¼å¼ {"alg_id": int, "split_k": int, "workspace": int} å’Œæ—§æ ¼å¼ int
            if isinstance(first_entry, dict):
                return first_entry
            else:
                # å…¼å®¹æ—§æ ¼å¼ï¼ˆä»… alg_idï¼‰
                return {"alg_id": first_entry, "split_k": 1, "workspace": 0}
    
    return None


# =============================================================================
# æ–‡ä»¶ä¿å­˜
# =============================================================================

def save_json(
    data: Any,
    prefix: str,
    dtype: Optional[str] = None,
    save_dir: Union[str, Path] = ".",
    *,
    indent: int = 2,
    **kwargs
) -> Path:
    """
    ä¿å­˜æ•°æ®ä¸º JSON æ–‡ä»¶
    
    Args:
        data: è¦ä¿å­˜çš„æ•°æ®
        prefix: æ–‡ä»¶å‰ç¼€
        dtype: æ•°æ®ç±»å‹
        save_dir: ä¿å­˜ç›®å½•
        indent: JSON ç¼©è¿›
        **kwargs: ä¼ é€’ç»™ build_filename çš„å‚æ•°
        
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    import json
    
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    filename = build_filename(prefix, dtype=dtype, ext=".json", **kwargs)
    filepath = save_dir / filename
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=indent, ensure_ascii=False)
    
    return filepath


def load_json(
    prefix: str,
    dtype: Optional[str] = None,
    search_dir: Union[str, Path] = ".",
    **kwargs
) -> Any:
    """
    åŠ è½½ JSON æ–‡ä»¶
    
    Args:
        prefix: æ–‡ä»¶å‰ç¼€
        dtype: æ•°æ®ç±»å‹
        search_dir: æœç´¢ç›®å½•
        **kwargs: ä¼ é€’ç»™ find_file çš„å‚æ•°
        
    Returns:
        åŠ è½½çš„æ•°æ®
        
    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
    """
    import json
    
    filepath = find_file(prefix, dtype=dtype, search_dir=search_dir, ext=".json", **kwargs)
    
    if not filepath:
        expected_name = build_filename(prefix, dtype=dtype, ext=".json", **kwargs)
        raise FileNotFoundError(f"JSON æ–‡ä»¶ä¸å­˜åœ¨: {expected_name} in {search_dir}")
    
    with open(filepath, "r", encoding="utf-8") as f:
        return json.load(f)


def save_csv(
    data: List[Dict[str, Any]],
    prefix: str,
    dtype: Optional[str] = None,
    save_dir: Union[str, Path] = ".",
    **kwargs
) -> Path:
    """
    ä¿å­˜æ•°æ®ä¸º CSV æ–‡ä»¶
    
    Args:
        data: å­—å…¸åˆ—è¡¨å½¢å¼çš„æ•°æ®
        prefix: æ–‡ä»¶å‰ç¼€
        dtype: æ•°æ®ç±»å‹
        save_dir: ä¿å­˜ç›®å½•
        **kwargs: ä¼ é€’ç»™ build_filename çš„å‚æ•°
        
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    import csv
    
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    filename = build_filename(prefix, dtype=dtype, ext=".csv", **kwargs)
    filepath = save_dir / filename
    
    if not data:
        filepath.touch()
        return filepath
    
    fieldnames = list(data[0].keys())
    
    with open(filepath, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data)
    
    return filepath


# =============================================================================
# ç›®å½•ç®¡ç†
# =============================================================================

def ensure_result_dir(
    base_dir: Union[str, Path],
    dtype: Optional[str] = None,
    *,
    prefix: Optional[str] = None,
    gpu_name: Optional[str] = None,
    cc_tag: Optional[str] = None,
) -> Path:
    """
    ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨å¹¶è¿”å›è·¯å¾„
    
    åˆ›å»ºæ ¼å¼ä¸º {GPU}_{CC}_{dtype} çš„å­ç›®å½•ã€‚
    
    Args:
        base_dir: åŸºç¡€ç›®å½•
        dtype: æ•°æ®ç±»å‹
        prefix: å¯é€‰å‰ç¼€
        gpu_name: GPU åç§°è¦†ç›–
        cc_tag: CC æ ‡ç­¾è¦†ç›–
        
    Returns:
        åˆ›å»º/å·²å­˜åœ¨çš„ç›®å½•è·¯å¾„
        
    Examples:
        >>> result_dir = ensure_result_dir("results", dtype="FP8E4M3")
        >>> # Creates: results/H100_cc90_FP8E4M3/
    """
    base_dir = Path(base_dir)
    dir_name = build_dir_name(prefix=prefix, dtype=dtype, gpu_name=gpu_name, cc_tag=cc_tag)
    result_dir = base_dir / dir_name
    result_dir.mkdir(parents=True, exist_ok=True)
    return result_dir


# #############################################################################
#
#  PART 4: æ¨¡å‹ä¿¡æ¯ç®¡ç†
#
#  æœ¬éƒ¨åˆ†æä¾›æ¨¡å‹æ³¨å†Œè¡¨å’Œæ¨¡å‹ä¿¡æ¯æŸ¥è¯¢åŠŸèƒ½ã€‚
#
#  ä¸»è¦å†…å®¹ï¼š
#  =========
#  - MODEL_SIZE_GB: æ¨¡å‹å¤§å°å‚è€ƒè¡¨
#  - ModelEntry: æ¨¡å‹æ¡ç›®æ•°æ®ç±»
#  - ModelRegistry: æ¨¡å‹æ³¨å†Œè¡¨ï¼ˆå•ä¾‹ï¼‰
#  - ä¾¿æ·å‡½æ•°: get_model_info, list_models, check_model_downloaded ç­‰
#
#  ä½¿ç”¨ç¤ºä¾‹ï¼š
#  =========
#  >>> from slidesparse.utils import model_registry, get_model_info
#  >>> info = get_model_info("Qwen2.5-0.5B-FP8")
#  >>> models = model_registry.list(family="Qwen2.5")
#
# #############################################################################


# =============================================================================
# æ¨¡å‹å¤§å°å‚è€ƒ
# =============================================================================

# æ¨¡å‹å¤§å°å‚è€ƒï¼ˆç”¨äºä¼°ç®—æ˜¾å­˜éœ€æ±‚ï¼‰
MODEL_SIZE_GB = {
    "0.5B": 0.9,
    "1B": 1.9,
    "1.5B": 2.1,
    "2B": 4.8,    # BitNet 2B-BF16
    "3B": 4.0,
    "7B": 8.1,
    "14B": 15.2,
}


@dataclass
class ModelEntry:
    """
    å•ä¸ªæ¨¡å‹çš„ä¿¡æ¯æ¡ç›®
    
    Attributes:
        key: æ¨¡å‹çŸ­é”®åï¼ˆå¦‚ "qwen2.5-7b-fp8"ï¼‰
        family: æ¨¡å‹ç³»åˆ—ï¼ˆå¦‚ "qwen", "llama"ï¼‰
        version: ç‰ˆæœ¬å·ï¼ˆå¦‚ "2.5", "3.2"ï¼‰
        size: æ¨¡å‹å¤§å°ï¼ˆå¦‚ "7B", "1.5B"ï¼‰
        quant: é‡åŒ–ç±»å‹ï¼ˆå¦‚ "fp8", "int8"ï¼‰
        hf_name: HuggingFace æ¨¡å‹åï¼ˆå¦‚ "Qwen2.5-7B-Instruct-FP8-dynamic"ï¼‰
        local_name: æœ¬åœ°æ–‡ä»¶å¤¹åï¼ˆå¦‚ "Qwen2.5-7B-FP8"ï¼‰
        hf_org: HuggingFace ç»„ç»‡åï¼ˆé»˜è®¤ "RedHatAI"ï¼‰
    """
    key: str
    family: str
    version: str
    size: str
    quant: str
    hf_name: str
    local_name: str
    hf_org: str = "RedHatAI"
    
    @property
    def hf_path(self) -> str:
        """å®Œæ•´çš„ HuggingFace è·¯å¾„"""
        return f"{self.hf_org}/{self.hf_name}"
    
    @property
    def quant_normalized(self) -> str:
        """æ ‡å‡†åŒ–çš„é‡åŒ–ç±»å‹"""
        return normalize_dtype(self.quant) if self.quant.lower() not in ("int8",) else "INT8"
    
    @property
    def estimated_gb(self) -> float:
        """ä¼°ç®—çš„æ˜¾å­˜éœ€æ±‚ (GB)"""
        size_upper = self.size.upper()
        return MODEL_SIZE_GB.get(size_upper, 0.0)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "key": self.key,
            "family": self.family,
            "version": self.version,
            "size": self.size,
            "quant": self.quant,
            "quant_normalized": self.quant_normalized,
            "hf_name": self.hf_name,
            "hf_path": self.hf_path,
            "local_name": self.local_name,
            "estimated_gb": self.estimated_gb,
        }


class ModelRegistry:
    """
    æ¨¡å‹æ³¨å†Œè¡¨
    
    ç®¡ç†æ‰€æœ‰æ”¯æŒçš„é‡åŒ–æ¨¡å‹ï¼Œæä¾›æœç´¢ã€è¿‡æ»¤ã€è·¯å¾„æ„å»ºç­‰åŠŸèƒ½ã€‚
    
    å‘½åè§„èŒƒï¼š
        - key: {family}{version}-{size}-{quant}  ä¾‹å¦‚ "qwen2.5-7b-fp8"
        - local_name: {Family}{Version}-{Size}-{QUANT}  ä¾‹å¦‚ "Qwen2.5-7B-FP8"
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        >>> registry = ModelRegistry()
        >>> 
        >>> # è·å–æ‰€æœ‰ FP8 æ¨¡å‹
        >>> for entry in registry.list(quant="fp8"):
        ...     print(entry.key, entry.local_name)
        >>> 
        >>> # è·å–ç‰¹å®šæ¨¡å‹
        >>> entry = registry.get("qwen2.5-7b-fp8")
        >>> print(entry.hf_path)
        >>> 
        >>> # æŒ‰ family è¿‡æ»¤
        >>> for entry in registry.list(family="llama"):
        ...     print(entry.key)
    """
    
    # å†…ç½®æ¨¡å‹å®šä¹‰
    # æ ¼å¼: (family, version, size, quant, hf_name, local_name)
    _BUILTIN_MODELS = [
        # Qwen2.5 INT8
        ("qwen", "2.5", "0.5B", "int8", "Qwen2.5-0.5B-Instruct-quantized.w8a8", "Qwen2.5-0.5B-INT8"),
        ("qwen", "2.5", "1.5B", "int8", "Qwen2.5-1.5B-Instruct-quantized.w8a8", "Qwen2.5-1.5B-INT8"),
        ("qwen", "2.5", "3B", "int8", "Qwen2.5-3B-Instruct-quantized.w8a8", "Qwen2.5-3B-INT8"),
        ("qwen", "2.5", "7B", "int8", "Qwen2.5-7B-Instruct-quantized.w8a8", "Qwen2.5-7B-INT8"),
        ("qwen", "2.5", "14B", "int8", "Qwen2.5-14B-Instruct-quantized.w8a8", "Qwen2.5-14B-INT8"),
        # Qwen2.5 FP8
        ("qwen", "2.5", "0.5B", "fp8", "Qwen2.5-0.5B-Instruct-FP8-dynamic", "Qwen2.5-0.5B-FP8"),
        ("qwen", "2.5", "1.5B", "fp8", "Qwen2.5-1.5B-Instruct-FP8-dynamic", "Qwen2.5-1.5B-FP8"),
        ("qwen", "2.5", "3B", "fp8", "Qwen2.5-3B-Instruct-FP8-dynamic", "Qwen2.5-3B-FP8"),
        ("qwen", "2.5", "7B", "fp8", "Qwen2.5-7B-Instruct-FP8-dynamic", "Qwen2.5-7B-FP8"),
        ("qwen", "2.5", "14B", "fp8", "Qwen2.5-14B-Instruct-FP8-dynamic", "Qwen2.5-14B-FP8"),
        # Llama3.2 INT8
        ("llama", "3.2", "1B", "int8", "Llama-3.2-1B-Instruct-quantized.w8a8", "Llama3.2-1B-INT8"),
        ("llama", "3.2", "3B", "int8", "Llama-3.2-3B-Instruct-quantized.w8a8", "Llama3.2-3B-INT8"),
        # Llama3.2 FP8
        ("llama", "3.2", "1B", "fp8", "Llama-3.2-1B-Instruct-FP8-dynamic", "Llama3.2-1B-FP8"),
        ("llama", "3.2", "3B", "fp8", "Llama-3.2-3B-Instruct-FP8-dynamic", "Llama3.2-3B-FP8"),
        # BitNet BF16 (microsoft)
        ("bitnet", "1.58", "2B", "bf16", "bitnet-b1.58-2B-4T-bf16", "BitNet-2B-BF16", "microsoft"),
    ]
    
    def __init__(self, hf_org: str = "RedHatAI"):
        """
        åˆå§‹åŒ–æ¨¡å‹æ³¨å†Œè¡¨
        
        Args:
            hf_org: é»˜è®¤çš„ HuggingFace ç»„ç»‡å
        """
        self.hf_org = hf_org
        self._models: Dict[str, ModelEntry] = {}
        
        # åŠ è½½å†…ç½®æ¨¡å‹
        # æ”¯æŒ 6 å…ƒç»„ (family, version, size, quant, hf_name, local_name)
        # å’Œ 7 å…ƒç»„ (family, version, size, quant, hf_name, local_name, custom_hf_org)
        for model_tuple in self._BUILTIN_MODELS:
            if len(model_tuple) == 7:
                family, version, size, quant, hf_name, local_name, custom_org = model_tuple
            else:
                family, version, size, quant, hf_name, local_name = model_tuple
                custom_org = hf_org
            
            key = self._make_key(family, version, size, quant)
            self._models[key] = ModelEntry(
                key=key,
                family=family,
                version=version,
                size=size,
                quant=quant,
                hf_name=hf_name,
                local_name=local_name,
                hf_org=custom_org,
            )
    
    @staticmethod
    def _make_key(family: str, version: str, size: str, quant: str) -> str:
        """ç”Ÿæˆæ¨¡å‹ key"""
        return f"{family}{version}-{size.lower()}-{quant.lower()}"
    
    def register(
        self,
        family: str,
        version: str,
        size: str,
        quant: str,
        hf_name: str,
        local_name: str,
        hf_org: Optional[str] = None,
    ) -> ModelEntry:
        """
        æ³¨å†Œæ–°æ¨¡å‹
        
        Args:
            family: æ¨¡å‹ç³»åˆ—
            version: ç‰ˆæœ¬å·
            size: æ¨¡å‹å¤§å°
            quant: é‡åŒ–ç±»å‹
            hf_name: HuggingFace æ¨¡å‹å
            local_name: æœ¬åœ°æ–‡ä»¶å¤¹å
            hf_org: HuggingFace ç»„ç»‡å
            
        Returns:
            æ³¨å†Œçš„æ¨¡å‹æ¡ç›®
        """
        key = self._make_key(family, version, size, quant)
        entry = ModelEntry(
            key=key,
            family=family,
            version=version,
            size=size,
            quant=quant,
            hf_name=hf_name,
            local_name=local_name,
            hf_org=hf_org or self.hf_org,
        )
        self._models[key] = entry
        return entry
    
    def get(self, key: str) -> Optional[ModelEntry]:
        """
        è·å–æ¨¡å‹æ¡ç›®
        
        Args:
            key: æ¨¡å‹ keyï¼ˆå¦‚ "qwen2.5-7b-fp8"ï¼‰
            
        Returns:
            æ¨¡å‹æ¡ç›®ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        return self._models.get(key.lower())
    
    def __getitem__(self, key: str) -> ModelEntry:
        """é€šè¿‡ key è·å–æ¨¡å‹ï¼ˆKeyError if not foundï¼‰"""
        entry = self.get(key)
        if entry is None:
            raise KeyError(f"æ¨¡å‹ä¸å­˜åœ¨: {key}")
        return entry
    
    def __contains__(self, key: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
        return key.lower() in self._models
    
    def __len__(self) -> int:
        """æ¨¡å‹æ•°é‡"""
        return len(self._models)
    
    def __iter__(self):
        """è¿­ä»£æ‰€æœ‰æ¨¡å‹"""
        return iter(self._models.values())
    
    def list(
        self,
        *,
        family: Optional[str] = None,
        version: Optional[str] = None,
        size: Optional[str] = None,
        quant: Optional[str] = None,
        sort_by_size: bool = True,
    ) -> List[ModelEntry]:
        """
        åˆ—å‡ºç¬¦åˆæ¡ä»¶çš„æ¨¡å‹
        
        Args:
            family: è¿‡æ»¤æ¨¡å‹ç³»åˆ—ï¼ˆqwen, llamaï¼‰
            version: è¿‡æ»¤ç‰ˆæœ¬å·ï¼ˆ2.5, 3.2ï¼‰
            size: è¿‡æ»¤æ¨¡å‹å¤§å°ï¼ˆ0.5B, 7Bï¼‰
            quant: è¿‡æ»¤é‡åŒ–ç±»å‹ï¼ˆfp8, int8ï¼‰
            sort_by_size: æ˜¯å¦æŒ‰æ¨¡å‹å¤§å°æ’åº
            
        Returns:
            ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹åˆ—è¡¨
        """
        results = []
        
        for entry in self._models.values():
            if family and entry.family.lower() != family.lower():
                continue
            if version and entry.version != version:
                continue
            if size and entry.size.lower() != size.lower():
                continue
            if quant and entry.quant.lower() != quant.lower():
                continue
            results.append(entry)
        
        if sort_by_size:
            # æŒ‰æ¨¡å‹å¤§å°æ’åº
            def size_key(e: ModelEntry) -> float:
                s = e.size.upper().replace("B", "")
                try:
                    return float(s)
                except ValueError:
                    return 0.0
            results.sort(key=size_key)
        
        return results
    
    def keys(
        self,
        *,
        family: Optional[str] = None,
        quant: Optional[str] = None,
    ) -> List[str]:
        """
        è·å–æ¨¡å‹ key åˆ—è¡¨
        
        Args:
            family: è¿‡æ»¤æ¨¡å‹ç³»åˆ—
            quant: è¿‡æ»¤é‡åŒ–ç±»å‹
            
        Returns:
            æ¨¡å‹ key åˆ—è¡¨
        """
        return [e.key for e in self.list(family=family, quant=quant)]
    
    def to_dict(self) -> Dict[str, Dict[str, Any]]:
        """å¯¼å‡ºä¸ºå­—å…¸"""
        return {k: v.to_dict() for k, v in self._models.items()}


# å…¨å±€æ¨¡å‹æ³¨å†Œè¡¨å®ä¾‹
model_registry = ModelRegistry()


# =============================================================================
# æ¨¡å‹è·¯å¾„å’Œæ£€æŸ¥ä¾¿æ·å‡½æ•°
# =============================================================================

def get_model_registry() -> ModelRegistry:
    """è·å–å…¨å±€æ¨¡å‹æ³¨å†Œè¡¨"""
    return model_registry


def get_model_info(key: str) -> Dict[str, Any]:
    """
    è·å–æ¨¡å‹ä¿¡æ¯
    
    Args:
        key: æ¨¡å‹ key
        
    Returns:
        æ¨¡å‹ä¿¡æ¯å­—å…¸
        
    Raises:
        KeyError: æ¨¡å‹ä¸å­˜åœ¨
    """
    entry = model_registry.get(key)
    if entry is None:
        raise KeyError(f"æ¨¡å‹ä¸å­˜åœ¨: {key}. å¯ç”¨æ¨¡å‹: {', '.join(model_registry.keys())}")
    return entry.to_dict()


def list_models(
    *,
    family: Optional[str] = None,
    quant: Optional[str] = None,
) -> List[str]:
    """
    åˆ—å‡ºæ¨¡å‹ key
    
    Args:
        family: è¿‡æ»¤æ¨¡å‹ç³»åˆ—
        quant: è¿‡æ»¤é‡åŒ–ç±»å‹
        
    Returns:
        æ¨¡å‹ key åˆ—è¡¨
    """
    return model_registry.keys(family=family, quant=quant)


def build_model_dir_name(
    family: str,
    version: str,
    size: str,
    quant: str,
) -> str:
    """
    æ„å»ºæ¨¡å‹ç›®å½•å
    
    æ ¼å¼: {Family}{Version}-{Size}-{QUANT}
    ä¾‹å¦‚: Qwen2.5-7B-FP8, Llama3.2-1B-INT8
    
    Args:
        family: æ¨¡å‹ç³»åˆ—ï¼ˆqwen, llamaï¼‰
        version: ç‰ˆæœ¬å·ï¼ˆ2.5, 3.2ï¼‰
        size: æ¨¡å‹å¤§å°ï¼ˆ7B, 1Bï¼‰
        quant: é‡åŒ–ç±»å‹ï¼ˆfp8, int8ï¼‰
        
    Returns:
        ç›®å½•åå­—ç¬¦ä¸²
    """
    # é¦–å­—æ¯å¤§å†™
    family_cap = family.capitalize()
    size_upper = size.upper()
    quant_upper = "FP8" if quant.lower() == "fp8" else "INT8"
    return f"{family_cap}{version}-{size_upper}-{quant_upper}"


def parse_model_key(key: str) -> Dict[str, str]:
    """
    è§£ææ¨¡å‹ key
    
    Args:
        key: æ¨¡å‹ keyï¼ˆå¦‚ "qwen2.5-7b-fp8"ï¼‰
        
    Returns:
        è§£æç»“æœå­—å…¸ {"family", "version", "size", "quant"}
        
    Raises:
        ValueError: æ— æ³•è§£æ
    """
    # å°è¯•ä»æ³¨å†Œè¡¨è·å–
    entry = model_registry.get(key)
    if entry:
        return {
            "family": entry.family,
            "version": entry.version,
            "size": entry.size,
            "quant": entry.quant,
        }
    
    # å°è¯•æ‰‹åŠ¨è§£æ: {family}{version}-{size}-{quant}
    # ä¾‹å¦‚: qwen2.5-7b-fp8, llama3.2-1b-int8
    import re
    match = re.match(r'^([a-z]+)([\d.]+)-(\d+\.?\d*b)-([a-z0-9]+)$', key.lower())
    if match:
        return {
            "family": match.group(1),
            "version": match.group(2),
            "size": match.group(3).upper(),
            "quant": match.group(4),
        }
    
    raise ValueError(f"æ— æ³•è§£ææ¨¡å‹ key: {key}")


def check_quant_support(quant: str) -> Tuple[bool, str]:
    """
    æ£€æŸ¥å½“å‰ GPU æ˜¯å¦æ”¯æŒæŒ‡å®šçš„é‡åŒ–ç±»å‹
    
    Args:
        quant: é‡åŒ–ç±»å‹ï¼ˆfp8, int8ï¼‰
        
    Returns:
        (supported, message)
    """
    quant_lower = quant.lower()
    
    if quant_lower == "int8":
        # INT8: CC >= 8.0 (Ampere+)
        if hw_info.cc_major >= 8:
            return True, f"GPU {hw_info.gpu_name} (CC {hw_info.cc_tag}) supports INT8"
        return False, (
            f"GPU {hw_info.gpu_name} (CC {hw_info.cc_tag}) does not support efficient INT8 Tensor Core.\n"
            f"INT8 requires Ampere (CC 8.0+) or newer."
        )
    
    elif quant_lower == "fp8":
        # FP8: CC >= 8.9 (Ada/Hopper+)
        if hw_info.supports_fp8:
            return True, f"GPU {hw_info.gpu_name} (CC {hw_info.cc_tag}) supports FP8"
        return False, (
            f"GPU {hw_info.gpu_name} (CC {hw_info.cc_tag}) does not support native FP8.\n"
            f"FP8 requires Ada (CC 8.9+) or Hopper (CC 9.0+) or newer."
        )
    
    else:
        return False, f"Unknown quantization type: {quant}"


def get_model_local_path(
    key: str,
    checkpoint_dir: Union[str, Path] = "checkpoints",
) -> Path:
    """
    è·å–æ¨¡å‹æœ¬åœ°è·¯å¾„
    
    Args:
        key: æ¨¡å‹ key
        checkpoint_dir: checkpoints æ ¹ç›®å½•
        
    Returns:
        æ¨¡å‹æœ¬åœ°ç›®å½•è·¯å¾„
        
    Raises:
        KeyError: æ¨¡å‹ä¸å­˜åœ¨
    """
    entry = model_registry[key]
    return Path(checkpoint_dir) / entry.local_name


def check_model_downloaded(
    key: str,
    checkpoint_dir: Union[str, Path] = "checkpoints",
) -> Tuple[bool, str]:
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
    
    Args:
        key: æ¨¡å‹ key
        checkpoint_dir: checkpoints æ ¹ç›®å½•
        
    Returns:
        (downloaded, message)
    """
    try:
        local_path = get_model_local_path(key, checkpoint_dir)
    except KeyError as e:
        return False, str(e)
    
    if local_path.is_dir() and (local_path / "config.json").exists():
        return True, f"Model exists: {local_path}"
    return False, f"Model not found: {local_path}"


# #############################################################################
#
#  PART 5: SlideSparse é…ç½®ä¸ç»´åº¦è®¡ç®—
#
#  æœ¬éƒ¨åˆ†æä¾› SlideSparse ç¨€ç–æ ¼å¼çš„é…ç½®å’Œç»´åº¦è®¡ç®—åŠŸèƒ½ã€‚
#
#  ç¨€ç–æ ¼å¼è¯´æ˜ï¼š
#  =============
#  Z:L è¡¨ç¤ºæ¯ L ä¸ªè¿ç»­å…ƒç´ ä¸­è‡³å°‘æœ‰ Z ä¸ªé›¶
#  ä¾‹å¦‚ 2:8 è¡¨ç¤ºæ¯ 8 ä¸ªå…ƒç´ è‡³å°‘ 2 ä¸ªé›¶ï¼ˆç¨€ç–åº¦ â‰¥ 25%ï¼‰
#
#  ä¸»è¦åŠŸèƒ½ï¼š
#  =========
#  - SlideSparseConfig: é…ç½®æ•°æ®ç±»
#  - compute_output_k: è®¡ç®— slided åçš„ K ç»´åº¦
#  - compute_compressed_k: è®¡ç®— 2:4 å‹ç¼©åçš„ K ç»´åº¦
#  - get_model_nk_sizes: æå–æ¨¡å‹çš„ NK å°ºå¯¸
#
# #############################################################################


# =============================================================================
# SlideSparse é…ç½®
# =============================================================================

@dataclass
class SlideSparseConfig:
    """
    SlideSparse è½¬æ¢é…ç½®
    
    ç¨€ç–æ ¼å¼è¯´æ˜ï¼š
        Z:L è¡¨ç¤ºæ¯ L ä¸ªè¿ç»­å…ƒç´ ä¸­è‡³å°‘æœ‰ Z ä¸ªé›¶
        ä¾‹å¦‚ 2:8 è¡¨ç¤ºæ¯ 8 ä¸ªå…ƒç´ è‡³å°‘ 2 ä¸ªé›¶ï¼ˆç¨€ç–åº¦ â‰¥ 25%ï¼‰
    
    Attributes:
        Z: æ¯ç»„ä¸­è‡³å°‘çš„é›¶å…ƒç´ æ•°é‡ï¼ˆå½“å‰å›ºå®šä¸º 2ï¼‰
        L: ç¨€ç–ç»„çš„å¤§å°ï¼ˆå¦‚ 6, 8, 10ï¼‰
        N: å†…éƒ¨å‚æ•°ï¼ŒN = L // 2
        window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼Œå›ºå®šä¸º 4ï¼ˆå¯¹åº” 2:4 ç¡¬ä»¶ï¼‰
        stride: æ»‘åŠ¨æ­¥é•¿ï¼Œå›ºå®šä¸º 2
        num_windows: æ¯ç»„å†…çš„çª—å£æ•°é‡ï¼Œ= N - 1
        expand_ratio: K ç»´åº¦çš„æ‰©å±•æ¯”ä¾‹
    """
    Z: int = 2
    L: int = 8
    
    # æ´¾ç”Ÿå‚æ•°ï¼ˆåœ¨ __post_init__ ä¸­è®¡ç®—ï¼‰
    N: int = field(init=False)
    window_size: int = field(init=False)
    stride: int = field(init=False)
    num_windows: int = field(init=False)
    expand_ratio: float = field(init=False)
    in_group_size: int = field(init=False)
    out_group_size: int = field(init=False)
    
    def __post_init__(self):
        if self.Z != 2:
            raise ValueError(f"å½“å‰ä»…æ”¯æŒ Z=2 çš„ç¨€ç–æ ¼å¼ï¼Œæ”¶åˆ° Z={self.Z}")
        if self.L % 2 != 0:
            raise ValueError(f"L å¿…é¡»ä¸ºå¶æ•°ï¼Œæ”¶åˆ° L={self.L}")
        if self.L < 4:
            raise ValueError(f"L å¿…é¡» >= 4ï¼Œæ”¶åˆ° L={self.L}")
        
        self.N = self.L // 2
        self.window_size = 4
        self.stride = 2
        self.num_windows = self.N - 1
        self.expand_ratio = (self.num_windows * self.window_size) / self.L
        self.in_group_size = self.L
        self.out_group_size = self.num_windows * self.window_size
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "Z": self.Z, "L": self.L, "N": self.N,
            "window_size": self.window_size, "stride": self.stride,
            "num_windows": self.num_windows, "expand_ratio": self.expand_ratio,
            "in_group_size": self.in_group_size, "out_group_size": self.out_group_size,
        }
    
    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "SlideSparseConfig":
        return cls(Z=d["Z"], L=d["L"])
    
    def __repr__(self):
        return (f"SlideSparseConfig(Z={self.Z}, L={self.L}, N={self.N}, "
                f"expand={self.expand_ratio:.3f})")


def compute_output_k(k_in: int, config: SlideSparseConfig, align_to: int = 32) -> Tuple[int, int]:
    """
    è®¡ç®—æ»‘åŠ¨æ‰©å±•åçš„ K ç»´åº¦
    
    Args:
        k_in: åŸå§‹è¾“å…¥ç»´åº¦ K
        config: SlideSparse é…ç½®
        align_to: è¾“å‡ºå¯¹é½è¦æ±‚ï¼ˆé»˜è®¤ 32
    
    Returns:
        (k_padded, k_out):
            - k_padded: padding åçš„è¾“å…¥ Kï¼ˆL çš„å€æ•°ï¼‰
            - k_out: æ»‘åŠ¨æ‰©å±•åçš„è¾“å‡º Kï¼ˆå¯¹é½åˆ° align_toï¼‰
    """
    L = config.L
    k_padded = ((k_in + L - 1) // L) * L
    num_groups = k_padded // L
    k_out_raw = num_groups * config.out_group_size
    k_out = ((k_out_raw + align_to - 1) // align_to) * align_to
    return k_padded, k_out


def compute_compressed_k(k_slided: int) -> int:
    """
    è®¡ç®— 2:4 å‹ç¼©åçš„ K ç»´åº¦
    
    2:4 å‹ç¼©å°† K å‡åŠï¼ˆæ¯ 4 ä¸ªå…ƒç´ å‹ç¼©ä¸º 2 ä¸ªå€¼ + metadataï¼‰
    """
    return k_slided // 2


# =============================================================================
# æ¨¡å‹ NK Size æå–å·¥å…·
# =============================================================================

# çº¿æ€§å±‚ç±»å‹æ˜ å°„ï¼ˆæ ‡å‡† HuggingFace æ ¼å¼ï¼‰
LINEAR_LAYER_TYPES = {
    # Attention å±‚ - æ˜ å°„åˆ°ç»Ÿä¸€çš„é”®å
    "q_proj": "qkv",       # Q projection -> qkvï¼ˆåˆå¹¶æŠ¥å‘Šï¼‰
    "k_proj": "qkv",       # K projection -> qkv
    "v_proj": "qkv",       # V projection -> qkv
    "qkv_proj": "qkv",     # QKV èåˆ
    "o_proj": "wo",        # Output projection
    # MLP å±‚
    "gate_proj": "w13",    # Gate projection (w1) -> w13ï¼ˆåˆå¹¶æŠ¥å‘Šï¼‰
    "up_proj": "w13",      # Up projection (w3) -> w13
    "gate_up_proj": "w13", # Gate+Up èåˆ
    "down_proj": "w2",     # Down projection (w2)
}


def get_model_nk_sizes(
    model_path: Union[str, Path],
    *,
    layer_index: int = 0,
) -> Dict[str, Tuple[int, int]]:
    """
    ä» safetensor æ–‡ä»¶æå–æ¨¡å‹çº¿æ€§å±‚çš„ N,K å°ºå¯¸
    
    Args:
        model_path: æ¨¡å‹ç›®å½•æˆ– safetensor æ–‡ä»¶è·¯å¾„
        layer_index: ä½¿ç”¨å“ªä¸€å±‚çš„å°ºå¯¸ï¼ˆé»˜è®¤ç¬¬ 0 å±‚ï¼Œå› ä¸ºæ‰€æœ‰å±‚å°ºå¯¸ç›¸åŒï¼‰
    
    Returns:
        Dict[str, Tuple[int, int]]: å±‚ç±»å‹ -> (N, K) å°ºå¯¸
        é”®ä¸º: "qkv", "wo", "w13", "w2"
        
    Example:
        >>> sizes = get_model_nk_sizes("checkpoints/Qwen2.5-7B-INT8")
        >>> sizes
        {
            'qkv': (4608, 3584),   # Q+K+V åˆå¹¶åçš„ N, å…±åŒçš„ K
            'wo': (3584, 3584),    # Output projection
            'w13': (18944, 3584),  # Gate+Up åˆå¹¶åçš„ N
            'w2': (3584, 9472),    # Down projection
        }
    """
    try:
        from safetensors import safe_open
    except ImportError:
        raise ImportError("safetensors is required: pip install safetensors")
    
    model_path = Path(model_path)
    
    # æ‰¾åˆ° safetensor æ–‡ä»¶
    if model_path.is_file() and model_path.suffix == ".safetensors":
        safetensor_files = [model_path]
    elif model_path.is_dir():
        safetensor_files = list(model_path.glob("*.safetensors"))
        if not safetensor_files:
            raise FileNotFoundError(f"No safetensors files in {model_path}")
    else:
        raise FileNotFoundError(f"Path not found: {model_path}")
    
    # æ”¶é›†å°ºå¯¸
    # æ ¼å¼: { "qkv": {"q": (N,K), "k": (N,K), "v": (N,K)}, "wo": (N,K), ... }
    raw_sizes: Dict[str, Dict[str, Tuple[int, int]]] = {
        "qkv": {}, "wo": {}, "w13": {}, "w2": {}
    }
    
    target_layer_prefix = f".{layer_index}."  # e.g., ".0."
    
    for sf_path in safetensor_files:
        with safe_open(sf_path, framework="pt") as f:
            for key in f.keys():
                # åªå¤„ç†ç›®æ ‡å±‚
                if target_layer_prefix not in key:
                    continue
                if "weight" not in key.lower() or "scale" in key.lower():
                    continue
                
                tensor = f.get_tensor(key)
                if tensor.dim() != 2:
                    continue
                
                N, K = tensor.shape
                
                # è¯†åˆ«å±‚ç±»å‹
                key_lower = key.lower()
                for pattern, group in LINEAR_LAYER_TYPES.items():
                    if pattern in key_lower:
                        if group == "qkv":
                            # åˆ†åˆ«è®°å½• q/k/v
                            if "q_proj" in key_lower or "qkv_proj" in key_lower:
                                raw_sizes["qkv"]["q"] = (N, K)
                            if "k_proj" in key_lower:
                                raw_sizes["qkv"]["k"] = (N, K)
                            if "v_proj" in key_lower:
                                raw_sizes["qkv"]["v"] = (N, K)
                            if "qkv_proj" in key_lower:
                                # èåˆçš„ QKV
                                raw_sizes["qkv"]["qkv"] = (N, K)
                        elif group == "w13":
                            if "gate_proj" in key_lower or "gate_up_proj" in key_lower:
                                raw_sizes["w13"]["gate"] = (N, K)
                            if "up_proj" in key_lower:
                                raw_sizes["w13"]["up"] = (N, K)
                            if "gate_up_proj" in key_lower:
                                raw_sizes["w13"]["gate_up"] = (N, K)
                        else:
                            raw_sizes[group] = (N, K)
                        break
    
    # åˆå¹¶å°ºå¯¸
    result: Dict[str, Tuple[int, int]] = {}
    
    # QKV å¤„ç†
    qkv_data = raw_sizes["qkv"]
    if "qkv" in qkv_data:
        # èåˆçš„ QKV
        result["qkv"] = qkv_data["qkv"]
    elif qkv_data:
        # åˆ†ç¦»çš„ Q, K, V - åˆå¹¶ N, K åº”è¯¥ç›¸åŒ
        q_size = qkv_data.get("q", (0, 0))
        k_size = qkv_data.get("k", (0, 0))
        v_size = qkv_data.get("v", (0, 0))
        total_n = q_size[0] + k_size[0] + v_size[0]
        common_k = q_size[1] or k_size[1] or v_size[1]
        result["qkv"] = (total_n, common_k)
    
    # W13 å¤„ç†
    w13_data = raw_sizes["w13"]
    if "gate_up" in w13_data:
        result["w13"] = w13_data["gate_up"]
    elif w13_data:
        gate_size = w13_data.get("gate", (0, 0))
        up_size = w13_data.get("up", (0, 0))
        total_n = gate_size[0] + up_size[0]
        common_k = gate_size[1] or up_size[1]
        result["w13"] = (total_n, common_k)
    
    # WO å’Œ W2 ç›´æ¥ä½¿ç”¨
    if isinstance(raw_sizes["wo"], tuple):
        result["wo"] = raw_sizes["wo"]
    if isinstance(raw_sizes["w2"], tuple):
        result["w2"] = raw_sizes["w2"]
    
    return result


def get_model_nk_sizes_slided(
    nk_sizes: Dict[str, Tuple[int, int]],
    Z: int,
    L: int,
    align_to: int = 32,
) -> Dict[str, Tuple[int, int]]:
    """
    è®¡ç®— slide åçš„ N,K å°ºå¯¸
    
    Args:
        nk_sizes: åŸå§‹ N,K å°ºå¯¸ï¼ˆæ¥è‡ª get_model_nk_sizesï¼‰
        Z: ç¨€ç–åº¦åˆ†å­
        L: ç¨€ç–åº¦åˆ†æ¯
        align_to: å¯¹é½è¦æ±‚
    
    Returns:
        Dict[str, Tuple[int, int]]: å±‚ç±»å‹ -> slide åçš„ (N, K_out)
        
    Example:
        >>> sizes = get_model_nk_sizes("checkpoints/Qwen2.5-7B-INT8")
        >>> slided = get_model_nk_sizes_slided(sizes, Z=2, L=8)
        >>> slided
        {
            'qkv': (4608, 5376),   # K æ‰©å±• 1.5x
            'wo': (3584, 5376),
            'w13': (18944, 5376),
            'w2': (3584, 14208),
        }
    """
    config = SlideSparseConfig(Z=Z, L=L)
    result = {}
    
    for layer_type, (N, K) in nk_sizes.items():
        _, k_out = compute_output_k(K, config, align_to)
        result[layer_type] = (N, k_out)
    
    return result


def get_model_nk_sizes_compressed(
    nk_sizes_slided: Dict[str, Tuple[int, int]],
) -> Dict[str, Tuple[int, int]]:
    """
    è®¡ç®— 2:4 å‹ç¼©åçš„ N,K å°ºå¯¸
    
    Args:
        nk_sizes_slided: slide åçš„ N,K å°ºå¯¸
    
    Returns:
        Dict[str, Tuple[int, int]]: å±‚ç±»å‹ -> å‹ç¼©åçš„ (N, K_compressed)
    """
    result = {}
    for layer_type, (N, K) in nk_sizes_slided.items():
        result[layer_type] = (N, compute_compressed_k(K))
    return result


def print_model_nk_summary(
    model_path: Union[str, Path],
    Z: int = 2,
    L: int = 8,
    align_to: int = 32,
) -> None:
    """
    æ‰“å°æ¨¡å‹çš„ NK å°ºå¯¸æ‘˜è¦ï¼ˆåŸå§‹ã€slideã€å‹ç¼©ï¼‰
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        Z, L: ç¨€ç–å‚æ•°
        align_to: å¯¹é½è¦æ±‚
    """
    print(f"Model NK Size Summary: {model_path}")
    print(f"SlideSparse Config: Z={Z}, L={L}, align={align_to}")
    print("=" * 70)
    
    config = SlideSparseConfig(Z=Z, L=L)
    print(f"Expand ratio: {config.expand_ratio:.4f}")
    print()
    
    original = get_model_nk_sizes(model_path)
    slided = get_model_nk_sizes_slided(original, Z, L, align_to)
    compressed = get_model_nk_sizes_compressed(slided)
    
    print(f"{'Layer':<8} {'Original N,K':<18} {'Slided N,K':<18} {'Compressed N,K':<18}")
    print("-" * 70)
    
    for layer in ["qkv", "wo", "w13", "w2"]:
        if layer in original:
            orig = original[layer]
            slid = slided[layer]
            comp = compressed[layer]
            print(f"{layer:<8} {str(orig):<18} {str(slid):<18} {str(comp):<18}")


# =============================================================================
# ç¨€ç–é…ç½®è§£æï¼ˆç¯å¢ƒå˜é‡ SPARSITYï¼‰
# =============================================================================

# ç¼“å­˜è§£æç»“æœ
_sparsity_config_cache = None


def parse_sparsity_env(sparsity_str: str = None) -> Tuple[int, int, float]:
    """
    è§£æç¨€ç–æ ¼å¼é…ç½®
    
    Args:
        sparsity_str: ç¨€ç–æ ¼å¼å­—ç¬¦ä¸²ï¼ˆå¦‚ "2_8"ï¼‰ï¼Œå¦‚æœä¸º None åˆ™è¯»å–ç¯å¢ƒå˜é‡ SPARSITY
    
    Returns:
        (Z, L, expand_ratio) å…ƒç»„:
            - Z: æ¯ç»„ä¸­çš„é›¶å…ƒç´ æ•°é‡ï¼ˆå›ºå®šä¸º 2ï¼‰
            - L: ç¨€ç–ç»„çš„å¤§å°ï¼ˆå¦‚ 6, 8, 10ï¼‰
            - expand_ratio: K ç»´åº¦æ‰©å±•æ¯”ä¾‹ = L / (L - Z)
        
        å¦‚æœæœªè®¾ç½®æˆ–æ ¼å¼é”™è¯¯ï¼Œè¿”å›é»˜è®¤å€¼ (2, 8, 1.333...)
    """
    if sparsity_str is None:
        sparsity_str = os.environ.get("SPARSITY", "2_8")
    
    try:
        parts = sparsity_str.split("_")
        if len(parts) != 2:
            raise ValueError(f"Invalid SPARSITY format: {sparsity_str}")
        
        Z = int(parts[0])
        L = int(parts[1])
        
        if Z != 2:
            Z = 2  # ä»…æ”¯æŒ Z=2
        
        if L < 4 or L % 2 != 0:
            L = 8  # å¿…é¡» >= 4 ä¸”ä¸ºå¶æ•°
        
        expand_ratio = L / (L - Z)
        return (Z, L, expand_ratio)
        
    except (ValueError, AttributeError):
        return (2, 8, 8 / 6)


def get_sparsity_config_cached() -> Tuple[int, int, float]:
    """
    è·å–ç¨€ç–æ ¼å¼é…ç½®ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    ä»ç¯å¢ƒå˜é‡ SPARSITY è§£æï¼Œç»“æœä¼šè¢«ç¼“å­˜
    """
    global _sparsity_config_cache
    
    if _sparsity_config_cache is None:
        _sparsity_config_cache = parse_sparsity_env()
    
    return _sparsity_config_cache


def clear_sparsity_cache() -> None:
    """æ¸…é™¤ç¨€ç–é…ç½®ç¼“å­˜ï¼ˆç”¨äºæµ‹è¯•æ—¶é‡æ–°è¯»å–ç¯å¢ƒå˜é‡ï¼‰"""
    global _sparsity_config_cache
    _sparsity_config_cache = None


def get_sparsity_str(Z: int = None, L: int = None) -> str:
    """
    è·å–ç¨€ç–æ ¼å¼å­—ç¬¦ä¸²
    
    Args:
        Z, L: å¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™ä»ç¼“å­˜/ç¯å¢ƒå˜é‡è·å–
    
    Returns:
        æ ¼å¼å¦‚ "2_8"ã€"2_6" ç­‰
    """
    if Z is None or L is None:
        Z, L, _ = get_sparsity_config_cached()
    return f"{Z}_{L}"


# =============================================================================
# SlideSparse æ¨¡å‹è·¯å¾„è§£æ
# =============================================================================

def get_slidesparse_checkpoints_dir() -> Path:
    """
    è·å– SlideSparse checkpoints ç›®å½•
    
    Returns:
        checkpoints_slidesparse ç›®å½•çš„ç»å¯¹è·¯å¾„
    """
    # ä»é¡¹ç›®æ ¹ç›®å½•å¯»æ‰¾
    project_root = Path(__file__).parent.parent
    slidesparse_dir = project_root / "checkpoints_slidesparse"
    
    if slidesparse_dir.exists():
        return slidesparse_dir
    
    # å°è¯•ä»å½“å‰å·¥ä½œç›®å½•
    cwd_dir = Path.cwd() / "checkpoints_slidesparse"
    if cwd_dir.exists():
        return cwd_dir
    
    # è¿”å›é»˜è®¤è·¯å¾„ï¼ˆå³ä½¿ä¸å­˜åœ¨ï¼‰
    return slidesparse_dir


def resolve_slidesparse_model_path(
    base_model_path: Union[str, Path],
    sparsity: str = None,
) -> Optional[Path]:
    """
    æ ¹æ®åŸºç¡€æ¨¡å‹è·¯å¾„å’Œç¨€ç–é…ç½®ï¼Œè§£æå¯¹åº”çš„ SlideSparse æ¨¡å‹è·¯å¾„
    
    å‘½åçº¦å®š:
        åŸºç¡€æ¨¡å‹: checkpoints/Qwen2.5-0.5B-FP8/
        SlideSparse: checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_8/
    
    Args:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¦‚ checkpoints/Qwen2.5-0.5B-FP8ï¼‰
        sparsity: ç¨€ç–é…ç½®ï¼ˆå¦‚ "2_8"ï¼‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
    
    Returns:
        SlideSparse æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
    """
    base_path = Path(base_model_path)
    model_name = base_path.name  # e.g., "Qwen2.5-0.5B-FP8"
    
    if sparsity is None:
        sparsity = get_sparsity_str()
    
    # æ„å»º SlideSparse æ¨¡å‹åç§°
    slidesparse_name = f"{model_name}-SlideSparse-{sparsity}"
    slidesparse_path = get_slidesparse_checkpoints_dir() / slidesparse_name
    
    if slidesparse_path.exists() and slidesparse_path.is_dir():
        return slidesparse_path
    
    return None


def find_slidesparse_model(
    dtype: str = "FP8",
    sparsity: str = None,
) -> Optional[Path]:
    """
    æŸ¥æ‰¾ SlideSparse æ¨¡å‹ï¼ˆä¼˜å…ˆé€‰æ‹©è¾ƒå°çš„æ¨¡å‹ï¼‰
    
    æœç´¢é¡ºåº: Qwen2.5-0.5B > Llama3.2-1B > Qwen2.5-1.5B > ...
    
    Args:
        dtype: æ•°æ®ç±»å‹ï¼ˆ"FP8" æˆ– "INT8"ï¼‰
        sparsity: ç¨€ç–é…ç½®ï¼ˆå¦‚ "2_8"ï¼‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
    
    Returns:
        æ‰¾åˆ°çš„ SlideSparse æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å› None
    """
    if sparsity is None:
        sparsity = get_sparsity_str()
    
    slidesparse_dir = get_slidesparse_checkpoints_dir()
    if not slidesparse_dir.exists():
        return None
    
    # æœç´¢ä¼˜å…ˆçº§ï¼ˆè¾ƒå°çš„æ¨¡å‹ä¼˜å…ˆï¼‰
    priority_patterns = [
        "Qwen2.5-0.5B",
        "Llama3.2-1B",
        "Qwen2.5-1.5B",
        "BitNet-2B",
        "Qwen2.5-3B",
        "Llama3.2-3B",
        "Qwen2.5-7B",
        "Qwen2.5-14B",
    ]
    
    dtype_upper = dtype.upper()
    
    for pattern in priority_patterns:
        # æ„å»ºé¢„æœŸçš„ç›®å½•å
        expected_name = f"{pattern}-{dtype_upper}-SlideSparse-{sparsity}"
        model_path = slidesparse_dir / expected_name
        
        if model_path.exists() and model_path.is_dir():
            return model_path
    
    # å¦‚æœæŒ‰ä¼˜å…ˆçº§æœªæ‰¾åˆ°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
    for model_dir in slidesparse_dir.iterdir():
        if not model_dir.is_dir():
            continue
        name = model_dir.name
        if dtype_upper in name and f"SlideSparse-{sparsity}" in name:
            return model_dir
    
    return None


# =============================================================================
# å¯¼å‡º
# =============================================================================

__all__ = [
    # æ•°æ®ç±»å‹
    "normalize_dtype",
    "DTYPE_ALIASES",
    
    # =========================================================================
    # PART 1: CUDA ç¼–è¯‘ã€é“¾æ¥ã€åº“åŠ è½½å·¥å…·
    # =========================================================================
    
    # ç³»ç»Ÿåº“è·¯å¾„
    "SYSTEM_LIB_PATHS",
    "get_system_lib_path",
    
    # NVCC æ¶æ„æ ‡å¿—
    "SUPPORTED_ARCHITECTURES",
    "get_nvcc_arch_flags",
    "get_current_arch_flag",
    
    # é“¾æ¥åº“é…ç½®
    "SUPPORTED_BACKENDS",
    "get_backend_ldflags",
    "BACKEND_LDFLAGS",
    "CUBLASLT_LDFLAGS",
    "CUSPARSELT_LDFLAGS",
    
    # è¿è¡Œæ—¶åº“åŠ è½½
    "ensure_cublaslt_loaded",
    "ensure_cusparselt_loaded",
    "BACKEND_LOADERS",
    
    # ç¼–è¯‘è¾…åŠ©
    "DEFAULT_CFLAGS",
    "DEFAULT_CUDA_CFLAGS",
    "should_rebuild",
    "clean_build_artifacts",
    
    # ç¼–è¯‘å‡½æ•°
    "build_cuda_extension",       # PyTorch æ‰©å±•ç¼–è¯‘
    "build_cuda_extension_direct", # ç›´æ¥ nvcc ç¼–è¯‘
    "load_cuda_extension",        # é«˜çº§åŠ è½½æ¥å£
    
    # =========================================================================
    # PART 2: ç¡¬ä»¶ä¿¡æ¯
    # =========================================================================
    "HardwareInfo",
    "hw_info",
    # ä¾¿æ·å‡½æ•°
    "get_gpu_name",
    "get_gpu_cc",
    "get_python_version_tag",
    "get_cuda_ver",
    "get_arch_tag",
    "get_sm_code",
    "print_system_info",
    
    # =========================================================================
    # PART 3: æ–‡ä»¶åä¸ IO
    # =========================================================================
    # æ–‡ä»¶åæ„å»º
    "build_filename",
    "build_stem",
    "build_dir_name",
    # æ–‡ä»¶æŸ¥æ‰¾
    "find_file",
    "find_files",
    "find_dir",
    # æ¨¡å—åŠ è½½
    "load_module",
    "clear_module_cache",
    # ç®—æ³•æŸ¥è¡¨
    "lookup_best_cublaslt_alg",
    "decode_cublaslt_algo_data",
    "lookup_best_cusparselt_alg",
    # æ•°æ®ä¿å­˜/åŠ è½½
    "save_json",
    "load_json",
    "save_csv",
    # ç›®å½•ç®¡ç†
    "ensure_result_dir",
    
    # =========================================================================
    # PART 4: æ¨¡å‹ä¿¡æ¯ç®¡ç†
    # =========================================================================
    "MODEL_SIZE_GB",
    "ModelEntry",
    "ModelRegistry",
    "model_registry",
    "get_model_registry",
    "get_model_info",
    "list_models",
    "build_model_dir_name",
    "parse_model_key",
    "check_quant_support",
    "get_model_local_path",
    "check_model_downloaded",
    
    # =========================================================================
    # PART 5: SlideSparse é…ç½®ä¸ç»´åº¦è®¡ç®—
    # =========================================================================
    "SlideSparseConfig",
    "compute_output_k",
    "compute_compressed_k",
    # æ¨¡å‹ NK Size å·¥å…·
    "LINEAR_LAYER_TYPES",
    "get_model_nk_sizes",
    "get_model_nk_sizes_slided",
    "get_model_nk_sizes_compressed",
    "print_model_nk_summary",
    # ç¨€ç–é…ç½®è§£æ
    "parse_sparsity_env",
    "get_sparsity_config_cached",
    "clear_sparsity_cache",
    "get_sparsity_str",
    # SlideSparse æ¨¡å‹è·¯å¾„è§£æ
    "get_slidesparse_checkpoints_dir",
    "resolve_slidesparse_model_path",
    "find_slidesparse_model",
]


# =============================================================================
# CLI
# =============================================================================

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="SlideSparse ç»Ÿä¸€å·¥å…·åº“",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    # æ˜¾ç¤ºç¡¬ä»¶ä¿¡æ¯
    python -m slidesparse.utils info
    
    # ç”Ÿæˆæ–‡ä»¶å
    python -m slidesparse.utils name cuBLASLt --dtype FP8E4M3 --ext .so
    
    # æŸ¥æ‰¾æ–‡ä»¶
    python -m slidesparse.utils find cuBLASLt --dtype FP8E4M3 --dir build
"""
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å‘½ä»¤")
    
    # info å‘½ä»¤
    info_parser = subparsers.add_parser("info", help="æ˜¾ç¤ºç¡¬ä»¶ä¿¡æ¯")
    info_parser.add_argument("--json", action="store_true", help="è¾“å‡º JSON æ ¼å¼")
    
    # name å‘½ä»¤
    name_parser = subparsers.add_parser("name", help="ç”Ÿæˆæ–‡ä»¶å")
    name_parser.add_argument("prefix", help="æ–‡ä»¶å‰ç¼€")
    name_parser.add_argument("--dtype", help="æ•°æ®ç±»å‹")
    name_parser.add_argument("--ext", default="", help="æ–‡ä»¶æ‰©å±•å")
    
    # find å‘½ä»¤
    find_parser = subparsers.add_parser("find", help="æŸ¥æ‰¾æ–‡ä»¶")
    find_parser.add_argument("prefix", help="æ–‡ä»¶å‰ç¼€")
    find_parser.add_argument("--dtype", help="æ•°æ®ç±»å‹")
    find_parser.add_argument("--dir", default=".", help="æœç´¢ç›®å½•")
    find_parser.add_argument("--ext", help="æ–‡ä»¶æ‰©å±•å")
    
    args = parser.parse_args()
    
    if args.command == "info":
        if args.json:
            import json
            print(json.dumps(hw_info.to_dict(), indent=2, ensure_ascii=False))
        else:
            hw_info.print_info()
    
    elif args.command == "name":
        name = build_filename(args.prefix, dtype=args.dtype, ext=args.ext)
        print(name)
    
    elif args.command == "find":
        result = find_file(args.prefix, dtype=args.dtype, search_dir=args.dir, ext=args.ext)
        if result:
            print(result)
        else:
            print(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶", file=sys.stderr)
            sys.exit(1)
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
