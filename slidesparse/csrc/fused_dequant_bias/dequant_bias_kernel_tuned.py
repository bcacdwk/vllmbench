"""
Auto-generated Dequant + Bias Triton Kernel (Tuned, No Autotune)
Generated by autotune_dequant_bias.py
DO NOT EDIT MANUALLY

功能：
- 输入：[M,N] BF16 或 FP32 GEMM输出（行主序）
- per-token scale: [M,1] FP32
- per-channel scale: [1,N] FP32  
- per-channel bias: [1,N] BF16
- 输出：[M,N] BF16
"""

import torch
import triton
import triton.language as tl


# =============================================================================
# 调优后的配置选择函数
# =============================================================================

def _get_tuned_config(M: int, N: int) -> tuple:
    """根据 M, N 返回最优配置: (BLOCK_M, BLOCK_N, num_warps, num_stages)"""
    if N == 2560:
        if M < 16:
            return 32, 32, 4, 2
        elif M < 192:
            return 32, 64, 4, 2
        elif M < 256:
            return 32, 64, 4, 4
        elif M < 512:
            return 64, 64, 4, 4
        elif M < 768:
            return 128, 128, 8, 4
        elif M < 1024:
            return 128, 128, 16, 4
        elif M < 1536:
            return 128, 128, 8, 2
        elif M < 2048:
            return 64, 128, 8, 2
        elif M < 4096:
            return 128, 128, 8, 4
        elif M < 20480:
            return 128, 128, 8, 2
        elif M < 24576:
            return 128, 128, 8, 4
        elif M < 32768:
            return 128, 128, 8, 2
        elif M < 40960:
            return 128, 128, 8, 4
        elif M < 49152:
            return 128, 128, 8, 2
        else:
            return 128, 128, 8, 4
    elif N == 3840:
        if M < 16:
            return 32, 32, 4, 4
        elif M < 32:
            return 32, 64, 4, 4
        elif M < 48:
            return 32, 64, 4, 2
        elif M < 64:
            return 32, 64, 4, 4
        elif M < 80:
            return 32, 32, 4, 4
        elif M < 112:
            return 32, 64, 4, 4
        elif M < 192:
            return 32, 64, 4, 2
        elif M < 512:
            return 64, 64, 4, 2
        elif M < 768:
            return 64, 64, 8, 2
        elif M < 1024:
            return 128, 128, 8, 2
        elif M < 1536:
            return 64, 128, 8, 2
        elif M < 3072:
            return 128, 128, 8, 4
        elif M < 4096:
            return 128, 128, 8, 2
        elif M < 8192:
            return 128, 128, 8, 4
        elif M < 12288:
            return 128, 128, 8, 2
        elif M < 16384:
            return 128, 128, 8, 4
        elif M < 24576:
            return 128, 128, 8, 2
        elif M < 32768:
            return 128, 128, 8, 4
        elif M < 65536:
            return 128, 128, 8, 2
        else:
            return 64, 256, 16, 4
    elif N == 13824:
        if M < 16:
            return 32, 32, 4, 2
        elif M < 64:
            return 32, 64, 4, 4
        elif M < 96:
            return 64, 64, 4, 2
        elif M < 112:
            return 128, 64, 8, 2
        elif M < 192:
            return 64, 64, 4, 2
        elif M < 256:
            return 64, 128, 8, 2
        elif M < 384:
            return 32, 64, 4, 2
        elif M < 768:
            return 128, 128, 8, 2
        elif M < 1024:
            return 128, 128, 8, 4
        elif M < 1536:
            return 128, 128, 8, 2
        elif M < 6144:
            return 128, 128, 8, 4
        elif M < 12288:
            return 128, 128, 8, 2
        elif M < 14336:
            return 128, 128, 8, 4
        elif M < 20480:
            return 128, 128, 8, 2
        elif M < 49152:
            return 128, 128, 8, 4
        else:
            return 64, 256, 16, 4
    else:
        # 默认配置
        if M <= 128:
            return 32, 64, 4, 4
        elif M <= 2048:
            return 64, 64, 4, 4
        else:
            return 128, 64, 8, 4


# =============================================================================
# Triton Kernel
# =============================================================================

@triton.jit
def _dequant_bias_kernel_tuned(
    gemm_output_ptr,
    scale_a_ptr,
    scale_b_ptr,
    bias_ptr,
    output_ptr,
    M,
    N,
    stride_gm,
    stride_gn,
    stride_om,
    stride_on,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    INPUT_FP32: tl.constexpr,
):
    """
    Dequant + Bias 融合 Kernel
    output = gemm_output * scale_a[:, None] * scale_b[None, :] + bias[None, :]
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    row_start = pid_m * BLOCK_M
    col_start = pid_n * BLOCK_N
    
    row_offs = row_start + tl.arange(0, BLOCK_M)
    col_offs = col_start + tl.arange(0, BLOCK_N)
    
    row_mask = row_offs < M
    col_mask = col_offs < N
    mask_2d = row_mask[:, None] & col_mask[None, :]
    
    # 加载 scale 和 bias
    scale_a = tl.load(scale_a_ptr + row_offs, mask=row_mask, other=1.0)
    scale_b = tl.load(scale_b_ptr + col_offs, mask=col_mask, other=1.0)
    bias = tl.load(bias_ptr + col_offs, mask=col_mask, other=0.0)
    bias = bias.to(tl.float32)
    
    # 加载 GEMM 输出
    gemm_offs = row_offs[:, None] * stride_gm + col_offs[None, :] * stride_gn
    gemm_val = tl.load(gemm_output_ptr + gemm_offs, mask=mask_2d, other=0.0)
    
    # 如果输入是 BF16，转换为 FP32
    if not INPUT_FP32:
        gemm_val = gemm_val.to(tl.float32)
    
    # 计算: output = gemm * scale_a * scale_b + bias
    output_val = gemm_val * scale_a[:, None] * scale_b[None, :]
    output_val = output_val + bias[None, :]
    
    # 转换为 BF16 输出
    output_val = output_val.to(tl.bfloat16)
    
    output_offs = row_offs[:, None] * stride_om + col_offs[None, :] * stride_on
    tl.store(output_ptr + output_offs, output_val, mask=mask_2d)


# =============================================================================
# 主接口函数
# =============================================================================

def dequant_bias_triton_tuned(
    gemm_output: torch.Tensor,
    scale_a: torch.Tensor,
    scale_b: torch.Tensor,
    bias: torch.Tensor,
    out_dtype: torch.dtype = torch.bfloat16,
) -> torch.Tensor:
    """
    调优版 Dequant + Bias 操作（无 autotune 开销）
    
    计算: output = gemm_output * scale_a[M,1] * scale_b[1,N] + bias[1,N]
    
    Args:
        gemm_output: GEMM 输出 [M, N]，BF16 或 FP32
        scale_a: per-token scale [M, 1] 或 [M] FP32
        scale_b: per-channel scale [1, N] 或 [N] FP32
        bias: per-channel 偏置 [N] BF16
        out_dtype: 输出数据类型
        
    Returns:
        dequant 后的输出 [M, N]
    """
    assert gemm_output.is_cuda, "gemm_output must be on CUDA"
    assert gemm_output.is_contiguous(), "gemm_output must be contiguous"
    assert gemm_output.dtype in [torch.bfloat16, torch.float32]
    
    M, N = gemm_output.shape
    input_fp32 = gemm_output.dtype == torch.float32
    
    # 准备输入
    if scale_a.numel() == 1:
        scale_a = scale_a.view(1).expand(M).contiguous().float()
    else:
        scale_a = scale_a.view(-1).contiguous().float()
    
    if scale_b.numel() == 1:
        scale_b = scale_b.view(1).expand(N).contiguous().float()
    else:
        scale_b = scale_b.view(-1).contiguous().float()
    
    bias = bias.view(-1).contiguous().to(torch.bfloat16)
    
    output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
    
    # 获取调优后的配置
    BLOCK_M, BLOCK_N, num_warps, num_stages = _get_tuned_config(M, N)
    
    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))
    
    stride_gm, stride_gn = gemm_output.stride()
    stride_om, stride_on = output.stride()
    
    _dequant_bias_kernel_tuned[grid](
        gemm_output,
        scale_a,
        scale_b,
        bias,
        output,
        M, N,
        stride_gm, stride_gn,
        stride_om, stride_on,
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        INPUT_FP32=input_fp32,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    
    if out_dtype != torch.bfloat16:
        output = output.to(out_dtype)
    
    return output


# =============================================================================
# PyTorch 参考实现
# =============================================================================

def dequant_bias_pytorch(
    gemm_output: torch.Tensor,
    scale_a: torch.Tensor,
    scale_b: torch.Tensor,
    bias: torch.Tensor,
    out_dtype: torch.dtype = torch.bfloat16,
) -> torch.Tensor:
    """PyTorch 参考实现"""
    M, N = gemm_output.shape
    
    if scale_a.numel() == 1:
        scale_a_broadcast = scale_a.view(1, 1).float()
    else:
        scale_a_broadcast = scale_a.view(-1, 1).float()
    
    if scale_b.numel() == 1:
        scale_b_broadcast = scale_b.view(1, 1).float()
    else:
        scale_b_broadcast = scale_b.view(1, -1).float()
    
    if gemm_output.dtype == torch.float32:
        output = gemm_output * scale_a_broadcast * scale_b_broadcast
    else:
        output = gemm_output.float() * scale_a_broadcast * scale_b_broadcast
    
    output = output + bias.float().view(1, -1)
    
    return output.to(out_dtype)


__all__ = ['dequant_bias_triton_tuned', 'dequant_bias_pytorch', '_get_tuned_config']
