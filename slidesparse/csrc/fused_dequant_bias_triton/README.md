# Dequant + Bias Triton Kernel

## æ¦‚è¿°

æœ¬æ¨¡å—å®ç°äº†ä¸€ä¸ªé«˜æ€§èƒ½çš„ Triton kernelï¼Œç”¨äºèåˆåé‡åŒ–ï¼ˆDequantï¼‰å’Œåç½®åŠ æ³•ï¼ˆBias Addï¼‰æ“ä½œï¼ŒæœåŠ¡äº dense å’Œ sparse çš„æ¨ç†é“¾è·¯ã€‚

### é“¾è·¯è¯´æ˜

- **Dense é“¾è·¯**: `Triton quant` â†’ `cuBLASLt (INT8/FP8 â†’ BF16)` â†’ **`Triton dequant`**
- **Sparse é“¾è·¯**: `Triton quantfusedslide` â†’ `cuSparseLt` â†’ **`Triton dequant`** (å¤ç”¨)

---

## æ–‡ä»¶ç»“æ„

| æ–‡ä»¶ | è¯´æ˜ | çŠ¶æ€ |
|------|------|------|
| `dequant_bias_kernel.py` | åŸºç¡€ Kernel å®ç°ï¼ˆæ‰‹åŠ¨é…ç½®é€‰æ‹©ï¼‰ | âœ… ä½¿ç”¨ |
| `dequant_bias_kernel_tuned.py` | **è°ƒä¼˜åçš„ Kernel**ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼Œå›ºå®šé…ç½®ï¼‰ | âœ… ä½¿ç”¨ |
| `autotune_dequant_bias.py` | Autotune è„šæœ¬ + autotune ç‰ˆæœ¬ Kernel | âœ… ä½¿ç”¨ |
| `run_benchmark.py` | **ç»Ÿä¸€æµ‹è¯•è„šæœ¬** | âœ… ä½¿ç”¨ |
| `dequant_torch.py` | PyTorch å‚è€ƒå®ç° (åŸå§‹ç‰ˆæœ¬) | ğŸ”§ å‚è€ƒ |

---

## Kernel åŠŸèƒ½

### è®¡ç®—å…¬å¼

```
output[M,N] = gemm_output[M,N] * scale_a[M,1] * scale_b[1,N] + bias[1,N]
```

### è¾“å…¥è¾“å‡ºè§„æ ¼

| å‚æ•° | å½¢çŠ¶ | æ•°æ®ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `gemm_output` | [M, N] | **BF16 æˆ– FP32** | GEMM è¾“å‡ºï¼ˆè¡Œä¸»åºï¼‰ |
| `scale_a` | [M, 1] | FP32 | per-token scale |
| `scale_b` | [1, N] | FP32 | per-channel scale |
| `bias` | [1, N] | BF16 | per-channel bias |
| `output` | [M, N] | BF16 | è¾“å‡ºç»“æœ |

### è®¡ç®—æµç¨‹

```
1. è¯»å– GEMM è¾“å‡º
   - å¦‚æœæ˜¯ BF16 â†’ è½¬æ¢ä¸º FP32
   - å¦‚æœæ˜¯ FP32 â†’ ç›´æ¥ä½¿ç”¨ï¼ˆçœå»è½¬æ¢å¼€é”€ï¼‰

2. FP32 é€ç‚¹å¤–ç§¯ä¹˜æ³•
   output = gemm_output * scale_a[:, None] * scale_b[None, :]

3. åŠ  biasï¼ˆBF16 â†’ FP32 â†’ åŠ æ³•ï¼‰
   output = output + bias[None, :]

4. è½¬æ¢å› BF16 è¾“å‡º
```

---

## æ ¸å¿ƒä¼˜åŒ–ï¼šKernel èåˆå‡å°‘å†…å­˜è®¿é—®

### ä¸ºä»€ä¹ˆ Triton æ¯” PyTorch å¿«ï¼Ÿ

**æ ¹æœ¬åŸå› ï¼šå‡å°‘å…¨å±€å†…å­˜è¯»å†™æ¬¡æ•°**

è¿™æ˜¯ä¸€ä¸ª **Memory-Boundï¼ˆå†…å­˜å¸¦å®½å—é™ï¼‰** çš„æ“ä½œï¼š
- ç®—æœ¯å¼ºåº¦ â‰ˆ 0.75 FLOP/byteï¼ˆH100 æ‹ç‚¹ ~200ï¼‰
- GPU å¤§éƒ¨åˆ†æ—¶é—´åœ¨ç­‰å†…å­˜ï¼Œä¸æ˜¯åœ¨è®¡ç®—

å› æ­¤ï¼Œ**å‡å°‘å†…å­˜è®¿é—®æ˜¯å”¯ä¸€æœ‰æ•ˆçš„ä¼˜åŒ–æ–¹å‘**ã€‚

---

### PyTorch å®ç°çš„é—®é¢˜

```python
# PyTorch: æ¯ä¸€æ­¥éƒ½è¦è¯»å†™å…¨å±€å†…å­˜
temp1 = gemm_output.float()           # è¯» MÃ—N (BF16), å†™ MÃ—N (FP32)
temp2 = temp1 * scale_a               # è¯» MÃ—N + M, å†™ MÃ—N
temp3 = temp2 * scale_b               # è¯» MÃ—N + N, å†™ MÃ—N
output = temp3 + bias                 # è¯» MÃ—N + N, å†™ MÃ—N
output = output.to(bfloat16)          # è¯» MÃ—N, å†™ MÃ—N
```

**å†…å­˜è®¿é—®é‡**ï¼š
| æ“ä½œ | è¯» | å†™ |
|------|-----|-----|
| float() | MÃ—NÃ—2 | MÃ—NÃ—4 |
| Ã— scale_a | MÃ—NÃ—4 + MÃ—4 | MÃ—NÃ—4 |
| Ã— scale_b | MÃ—NÃ—4 + NÃ—4 | MÃ—NÃ—4 |
| + bias | MÃ—NÃ—4 + NÃ—2 | MÃ—NÃ—4 |
| to(bf16) | MÃ—NÃ—4 | MÃ—NÃ—2 |
| **æ€»è®¡** | **~5 MÃ—NÃ—4** | **~5 MÃ—NÃ—4** |

ğŸ‘‰ **çº¦ 40 MÃ—N bytes å†…å­˜è®¿é—®**ï¼ˆåŠ ä¸Š 5 æ¬¡ kernel launch å¼€é”€ï¼‰

---

### Triton èåˆå®ç°

```python
@triton.jit
def _dequant_bias_kernel(...):
    # 1. ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰è¾“å…¥åˆ°å¯„å­˜å™¨
    scale_a = tl.load(scale_a_ptr + row_offs)     # [BLOCK_M] -> å¯„å­˜å™¨
    scale_b = tl.load(scale_b_ptr + col_offs)     # [BLOCK_N] -> å¯„å­˜å™¨  
    bias = tl.load(bias_ptr + col_offs)           # [BLOCK_N] -> å¯„å­˜å™¨
    gemm_val = tl.load(gemm_output_ptr + offs)    # [BLOCK_M, BLOCK_N] -> å¯„å­˜å™¨
    
    # 2. æ‰€æœ‰è®¡ç®—éƒ½åœ¨å¯„å­˜å™¨å†…å®Œæˆï¼ˆä¸è®¿é—®å…¨å±€å†…å­˜ï¼ï¼‰
    if not INPUT_FP32:
        gemm_val = gemm_val.to(tl.float32)        # å¯„å­˜å™¨å†…ç±»å‹è½¬æ¢
    
    output_val = gemm_val * scale_a[:, None]      # å¯„å­˜å™¨å†…å¹¿æ’­ä¹˜æ³•
    output_val = output_val * scale_b[None, :]    # å¯„å­˜å™¨å†…å¹¿æ’­ä¹˜æ³•
    output_val = output_val + bias[None, :]       # å¯„å­˜å™¨å†…å¹¿æ’­åŠ æ³•
    output_val = output_val.to(tl.bfloat16)       # å¯„å­˜å™¨å†…ç±»å‹è½¬æ¢
    
    # 3. ä¸€æ¬¡æ€§å†™å›å…¨å±€å†…å­˜
    tl.store(output_ptr + offs, output_val)       # å¯„å­˜å™¨ -> å…¨å±€å†…å­˜
```

**å†…å­˜è®¿é—®é‡**ï¼š
| æ“ä½œ | è¯» | å†™ |
|------|-----|-----|
| load gemm | MÃ—NÃ—2 (BF16) æˆ– MÃ—NÃ—4 (FP32) | 0 |
| load scales | MÃ—4 + NÃ—4 | 0 |
| load bias | NÃ—2 | 0 |
| store output | 0 | MÃ—NÃ—2 |
| **æ€»è®¡ (BF16è¾“å…¥)** | **~2 MÃ—N + å°é‡** | **~2 MÃ—N** |

ğŸ‘‰ **çº¦ 4 MÃ—N bytes å†…å­˜è®¿é—®**ï¼ˆ1 æ¬¡ kernel launchï¼‰

---

### å¯¹æ¯”æ€»ç»“

| æŒ‡æ ‡ | PyTorch | Triton | æå‡ |
|------|---------|--------|------|
| å…¨å±€å†…å­˜è®¿é—® | ~40 MÃ—N bytes | ~4 MÃ—N bytes | **10x å‡å°‘** |
| Kernel Launch | 5 æ¬¡ | 1 æ¬¡ | **5x å‡å°‘** |
| ä¸­é—´ç»“æœå­˜å‚¨ | 4 ä¸ªä¸´æ—¶å¼ é‡ | 0ï¼ˆå…¨åœ¨å¯„å­˜å™¨ï¼‰ | **âˆ** |
| ç†è®ºåŠ é€Ÿæ¯” | - | - | **~10x** |
| å®æµ‹åŠ é€Ÿæ¯” | - | - | **5-12x** |

---

### å…³é”®æŠ€æœ¯ç‚¹

#### 1. å¯„å­˜å™¨å†…è®¡ç®—ï¼ˆRegister-Level Fusionï¼‰
```python
# æ‰€æœ‰ä¸­é—´ç»“æœéƒ½åœ¨å¯„å­˜å™¨ï¼Œä¸å†™å› HBM
output_val = gemm_val * scale_a[:, None] * scale_b[None, :] + bias[None, :]
```

#### 2. ç¼–è¯‘æ—¶å¸¸é‡é¿å…åˆ†æ”¯
```python
INPUT_FP32: tl.constexpr  # ç¼–è¯‘æ—¶ç¡®å®šï¼Œç”Ÿæˆä¸¤ä¸ªç‰ˆæœ¬çš„ kernel
if not INPUT_FP32:        # ç¼–è¯‘æ—¶å±•å¼€ï¼Œæ— è¿è¡Œæ—¶å¼€é”€
    gemm_val = gemm_val.to(tl.float32)
```

#### 3. 2D åˆ†å—å¹¶è¡Œ
```python
# æ¯ä¸ª GPU thread block å¤„ç†ä¸€ä¸ª [BLOCK_M, BLOCK_N] tile
pid_m = tl.program_id(0)  # M æ–¹å‘çš„ block ID
pid_n = tl.program_id(1)  # N æ–¹å‘çš„ block ID
grid = (cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))  # æ€»å…±å¯åŠ¨çš„ block æ•°
```

#### 4. å‘é‡å¹¿æ’­ï¼ˆBroadcastingï¼‰
```python
# scale_a: [BLOCK_M] -> [BLOCK_M, 1] -> [BLOCK_M, BLOCK_N]
# scale_b: [BLOCK_N] -> [1, BLOCK_N] -> [BLOCK_M, BLOCK_N]
output = gemm_val * scale_a[:, None] * scale_b[None, :]
# å¹¿æ’­åœ¨å¯„å­˜å™¨å†…å®Œæˆï¼Œæ— é¢å¤–å†…å­˜è®¿é—®
```

---

### é…ç½®é€‰æ‹©ç­–ç•¥

æ ¹æ®çŸ©é˜µå¤§å°é€‰æ‹©æœ€ä¼˜ BLOCK å¤§å°ï¼Œé¿å… autotune å¼€é”€ï¼š

```python
def _get_best_config(M: int, N: int) -> tuple:
    # å° batch: å° block å‡å°‘æµªè´¹
    if M <= 128:
        return (32, 64, 4) if N <= 4096 else (32, 128, 4)
    # ä¸­ç­‰ batch: å¹³è¡¡é…ç½®
    elif M <= 2048:
        return (64, 64, 4) if N <= 4096 else (64, 128, 8)
    # å¤§ batch: å¤§ block æé«˜åå
    else:
        return (128, 64, 8) if N <= 4096 else (128, 128, 8)
```

---

### ä¸ºä»€ä¹ˆä¼˜åŒ–ç©ºé—´æœ‰é™ï¼Ÿ

è¿™ä¸ª kernel å·²ç»æ¥è¿‘ç†è®ºæœ€ä¼˜ï¼š

1. **ç®—æœ¯å¼ºåº¦å¤ªä½** (~0.75)ï¼Œæ— æ³•é€šè¿‡è®¡ç®—ä¼˜åŒ–æå‡
2. **å†…å­˜è®¿é—®å·²æœ€å°åŒ–**ï¼ˆåªæœ‰å¿…è¦çš„è¯»å†™ï¼‰
3. **æ²¡æœ‰æ•°æ®å¤ç”¨æœºä¼š**ï¼ˆæ¯ä¸ªå…ƒç´ åªç”¨ä¸€æ¬¡ï¼‰

è¿›ä¸€æ­¥ä¼˜åŒ–æ–¹å‘ï¼ˆæ”¶ç›Šæœ‰é™ï¼‰ï¼š
- Swizzling ä¼˜åŒ– L2 Cache å‘½ä¸­ç‡ï¼šé¢„æœŸ 5-10%
- å¼‚æ­¥é¢„å–ï¼ˆTriton 3.xï¼‰ï¼šé¢„æœŸ < 5%

---

## æµ‹è¯•æ–¹æ³•

### è¿è¡Œæµ‹è¯•

```bash
cd /root/vllmbench/slidesparse/kernels/dequant_kernals

# ä½¿ç”¨ autotune ç‰ˆæœ¬ï¼ˆé¢„çƒ­æ—¶è‡ªåŠ¨è°ƒä¼˜ï¼‰
python3 run_benchmark.py

# ä½¿ç”¨å·²è°ƒä¼˜çš„å›ºå®šé…ç½®ç‰ˆæœ¬ï¼ˆæ¨èï¼Œæ—  autotune å¼€é”€ï¼‰
python3 run_benchmark.py --tuned

# åªæµ‹æ­£ç¡®æ€§
python3 run_benchmark.py --correctness
python3 run_benchmark.py --tuned --correctness

# åªæµ‹ BF16 æˆ– FP32
python3 run_benchmark.py --tuned --dtype bf16
python3 run_benchmark.py --tuned --dtype fp32
```

### ç”Ÿæˆè°ƒä¼˜é…ç½®

å¦‚éœ€é‡æ–°è°ƒä¼˜ï¼ˆæ›´æ¢ GPU åï¼‰ï¼Œè¿è¡Œï¼š
```bash
python3 autotune_dequant_bias.py
# ä¼šç”Ÿæˆæ–°çš„ dequant_bias_kernel_tuned.py
```

### æµ‹è¯•é…ç½®ï¼ˆå‚è€ƒ autotune_exampleï¼‰

```python
# BitNet æ¨¡å‹å¸¸è§éšè—å±‚å¤§å°
N_VALUES = [2560, 3840, 13824]

# Batch size / sequence length å˜åŒ–
M_VALUES = [1, 16, 32, 48, 64, 80, 96, 112, 128,
            192, 256, 384, 512, 768, 1024, 1536, 2048, 3072, 4096,
            6144, 8192, 10240, 12288, 14336, 16384, 20480, 24576, 
            32768, 40960, 49152, 65536]
```

---

## æ€§èƒ½ç»“æœï¼ˆH100 PCIeï¼Œ--tuned æ¨¡å¼ï¼‰

### BF16 è¾“å…¥

| N | å¹³å‡åŠ é€Ÿæ¯” | æœ€å¤§åŠ é€Ÿæ¯” | æœ€å°åŠ é€Ÿæ¯” |
|---|-----------|-----------|-----------|
| 2560 | **7.13x** | 9.49x | 4.79x |
| 3840 | **7.35x** | 9.51x | 4.68x |
| 13824 | **7.84x** | 9.50x | 4.79x |

### FP32 è¾“å…¥

| N | å¹³å‡åŠ é€Ÿæ¯” | æœ€å¤§åŠ é€Ÿæ¯” | æœ€å°åŠ é€Ÿæ¯” |
|---|-----------|-----------|-----------|
| 2560 | **4.34x** | 5.26x | 3.44x |
| 3840 | **4.43x** | 5.26x | 3.42x |
| 13824 | **4.57x** | 5.29x | 3.45x |

### ç»“è®º

- **BF16 è¾“å…¥åŠ é€Ÿæ›´æ˜æ˜¾** - å¹³å‡ 7-8xï¼Œå› ä¸ºæ•°æ®é‡æ›´å°ï¼Œå†…å­˜å¸¦å®½å‹åŠ›æ›´ä½
- **å¤§ M å€¼åŠ é€Ÿæ›´å¤§** - å° M (1-128) åŠ é€Ÿ ~5xï¼Œå¤§ M (4096+) åŠ é€Ÿ 9-10x
- **Tuned ç‰ˆæœ¬æ—  autotune å¼€é”€** - é¦–æ¬¡è°ƒç”¨å³ä¸ºæœ€ä¼˜æ€§èƒ½

---

## ä½¿ç”¨ç¤ºä¾‹

```python
from dequant_bias_kernel import dequant_bias_triton, dequant_bias_triton_tuned

# å‡†å¤‡æ•°æ®
gemm_output = torch.randn(1024, 2560, dtype=torch.bfloat16, device='cuda')
scale_a = torch.rand(1024, 1, dtype=torch.float32, device='cuda')
scale_b = torch.rand(1, 2560, dtype=torch.float32, device='cuda')
bias = torch.randn(2560, dtype=torch.bfloat16, device='cuda')

# æ–¹å¼1: å›ºå®šé…ç½® (64x64)
output = dequant_bias_triton(gemm_output, scale_a, scale_b, bias)

# æ–¹å¼2: è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é…ç½®ï¼ˆæ¨èï¼‰
output = dequant_bias_triton_tuned(gemm_output, scale_a, scale_b, bias)

# æ”¯æŒ FP32 è¾“å…¥ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼Œæ— éœ€é¢å¤–è½¬æ¢ï¼‰
gemm_fp32 = torch.randn(1024, 2560, dtype=torch.float32, device='cuda')
output = dequant_bias_triton_tuned(gemm_fp32, scale_a, scale_b, bias)
```
