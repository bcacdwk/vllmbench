# Auto-generated by autotune_autogen_dequant_bias.py
# Target: RTX5080 (cc120)
# Supports BF16, FP32 and INT32 input (auto-converts to FP32 for computation)
# DO NOT EDIT

import torch
import triton
import triton.language as tl

def _get_config(M: int, N: int) -> tuple:
    """Returns (BLOCK_M, BLOCK_N, num_warps, num_stages)"""
    if N == 2560:
        if M < 128:
            return 32, 128, 8, 3
        elif M < 1024:
            return 32, 64, 8, 2
        elif M < 4096:
            return 64, 256, 16, 3
        elif M < 16384:
            return 128, 128, 8, 2
        return 64, 256, 8, 3
    elif N == 3840:
        if M < 128:
            return 32, 32, 4, 3
        elif M < 1024:
            return 128, 32, 8, 4
        elif M < 4096:
            return 32, 128, 2, 2
        return 64, 256, 8, 3
    elif N == 13824:
        if M < 128:
            return 64, 64, 4, 3
        elif M < 1024:
            return 16, 256, 4, 3
        elif M < 4096:
            return 64, 128, 4, 4
        elif M < 16384:
            return 32, 256, 8, 4
        return 32, 256, 8, 3
    if M <= 128:
        return 32, 64, 4, 4
    elif M <= 4096:
        return 64, 64, 8, 4
    return 128, 64, 8, 4


@triton.jit
def _dequant_bias_kernel(
    gemm_ptr, scale_a_ptr, scale_b_ptr, bias_ptr, out_ptr,
    M, N, stride_gm, stride_gn, stride_om, stride_on,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    mask_m = offs_m < M
    mask_n = offs_n < N
    mask_2d = mask_m[:, None] & mask_n[None, :]
    
    scale_a = tl.load(scale_a_ptr + offs_m, mask=mask_m, other=1.0)
    scale_b = tl.load(scale_b_ptr + offs_n, mask=mask_n, other=1.0)
    bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0).to(tl.float32)
    
    gemm_offs = offs_m[:, None] * stride_gm + offs_n[None, :] * stride_gn
    val = tl.load(gemm_ptr + gemm_offs, mask=mask_2d, other=0.0)
    val = val.to(tl.float32) * scale_a[:, None] * scale_b[None, :] + bias[None, :]
    
    out_offs = offs_m[:, None] * stride_om + offs_n[None, :] * stride_on
    tl.store(out_ptr + out_offs, val.to(tl.bfloat16), mask=mask_2d)


def dequant_bias_triton(
    gemm_output: torch.Tensor,
    scale_a: torch.Tensor,
    scale_b: torch.Tensor,
    bias: torch.Tensor,
    out_dtype: torch.dtype = torch.bfloat16,
) -> torch.Tensor:
    assert gemm_output.is_cuda and gemm_output.is_contiguous()
    M, N = gemm_output.shape
    
    scale_a = scale_a.view(-1).contiguous().float() if scale_a.numel() > 1 else scale_a.expand(M).contiguous().float()
    scale_b = scale_b.view(-1).contiguous().float() if scale_b.numel() > 1 else scale_b.expand(N).contiguous().float()
    bias = bias.view(-1).contiguous().to(torch.bfloat16)
    output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
    
    BLOCK_M, BLOCK_N, num_warps, num_stages = _get_config(M, N)
    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))
    
    _dequant_bias_kernel[grid](
        gemm_output, scale_a, scale_b, bias, output,
        M, N,
        gemm_output.stride(0), gemm_output.stride(1),
        output.stride(0), output.stride(1),
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,
        num_warps=num_warps, num_stages=num_stages,
    )
    return output.to(out_dtype) if out_dtype != torch.bfloat16 else output


__all__ = ['dequant_bias_triton', '_get_config']
