# Auto-generated by autotune_autogen_dequant_bias.py
# Target: A100 (cc80)
# Supports BF16, FP32 and INT32 input (auto-converts to FP32 for computation)
# DO NOT EDIT

import torch
import triton
import triton.language as tl


def _prepare_scale(scale: torch.Tensor, size: int) -> torch.Tensor:
    """Prepare scale tensor: view as 1D, ensure float32, contiguous."""
    if scale.numel() == 1:
        scale = scale.view(1).expand(size)
    else:
        scale = scale.view(-1)
    # Only convert if not already float32 and contiguous
    if scale.dtype != torch.float32:
        return scale.contiguous().float()
    return scale.contiguous() if not scale.is_contiguous() else scale


def _get_config(M: int, N: int) -> tuple:
    """Returns (BLOCK_M, BLOCK_N, num_warps, num_stages)"""
    if N == 2048:
        if M < 128:
            return 32, 64, 4, 2
        elif M < 256:
            return 64, 64, 4, 2
        elif M < 512:
            return 32, 64, 4, 3
        elif M < 1024:
            return 64, 64, 4, 2
        elif M < 2048:
            return 32, 64, 4, 2
        elif M < 4096:
            return 128, 64, 8, 3
        elif M < 16384:
            return 128, 128, 16, 3
        elif M < 32768:
            return 32, 128, 4, 2
        elif M < 65536:
            return 128, 128, 16, 3
        return 64, 256, 8, 3
    elif N == 3072:
        if M < 1024:
            return 32, 64, 4, 2
        elif M < 2048:
            return 32, 64, 4, 3
        elif M < 4096:
            return 128, 64, 4, 3
        elif M < 8192:
            return 64, 128, 8, 2
        elif M < 16384:
            return 32, 128, 4, 3
        elif M < 32768:
            return 128, 128, 16, 3
        elif M < 65536:
            return 64, 128, 8, 3
        return 64, 256, 8, 3
    elif N == 16384:
        if M < 128:
            return 32, 64, 4, 2
        elif M < 256:
            return 64, 64, 4, 2
        elif M < 512:
            return 64, 64, 4, 3
        elif M < 1024:
            return 128, 64, 8, 3
        elif M < 2048:
            return 64, 128, 8, 3
        elif M < 4096:
            return 64, 128, 8, 2
        elif M < 8192:
            return 64, 256, 8, 3
        elif M < 16384:
            return 64, 128, 8, 3
        return 64, 256, 8, 3
    if M <= 128:
        return 32, 64, 4, 4
    elif M <= 4096:
        return 64, 64, 8, 4
    return 128, 64, 8, 4


@triton.jit
def _dequant_bias_kernel(
    gemm_ptr, scale_a_ptr, scale_b_ptr, bias_ptr, out_ptr,
    M, N, stride_gm, stride_gn, stride_om, stride_on,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    mask_m = offs_m < M
    mask_n = offs_n < N
    mask_2d = mask_m[:, None] & mask_n[None, :]
    
    scale_a = tl.load(scale_a_ptr + offs_m, mask=mask_m, other=1.0)
    scale_b = tl.load(scale_b_ptr + offs_n, mask=mask_n, other=1.0)
    bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0).to(tl.float32)
    
    gemm_offs = offs_m[:, None] * stride_gm + offs_n[None, :] * stride_gn
    val = tl.load(gemm_ptr + gemm_offs, mask=mask_2d, other=0.0)
    val = val.to(tl.float32) * scale_a[:, None] * scale_b[None, :] + bias[None, :]
    
    out_offs = offs_m[:, None] * stride_om + offs_n[None, :] * stride_on
    tl.store(out_ptr + out_offs, val.to(tl.bfloat16), mask=mask_2d)


def dequant_bias_triton(
    gemm_output: torch.Tensor,
    scale_a: torch.Tensor,
    scale_b: torch.Tensor,
    bias: torch.Tensor,
    out_dtype: torch.dtype = torch.bfloat16,
) -> torch.Tensor:
    assert gemm_output.is_cuda and gemm_output.is_contiguous()
    M, N = gemm_output.shape
    
    scale_a = _prepare_scale(scale_a, M)
    scale_b = _prepare_scale(scale_b, N)
    
    bias = bias.view(-1)
    if bias.dtype != torch.bfloat16:
        bias = bias.to(torch.bfloat16)
    bias = bias.contiguous() if not bias.is_contiguous() else bias
    
    output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
    
    BLOCK_M, BLOCK_N, num_warps, num_stages = _get_config(M, N)
    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))
    
    _dequant_bias_kernel[grid](
        gemm_output, scale_a, scale_b, bias, output,
        M, N,
        gemm_output.stride(0), gemm_output.stride(1),
        output.stride(0), output.stride(1),
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,
        num_warps=num_warps, num_stages=num_stages,
    )
    return output.to(out_dtype) if out_dtype != torch.bfloat16 else output


__all__ = ['dequant_bias_triton', '_get_config']
