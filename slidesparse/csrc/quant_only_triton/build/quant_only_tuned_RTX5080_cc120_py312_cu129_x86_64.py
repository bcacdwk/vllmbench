# Auto-generated by autotune_autogen_quant_only.py
# Target: RTX5080 (cc120)
# Design: Per-row kernel (grid = M), Unified FP8/INT8
# DO NOT EDIT

import torch
import triton
import triton.language as tl

def _get_config(M: int, K: int) -> tuple:
    """Returns (BLOCK_K, num_warps, num_stages)"""
    if K == 2560:
        if M <= 16:
            return 4096, 16, 4
        elif M <= 128:
            return 4096, 8, 2
        elif M <= 1024:
            return 4096, 8, 3
        elif M <= 4096:
            return 8192, 16, 3
        else:
            return 4096, 1, 3
    elif K == 6912:
        if M <= 16:
            return 8192, 16, 1
        elif M <= 128:
            return 8192, 4, 1
        elif M <= 1024:
            return 8192, 2, 1
        elif M <= 4096:
            return 8192, 4, 1
        else:
            return 8192, 16, 1
    # Default fallback
    if K <= 4096:
        return 4096, 8, 2
    return 8192, 8, 2


# =============================================================================
# FP8 Kernel
# =============================================================================

@triton.jit
def _quant_only_fp8_kernel(
    x_ptr, out_ptr, scale_ptr,
    M, K: tl.constexpr,
    stride_xm, stride_om,
    BLOCK_K: tl.constexpr,
):
    """Per-token FP8 quantization kernel - one program per row"""
    row = tl.program_id(0)
    
    FP8_MAX: tl.constexpr = 448.0
    MIN_SCALE: tl.constexpr = 1.0 / (448.0 * 512.0)
    
    x_row_ptr = x_ptr + row * stride_xm
    out_row_ptr = out_ptr + row * stride_om
    
    # Pass 1: 计算 absmax
    absmax = tl.zeros((), dtype=tl.float32)
    
    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(x_val)))
    
    # 计算 scale
    absmax = tl.maximum(absmax, 1e-12)
    scale = tl.maximum(absmax / FP8_MAX, MIN_SCALE)
    inv_scale = FP8_MAX / absmax
    
    tl.store(scale_ptr + row, scale)
    
    # Pass 2: 量化
    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0).to(tl.float32)
        y_val = tl.clamp(x_val * inv_scale, -FP8_MAX, FP8_MAX)
        tl.store(out_row_ptr + offs_k, y_val.to(tl.float8e4nv), mask=mask_k)


def quant_only_fp8_triton(
    x: torch.Tensor,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Per-token FP8 quantization using tuned kernel
    
    Args:
        x: Input tensor [M, K], BF16/FP16/FP32, must be contiguous
        
    Returns:
        out: Quantized tensor [M, K], FP8E4M3
        scale: Per-token scale [M], FP32
    """
    assert x.is_cuda and x.is_contiguous()
    M, K = x.shape
    
    out = torch.empty(M, K, dtype=torch.float8_e4m3fn, device=x.device)
    scale = torch.empty(M, dtype=torch.float32, device=x.device)
    
    BLOCK_K, num_warps, num_stages = _get_config(M, K)
    
    # Per-row: grid = (M,)
    _quant_only_fp8_kernel[(M,)](
        x, out, scale,
        M, K,
        x.stride(0), out.stride(0),
        BLOCK_K=BLOCK_K,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return out, scale


# =============================================================================
# INT8 Kernel
# =============================================================================

@triton.jit
def _quant_only_int8_kernel(
    x_ptr, out_ptr, scale_ptr,
    M, K: tl.constexpr,
    stride_xm, stride_om,
    BLOCK_K: tl.constexpr,
):
    """Per-token INT8 quantization kernel - one program per row"""
    row = tl.program_id(0)
    
    INT8_MAX: tl.constexpr = 127.0
    MIN_SCALE: tl.constexpr = 1.0 / (127.0 * 512.0)
    
    x_row_ptr = x_ptr + row * stride_xm
    out_row_ptr = out_ptr + row * stride_om
    
    # Pass 1: 计算 absmax
    absmax = tl.zeros((), dtype=tl.float32)
    
    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(x_val)))
    
    # 计算 scale
    absmax = tl.maximum(absmax, 1e-12)
    scale = tl.maximum(absmax / INT8_MAX, MIN_SCALE)
    inv_scale = INT8_MAX / absmax
    
    tl.store(scale_ptr + row, scale)
    
    # Pass 2: 量化
    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0).to(tl.float32)
        y_val = tl.clamp(tl.extra.cuda.libdevice.rint(x_val * inv_scale), -128.0, 127.0)
        tl.store(out_row_ptr + offs_k, y_val.to(tl.int8), mask=mask_k)


def quant_only_int8_triton(
    x: torch.Tensor,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Per-token INT8 quantization using tuned kernel
    
    Args:
        x: Input tensor [M, K], BF16/FP16/FP32, must be contiguous
        
    Returns:
        out: Quantized tensor [M, K], INT8
        scale: Per-token scale [M], FP32
    """
    assert x.is_cuda and x.is_contiguous()
    M, K = x.shape
    
    out = torch.empty(M, K, dtype=torch.int8, device=x.device)
    scale = torch.empty(M, dtype=torch.float32, device=x.device)
    
    BLOCK_K, num_warps, num_stages = _get_config(M, K)
    
    # Per-row: grid = (M,)
    _quant_only_int8_kernel[(M,)](
        x, out, scale,
        M, K,
        x.stride(0), out.stride(0),
        BLOCK_K=BLOCK_K,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return out, scale


__all__ = ['quant_only_fp8_triton', 'quant_only_int8_triton', '_get_config']
