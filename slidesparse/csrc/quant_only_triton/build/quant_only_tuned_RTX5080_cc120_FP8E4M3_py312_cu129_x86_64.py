# Auto-generated by autotune_autogen_quant_only.py
# Target: RTX5080 (cc120), Output: FP8
# DO NOT EDIT - Per-row quantization kernel

import torch
import triton
import triton.language as tl

def _get_config(K: int) -> tuple:
    """Returns (BLOCK_K, num_warps, num_stages) for given K"""
    if K == 2560:
        return 4096, 2, 2
    elif K == 6912:
        return 8192, 4, 1
    # Default config for unknown K
    if K <= 2048:
        return 2048, 8, 3
    return 4096, 8, 3


@triton.jit
def _quant_kernel(
    x_ptr, out_ptr, scale_ptr,
    M, K: tl.constexpr,
    stride_xm, stride_om,
    BLOCK_K: tl.constexpr,
):
    """
    Per-row FP8 quantization kernel
    
    Each program handles one row (token).
    Optimized for coalesced memory access with contiguous input.
    """
    row = tl.program_id(0)
    
    QMAX: tl.constexpr = 448.0
    MIN_SCALE: tl.constexpr = 1.0 / (448.0 * 512.0)
    
    x_row_ptr = x_ptr + row * stride_xm
    out_row_ptr = out_ptr + row * stride_om
    
    # Pass 1: 计算 absmax
    absmax = tl.zeros((), dtype=tl.float32)
    
    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(x_val)))
    
    # 计算 scale
    absmax = tl.maximum(absmax, 1e-12)
    scale = tl.maximum(absmax / QMAX, MIN_SCALE)
    inv_scale = QMAX / absmax
    
    tl.store(scale_ptr + row, scale)
    
    # Pass 2: 量化
    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0).to(tl.float32)
        y_val = x_val * inv_scale
        y_val = tl.clamp(y_val, -448.0, 448.0)
        tl.store(out_row_ptr + offs_k, y_val.to(tl.float8e4nv), mask=mask_k)


def quant_triton(
    x: torch.Tensor,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Per-token FP8 quantization using tuned per-row kernel
    
    Args:
        x: Input tensor [M, K], BF16/FP16/FP32 (must be contiguous)
        
    Returns:
        out: Quantized tensor [M, K], FP8
        scale: Per-token scale [M], FP32
    """
    assert x.is_cuda and x.is_contiguous()
    M, K = x.shape
    
    out = torch.empty(M, K, dtype=torch.float8_e4m3fn, device=x.device)
    scale = torch.empty(M, dtype=torch.float32, device=x.device)
    
    BLOCK_K, num_warps, num_stages = _get_config(K)
    
    # Per-row: M programs
    _quant_kernel[(M,)](
        x, out, scale,
        M, K,
        x.stride(0), out.stride(0),
        BLOCK_K=BLOCK_K,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return out, scale


__all__ = ['quant_triton', '_get_config']
