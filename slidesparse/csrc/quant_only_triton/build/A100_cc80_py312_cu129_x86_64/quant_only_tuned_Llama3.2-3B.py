# Auto-generated by autotune_autogen_quant_only.py
# Target: A100 (cc80)
# Design: Per-row kernel (grid = M), Unified FP8/INT8
# DO NOT EDIT

import torch
import triton
import triton.language as tl




def _get_config(M: int, K: int) -> tuple:
    """Returns (BLOCK_K, num_warps, num_stages)"""
    if K == 3072:
        if M <= 256:
            return 4096, 4, 3
        elif M <= 4096:
            return 4096, 8, 2
        elif M <= 16384:
            return 4096, 4, 2
        else:
            return 4096, 8, 2
    elif K == 4096:
        if M <= 256:
            return 4096, 8, 2
        elif M <= 1024:
            return 4096, 8, 3
        elif M <= 4096:
            return 8192, 8, 2
        else:
            return 8192, 8, 3
    elif K == 4608:
        if M <= 256:
            return 8192, 8, 2
        elif M <= 1024:
            return 8192, 16, 2
        elif M <= 16384:
            return 8192, 8, 2
        else:
            return 8192, 8, 3
    elif K == 4928:
        if M <= 256:
            return 8192, 8, 3
        elif M <= 1024:
            return 8192, 16, 2
        else:
            return 8192, 8, 2
    elif K == 8192:
        if M <= 256:
            return 4096, 16, 2
        else:
            return 8192, 16, 2
    elif K == 10944:
        if M <= 1024:
            return 4096, 16, 2
        else:
            return 8192, 16, 2
    elif K == 12288:
        if M <= 256:
            return 4096, 8, 2
        else:
            return 8192, 16, 2
    elif K == 13120:
        if M <= 32768:
            return 8192, 16, 2
    # Default fallback
    if K <= 4096:
        return 4096, 8, 2
    return 8192, 8, 2


# =============================================================================
# FP8 Kernel
# =============================================================================

@triton.jit
def _quant_only_fp8_kernel(
    x_ptr, out_ptr, scale_ptr,
    M, K: tl.constexpr,
    stride_xm, stride_om,
    BLOCK_K: tl.constexpr,
):
    """Per-token FP8 quantization kernel - one program per row"""
    row = tl.program_id(0)
    
    FP8_MAX: tl.constexpr = 448.0
    MIN_SCALE: tl.constexpr = 1.0 / (448.0 * 512.0)
    
    x_row_ptr = x_ptr + row * stride_xm
    out_row_ptr = out_ptr + row * stride_om
    
    # Pass 1: 计算 absmax
    absmax = tl.zeros((), dtype=tl.float32)
    
    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(x_val)))
    
    # 计算 scale
    absmax = tl.maximum(absmax, 1e-12)
    scale = tl.maximum(absmax / FP8_MAX, MIN_SCALE)
    inv_scale = FP8_MAX / absmax
    
    tl.store(scale_ptr + row, scale)
    
    # Pass 2: 量化
    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0).to(tl.float32)
        y_val = tl.clamp(x_val * inv_scale, -FP8_MAX, FP8_MAX)
        tl.store(out_row_ptr + offs_k, y_val.to(tl.float8e4nv), mask=mask_k)


def quant_only_fp8_triton(
    x: torch.Tensor,
) -> tuple[torch.Tensor, torch.Tensor]:

    assert x.is_cuda and x.is_contiguous()
    M, K = x.shape
    
    # Padding: K -> 32 aligned, M -> 16 aligned
    K_padded = ((K + 31) // 32) * 32
    M_padded = ((M + 15) // 16) * 16
    
    # 使用 zeros 分配，padding 区域天然为 0（torch.compile 友好）
    out = torch.zeros(M_padded, K_padded, dtype=torch.float8_e4m3fn, device=x.device)
    # scale padding 为 1.0，避免 dequant 时除以 0
    scale = torch.ones(M_padded, dtype=torch.float32, device=x.device)
    
    BLOCK_K, num_warps, num_stages = _get_config(M, K)
    
    # Per-row: grid = (M,) - 只处理有效的 M 行
    _quant_only_fp8_kernel[(M,)](
        x, out, scale,
        M, K,
        x.stride(0), K_padded,  # output stride 使用 K_padded
        BLOCK_K=BLOCK_K,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return out, scale


# =============================================================================
# INT8 Kernel
# =============================================================================

@triton.jit
def _quant_only_int8_kernel(
    x_ptr, out_ptr, scale_ptr,
    M, K: tl.constexpr,
    stride_xm, stride_om,
    BLOCK_K: tl.constexpr,
):
    """Per-token INT8 quantization kernel - one program per row"""
    row = tl.program_id(0)
    
    INT8_MAX: tl.constexpr = 127.0
    MIN_SCALE: tl.constexpr = 1.0 / (127.0 * 512.0)
    
    x_row_ptr = x_ptr + row * stride_xm
    out_row_ptr = out_ptr + row * stride_om
    
    # Pass 1: 计算 absmax
    absmax = tl.zeros((), dtype=tl.float32)
    
    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(x_val)))
    
    # 计算 scale
    absmax = tl.maximum(absmax, 1e-12)
    scale = tl.maximum(absmax / INT8_MAX, MIN_SCALE)
    inv_scale = INT8_MAX / absmax
    
    tl.store(scale_ptr + row, scale)
    
    # Pass 2: 量化
    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0).to(tl.float32)
        y_val = tl.clamp(tl.extra.cuda.libdevice.rint(x_val * inv_scale), -128.0, 127.0)
        tl.store(out_row_ptr + offs_k, y_val.to(tl.int8), mask=mask_k)


def quant_only_int8_triton(
    x: torch.Tensor,
) -> tuple[torch.Tensor, torch.Tensor]:

    assert x.is_cuda and x.is_contiguous()
    M, K = x.shape
    
    # Padding: K -> 32 aligned, M -> 16 aligned
    K_padded = ((K + 31) // 32) * 32
    M_padded = ((M + 15) // 16) * 16
    
    # 使用 zeros 分配，padding 区域天然为 0（torch.compile 友好）
    out = torch.zeros(M_padded, K_padded, dtype=torch.int8, device=x.device)
    # scale padding 为 1.0，避免 dequant 时除以 0
    scale = torch.ones(M_padded, dtype=torch.float32, device=x.device)
    
    BLOCK_K, num_warps, num_stages = _get_config(M, K)
    
    # Per-row: grid = (M,) - 只处理有效的 M 行
    _quant_only_int8_kernel[(M,)](
        x, out, scale,
        M, K,
        x.stride(0), K_padded,  # output stride 使用 K_padded
        BLOCK_K=BLOCK_K,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return out, scale


__all__ = ['quant_only_fp8_triton', 'quant_only_int8_triton', '_get_config']
