#!/usr/bin/env python3
"""
SlideSparse Kernel Code Generator (Unified Autotune Config)

生成 fused_quant_slide kernel，支持:
- dtype: int8 或 fp8 (E4M3)
- L: 任意偶数 >= 4
- 统一使用 BLOCK_GROUPS 作为 autotune 参数

用法:
    python3 codegen_unified.py 8 --int8
    python3 codegen_unified.py 6 --fp8
"""

import argparse
from pathlib import Path
from datetime import datetime


def is_power_of_2(n: int) -> bool:
    return n > 0 and (n & (n - 1)) == 0


# ============================================================================
# 统一的 autotune 配置 (最全面)
# ============================================================================
UNIFIED_AUTOTUNE_CONFIGS = """
def get_autotune_configs():
    \"\"\"统一的 autotune 配置 (BLOCK_GROUPS 为 2 的幂次)\"\"\"
    return [
        # Tier 1: 小 BLOCK_GROUPS (适合小 K 或高寄存器压力场景)
        triton.Config({'BLOCK_GROUPS': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 64}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=8, num_stages=3),
        # Tier 2: 中等 BLOCK_GROUPS (通用场景)
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=4, num_stages=4),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=8, num_stages=4),
        # Tier 3: 大 BLOCK_GROUPS (H100 高带宽利用)
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=4),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=5),
        # Tier 4: 超大 BLOCK_GROUPS (大 K 高吞吐场景)
        triton.Config({'BLOCK_GROUPS': 1024}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 1024}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 1024}, num_warps=8, num_stages=4),
        triton.Config({'BLOCK_GROUPS': 2048}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 2048}, num_warps=8, num_stages=3),
    ]
"""


def generate_int8_kernel(L: int) -> str:
    """生成 INT8 kernel (统一使用 BLOCK_GROUPS)"""
    N = L // 2
    NUM_WINDOWS = N - 1
    expand_ratio = (NUM_WINDOWS * 4) / L
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    L_PAD = 1 << (L - 1).bit_length()  # 下一个 2 的幂次: 6->8, 8->8
    
    # load 代码：用 (base + i) < K_in_orig 做边界检查，越界读 0
    load_lines = [f"        x{i} = tl.load(x_row + base + {i}, mask=mask_group & ((base + {i}) < K_in_orig), other=0.0).to(tl.float32)" for i in range(L)]
    load_code = "\n".join(load_lines)
    
    # quant 代码
    quant_lines = [f"        b{i} = tl.extra.cuda.libdevice.rint(x{i} * inv_scale).to(tl.int32) & 0xFF" for i in range(L)]
    quant_code = "\n".join(quant_lines)
    
    # store 代码
    store_lines = []
    for w in range(NUM_WINDOWS):
        i0, i1, i2, i3 = 2*w, 2*w+1, 2*w+2, 2*w+3
        store_lines.append(
            f"        tl.store(y_row32 + out_base + {w}, "
            f"(b{i0} | (b{i1} << 8) | (b{i2} << 16) | (b{i3} << 24)).to(tl.int32), mask=mask_group)"
        )
    store_code = "\n".join(store_lines)
    
    # Pass 1 代码：统一使用 BLOCK_K = BLOCK_GROUPS * L_PAD (L_PAD 是 >= L 的最小 2 的幂次)
    # 这样 BLOCK_K 总是 2 的幂次，autotune 可以正常工作
    L_PAD = 1 << (L - 1).bit_length()  # 下一个 2 的幂次: 6->8, 8->8
    pass1_code = f'''    # BLOCK_K = BLOCK_GROUPS * {L_PAD} (L_PAD={L_PAD} 是 >= L={L} 的最小 2 的幂次)
    BLOCK_K: tl.constexpr = BLOCK_GROUPS * {L_PAD}

    # Pass 1: absmax (向量化，用 K_in_orig 做边界检查避免越界)
    absmax = tl.zeros((), dtype=tl.float32)
    for k in range(0, K_in, BLOCK_K):
        offs = k + tl.arange(0, BLOCK_K)
        mask = offs < K_in_orig  # 使用原始 K，越界位置 load 0
        xb = tl.load(x_row + offs, mask=mask, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(xb), axis=0))'''
    store_code = "\n".join(store_lines)
    
    return f'''"""
Fused INT8 Quantization + SlideSparse Slide Kernel

Auto-generated by codegen_unified.py at {timestamp}

Configuration:
    L = {L}  (2:{L} sparsity)
    L_PAD = {L_PAD}  (for vectorization)
    dtype = int8
    expand_ratio = {expand_ratio:.3f}x

Regenerate: python3 codegen_unified.py {L} --int8
"""

import torch
import triton
import triton.language as tl


# Configuration
L = {L}
L_PAD = {L_PAD}
N = {N}
NUM_WINDOWS = {NUM_WINDOWS}
EXPAND_RATIO = {expand_ratio}
DTYPE = "int8"

{UNIFIED_AUTOTUNE_CONFIGS}

@triton.autotune(configs=get_autotune_configs(), key=['num_groups'])
@triton.jit
def _fused_quant_slide_kernel(
    x_ptr, y_ptr, scale_ptr,
    M, K_in_orig, K_in, K_out, num_groups,
    stride_x, stride_y,
    BLOCK_GROUPS: tl.constexpr,
):
    """Fused INT8 Quant + Slide for L={L}"""
    row = tl.program_id(0)
    x_row = x_ptr + row * stride_x
    y_row = y_ptr + row * stride_y
    y_row32 = y_row.to(tl.pointer_type(tl.int32))

{pass1_code}

    absmax = tl.maximum(absmax, 1e-5)
    scale = absmax / 127.0
    inv_scale = 127.0 / absmax
    tl.store(scale_ptr + row, scale)

    # Pass 2: Quant + Slide
    for g_start in range(0, num_groups, BLOCK_GROUPS):
        offs_g = tl.arange(0, BLOCK_GROUPS)
        gid = g_start + offs_g
        mask_group = gid < num_groups
        base = gid * {L}

        # Load {L} elements per group (用 K_in_orig 做边界检查)
{load_code}

        # Quantize to int8
{quant_code}

        # Slide windows output
        out_base = gid * {NUM_WINDOWS}
{store_code}


def fused_quant_slide(x: torch.Tensor):
    """
    Fused INT8 Quantization + SlideSparse Slide
    
    Args:
        x: Input tensor [M, K_in], bf16/fp16/fp32
    
    Returns:
        y: Output tensor [M, K_out_padded] (int8), padded to 16-byte alignment
        scale: Scale tensor [M], fp32
    """
    assert x.dim() == 2
    M, K_in_orig = x.shape
    
    # 计算 padded K (不实际 pad，在 kernel 里用 mask 处理)
    K_in = ((K_in_orig + L - 1) // L) * L
    
    num_groups = K_in // L
    K_out = num_groups * NUM_WINDOWS * 4
    
    # Pad output to 16-byte alignment
    K_out_padded = ((K_out + 15) // 16) * 16
    y = torch.empty(M, K_out_padded, dtype=torch.int8, device=x.device)
    if K_out_padded > K_out:
        y[:, K_out:].zero_()
    scale = torch.empty(M, dtype=torch.float32, device=x.device)
    
    # 传递 K_in_orig 给 kernel，在 kernel 里用 mask 处理越界访问
    _fused_quant_slide_kernel[(M,)](
        x, y, scale, M, K_in_orig, K_in, K_out, num_groups, x.stride(0), y.stride(0),
    )
    return y, scale


def get_config():
    return {{'L': L, 'L_PAD': L_PAD, 'N': N, 'NUM_WINDOWS': NUM_WINDOWS, 'EXPAND_RATIO': EXPAND_RATIO, 'DTYPE': DTYPE}}
'''


def generate_fp8_kernel(L: int) -> str:
    """生成 FP8 E4M3 kernel (统一使用 BLOCK_GROUPS)"""
    N = L // 2
    NUM_WINDOWS = N - 1
    expand_ratio = (NUM_WINDOWS * 4) / L
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    L_PAD = 1 << (L - 1).bit_length()  # 下一个 2 的幂次: 6->8, 8->8
    
    # load 代码：用 (base + i) < K_in_orig 做边界检查，越界读 0
    load_lines = [f"        x{i} = tl.load(x_row + base + {i}, mask=mask_group & ((base + {i}) < K_in_orig), other=0.0).to(tl.float32)" for i in range(L)]
    load_code = "\n".join(load_lines)
    
    # FP8 quant 代码
    quant_lines = [f"        q{i} = (x{i} * inv_scale).to(tl.float8e4nv)" for i in range(L)]
    quant_code = "\n".join(quant_lines)
    
    # FP8 打包 store 代码
    store_lines = []
    for w in range(NUM_WINDOWS):
        i0, i1, i2, i3 = 2*w, 2*w+1, 2*w+2, 2*w+3
        store_lines.append(
            f"        b{i0} = q{i0}.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF\n"
            f"        b{i1} = q{i1}.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF\n"
            f"        b{i2} = q{i2}.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF\n"
            f"        b{i3} = q{i3}.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF\n"
            f"        tl.store(y_row32 + out_base + {w}, "
            f"(b{i0} | (b{i1} << 8) | (b{i2} << 16) | (b{i3} << 24)).to(tl.int32), mask=mask_group)"
        )
    store_code = "\n".join(store_lines)
    
    # Pass 1 代码：统一使用 BLOCK_K = BLOCK_GROUPS * L_PAD (L_PAD 是 >= L 的最小 2 的幂次)
    pass1_code = f'''    # BLOCK_K = BLOCK_GROUPS * {L_PAD} (L_PAD={L_PAD} 是 >= L={L} 的最小 2 的幂次)
    BLOCK_K: tl.constexpr = BLOCK_GROUPS * {L_PAD}

    # Pass 1: absmax (向量化，用 K_in_orig 做边界检查避免越界)
    absmax = tl.zeros((), dtype=tl.float32)
    for k in range(0, K_in, BLOCK_K):
        offs = k + tl.arange(0, BLOCK_K)
        mask = offs < K_in_orig  # 使用原始 K，越界位置 load 0
        xb = tl.load(x_row + offs, mask=mask, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(xb), axis=0))'''
    
    return f'''"""
Fused FP8 (E4M3) Quantization + SlideSparse Slide Kernel

Auto-generated by codegen_unified.py at {timestamp}

Configuration:
    L = {L}  (2:{L} sparsity)
    L_PAD = {L_PAD}  (for vectorization)
    dtype = fp8 (E4M3)
    expand_ratio = {expand_ratio:.3f}x

Regenerate: python3 codegen_unified.py {L} --fp8
"""

import torch
import triton
import triton.language as tl


# Configuration
L = {L}
L_PAD = {L_PAD}
N = {N}
NUM_WINDOWS = {NUM_WINDOWS}
EXPAND_RATIO = {expand_ratio}
DTYPE = "fp8"

# FP8 E4M3 max value
FP8_E4M3_MAX = 448.0

{UNIFIED_AUTOTUNE_CONFIGS}

@triton.autotune(configs=get_autotune_configs(), key=['num_groups'])
@triton.jit
def _fused_quant_slide_kernel(
    x_ptr, y_ptr, scale_ptr,
    M, K_in_orig, K_in, K_out, num_groups,
    stride_x, stride_y,
    BLOCK_GROUPS: tl.constexpr,
):
    """Fused FP8 E4M3 Quant + Slide for L={L}"""
    row = tl.program_id(0)
    x_row = x_ptr + row * stride_x
    y_row = y_ptr + row * stride_y
    y_row32 = y_row.to(tl.pointer_type(tl.int32))

    FP8_MAX: tl.constexpr = 448.0

{pass1_code}

    absmax = tl.maximum(absmax, 1e-5)
    scale = absmax / FP8_MAX
    inv_scale = FP8_MAX / absmax
    tl.store(scale_ptr + row, scale)

    # Pass 2: Quant + Slide
    for g_start in range(0, num_groups, BLOCK_GROUPS):
        offs_g = tl.arange(0, BLOCK_GROUPS)
        gid = g_start + offs_g
        mask_group = gid < num_groups
        base = gid * {L}

        # Load {L} elements per group (用 K_in_orig 做边界检查)
{load_code}

        # Quantize to FP8 E4M3
{quant_code}

        # Slide windows output
        out_base = gid * {NUM_WINDOWS}
{store_code}


def fused_quant_slide(x: torch.Tensor):
    """
    Fused FP8 E4M3 Quantization + SlideSparse Slide
    
    Args:
        x: Input tensor [M, K_in], bf16/fp16/fp32
    
    Returns:
        y: Output tensor [M, K_out_padded] (fp8_e4m3fn), padded to 16-byte alignment
        scale: Scale tensor [M], fp32
    """
    assert x.dim() == 2
    M, K_in_orig = x.shape
    
    # 计算 padded K (不实际 pad，在 kernel 里用 mask 处理)
    K_in = ((K_in_orig + L - 1) // L) * L
    
    num_groups = K_in // L
    K_out = num_groups * NUM_WINDOWS * 4
    
    # Pad output to 16-byte alignment
    K_out_padded = ((K_out + 15) // 16) * 16
    y = torch.empty(M, K_out_padded, dtype=torch.float8_e4m3fn, device=x.device)
    if K_out_padded > K_out:
        y[:, K_out:].zero_()
    scale = torch.empty(M, dtype=torch.float32, device=x.device)
    
    # 传递 K_in_orig 给 kernel，在 kernel 里用 mask 处理越界访问
    _fused_quant_slide_kernel[(M,)](
        x, y, scale, M, K_in_orig, K_in, K_out, num_groups, x.stride(0), y.stride(0),
    )
    return y, scale


def get_config():
    return {{'L': L, 'L_PAD': L_PAD, 'N': N, 'NUM_WINDOWS': NUM_WINDOWS, 'EXPAND_RATIO': EXPAND_RATIO, 'DTYPE': DTYPE}}
'''


def main():
    parser = argparse.ArgumentParser(
        description="Generate SlideSparse kernel (unified autotune config)",
        epilog="""
Examples:
    python3 codegen_unified.py 8 --int8
    python3 codegen_unified.py 6 --int8
    python3 codegen_unified.py 8 --fp8
    python3 codegen_unified.py 6 --fp8
        """)
    parser.add_argument('L', type=int, help='Group size (even >= 4)')
    parser.add_argument('--int8', dest='dtype', action='store_const', const='int8', help='INT8 quantization')
    parser.add_argument('--fp8', dest='dtype', action='store_const', const='fp8', help='FP8 E4M3 quantization')
    parser.add_argument('-o', '--output', type=str, default=None)
    parser.set_defaults(dtype='int8')
    args = parser.parse_args()
    
    if args.L < 4 or args.L % 2 != 0:
        parser.error(f"L must be even >= 4, got {args.L}")
    
    # 生成代码
    if args.dtype == 'int8':
        code = generate_int8_kernel(args.L)
    else:
        code = generate_fp8_kernel(args.L)
    
    # 输出文件名
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = Path(__file__).parent / f"slide_L{args.L}_{args.dtype}.py"
    
    output_path.write_text(code)
    
    N = args.L // 2
    NUM_WINDOWS = N - 1
    expand = (NUM_WINDOWS * 4) / args.L
    mode = "vectorized" if is_power_of_2(args.L) else "block-parallel"
    
    print(f"Generated: {output_path}")
    print(f"  L={args.L}, dtype={args.dtype}, mode={mode}, expand={expand:.3f}x")


if __name__ == "__main__":
    main()
