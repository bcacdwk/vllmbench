"""
Fused FP8 (E4M3) Quantization + SlideSparse Slide Kernel

Auto-generated by codegen_unified.py at 2026-01-07 11:52:09

Configuration:
    L = 8  (2:8 sparsity)
    L_PAD = 8  (for vectorization)
    dtype = fp8 (E4M3)
    expand_ratio = 1.500x

Regenerate: python3 codegen_unified.py 8 --fp8
"""

import torch
import triton
import triton.language as tl


# Configuration
L = 8
L_PAD = 8
N = 4
NUM_WINDOWS = 3
EXPAND_RATIO = 1.5
DTYPE = "fp8"

# FP8 E4M3 max value
FP8_E4M3_MAX = 448.0


def get_autotune_configs():
    """统一的 autotune 配置 (BLOCK_GROUPS 为 2 的幂次)"""
    return [
        # Tier 1: 小 BLOCK_GROUPS (适合小 K 或高寄存器压力场景)
        triton.Config({'BLOCK_GROUPS': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 64}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=8, num_stages=3),
        # Tier 2: 中等 BLOCK_GROUPS (通用场景)
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=4, num_stages=4),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=8, num_stages=4),
        # Tier 3: 大 BLOCK_GROUPS (H100 高带宽利用)
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=4),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=5),
        # Tier 4: 超大 BLOCK_GROUPS (大 K 高吞吐场景)
        triton.Config({'BLOCK_GROUPS': 1024}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 1024}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 1024}, num_warps=8, num_stages=4),
        triton.Config({'BLOCK_GROUPS': 2048}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 2048}, num_warps=8, num_stages=3),
    ]


@triton.autotune(configs=get_autotune_configs(), key=['num_groups'])
@triton.jit
def _fused_quant_slide_kernel(
    x_ptr, y_ptr, scale_ptr,
    M, K_in_orig, K_in, K_out, num_groups,
    stride_x, stride_y,
    BLOCK_GROUPS: tl.constexpr,
):
    """Fused FP8 E4M3 Quant + Slide for L=8"""
    row = tl.program_id(0)
    x_row = x_ptr + row * stride_x
    y_row = y_ptr + row * stride_y
    y_row32 = y_row.to(tl.pointer_type(tl.int32))

    FP8_MAX: tl.constexpr = 448.0

    # BLOCK_K = BLOCK_GROUPS * 8 (L_PAD=8 是 >= L=8 的最小 2 的幂次)
    BLOCK_K: tl.constexpr = BLOCK_GROUPS * 8

    # Pass 1: absmax (向量化，用 K_in_orig 做边界检查避免越界)
    absmax = tl.zeros((), dtype=tl.float32)
    for k in range(0, K_in, BLOCK_K):
        offs = k + tl.arange(0, BLOCK_K)
        mask = offs < K_in_orig  # 使用原始 K，越界位置 load 0
        xb = tl.load(x_row + offs, mask=mask, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(xb), axis=0))

    absmax = tl.maximum(absmax, 1e-5)
    scale = absmax / FP8_MAX
    inv_scale = FP8_MAX / absmax
    tl.store(scale_ptr + row, scale)

    # Pass 2: Quant + Slide
    for g_start in range(0, num_groups, BLOCK_GROUPS):
        offs_g = tl.arange(0, BLOCK_GROUPS)
        gid = g_start + offs_g
        mask_group = gid < num_groups
        base = gid * 8

        # Load 8 elements per group (用 K_in_orig 做边界检查)
        x0 = tl.load(x_row + base + 0, mask=mask_group & ((base + 0) < K_in_orig), other=0.0).to(tl.float32)
        x1 = tl.load(x_row + base + 1, mask=mask_group & ((base + 1) < K_in_orig), other=0.0).to(tl.float32)
        x2 = tl.load(x_row + base + 2, mask=mask_group & ((base + 2) < K_in_orig), other=0.0).to(tl.float32)
        x3 = tl.load(x_row + base + 3, mask=mask_group & ((base + 3) < K_in_orig), other=0.0).to(tl.float32)
        x4 = tl.load(x_row + base + 4, mask=mask_group & ((base + 4) < K_in_orig), other=0.0).to(tl.float32)
        x5 = tl.load(x_row + base + 5, mask=mask_group & ((base + 5) < K_in_orig), other=0.0).to(tl.float32)
        x6 = tl.load(x_row + base + 6, mask=mask_group & ((base + 6) < K_in_orig), other=0.0).to(tl.float32)
        x7 = tl.load(x_row + base + 7, mask=mask_group & ((base + 7) < K_in_orig), other=0.0).to(tl.float32)

        # Quantize to FP8 E4M3
        q0 = (x0 * inv_scale).to(tl.float8e4nv)
        q1 = (x1 * inv_scale).to(tl.float8e4nv)
        q2 = (x2 * inv_scale).to(tl.float8e4nv)
        q3 = (x3 * inv_scale).to(tl.float8e4nv)
        q4 = (x4 * inv_scale).to(tl.float8e4nv)
        q5 = (x5 * inv_scale).to(tl.float8e4nv)
        q6 = (x6 * inv_scale).to(tl.float8e4nv)
        q7 = (x7 * inv_scale).to(tl.float8e4nv)

        # Slide windows output
        out_base = gid * 3
        b0 = q0.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b1 = q1.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b2 = q2.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b3 = q3.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        tl.store(y_row32 + out_base + 0, (b0 | (b1 << 8) | (b2 << 16) | (b3 << 24)).to(tl.int32), mask=mask_group)
        b2 = q2.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b3 = q3.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b4 = q4.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b5 = q5.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        tl.store(y_row32 + out_base + 1, (b2 | (b3 << 8) | (b4 << 16) | (b5 << 24)).to(tl.int32), mask=mask_group)
        b4 = q4.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b5 = q5.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b6 = q6.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b7 = q7.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        tl.store(y_row32 + out_base + 2, (b4 | (b5 << 8) | (b6 << 16) | (b7 << 24)).to(tl.int32), mask=mask_group)


def fused_quant_slide(x: torch.Tensor):
    """
    Fused FP8 E4M3 Quantization + SlideSparse Slide
    
    Args:
        x: Input tensor [M, K_in], bf16/fp16/fp32
    
    Returns:
        y: Output tensor [M, K_out_padded] (fp8_e4m3fn), padded to 16-byte alignment
        scale: Scale tensor [M], fp32
    """
    assert x.dim() == 2
    M, K_in_orig = x.shape
    
    # 计算 padded K (不实际 pad，在 kernel 里用 mask 处理)
    K_in = ((K_in_orig + L - 1) // L) * L
    
    num_groups = K_in // L
    K_out = num_groups * NUM_WINDOWS * 4
    
    # Pad output to 16-byte alignment
    K_out_padded = ((K_out + 15) // 16) * 16
    y = torch.empty(M, K_out_padded, dtype=torch.float8_e4m3fn, device=x.device)
    if K_out_padded > K_out:
        y[:, K_out:].zero_()
    scale = torch.empty(M, dtype=torch.float32, device=x.device)
    
    # 传递 K_in_orig 给 kernel，在 kernel 里用 mask 处理越界访问
    _fused_quant_slide_kernel[(M,)](
        x, y, scale, M, K_in_orig, K_in, K_out, num_groups, x.stride(0), y.stride(0),
    )
    return y, scale


def get_config():
    return {'L': L, 'L_PAD': L_PAD, 'N': N, 'NUM_WINDOWS': NUM_WINDOWS, 'EXPAND_RATIO': EXPAND_RATIO, 'DTYPE': DTYPE}
