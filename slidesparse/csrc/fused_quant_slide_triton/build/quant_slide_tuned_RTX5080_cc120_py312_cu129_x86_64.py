# Auto-generated by autotune_autogen_quant_slide.py
# Target: RTX5080 (cc120)
# Design: Per-row kernel (grid = M), Unified FP8/INT8, L as constexpr
# DO NOT EDIT

import torch
import triton
import triton.language as tl
from typing import Tuple

# Tensor Cache for output allocation

_fp8_slide_out_cache: dict = {}
_fp8_slide_scale_cache: dict = {}
_int8_slide_out_cache: dict = {}
_int8_slide_scale_cache: dict = {}


def _get_cached_fp8_slide_tensors(M_padded: int, K_out_padded: int, device: torch.device):
    """Get or create cached FP8 output tensors for quant_slide."""
    key = (M_padded, K_out_padded, device.index if device.index is not None else 0)
    if key not in _fp8_slide_out_cache:
        _fp8_slide_out_cache[key] = torch.empty(M_padded, K_out_padded, dtype=torch.float8_e4m3fn, device=device)
        _fp8_slide_scale_cache[key] = torch.empty(M_padded, dtype=torch.float32, device=device)
    # Must zero/fill every call since kernel only writes valid M rows
    out, scale = _fp8_slide_out_cache[key], _fp8_slide_scale_cache[key]
    out.zero_()
    scale.fill_(1.0)
    return out, scale


def _get_cached_int8_slide_tensors(M_padded: int, K_out_padded: int, device: torch.device):
    """Get or create cached INT8 output tensors for quant_slide."""
    key = (M_padded, K_out_padded, device.index if device.index is not None else 0)
    if key not in _int8_slide_out_cache:
        _int8_slide_out_cache[key] = torch.empty(M_padded, K_out_padded, dtype=torch.int8, device=device)
        _int8_slide_scale_cache[key] = torch.empty(M_padded, dtype=torch.float32, device=device)
    # Must zero/fill every call since kernel only writes valid M rows
    out, scale = _int8_slide_out_cache[key], _int8_slide_scale_cache[key]
    out.zero_()
    scale.fill_(1.0)
    return out, scale


def _get_config(M: int, K: int) -> tuple:
    """Returns (BLOCK_GROUPS, num_warps, num_stages)"""
    if K == 896:
        if M <= 32:
            return 256, 8, 1
        elif M <= 256:
            return 128, 4, 3
        else:
            return 256, 4, 3
    elif K == 4864:
        if M <= 32:
            return 256, 4, 3
        elif M <= 256:
            return 256, 8, 3
        else:
            return 256, 8, 1
    # Default fallback
    if K <= 4096:
        return 256, 8, 2
    return 256, 8, 2


def _get_num_windows(L: int) -> int:
    """Calculate number of windows: L/2 - 1"""
    return L // 2 - 1


def _compute_output_k(K_in: int, L: int) -> Tuple[int, int, int]:
    """Compute output dimensions"""
    K_in_padded = ((K_in + L - 1) // L) * L
    num_groups = K_in_padded // L
    num_windows = _get_num_windows(L)
    K_out = num_groups * num_windows * 4
    return K_in_padded, K_out, num_groups


def _get_block_k(K: int) -> int:
    """Get BLOCK_K for Pass 1"""
    if K <= 2048:
        return 2048
    elif K <= 4096:
        return 4096
    else:
        return 4096


# =============================================================================
# FP8 Kernel
# =============================================================================

@triton.jit
def _quant_slide_fp8_kernel(
    x_ptr, out_ptr, scale_ptr,
    M, K_in_orig, K_in_padded, K_out, num_groups,
    stride_x, stride_out,
    L: tl.constexpr,
    NUM_WINDOWS: tl.constexpr,
    BLOCK_GROUPS: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """Per-token FP8 Quant + Slide kernel"""
    row = tl.program_id(0)
    
    FP8_MAX: tl.constexpr = 448.0
    MIN_SCALE: tl.constexpr = 1.0 / (448.0 * 512.0)
    
    x_row = x_ptr + row * stride_x
    out_row32 = out_ptr.to(tl.pointer_type(tl.int32)) + row * (stride_out // 4)
    
    # Pass 1: Compute absmax
    absmax = tl.zeros((), dtype=tl.float32)
    for k in range(0, K_in_padded, BLOCK_K):
        offs = k + tl.arange(0, BLOCK_K)
        mask = offs < K_in_orig
        xb = tl.load(x_row + offs, mask=mask, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(xb)))
    
    absmax = tl.maximum(absmax, 1e-12)
    scale = tl.maximum(absmax / FP8_MAX, MIN_SCALE)
    inv_scale = FP8_MAX / absmax
    tl.store(scale_ptr + row, scale)
    
    # Pass 2: Quant + Slide
    for g_start in range(0, num_groups, BLOCK_GROUPS):
        offs_g = tl.arange(0, BLOCK_GROUPS)
        gid = g_start + offs_g
        mask_g = gid < num_groups
        base_in = gid * L
        base_out = gid * NUM_WINDOWS
        
        for w in tl.static_range(NUM_WINDOWS):
            win_start = 2 * w
            
            x0 = tl.load(x_row + base_in + win_start + 0, 
                        mask=mask_g & ((base_in + win_start + 0) < K_in_orig), other=0.0).to(tl.float32)
            x1 = tl.load(x_row + base_in + win_start + 1,
                        mask=mask_g & ((base_in + win_start + 1) < K_in_orig), other=0.0).to(tl.float32)
            x2 = tl.load(x_row + base_in + win_start + 2,
                        mask=mask_g & ((base_in + win_start + 2) < K_in_orig), other=0.0).to(tl.float32)
            x3 = tl.load(x_row + base_in + win_start + 3,
                        mask=mask_g & ((base_in + win_start + 3) < K_in_orig), other=0.0).to(tl.float32)
            
            q0 = tl.clamp(x0 * inv_scale, -FP8_MAX, FP8_MAX).to(tl.float8e4nv)
            q1 = tl.clamp(x1 * inv_scale, -FP8_MAX, FP8_MAX).to(tl.float8e4nv)
            q2 = tl.clamp(x2 * inv_scale, -FP8_MAX, FP8_MAX).to(tl.float8e4nv)
            q3 = tl.clamp(x3 * inv_scale, -FP8_MAX, FP8_MAX).to(tl.float8e4nv)
            
            b0 = q0.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
            b1 = q1.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
            b2 = q2.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
            b3 = q3.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
            
            packed = (b0 | (b1 << 8) | (b2 << 16) | (b3 << 24)).to(tl.int32)
            tl.store(out_row32 + base_out + w, packed, mask=mask_g)


def quant_slide_fp8_triton(
    x: torch.Tensor,
    L: int = 8,
) -> Tuple[torch.Tensor, torch.Tensor]:

    assert x.is_cuda and x.is_contiguous()
    assert L >= 4 and L % 2 == 0
    
    M, K_in_orig = x.shape
    K_in_padded, K_out, num_groups = _compute_output_k(K_in_orig, L)
    num_windows = _get_num_windows(L)
    
    K_out_padded = ((K_out + 31) // 32) * 32
    M_padded = ((M + 15) // 16) * 16
    
    out, scale = _get_cached_fp8_slide_tensors(M_padded, K_out_padded, x.device)
    
    BLOCK_GROUPS, num_warps, num_stages = _get_config(M, K_in_orig)
    block_k = _get_block_k(K_in_orig)
    
    _quant_slide_fp8_kernel[(M,)](
        x, out, scale,
        M, K_in_orig, K_in_padded, K_out, num_groups,
        x.stride(0), K_out_padded,  # output stride 使用 K_out_padded
        L=L,
        NUM_WINDOWS=num_windows,
        BLOCK_GROUPS=BLOCK_GROUPS,
        BLOCK_K=block_k,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return out, scale


# =============================================================================
# INT8 Kernel
# =============================================================================

@triton.jit
def _quant_slide_int8_kernel(
    x_ptr, out_ptr, scale_ptr,
    M, K_in_orig, K_in_padded, K_out, num_groups,
    stride_x, stride_out,
    L: tl.constexpr,
    NUM_WINDOWS: tl.constexpr,
    BLOCK_GROUPS: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """Per-token INT8 Quant + Slide kernel"""
    row = tl.program_id(0)
    
    INT8_MAX: tl.constexpr = 127.0
    MIN_SCALE: tl.constexpr = 1.0 / (127.0 * 512.0)
    
    x_row = x_ptr + row * stride_x
    out_row32 = out_ptr.to(tl.pointer_type(tl.int32)) + row * (stride_out // 4)
    
    # Pass 1: Compute absmax
    absmax = tl.zeros((), dtype=tl.float32)
    for k in range(0, K_in_padded, BLOCK_K):
        offs = k + tl.arange(0, BLOCK_K)
        mask = offs < K_in_orig
        xb = tl.load(x_row + offs, mask=mask, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(xb)))
    
    absmax = tl.maximum(absmax, 1e-12)
    scale = tl.maximum(absmax / INT8_MAX, MIN_SCALE)
    inv_scale = INT8_MAX / absmax
    tl.store(scale_ptr + row, scale)
    
    # Pass 2: Quant + Slide
    for g_start in range(0, num_groups, BLOCK_GROUPS):
        offs_g = tl.arange(0, BLOCK_GROUPS)
        gid = g_start + offs_g
        mask_g = gid < num_groups
        base_in = gid * L
        base_out = gid * NUM_WINDOWS
        
        for w in tl.static_range(NUM_WINDOWS):
            win_start = 2 * w
            
            x0 = tl.load(x_row + base_in + win_start + 0, 
                        mask=mask_g & ((base_in + win_start + 0) < K_in_orig), other=0.0).to(tl.float32)
            x1 = tl.load(x_row + base_in + win_start + 1,
                        mask=mask_g & ((base_in + win_start + 1) < K_in_orig), other=0.0).to(tl.float32)
            x2 = tl.load(x_row + base_in + win_start + 2,
                        mask=mask_g & ((base_in + win_start + 2) < K_in_orig), other=0.0).to(tl.float32)
            x3 = tl.load(x_row + base_in + win_start + 3,
                        mask=mask_g & ((base_in + win_start + 3) < K_in_orig), other=0.0).to(tl.float32)
            
            q0 = tl.clamp(tl.extra.cuda.libdevice.rint(x0 * inv_scale), -128.0, 127.0).to(tl.int32) & 0xFF
            q1 = tl.clamp(tl.extra.cuda.libdevice.rint(x1 * inv_scale), -128.0, 127.0).to(tl.int32) & 0xFF
            q2 = tl.clamp(tl.extra.cuda.libdevice.rint(x2 * inv_scale), -128.0, 127.0).to(tl.int32) & 0xFF
            q3 = tl.clamp(tl.extra.cuda.libdevice.rint(x3 * inv_scale), -128.0, 127.0).to(tl.int32) & 0xFF
            
            packed = (q0 | (q1 << 8) | (q2 << 16) | (q3 << 24)).to(tl.int32)
            tl.store(out_row32 + base_out + w, packed, mask=mask_g)


def quant_slide_int8_triton(
    x: torch.Tensor,
    L: int = 8,
) -> Tuple[torch.Tensor, torch.Tensor]:

    assert x.is_cuda and x.is_contiguous()
    assert L >= 4 and L % 2 == 0
    
    M, K_in_orig = x.shape
    K_in_padded, K_out, num_groups = _compute_output_k(K_in_orig, L)
    num_windows = _get_num_windows(L)
    
    K_out_padded = ((K_out + 31) // 32) * 32
    M_padded = ((M + 15) // 16) * 16
    
    out, scale = _get_cached_int8_slide_tensors(M_padded, K_out_padded, x.device)
    
    BLOCK_GROUPS, num_warps, num_stages = _get_config(M, K_in_orig)
    block_k = _get_block_k(K_in_orig)
    
    _quant_slide_int8_kernel[(M,)](
        x, out, scale,
        M, K_in_orig, K_in_padded, K_out, num_groups,
        x.stride(0), K_out_padded,  # output stride 使用 K_out_padded
        L=L,
        NUM_WINDOWS=num_windows,
        BLOCK_GROUPS=BLOCK_GROUPS,
        BLOCK_K=block_k,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return out, scale


__all__ = ['quant_slide_fp8_triton', 'quant_slide_int8_triton', '_get_config', '_compute_output_k', '_get_num_windows']
