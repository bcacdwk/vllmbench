# Auto-generated by autotune_autogen_quant_slide.py
# Target: B200 (cc100)
# Design: Per-row kernel (grid = M), Unified FP8/INT8, L as constexpr
# DO NOT EDIT

import torch
import triton
import triton.language as tl
from typing import Tuple




def _get_config(M: int, K: int) -> tuple:
    """Returns (BLOCK_OUT, BLOCK_K, num_warps, num_stages)"""
    if K == 2560:
        if M <= 64:
            return 512, 4096, 16, 2
        elif M <= 128:
            return 256, 4096, 8, 2
        elif M <= 256:
            return 256, 8192, 8, 2
        elif M <= 512:
            return 256, 4096, 4, 2
        elif M <= 1024:
            return 512, 4096, 8, 3
        elif M <= 2048:
            return 256, 8192, 8, 2
        elif M <= 16384:
            return 256, 4096, 4, 2
        else:
            return 256, 4096, 4, 3
    elif K == 3424:
        if M <= 64:
            return 256, 4096, 8, 2
        elif M <= 128:
            return 512, 4096, 16, 2
        elif M <= 256:
            return 512, 4096, 8, 2
        elif M <= 512:
            return 128, 4096, 4, 3
        elif M <= 1024:
            return 256, 4096, 4, 2
        elif M <= 8192:
            return 128, 4096, 4, 2
        elif M <= 16384:
            return 128, 4096, 4, 3
        else:
            return 128, 4096, 4, 2
    elif K == 3840:
        if M <= 64:
            return 256, 4096, 4, 3
        elif M <= 128:
            return 256, 4096, 8, 2
        elif M <= 512:
            return 512, 4096, 8, 3
        elif M <= 1024:
            return 256, 4096, 4, 2
        elif M <= 2048:
            return 256, 4096, 4, 3
        elif M <= 4096:
            return 256, 4096, 4, 2
        else:
            return 256, 4096, 4, 3
    elif K == 4096:
        if M <= 64:
            return 1024, 4096, 16, 2
        elif M <= 128:
            return 512, 4096, 8, 3
        elif M <= 256:
            return 512, 8192, 8, 2
        elif M <= 512:
            return 256, 4096, 4, 3
        elif M <= 1024:
            return 512, 8192, 8, 2
        elif M <= 2048:
            return 256, 4096, 4, 3
        elif M <= 16384:
            return 256, 4096, 4, 2
        else:
            return 256, 4096, 4, 3
    elif K == 6912:
        if M <= 128:
            return 512, 8192, 16, 2
        elif M <= 512:
            return 512, 8192, 8, 2
        elif M <= 16384:
            return 256, 8192, 8, 2
        else:
            return 128, 4096, 4, 2
    elif K == 9216:
        if M <= 128:
            return 512, 8192, 16, 2
        elif M <= 256:
            return 512, 8192, 8, 2
        elif M <= 512:
            return 512, 8192, 16, 2
        elif M <= 1024:
            return 512, 4096, 8, 3
        else:
            return 512, 8192, 8, 2
    elif K == 10368:
        if M <= 64:
            return 512, 8192, 8, 2
        elif M <= 128:
            return 1024, 8192, 16, 2
        elif M <= 256:
            return 512, 8192, 16, 2
        elif M <= 512:
            return 1024, 8192, 16, 2
        else:
            return 512, 8192, 8, 2
    elif K == 11072:
        if M <= 64:
            return 512, 8192, 16, 2
        elif M <= 512:
            return 512, 8192, 8, 2
        elif M <= 1024:
            return 256, 4096, 8, 3
        elif M <= 2048:
            return 512, 8192, 8, 2
        else:
            return 256, 8192, 8, 2
    # Default fallback
    if K <= 4096:
        return 256, 4096, 8, 2
    return 256, 4096, 8, 2


def _get_num_windows(L: int) -> int:
    """Calculate number of windows: L/2 - 1"""
    return L // 2 - 1


def _compute_output_k(K_in: int, L: int) -> Tuple[int, int, int]:
    """Compute output dimensions"""
    K_in_padded = ((K_in + L - 1) // L) * L
    num_groups = K_in_padded // L
    num_windows = _get_num_windows(L)
    K_out = num_groups * num_windows * 4
    return K_in_padded, K_out, num_groups


# =============================================================================
# FP8 Kernel
# =============================================================================

@triton.jit
def _quant_slide_fp8_kernel(
    x_ptr, out_ptr, scale_ptr,
    M, K_in_orig, K_in_padded, K_out, num_groups,
    stride_x, stride_out,
    L: tl.constexpr,
    NUM_WINDOWS: tl.constexpr,
    BLOCK_OUT: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """Per-token FP8 Quant + Slide kernel"""
    row = tl.program_id(0)
    
    FP8_MAX: tl.constexpr = 448.0
    MIN_SCALE: tl.constexpr = 1.0 / (448.0 * 512.0)
    
    x_row = x_ptr + row * stride_x
    out_row32 = out_ptr.to(tl.pointer_type(tl.int32)) + row * (stride_out // 4)
    
    # Pass 1: Compute absmax
    absmax = tl.zeros((), dtype=tl.float32)
    for k in range(0, K_in_padded, BLOCK_K):
        offs = k + tl.arange(0, BLOCK_K)
        mask = offs < K_in_orig
        xb = tl.load(x_row + offs, mask=mask, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(xb)))
    
    absmax = tl.maximum(absmax, 1e-12)
    scale = tl.maximum(absmax / FP8_MAX, MIN_SCALE)
    inv_scale = FP8_MAX / absmax
    tl.store(scale_ptr + row, scale)
    
    # Pass 2: Output-Oriented Quant + Slide
    total_out = num_groups * NUM_WINDOWS
    
    for out_start in range(0, total_out, BLOCK_OUT):
        offs_out = out_start + tl.arange(0, BLOCK_OUT)
        mask_out = offs_out < total_out
        
        # Direct input position calculation
        g = offs_out // NUM_WINDOWS
        w = offs_out % NUM_WINDOWS
        base_in = g * L + 2 * w
        
        x0 = tl.load(x_row + base_in + 0, 
                    mask=mask_out & ((base_in + 0) < K_in_orig), other=0.0).to(tl.float32)
        x1 = tl.load(x_row + base_in + 1,
                    mask=mask_out & ((base_in + 1) < K_in_orig), other=0.0).to(tl.float32)
        x2 = tl.load(x_row + base_in + 2,
                    mask=mask_out & ((base_in + 2) < K_in_orig), other=0.0).to(tl.float32)
        x3 = tl.load(x_row + base_in + 3,
                    mask=mask_out & ((base_in + 3) < K_in_orig), other=0.0).to(tl.float32)
        
        q0 = tl.clamp(x0 * inv_scale, -FP8_MAX, FP8_MAX).to(tl.float8e4nv)
        q1 = tl.clamp(x1 * inv_scale, -FP8_MAX, FP8_MAX).to(tl.float8e4nv)
        q2 = tl.clamp(x2 * inv_scale, -FP8_MAX, FP8_MAX).to(tl.float8e4nv)
        q3 = tl.clamp(x3 * inv_scale, -FP8_MAX, FP8_MAX).to(tl.float8e4nv)
        
        b0 = q0.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b1 = q1.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b2 = q2.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        b3 = q3.to(tl.int8, bitcast=True).to(tl.int32) & 0xFF
        
        packed = (b0 | (b1 << 8) | (b2 << 16) | (b3 << 24)).to(tl.int32)
        tl.store(out_row32 + offs_out, packed, mask=mask_out)


def quant_slide_fp8_triton(
    x: torch.Tensor,
    L: int = 8,
    block_groups: int = None,  # Ignored, kept for API compatibility
    num_warps: int = None,     # Ignored, kept for API compatibility
    num_stages: int = None,    # Ignored, kept for API compatibility
) -> Tuple[torch.Tensor, torch.Tensor]:

    # Note: block_groups, num_warps, num_stages are ignored (tuned values used)
    assert x.is_cuda and x.is_contiguous()
    assert L >= 4 and L % 2 == 0
    
    M, K_in_orig = x.shape
    K_in_padded, K_out, num_groups = _compute_output_k(K_in_orig, L)
    num_windows = _get_num_windows(L)
    
    K_out_padded = ((K_out + 31) // 32) * 32
    M_padded = ((M + 15) // 16) * 16
    
    # 使用 zeros 分配，padding 区域天然为 0（torch.compile 友好）
    out = torch.zeros(M_padded, K_out_padded, dtype=torch.float8_e4m3fn, device=x.device)
    # scale padding 为 1.0，避免 dequant 时除以 0
    scale = torch.ones(M_padded, dtype=torch.float32, device=x.device)
    
    BLOCK_OUT, BLOCK_K, num_warps, num_stages = _get_config(M, K_in_orig)
    
    _quant_slide_fp8_kernel[(M,)](
        x, out, scale,
        M, K_in_orig, K_in_padded, K_out, num_groups,
        x.stride(0), K_out_padded,  # output stride 使用 K_out_padded
        L=L,
        NUM_WINDOWS=num_windows,
        BLOCK_OUT=BLOCK_OUT,
        BLOCK_K=BLOCK_K,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return out, scale


# =============================================================================
# INT8 Kernel
# =============================================================================

@triton.jit
def _quant_slide_int8_kernel(
    x_ptr, out_ptr, scale_ptr,
    M, K_in_orig, K_in_padded, K_out, num_groups,
    stride_x, stride_out,
    L: tl.constexpr,
    NUM_WINDOWS: tl.constexpr,
    BLOCK_OUT: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """Per-token INT8 Quant + Slide kernel"""
    row = tl.program_id(0)
    
    INT8_MAX: tl.constexpr = 127.0
    MIN_SCALE: tl.constexpr = 1.0 / (127.0 * 512.0)
    
    x_row = x_ptr + row * stride_x
    out_row32 = out_ptr.to(tl.pointer_type(tl.int32)) + row * (stride_out // 4)
    
    # Pass 1: Compute absmax
    absmax = tl.zeros((), dtype=tl.float32)
    for k in range(0, K_in_padded, BLOCK_K):
        offs = k + tl.arange(0, BLOCK_K)
        mask = offs < K_in_orig
        xb = tl.load(x_row + offs, mask=mask, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(xb)))
    
    absmax = tl.maximum(absmax, 1e-12)
    scale = tl.maximum(absmax / INT8_MAX, MIN_SCALE)
    inv_scale = INT8_MAX / absmax
    tl.store(scale_ptr + row, scale)
    
    # Pass 2: Output-Oriented Quant + Slide
    total_out = num_groups * NUM_WINDOWS
    
    for out_start in range(0, total_out, BLOCK_OUT):
        offs_out = out_start + tl.arange(0, BLOCK_OUT)
        mask_out = offs_out < total_out
        
        # Direct input position calculation
        g = offs_out // NUM_WINDOWS
        w = offs_out % NUM_WINDOWS
        base_in = g * L + 2 * w
        
        x0 = tl.load(x_row + base_in + 0, 
                    mask=mask_out & ((base_in + 0) < K_in_orig), other=0.0).to(tl.float32)
        x1 = tl.load(x_row + base_in + 1,
                    mask=mask_out & ((base_in + 1) < K_in_orig), other=0.0).to(tl.float32)
        x2 = tl.load(x_row + base_in + 2,
                    mask=mask_out & ((base_in + 2) < K_in_orig), other=0.0).to(tl.float32)
        x3 = tl.load(x_row + base_in + 3,
                    mask=mask_out & ((base_in + 3) < K_in_orig), other=0.0).to(tl.float32)
        
        q0 = tl.clamp(tl.extra.cuda.libdevice.rint(x0 * inv_scale), -128.0, 127.0).to(tl.int32) & 0xFF
        q1 = tl.clamp(tl.extra.cuda.libdevice.rint(x1 * inv_scale), -128.0, 127.0).to(tl.int32) & 0xFF
        q2 = tl.clamp(tl.extra.cuda.libdevice.rint(x2 * inv_scale), -128.0, 127.0).to(tl.int32) & 0xFF
        q3 = tl.clamp(tl.extra.cuda.libdevice.rint(x3 * inv_scale), -128.0, 127.0).to(tl.int32) & 0xFF
        
        packed = (q0 | (q1 << 8) | (q2 << 16) | (q3 << 24)).to(tl.int32)
        tl.store(out_row32 + offs_out, packed, mask=mask_out)


def quant_slide_int8_triton(
    x: torch.Tensor,
    L: int = 8,
    block_groups: int = None,  # Ignored, kept for API compatibility
    num_warps: int = None,     # Ignored, kept for API compatibility
    num_stages: int = None,    # Ignored, kept for API compatibility
) -> Tuple[torch.Tensor, torch.Tensor]:

    # Note: block_groups, num_warps, num_stages are ignored (tuned values used)
    assert x.is_cuda and x.is_contiguous()
    assert L >= 4 and L % 2 == 0
    
    M, K_in_orig = x.shape
    K_in_padded, K_out, num_groups = _compute_output_k(K_in_orig, L)
    num_windows = _get_num_windows(L)
    
    K_out_padded = ((K_out + 31) // 32) * 32
    M_padded = ((M + 15) // 16) * 16
    
    # 使用 zeros 分配，padding 区域天然为 0（torch.compile 友好）
    out = torch.zeros(M_padded, K_out_padded, dtype=torch.int8, device=x.device)
    # scale padding 为 1.0，避免 dequant 时除以 0
    scale = torch.ones(M_padded, dtype=torch.float32, device=x.device)
    
    BLOCK_OUT, BLOCK_K, num_warps, num_stages = _get_config(M, K_in_orig)
    
    _quant_slide_int8_kernel[(M,)](
        x, out, scale,
        M, K_in_orig, K_in_padded, K_out, num_groups,
        x.stride(0), K_out_padded,  # output stride 使用 K_out_padded
        L=L,
        NUM_WINDOWS=num_windows,
        BLOCK_OUT=BLOCK_OUT,
        BLOCK_K=BLOCK_K,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return out, scale


__all__ = ['quant_slide_fp8_triton', 'quant_slide_int8_triton', '_get_config', '_compute_output_k', '_get_num_windows']
