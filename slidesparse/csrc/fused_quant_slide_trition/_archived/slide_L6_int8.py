"""
Fused INT8 Quantization + SlideSparse Slide Kernel

Auto-generated by codegen_unified.py at 2026-01-07 11:52:09

Configuration:
    L = 6  (2:6 sparsity)
    L_PAD = 8  (for vectorization)
    dtype = int8
    expand_ratio = 1.333x

Regenerate: python3 codegen_unified.py 6 --int8
"""

import torch
import triton
import triton.language as tl


# Configuration
L = 6
L_PAD = 8
N = 3
NUM_WINDOWS = 2
EXPAND_RATIO = 1.3333333333333333
DTYPE = "int8"


def get_autotune_configs():
    """统一的 autotune 配置 (BLOCK_GROUPS 为 2 的幂次)"""
    return [
        # Tier 1: 小 BLOCK_GROUPS (适合小 K 或高寄存器压力场景)
        triton.Config({'BLOCK_GROUPS': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 64}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 128}, num_warps=8, num_stages=3),
        # Tier 2: 中等 BLOCK_GROUPS (通用场景)
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=4, num_stages=4),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 256}, num_warps=8, num_stages=4),
        # Tier 3: 大 BLOCK_GROUPS (H100 高带宽利用)
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=4),
        triton.Config({'BLOCK_GROUPS': 512}, num_warps=8, num_stages=5),
        # Tier 4: 超大 BLOCK_GROUPS (大 K 高吞吐场景)
        triton.Config({'BLOCK_GROUPS': 1024}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 1024}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_GROUPS': 1024}, num_warps=8, num_stages=4),
        triton.Config({'BLOCK_GROUPS': 2048}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_GROUPS': 2048}, num_warps=8, num_stages=3),
    ]


@triton.autotune(configs=get_autotune_configs(), key=['num_groups'])
@triton.jit
def _fused_quant_slide_kernel(
    x_ptr, y_ptr, scale_ptr,
    M, K_in_orig, K_in, K_out, num_groups,
    stride_x, stride_y,
    BLOCK_GROUPS: tl.constexpr,
):
    """Fused INT8 Quant + Slide for L=6"""
    row = tl.program_id(0)
    x_row = x_ptr + row * stride_x
    y_row = y_ptr + row * stride_y
    y_row32 = y_row.to(tl.pointer_type(tl.int32))

    # BLOCK_K = BLOCK_GROUPS * 8 (L_PAD=8 是 >= L=6 的最小 2 的幂次)
    BLOCK_K: tl.constexpr = BLOCK_GROUPS * 8

    # Pass 1: absmax (向量化，用 K_in_orig 做边界检查避免越界)
    absmax = tl.zeros((), dtype=tl.float32)
    for k in range(0, K_in, BLOCK_K):
        offs = k + tl.arange(0, BLOCK_K)
        mask = offs < K_in_orig  # 使用原始 K，越界位置 load 0
        xb = tl.load(x_row + offs, mask=mask, other=0.0).to(tl.float32)
        absmax = tl.maximum(absmax, tl.max(tl.abs(xb), axis=0))

    absmax = tl.maximum(absmax, 1e-5)
    scale = absmax / 127.0
    inv_scale = 127.0 / absmax
    tl.store(scale_ptr + row, scale)

    # Pass 2: Quant + Slide
    for g_start in range(0, num_groups, BLOCK_GROUPS):
        offs_g = tl.arange(0, BLOCK_GROUPS)
        gid = g_start + offs_g
        mask_group = gid < num_groups
        base = gid * 6

        # Load 6 elements per group (用 K_in_orig 做边界检查)
        x0 = tl.load(x_row + base + 0, mask=mask_group & ((base + 0) < K_in_orig), other=0.0).to(tl.float32)
        x1 = tl.load(x_row + base + 1, mask=mask_group & ((base + 1) < K_in_orig), other=0.0).to(tl.float32)
        x2 = tl.load(x_row + base + 2, mask=mask_group & ((base + 2) < K_in_orig), other=0.0).to(tl.float32)
        x3 = tl.load(x_row + base + 3, mask=mask_group & ((base + 3) < K_in_orig), other=0.0).to(tl.float32)
        x4 = tl.load(x_row + base + 4, mask=mask_group & ((base + 4) < K_in_orig), other=0.0).to(tl.float32)
        x5 = tl.load(x_row + base + 5, mask=mask_group & ((base + 5) < K_in_orig), other=0.0).to(tl.float32)

        # Quantize to int8
        b0 = tl.extra.cuda.libdevice.rint(x0 * inv_scale).to(tl.int32) & 0xFF
        b1 = tl.extra.cuda.libdevice.rint(x1 * inv_scale).to(tl.int32) & 0xFF
        b2 = tl.extra.cuda.libdevice.rint(x2 * inv_scale).to(tl.int32) & 0xFF
        b3 = tl.extra.cuda.libdevice.rint(x3 * inv_scale).to(tl.int32) & 0xFF
        b4 = tl.extra.cuda.libdevice.rint(x4 * inv_scale).to(tl.int32) & 0xFF
        b5 = tl.extra.cuda.libdevice.rint(x5 * inv_scale).to(tl.int32) & 0xFF

        # Slide windows output
        out_base = gid * 2
        tl.store(y_row32 + out_base + 0, (b0 | (b1 << 8) | (b2 << 16) | (b3 << 24)).to(tl.int32), mask=mask_group)
        tl.store(y_row32 + out_base + 1, (b2 | (b3 << 8) | (b4 << 16) | (b5 << 24)).to(tl.int32), mask=mask_group)


def fused_quant_slide(x: torch.Tensor):
    """
    Fused INT8 Quantization + SlideSparse Slide
    
    Args:
        x: Input tensor [M, K_in], bf16/fp16/fp32
    
    Returns:
        y: Output tensor [M, K_out_padded] (int8), padded to 16-byte alignment
        scale: Scale tensor [M], fp32
    """
    assert x.dim() == 2
    M, K_in_orig = x.shape
    
    # 计算 padded K (不实际 pad，在 kernel 里用 mask 处理)
    K_in = ((K_in_orig + L - 1) // L) * L
    
    num_groups = K_in // L
    K_out = num_groups * NUM_WINDOWS * 4
    
    # Pad output to 16-byte alignment
    K_out_padded = ((K_out + 15) // 16) * 16
    y = torch.empty(M, K_out_padded, dtype=torch.int8, device=x.device)
    if K_out_padded > K_out:
        y[:, K_out:].zero_()
    scale = torch.empty(M, dtype=torch.float32, device=x.device)
    
    # 传递 K_in_orig 给 kernel，在 kernel 里用 mask 处理越界访问
    _fused_quant_slide_kernel[(M,)](
        x, y, scale, M, K_in_orig, K_in, K_out, num_groups, x.stride(0), y.stride(0),
    )
    return y, scale


def get_config():
    return {'L': L, 'L_PAD': L_PAD, 'N': N, 'NUM_WINDOWS': NUM_WINDOWS, 'EXPAND_RATIO': EXPAND_RATIO, 'DTYPE': DTYPE}
