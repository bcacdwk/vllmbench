#!/usr/bin/env python3
# SPDX-License-Identifier: Apache-2.0
"""
SlideSparse CSRC ÁºñËØëÂ∑•ÂÖ∑Â∫ì

Êú¨Ê®°ÂùóÊèê‰æõ CUDA Êâ©Â±ïÁºñËØëÁõ∏ÂÖ≥ÁöÑÈÄöÁî®Â∑•ÂÖ∑ÂáΩÊï∞„ÄÇ

Ê≥®ÊÑèÔºöÊñá‰ª∂Âêç„ÄÅÁ°¨‰ª∂‰ø°ÊÅØÁ≠âÂäüËÉΩËØ∑‰ΩøÁî®È°∂Â±Ç slidesparse.utils Ê®°Âùó„ÄÇ

‰∏ªË¶ÅÂäüËÉΩ
========
1. NVCC Êû∂ÊûÑÊ†áÂøóÁîüÊàê
2. CUDA Êâ©Â±ïÁºñËØëÂô®ÔºàÊîØÊåÅ cuBLASLt, cuSPARSELt Á≠âÔºâ
3. ÁºñËØë‰∫ßÁâ©Ê∏ÖÁêÜ
4. Triton Autotune ÈÖçÁΩÆ

‰ΩøÁî®Á§∫‰æã
========
>>> from slidesparse.csrc.utils import build_cuda_extension, get_nvcc_arch_flags
>>>
>>> # ÁºñËØë cuBLASLt Êâ©Â±ï
>>> so_path = build_cuda_extension(
...     name="cublaslt_gemm",
...     source_file=Path("cublaslt_gemm.cu"),
...     build_dir=Path("build"),
...     extra_ldflags=["-lcublasLt", "-lcublas"],
... )
"""

import os
import sys
import shutil
from pathlib import Path
from typing import List, Optional, Callable

import torch
from torch.utils.cpp_extension import load


# =============================================================================
# NVCC Êû∂ÊûÑÊ†áÂøó
# =============================================================================

# ÊîØÊåÅÁöÑ GPU Êû∂ÊûÑÂàóË°®
SUPPORTED_ARCHITECTURES = [
    ("80", "sm_80"),   # Ampere (A100, A10, A30)
    ("86", "sm_86"),   # Ampere (RTX 30xx)
    ("89", "sm_89"),   # Ada Lovelace (RTX 40xx)
    ("90", "sm_90"),   # Hopper (H100, H200)
    ("100", "sm_100"), # Blackwell (B100, B200)
    ("120", "sm_120"), # Blackwell (RTX 50xx, GB10)
]


def get_nvcc_arch_flags(
    min_compute: int = 80,
    max_compute: int = 120,
) -> List[str]:
    """
    ÁîüÊàê nvcc Êû∂ÊûÑÁºñËØëÈÄâÈ°π
    
    ÊîØÊåÅ‰ªé SM 80 (Ampere) Âà∞ SM 120 (Blackwell)
    
    Args:
        min_compute: ÊúÄÂ∞èÊîØÊåÅÁöÑ compute capability (ÈªòËÆ§ 80)
        max_compute: ÊúÄÂ§ßÊîØÊåÅÁöÑ compute capability (ÈªòËÆ§ 120)
        
    Returns:
        nvcc -gencode Ê†áÂøóÂàóË°®
        
    Example:
        >>> get_nvcc_arch_flags()
        ['-gencode=arch=compute_80,code=sm_80', ...]
    """
    flags = []
    for compute, sm in SUPPORTED_ARCHITECTURES:
        cc = int(compute)
        if min_compute <= cc <= max_compute:
            flags.append(f"-gencode=arch=compute_{compute},code={sm}")
    return flags


def get_current_arch_flag() -> str:
    """
    Ëé∑ÂèñÂΩìÂâç GPU Êû∂ÊûÑÁöÑ nvcc ÁºñËØëÊ†áÂøó
    
    Returns:
        Âçï‰∏™ -gencode Ê†áÂøóÔºåÈíàÂØπÂΩìÂâç GPU
        
    Raises:
        RuntimeError: CUDA ‰∏çÂèØÁî®
    """
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA not available")
    prop = torch.cuda.get_device_properties(0)
    compute = f"{prop.major}{prop.minor}"
    return f"-gencode=arch=compute_{compute},code=sm_{compute}"


# =============================================================================
# CUDA Êâ©Â±ïÁºñËØëÂô®
# =============================================================================

# ÈªòËÆ§ÁºñËØëÈÄâÈ°π
DEFAULT_CFLAGS = ['-O3', '-std=c++17']

DEFAULT_CUDA_CFLAGS = [
    '-O3',
    '-std=c++17',
    '--expt-relaxed-constexpr',
    '--expt-extended-lambda',
    '-U__CUDA_NO_HALF_OPERATORS__',
    '-U__CUDA_NO_HALF_CONVERSIONS__',
    '-U__CUDA_NO_BFLOAT16_CONVERSIONS__',
]


def should_rebuild(so_path: Path, source_paths: List[Path]) -> bool:
    """
    Âà§Êñ≠ÊòØÂê¶ÈúÄË¶ÅÈáçÊñ∞ÁºñËØë
    
    Â¶ÇÊûú .so ‰∏çÂ≠òÂú®ÊàñÊØî‰ªª‰∏ÄÊ∫êÊñá‰ª∂ÊóßÔºåËøîÂõû True
    
    Args:
        so_path: .so Êñá‰ª∂Ë∑ØÂæÑ
        source_paths: Ê∫êÊñá‰ª∂Ë∑ØÂæÑÂàóË°®
        
    Returns:
        ÊòØÂê¶ÈúÄË¶ÅÈáçÊñ∞ÁºñËØë
    """
    if not so_path.exists():
        return True
    
    so_mtime = so_path.stat().st_mtime
    for src in source_paths:
        if src.exists() and src.stat().st_mtime > so_mtime:
            return True
    return False


def clean_build_artifacts(build_dir: Path, keep_extensions: List[str] = None):
    """
    Ê∏ÖÁêÜÁºñËØë‰∏≠Èó¥Êñá‰ª∂
    
    ÈªòËÆ§‰øùÁïô .so Âíå .py Êñá‰ª∂ÔºåÂà†Èô§ÂÖ∂‰ªñÊâÄÊúâÂÜÖÂÆπ„ÄÇ
    
    Args:
        build_dir: ÊûÑÂª∫ÁõÆÂΩï
        keep_extensions: Ë¶Å‰øùÁïôÁöÑÊñá‰ª∂Êâ©Â±ïÂêçÂàóË°®ÔºàÈªòËÆ§ ['.so', '.py']Ôºâ
    """
    if keep_extensions is None:
        keep_extensions = ['.so', '.py']
    
    if not build_dir.exists():
        return
    
    for item in build_dir.iterdir():
        # ‰øùÁïôÊåáÂÆöÊâ©Â±ïÂêçÁöÑÊñá‰ª∂
        if item.suffix in keep_extensions:
            continue
        
        # Âà†Èô§ÂÖ∂‰ªñÊñá‰ª∂ÂíåÁõÆÂΩï
        if item.is_dir():
            shutil.rmtree(item)
        else:
            item.unlink()


def build_cuda_extension(
    name: str,
    source_file: Path,
    build_dir: Path,
    *,
    extra_cflags: List[str] = None,
    extra_cuda_cflags: List[str] = None,
    extra_ldflags: List[str] = None,
    extra_include_paths: List[str] = None,
    force: bool = False,
    verbose: bool = True,
    clean_after_build: bool = True,
) -> Path:
    """
    ÁºñËØë CUDA Êâ©Â±ïÁöÑÈÄöÁî®ÂáΩÊï∞
    
    ÊîØÊåÅÁºñËØë cuBLASLt, cuSPARSELt Á≠â CUDA Êâ©Â±ï„ÄÇ
    
    Args:
        name: Êâ©Â±ïÂêçÁß∞Ôºà‰∏çÂê´ .so ÂêéÁºÄÔºâ
        source_file: Ê∫êÊñá‰ª∂Ë∑ØÂæÑ (.cu Êàñ .cpp)
        build_dir: ÊûÑÂª∫ÁõÆÂΩï
        extra_cflags: È¢ùÂ§ñÁöÑ C++ ÁºñËØëÊ†áÂøó
        extra_cuda_cflags: È¢ùÂ§ñÁöÑ CUDA ÁºñËØëÊ†áÂøó
        extra_ldflags: È¢ùÂ§ñÁöÑÈìæÊé•Ê†áÂøóÔºàÂ¶Ç -lcublasLtÔºâ
        extra_include_paths: È¢ùÂ§ñÁöÑÂ§¥Êñá‰ª∂ÊêúÁ¥¢Ë∑ØÂæÑ
        force: ÊòØÂê¶Âº∫Âà∂ÈáçÊñ∞ÁºñËØë
        verbose: ÊòØÂê¶ÊòæÁ§∫ËØ¶ÁªÜËæìÂá∫
        clean_after_build: ÁºñËØëÂêéÊòØÂê¶Ê∏ÖÁêÜ‰∏≠Èó¥Êñá‰ª∂
        
    Returns:
        ÁºñËØëÁîüÊàêÁöÑ .so Êñá‰ª∂Ë∑ØÂæÑ
        
    Raises:
        FileNotFoundError: Ê∫êÊñá‰ª∂‰∏çÂ≠òÂú®
        RuntimeError: ÁºñËØëÂ§±Ë¥•
        
    Example:
        >>> so_path = build_cuda_extension(
        ...     name="cublaslt_gemm_H100_cc90_FP8E4M3_py312_cu124_x86_64",
        ...     source_file=Path("cublaslt_gemm.cu"),
        ...     build_dir=Path("build"),
        ...     extra_ldflags=["-lcublasLt", "-lcublas", "-lcuda"],
        ... )
    """
    # È™åËØÅÊ∫êÊñá‰ª∂
    if not source_file.exists():
        raise FileNotFoundError(f"Ê∫êÊñá‰ª∂‰∏çÂ≠òÂú®: {source_file}")
    
    # Á°Æ‰øù build ÁõÆÂΩïÂ≠òÂú®
    build_dir.mkdir(parents=True, exist_ok=True)
    
    # Êü•ÊâæÂ∑≤Â≠òÂú®ÁöÑ .so
    so_pattern = f"{name}*.so"
    existing_sos = list(build_dir.glob(so_pattern))
    
    if existing_sos and not force:
        so_path = existing_sos[0]
        if not should_rebuild(so_path, [source_file]):
            if verbose:
                print(f"‚úì Using existing: {so_path.name}")
            return so_path
        elif verbose:
            print(f"‚ö† Source changed, rebuilding...")
    
    if verbose:
        print(f"üî® Building {name}...")
    
    # CUDA Ë∑ØÂæÑ
    cuda_home = os.environ.get('CUDA_HOME', '/usr/local/cuda')
    
    # ÂêàÂπ∂ÁºñËØëÈÄâÈ°π
    cflags = DEFAULT_CFLAGS + (extra_cflags or [])
    cuda_cflags = DEFAULT_CUDA_CFLAGS + get_nvcc_arch_flags() + (extra_cuda_cflags or [])
    ldflags = extra_ldflags or []
    include_paths = [os.path.join(cuda_home, 'include')] + (extra_include_paths or [])
    
    # ÁºñËØë
    try:
        load(
            name=name,
            sources=[str(source_file)],
            extra_cflags=cflags,
            extra_cuda_cflags=cuda_cflags,
            extra_ldflags=ldflags,
            extra_include_paths=include_paths,
            build_directory=str(build_dir),
            verbose=verbose,
        )
    except Exception as e:
        raise RuntimeError(f"ÁºñËØëÂ§±Ë¥•: {e}") from e
    
    # Êü•ÊâæÁîüÊàêÁöÑ .so
    new_sos = list(build_dir.glob(so_pattern))
    if not new_sos:
        raise RuntimeError(f"ÁºñËØëÂÆåÊàê‰ΩÜÊú™ÊâæÂà∞ .so Êñá‰ª∂: {so_pattern}")
    
    so_path = new_sos[0]
    
    if verbose:
        print(f"‚úì Built: {so_path.name}")
    
    # Ê∏ÖÁêÜ‰∏≠Èó¥Êñá‰ª∂
    if clean_after_build:
        if verbose:
            print(f"üßπ Cleaning build artifacts...")
        clean_build_artifacts(build_dir)
    
    return so_path


# =============================================================================
# ÁâπÂÆöÊâ©Â±ïÁöÑÈìæÊé•Â∫ìÈÖçÁΩÆ
# =============================================================================

# cuBLASLt Êâ©Â±ïÊâÄÈúÄÁöÑÈìæÊé•Â∫ì
CUBLASLT_LDFLAGS = ['-lcublasLt', '-lcublas', '-lcuda']

# cuSPARSELt Êâ©Â±ïÊâÄÈúÄÁöÑÈìæÊé•Â∫ì
CUSPARSELT_LDFLAGS = ['-lcusparseLt', '-lcusparse', '-lcuda']


def get_gemm_ldflags(backend: str) -> List[str]:
    """
    Ëé∑Âèñ GEMM ÂêéÁ´ØÊâÄÈúÄÁöÑÈìæÊé•Â∫ìÊ†áÂøó
    
    Args:
        backend: ÂêéÁ´ØÂêçÁß∞ ("cublaslt" Êàñ "cusparselt")
        
    Returns:
        ÈìæÊé•Â∫ìÊ†áÂøóÂàóË°®
        
    Raises:
        ValueError: Êú™Áü•ÁöÑÂêéÁ´Ø
    """
    if backend.lower() == "cublaslt":
        return CUBLASLT_LDFLAGS.copy()
    elif backend.lower() == "cusparselt":
        return CUSPARSELT_LDFLAGS.copy()
    else:
        raise ValueError(f"Êú™Áü•ÁöÑ GEMM ÂêéÁ´Ø: {backend}")


# =============================================================================
# Triton Autotune ÈÖçÁΩÆ
# =============================================================================

def get_dequant_bias_autotune_configs():
    """
    Ëé∑Âèñ dequant+bias kernel ÁöÑ Triton autotune ÈÖçÁΩÆ
    
    Ë¶ÜÁõñ: SM80(A100), SM89(4090), SM90(H100), SM100(B200), SM120(5080)
    
    ÈÖçÁΩÆÊåâ A100 È™åËØÅÁöÑÊÄßËÉΩÊéíÂ∫è„ÄÇ
    
    Returns:
        triton.Config ÂØπË±°ÂàóË°®
    """
    import triton
    
    return [
        # =====================================================================
        # Tier 1: Proven Winners (A100 validated)
        # =====================================================================
        # Small M King (M=1~128): 32x32
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4, num_stages=4),
        # Medium M King (M=256~8192): 64x32
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=4, num_stages=4),
        # Large M King (M=12288+): 128x64
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=8, num_stages=4),

        # =====================================================================
        # Tier 2: Basic kernel heuristics
        # =====================================================================
        # Small M, N<=4096: (32, 64, 4)
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=4, num_stages=4),
        # Small M, N>4096: (32, 128, 4)
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}, num_warps=4, num_stages=4),
        # Medium M, N<=4096: (64, 64, 4)
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=4, num_stages=4),
        # Medium M, N>4096: (64, 128, 8)
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128}, num_warps=8, num_stages=4),
        # Large M, N>4096: (128, 128, 8)
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=8, num_stages=4),

        # =====================================================================
        # Tier 3: Read/Write bias exploration
        # =====================================================================
        # Write Heavy (tall blocks): 128x32
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32}, num_warps=8, num_stages=4),
        # Read Heavy (wide blocks): 64x128 with lower warps
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128}, num_warps=4, num_stages=4),
        # Balanced high warp: 64x64 w=8
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=8, num_stages=4),
        # Low warp large block
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=4, num_stages=4),

        # =====================================================================
        # Tier 4: H100/Blackwell exploration (SM90/100/120)
        # =====================================================================
        # Super Wide: 256x64
        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64}, num_warps=16, num_stages=3),
        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64}, num_warps=8, num_stages=3),
        # Super Tall: 64x256
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256}, num_warps=16, num_stages=3),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256}, num_warps=8, num_stages=3),
        # Super Square: 128x128 high warp
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=16, num_stages=3),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=16, num_stages=4),
        # Wide variants for large N
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 256}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 256}, num_warps=8, num_stages=4),
        # Extreme Wide: 256x32
        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 32}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 32}, num_warps=8, num_stages=4),

        # =====================================================================
        # Tier 5: Small M + Large N special cases
        # =====================================================================
        # 32x128 with various warps
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}, num_warps=2, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}, num_warps=2, num_stages=3),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}, num_warps=8, num_stages=3),
        # 32x64 with high warps
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=8, num_stages=4),
        # 32x64 with low warps
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=2, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=2, num_stages=3),

        # =====================================================================
        # Tier 6: Tiny M = 16 special cases
        # =====================================================================
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 64}, num_warps=2, num_stages=2),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 64}, num_warps=2, num_stages=3),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 64}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 128}, num_warps=2, num_stages=2),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 128}, num_warps=2, num_stages=3),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 128}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 128}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 32}, num_warps=2, num_stages=2),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 32}, num_warps=2, num_stages=3),
        # Very wide for large N
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 256}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 256}, num_warps=4, num_stages=3),
    ]


# =============================================================================
# ÂØºÂá∫
# =============================================================================

__all__ = [
    # NVCC Êû∂ÊûÑÊ†áÂøó
    'SUPPORTED_ARCHITECTURES',
    'get_nvcc_arch_flags',
    'get_current_arch_flag',
    # CUDA ÁºñËØë
    'DEFAULT_CFLAGS',
    'DEFAULT_CUDA_CFLAGS',
    'should_rebuild',
    'clean_build_artifacts',
    'build_cuda_extension',
    # GEMM ÈìæÊé•Â∫ì
    'CUBLASLT_LDFLAGS',
    'CUSPARSELT_LDFLAGS',
    'get_gemm_ldflags',
    # Triton ÈÖçÁΩÆ
    'get_dequant_bias_autotune_configs',
]
