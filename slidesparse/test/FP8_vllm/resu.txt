root@spark-1061:~/vllmbench/slidesparse/test/FP8_vllm# python3 run_cublaslt_tests.py

════════════════════════════════════════════════════════════════════════════════
                    SlideSparse cuBLASLt 一键测试
════════════════════════════════════════════════════════════════════════════════
  后端: cuBLASLt + BF16
════════════════════════════════════════════════════════════════════════════════


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[1/4] 桥接与集成测试 (test_01_bridge.py)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

>>> 运行: python3 test_01_bridge.py 

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
环境信息
----------------------------------------------------------------------
  Python: 3.12.12
  PyTorch: 2.9.0+cu129
  vLLM: 0.13.1.dev52+gc34425c3d
  CUDA: 可用
  GPU: NVIDIA GB10
  Compute Capability: sm_121
  FP8: 支持
----------------------------------------------------------------------
  SlideSparse: 启用
  Kernel 后端: CUTLASS (fallback)
======================================================================
============================================================
测试套件: SlideSparse 桥接与集成测试
============================================================

  运行: test_import_main...
    ✓ 导入 slidesparse 主模块: v0.1.0 (0.00s)

  运行: test_import_core...
    ✓ 导入 slidesparse.core: 核心接口导入成功 (0.00s)

  运行: test_import_config...
    ✓ 导入配置模块: 配置解析正常 (0.00s)

  运行: test_import_linear_method...
    ✓ 导入 LinearMethod 模块: 类结构正确 (0.00s)

  运行: test_env_vars...
    ✓ 环境变量解析: SlideSparse=True, cuBLASLt=False, cuSPARSELt=False, FP32=False (0.00s)

  运行: test_status_format...
    ✓ get_slidesparse_status 格式: SlideSparse ENABLED, CUTLASS kernel (fallback) (0.00s)

  运行: test_mutual_exclusion...
    ✓ 互斥配置校验: 互斥校验正常 (0.00s)

  运行: test_vllm_bridge_import...
    ✓ vLLM 桥接文件导入: vLLM 桥接模块正常 (0.00s)

  运行: test_bridge_reference_consistency...
    ✓ 桥接函数引用一致性: 函数引用一致 (0.00s)

  运行: test_compressed_tensors_import...
    ✓ CompressedTensors 模块导入: CompressedTensors 可用 (0.13s)

  运行: test_fp8_scheme...
    ✓ FP8 Scheme 类可用: FP8 scheme 可用 (0.00s)

  运行: test_create_op...
WARNING 01-14 23:00:42 [vllm.py:1403] Current vLLM config is not set.
INFO 01-14 23:00:42 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
    ✓ SlideSparseFp8LinearOp 创建: Op 创建成功 (kernel=CUTLASS) (0.03s)

  运行: test_quant_fp8_config...
    ✓ QuantFP8 配置一致性: 配置一致 (0.00s)

  运行: test_wrap_scheme_fp8...
SlideSparse wrapper not supported for MockScheme, using original scheme
    ✓ wrap_scheme_fp8 统一入口: 统一入口正常 (0.00s)

  运行: test_cublaslt_extension_load...
    ✓ cuBLASLt Extension 加载状态: Extension 已加载，cublaslt_fp8_mm 可用 (0.00s)

  运行: test_cublaslt_extension_signatures...
    ✓ cuBLASLt Extension 函数签名: 函数签名正确 (0.00s)

  运行: test_cusparselt_extension_load...
    ✓ cuSPARSELt Extension 加载状态: Extension 加载异常: Build script not found: /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py (0.00s)

  运行: test_cusparselt_extension_signatures...
    ⚠ cuSPARSELt Extension 函数签名: Extension 加载异常: Build script not found: /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py (0.00s)

============================================================
测试套件: SlideSparse 桥接与集成测试
------------------------------------------------------------
通过: 17  失败: 0  跳过: 0
耗时: 0.16s
------------------------------------------------------------
  ✓ 导入 slidesparse 主模块: v0.1.0 (0.00s)
  ✓ 导入 slidesparse.core: 核心接口导入成功 (0.00s)
  ✓ 导入配置模块: 配置解析正常 (0.00s)
  ✓ 导入 LinearMethod 模块: 类结构正确 (0.00s)
  ✓ 环境变量解析: SlideSparse=True, cuBLASLt=False, cuSPARSELt=False, FP32=False (0.00s)
  ✓ get_slidesparse_status 格式: SlideSparse ENABLED, CUTLASS kernel (fallback) (0.00s)
  ✓ 互斥配置校验: 互斥校验正常 (0.00s)
  ✓ vLLM 桥接文件导入: vLLM 桥接模块正常 (0.00s)
  ✓ 桥接函数引用一致性: 函数引用一致 (0.00s)
  ✓ CompressedTensors 模块导入: CompressedTensors 可用 (0.13s)
  ✓ FP8 Scheme 类可用: FP8 scheme 可用 (0.00s)
  ✓ SlideSparseFp8LinearOp 创建: Op 创建成功 (kernel=CUTLASS) (0.03s)
  ✓ QuantFP8 配置一致性: 配置一致 (0.00s)
  ✓ wrap_scheme_fp8 统一入口: 统一入口正常 (0.00s)
  ✓ cuBLASLt Extension 加载状态: Extension 已加载，cublaslt_fp8_mm 可用 (0.00s)
  ✓ cuBLASLt Extension 函数签名: 函数签名正确 (0.00s)
  ✓ cuSPARSELt Extension 加载状态: Extension 加载异常: Build script not found: /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py (0.00s)
  ⚠ cuSPARSELt Extension 函数签名: Extension 加载异常: Build script not found: /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py (0.00s)
============================================================

✓ test_01_bridge.py 完成

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2/4] Kernel 正确性测试 (test_02_kernel.py)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

>>> 运行: python3 test_02_kernel.py --use-cublaslt

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
环境信息
----------------------------------------------------------------------
  Python: 3.12.12
  PyTorch: 2.9.0+cu129
  vLLM: 0.13.1.dev52+gc34425c3d
  CUDA: 可用
  GPU: NVIDIA GB10
  Compute Capability: sm_121
  FP8: 支持
----------------------------------------------------------------------
  SlideSparse: 启用
  Kernel 后端: cuBLASLt
  GEMM Inner Dtype: BF16
======================================================================
============================================================
测试套件: SlideSparse Kernel 正确性测试
============================================================

  运行: test_cuda_available...
    ✓ CUDA 可用性: NVIDIA GB10 (sm_121) (0.00s)

  运行: test_fp8_support...
    ✓ FP8 支持: sm_121 >= sm_89 (0.00s)

  运行: test_op_basic...
如果quant和dequant Triton kernel没有提前搜索, 会有首次运行的搜索开销
    ✓ SlideSparseFp8LinearOp 基本功能: 输出形状 torch.Size([64, 512]), kernel=cuBLASLt (0.64s)

  运行: test_single_correctness...
    ✓ 单次正确性验证: CUTLASS 不支持当前 GPU，跳过对比 (test 输出 shape=torch.Size([128, 1024])) (0.05s)

  运行: test_batch_correctness...

====================================================================================================
vLLM 原生 vs SlideSparse + cuBLASLt 正确性对比
====================================================================================================
测试用例                 |      M |      N |      K |   Max Diff |    Mean Diff |   Status
----------------------------------------------------------------------------------------------------

注意: CUTLASS 不支持当前 GPU (sm_121)，跳过 baseline 对比
只验证 SlideSparse 路径能正常运行

Small (M=16)         |     16 |    896 |    896 |        N/A |          N/A | SKIP
Small (M=32)         |     32 |    896 |    896 |        N/A |          N/A | SKIP
Medium (M=128)       |    128 |   4096 |   4096 |        N/A |          N/A | SKIP
Medium (M=256)       |    256 |   4096 |   4096 |        N/A |          N/A | SKIP
Large (M=1024)       |   1024 |   4096 |   4096 |        N/A |          N/A | SKIP
Large (M=4096)       |   4096 |   4096 |   4096 |        N/A |          N/A | SKIP
Qwen-0.5B QKV        |     64 |   2688 |    896 |        N/A |          N/A | SKIP
Qwen-0.5B FFN        |     64 |   4864 |    896 |        N/A |          N/A | SKIP
Llama-1B QKV         |     64 |   6144 |   2048 |        N/A |          N/A | SKIP
Llama-1B FFN         |     64 |   8192 |   2048 |        N/A |          N/A | SKIP
----------------------------------------------------------------------------------------------------
总计: 10/10 通过
====================================================================================================
    ✓ 批量正确性测试: 全部 10 个测试通过 (0.10s)

  运行: test_performance_comparison...

==================================================================================================================================
SlideSparse + cuBLASLt 性能测试
注意: CUTLASS 不支持当前 GPU (sm_121)，跳过 baseline 对比
==================================================================================================================================
Warmup: 10, Repeat: 50
----------------------------------------------------------------------------------------------------------------------------------
测试用例                 |      M |      N |      K |     Test(ms)
----------------------------------------------------------------------------------------------------------------------------------
Small (M=16)         |     16 |    896 |    896 |       0.0599
Small (M=32)         |     32 |    896 |    896 |       0.0588
Medium (M=128)       |    128 |   4096 |   4096 |       0.1220
Medium (M=256)       |    256 |   4096 |   4096 |       0.2333
Large (M=1024)       |   1024 |   4096 |   4096 |       0.5757
Large (M=4096)       |   4096 |   4096 |   4096 |       1.5537
Qwen-0.5B QKV        |     64 |   2688 |    896 |       0.0575
Qwen-0.5B FFN        |     64 |   4864 |    896 |       0.0582
Llama-1B QKV         |     64 |   6144 |   2048 |       0.0597
Llama-1B FFN         |     64 |   8192 |   2048 |       0.0984
----------------------------------------------------------------------------------------------------------------------------------
(无 baseline 对比，仅显示 SlideSparse + cuBLASLt 性能)
==================================================================================================================================
    ✓ 性能对比测试: CUTLASS 不支持当前 GPU，已完成 10 个性能测试 (0.27s)

============================================================
测试套件: SlideSparse Kernel 正确性测试
------------------------------------------------------------
通过: 6  失败: 0  跳过: 0
耗时: 1.06s
------------------------------------------------------------
  ✓ CUDA 可用性: NVIDIA GB10 (sm_121) (0.00s)
  ✓ FP8 支持: sm_121 >= sm_89 (0.00s)
  ✓ SlideSparseFp8LinearOp 基本功能: 输出形状 torch.Size([64, 512]), kernel=cuBLASLt (0.64s)
  ✓ 单次正确性验证: CUTLASS 不支持当前 GPU，跳过对比 (test 输出 shape=torch.Size([128, 1024])) (0.05s)
  ✓ 批量正确性测试: 全部 10 个测试通过 (0.10s)
  ✓ 性能对比测试: CUTLASS 不支持当前 GPU，已完成 10 个性能测试 (0.27s)
============================================================

✓ test_02_kernel.py 完成

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[3/4] 端到端推理对比 (test_03_inference.py)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

>>> 运行: python3 test_03_inference.py --use-cublaslt

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
环境信息
----------------------------------------------------------------------
  Python: 3.12.12
  PyTorch: 2.9.0+cu129
  vLLM: 0.13.1.dev52+gc34425c3d
  CUDA: 可用
  GPU: NVIDIA GB10
  Compute Capability: sm_121
  FP8: 支持
----------------------------------------------------------------------
  SlideSparse: 启用
  Kernel 后端: CUTLASS (fallback)
======================================================================

================================================================================
SlideSparse + cuBLASLt 推理测试
================================================================================
模型: Qwen2.5-0.5B-FP8
注意: CUTLASS 不支持当前 GPU (sm_121)，跳过 baseline
测试: SlideSparse + cuBLASLt
采样: temperature=0.0, max_tokens=64
================================================================================

[1/1] 运行 SlideSparse + cuBLASLt...
(EngineCore_DP0 pid=812059) /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
(EngineCore_DP0 pid=812059)     Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
(EngineCore_DP0 pid=812059)     Minimum and Maximum cuda capability supported by this version of PyTorch is
(EngineCore_DP0 pid=812059)     (8.0) - (12.0)
(EngineCore_DP0 pid=812059)     
(EngineCore_DP0 pid=812059)   warnings.warn(
(EngineCore_DP0 pid=812059) [2026-01-14 23:00:53] INFO SlideSparseLinearMethod_FP8.py:727: Wrapping CompressedTensorsW8A8Fp8 with SlideSparseFp8LinearMethod (cuBLASLt)
(EngineCore_DP0 pid=812059) [2026-01-14 23:00:53] INFO SlideSparseLinearMethod_FP8.py:537: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt, inner_dtype=bf16, static=False, group_shape=GroupShape(row=1, col=-1))
(EngineCore_DP0 pid=812059) [2026-01-14 23:00:53] INFO SlideSparseLinearMethod_FP8.py:631: SlideSparseFp8LinearMethod initialized, wrapping: CompressedTensorsW8A8Fp8, kernel: cuBLASLt
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.54s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.54s/it]
(EngineCore_DP0 pid=812059) 
(EngineCore_DP0 pid=812059) [2026-01-14 23:00:58] INFO SlideSparseLinearMethod_FP8.py:219: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=812059) [2026-01-14 23:00:58] INFO SlideSparseLinearMethod_FP8.py:329: FP8 quant kernel loaded
(EngineCore_DP0 pid=812059) [2026-01-14 23:00:58] INFO SlideSparseLinearMethod_FP8.py:260: Dequant+bias kernel loaded
(EngineCore_DP0 pid=812059) 2026-01-14 23:01:19,356 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=812059) 2026-01-14 23:01:19,834 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1409.69it/s]
Processed prompts: 100%|██████████████████████████████████████████████████████████| 3/3 [01:38<00:00, 32.97s/it, est. speed input: 0.22 toks/s, output: 1.94 toks/s]
[rank0]:[W114 23:03:00.094454464 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

================================================================================
SlideSparse + cuBLASLt 输出
================================================================================

[Prompt 1] What is the capital of France?
--------------------------------------------------------------------------------
SlideSparse + cuBLASLt: The capital of France is Paris. 

To verify this, I will provide a simple Python code snippet that prints the capital of France:

```python
def capital_of_france():
    return "Paris"

capital = capital_of_france()
print(capital)
```

When I run this code, it will output:

[Prompt 2] Explain quantum computing in one sentence.
--------------------------------------------------------------------------------
SlideSparse + cuBLASLt: Quantum computing uses quantum bits, or qubits, to perform multiple calculations simultaneously, allowing for exponential speedup over classical computers. 

This sentence encapsulates the core concept of quantum computing, highlighting its ability to process information in parallel, which is a key advantage over classical computers. It also emphasizes the qubit's unique property

[Prompt 3] Write a haiku about programming.
--------------------------------------------------------------------------------
SlideSparse + cuBLASLt: Programming is a journey,
Where code is the map and logic the way,
Innovation and efficiency, it's the goal. 

Programming is a journey,
Where code is the map and logic the way,
Innovation and efficiency, it's the goal. 

(Translation: Programming is a quest,
Where code is the

================================================================================
已完成 3 个推理测试 (无 baseline 对比)
================================================================================

✓ test_03_inference.py 完成

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[4/4] 吞吐量对比测试 (test_04_throughput.py)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

>>> 运行: python3 test_04_throughput.py --use-cublaslt

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
环境信息
----------------------------------------------------------------------
  Python: 3.12.12
  PyTorch: 2.9.0+cu129
  vLLM: 0.13.1.dev52+gc34425c3d
  CUDA: 可用
  GPU: NVIDIA GB10
  Compute Capability: sm_121
  FP8: 支持
----------------------------------------------------------------------
  SlideSparse: 启用
  Kernel 后端: CUTLASS (fallback)
======================================================================

==========================================================================================
vLLM 原生路径 vs SlideSparse 吞吐量对比
==========================================================================================
模型: Qwen2.5-0.5B-FP8
基准: vLLM 原生路径 (DISABLE_SLIDESPARSE=1)
测试: SlideSparse + cuBLASLt
==========================================================================================

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Prefill 吞吐量测试
配置: 8 prompts × 256 input tokens × 1 output token
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

  [基准] vLLM 原生路径...
(EngineCore_DP0 pid=813895) /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
(EngineCore_DP0 pid=813895)     Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
(EngineCore_DP0 pid=813895)     Minimum and Maximum cuda capability supported by this version of PyTorch is
(EngineCore_DP0 pid=813895)     (8.0) - (12.0)
(EngineCore_DP0 pid=813895)     
(EngineCore_DP0 pid=813895)   warnings.warn(
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.76s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.76s/it]
(EngineCore_DP0 pid=813895) 
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]                     ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py", line 193, in apply_weights
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return self.fp8_linear.apply(
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/utils/w8a8_utils.py", line 487, in apply
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return w8a8_scaled_mm_func(
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/utils/w8a8_utils.py", line 162, in cutlass_w8a8_scaled_mm
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     output = ops.cutlass_scaled_mm(
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/root/vllmbench/vllm/_custom_ops.py", line 874, in cutlass_scaled_mm
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) ERROR 01-14 23:03:15 [core.py:866] RuntimeError: Error Internal
(EngineCore_DP0 pid=813895) Process EngineCore_DP0:
(EngineCore_DP0 pid=813895) Traceback (most recent call last):
(EngineCore_DP0 pid=813895)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=813895)     self.run()
(EngineCore_DP0 pid=813895)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=813895)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=813895)     raise e
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=813895)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=813895)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=813895)     super().__init__(
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=813895)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=813895)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=813895)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=813895)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=813895)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=813895)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=813895)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=813895)     return func(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=813895)     return func(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=813895)     self.model_runner.profile_run()
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=813895)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=813895)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=813895)     return func(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=813895)     outputs = self.model(
(EngineCore_DP0 pid=813895)               ^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=813895)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=813895)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=813895)     hidden_states = self.model(
(EngineCore_DP0 pid=813895)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=813895)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=813895)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=813895)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=813895)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=813895)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=813895)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=813895)                     ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=813895)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=813895)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=813895)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=813895)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=813895)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=813895)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=813895)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=813895)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=813895)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py", line 193, in apply_weights
(EngineCore_DP0 pid=813895)     return self.fp8_linear.apply(
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/model_executor/layers/quantization/utils/w8a8_utils.py", line 487, in apply
(EngineCore_DP0 pid=813895)     return w8a8_scaled_mm_func(
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/model_executor/layers/quantization/utils/w8a8_utils.py", line 162, in cutlass_w8a8_scaled_mm
(EngineCore_DP0 pid=813895)     output = ops.cutlass_scaled_mm(
(EngineCore_DP0 pid=813895)              ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895)   File "/root/vllmbench/vllm/_custom_ops.py", line 874, in cutlass_scaled_mm
(EngineCore_DP0 pid=813895)     torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias)
(EngineCore_DP0 pid=813895)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=813895)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=813895)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=813895) RuntimeError: Error Internal
[rank0]:[W114 23:03:15.483545537 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
    跳过 (CUTLASS 不支持当前 GPU): Engine core initialization failed. See root cause above. Failed core proc(s): {}
  [测试] SlideSparse + cuBLASLt...
(EngineCore_DP0 pid=814151) /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
(EngineCore_DP0 pid=814151)     Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
(EngineCore_DP0 pid=814151)     Minimum and Maximum cuda capability supported by this version of PyTorch is
(EngineCore_DP0 pid=814151)     (8.0) - (12.0)
(EngineCore_DP0 pid=814151)     
(EngineCore_DP0 pid=814151)   warnings.warn(
(EngineCore_DP0 pid=814151) [2026-01-14 23:03:19] INFO SlideSparseLinearMethod_FP8.py:727: Wrapping CompressedTensorsW8A8Fp8 with SlideSparseFp8LinearMethod (cuBLASLt)
(EngineCore_DP0 pid=814151) [2026-01-14 23:03:19] INFO SlideSparseLinearMethod_FP8.py:537: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt, inner_dtype=bf16, static=False, group_shape=GroupShape(row=1, col=-1))
(EngineCore_DP0 pid=814151) [2026-01-14 23:03:19] INFO SlideSparseLinearMethod_FP8.py:631: SlideSparseFp8LinearMethod initialized, wrapping: CompressedTensorsW8A8Fp8, kernel: cuBLASLt
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.74s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.74s/it]
(EngineCore_DP0 pid=814151) 
(EngineCore_DP0 pid=814151) [2026-01-14 23:03:24] INFO SlideSparseLinearMethod_FP8.py:219: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=814151) [2026-01-14 23:03:24] INFO SlideSparseLinearMethod_FP8.py:329: FP8 quant kernel loaded
(EngineCore_DP0 pid=814151) [2026-01-14 23:03:24] INFO SlideSparseLinearMethod_FP8.py:260: Dequant+bias kernel loaded
(EngineCore_DP0 pid=814151) 2026-01-14 23:03:37,948 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=814151) 2026-01-14 23:03:38,178 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
Adding requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.51it/s]
Processed prompts: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17.96it/s, est. speed input: 4602.11 toks/s, output: 17.98 toks/s]
Adding requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 23.49it/s]
Processed prompts: 100%|███████████████████████████████████████████████████| 2/2 [00:00<00:00, 178.89it/s, est. speed input: 46369.92 toks/s, output: 180.92 toks/s]
Adding requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 47.53it/s]
Processed prompts: 100%|██████████████████████████████████████████████████████| 8/8 [00:00<00:00, 25.34it/s, est. speed input: 6491.99 toks/s, output: 25.35 toks/s]
[rank0]:[W114 23:03:50.610270946 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

  结果:
    SlideSparse + cuBLASLt:     4230.2 tok/s
    (无 baseline 对比)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Decode 吞吐量测试
配置: 4 prompts × 16 input tokens × 128 output tokens
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

  [基准] vLLM 原生路径...
(EngineCore_DP0 pid=814822) /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
(EngineCore_DP0 pid=814822)     Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
(EngineCore_DP0 pid=814822)     Minimum and Maximum cuda capability supported by this version of PyTorch is
(EngineCore_DP0 pid=814822)     (8.0) - (12.0)
(EngineCore_DP0 pid=814822)     
(EngineCore_DP0 pid=814822)   warnings.warn(
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.27s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.27s/it]
(EngineCore_DP0 pid=814822) 
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]                     ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py", line 193, in apply_weights
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return self.fp8_linear.apply(
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/utils/w8a8_utils.py", line 487, in apply
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return w8a8_scaled_mm_func(
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/utils/w8a8_utils.py", line 162, in cutlass_w8a8_scaled_mm
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     output = ops.cutlass_scaled_mm(
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/root/vllmbench/vllm/_custom_ops.py", line 874, in cutlass_scaled_mm
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) ERROR 01-14 23:04:04 [core.py:866] RuntimeError: Error Internal
(EngineCore_DP0 pid=814822) Process EngineCore_DP0:
(EngineCore_DP0 pid=814822) Traceback (most recent call last):
(EngineCore_DP0 pid=814822)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=814822)     self.run()
(EngineCore_DP0 pid=814822)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=814822)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=814822)     raise e
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=814822)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=814822)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=814822)     super().__init__(
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=814822)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=814822)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=814822)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=814822)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=814822)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=814822)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=814822)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=814822)     return func(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=814822)     return func(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=814822)     self.model_runner.profile_run()
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=814822)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=814822)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=814822)     return func(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=814822)     outputs = self.model(
(EngineCore_DP0 pid=814822)               ^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=814822)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=814822)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=814822)     hidden_states = self.model(
(EngineCore_DP0 pid=814822)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=814822)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=814822)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=814822)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=814822)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=814822)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=814822)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=814822)                     ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=814822)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=814822)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=814822)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=814822)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=814822)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=814822)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=814822)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=814822)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=814822)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py", line 193, in apply_weights
(EngineCore_DP0 pid=814822)     return self.fp8_linear.apply(
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/model_executor/layers/quantization/utils/w8a8_utils.py", line 487, in apply
(EngineCore_DP0 pid=814822)     return w8a8_scaled_mm_func(
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/model_executor/layers/quantization/utils/w8a8_utils.py", line 162, in cutlass_w8a8_scaled_mm
(EngineCore_DP0 pid=814822)     output = ops.cutlass_scaled_mm(
(EngineCore_DP0 pid=814822)              ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822)   File "/root/vllmbench/vllm/_custom_ops.py", line 874, in cutlass_scaled_mm
(EngineCore_DP0 pid=814822)     torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias)
(EngineCore_DP0 pid=814822)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=814822)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=814822)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=814822) RuntimeError: Error Internal
[rank0]:[W114 23:04:04.012796423 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
    跳过 (CUTLASS 不支持当前 GPU): Engine core initialization failed. See root cause above. Failed core proc(s): {}
  [测试] SlideSparse + cuBLASLt...
(EngineCore_DP0 pid=815121) /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
(EngineCore_DP0 pid=815121)     Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
(EngineCore_DP0 pid=815121)     Minimum and Maximum cuda capability supported by this version of PyTorch is
(EngineCore_DP0 pid=815121)     (8.0) - (12.0)
(EngineCore_DP0 pid=815121)     
(EngineCore_DP0 pid=815121)   warnings.warn(
(EngineCore_DP0 pid=815121) [2026-01-14 23:04:09] INFO SlideSparseLinearMethod_FP8.py:727: Wrapping CompressedTensorsW8A8Fp8 with SlideSparseFp8LinearMethod (cuBLASLt)
(EngineCore_DP0 pid=815121) [2026-01-14 23:04:09] INFO SlideSparseLinearMethod_FP8.py:537: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt, inner_dtype=bf16, static=False, group_shape=GroupShape(row=1, col=-1))
(EngineCore_DP0 pid=815121) [2026-01-14 23:04:09] INFO SlideSparseLinearMethod_FP8.py:631: SlideSparseFp8LinearMethod initialized, wrapping: CompressedTensorsW8A8Fp8, kernel: cuBLASLt
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.57s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.57s/it]
(EngineCore_DP0 pid=815121) 
(EngineCore_DP0 pid=815121) [2026-01-14 23:04:13] INFO SlideSparseLinearMethod_FP8.py:219: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=815121) [2026-01-14 23:04:13] INFO SlideSparseLinearMethod_FP8.py:329: FP8 quant kernel loaded
(EngineCore_DP0 pid=815121) [2026-01-14 23:04:13] INFO SlideSparseLinearMethod_FP8.py:260: Dequant+bias kernel loaded
(EngineCore_DP0 pid=815121) 2026-01-14 23:04:18,281 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=815121) 2026-01-14 23:04:18,319 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1351.26it/s]
Processed prompts: 100%|███████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.16it/s, est. speed input: 18.53 toks/s, output: 148.27 toks/s]
Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1982.65it/s]
Processed prompts: 100%|███████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.01it/s, est. speed input: 32.14 toks/s, output: 257.08 toks/s]
Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 328.30it/s]
Processed prompts: 100%|███████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.99it/s, est. speed input: 63.77 toks/s, output: 510.18 toks/s]
[rank0]:[W114 23:04:23.511272201 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

  结果:
    SlideSparse + cuBLASLt:      503.8 tok/s
    (无 baseline 对比)

==========================================================================================
总结
==========================================================================================
  Prefill (长输入): 4230.2 tok/s (无 baseline)
  Decode  (长输出): 503.8 tok/s (无 baseline)
==========================================================================================

✓ test_04_throughput.py 完成

════════════════════════════════════════════════════════════════════════════════
                              测试汇总
════════════════════════════════════════════════════════════════════════════════
  后端:     cuBLASLt + BF16
  耗时:     3分56秒

  ✓ 全部 4 个测试通过！
════════════════════════════════════════════════════════════════════════════════