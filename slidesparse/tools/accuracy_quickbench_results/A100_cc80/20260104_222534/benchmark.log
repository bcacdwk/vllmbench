
==============================================
SlideSparse vLLM Accuracy Quick Benchmark Log
==============================================
Start Time: 2026-01-04 22:25:37

Hardware Information:
┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA A100 80GB PCIe                     │
│ GPU Count:        1                                         │
│ GPU Memory:       79.3 GB                                    │
│ Compute Cap:      8.0 (Ampere)                              ││
│ SM Code:          sm_80                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Driver:      13.0                                      │
│ CUDA Runtime:     12.9                                      │
│ NVIDIA Driver:    580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
└─────────────────────────────────────────────────────────────┘

Test Configuration:
  Prompt Input File:  /root/vllmbench/slidesparse/tools/accuracy_quickbench_prompts.jsonl
  Max Output Tokens:  64
  Max Model Length:   512
  Temperature:        0.0
  GPU Memory Util:    0.8
  vLLM Log Level:     WARNING
==============================================


========== Llama3.2-1B-INT8 ==========
Time: 2026-01-04 22:25:41
Model: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Output: /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Llama3.2-1B-INT8.json
Command: CUDA_VISIBLE_DEVICES=0 VLLM_LOGGING_LEVEL=WARNING vllm run-batch         --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8         --served-model-name model         --input-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_prompts.jsonl         --output-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Llama3.2-1B-INT8.json         --max-model-len 512         --gpu-memory-utilization 0.8         --override-generation-config '{"max_new_tokens": 64, "temperature": 0.0}'                  --disable-log-stats

(EngineCore_DP0 pid=1047491) Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1047491) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.01it/s]
(EngineCore_DP0 pid=1047491) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.00it/s]
(EngineCore_DP0 pid=1047491) 
(EngineCore_DP0 pid=1047491) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:00, 60.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:00, 26.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 35.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:00<00:00, 41.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:00<00:00, 47.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:00<00:00, 42.25it/s]
(EngineCore_DP0 pid=1047491) Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:00, 51.78it/s]Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:00<00:00, 54.99it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 59.00it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 57.54it/s]
WARNING 01-04 22:26:10 [model.py:1487] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
Running batch:   0% Completed | 0/16 [00:00<?, ?req/s]
Running batch: 100% Completed | 16/16 [00:00<00:00, 30.34req/s]

[rank0]:[W104 22:26:10.615313509 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

========== Llama3.2-3B-INT8 ==========
Time: 2026-01-04 22:26:12
Model: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Output: /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Llama3.2-3B-INT8.json
Command: CUDA_VISIBLE_DEVICES=0 VLLM_LOGGING_LEVEL=WARNING vllm run-batch         --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8         --served-model-name model         --input-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_prompts.jsonl         --output-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Llama3.2-3B-INT8.json         --max-model-len 512         --gpu-memory-utilization 0.8         --override-generation-config '{"max_new_tokens": 64, "temperature": 0.0}'                  --disable-log-stats

(EngineCore_DP0 pid=1048099) Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1048099) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=1048099) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=1048099) 
(EngineCore_DP0 pid=1048099) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:00, 35.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:00, 36.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 36.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:00<00:00, 19.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 23.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 25.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:01<00:00, 27.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:01<00:00, 30.25it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 28.34it/s]
(EngineCore_DP0 pid=1048099) Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 32.09it/s]Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:00, 34.14it/s]Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:00<00:00, 35.31it/s]Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 19.93it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 24.81it/s]
WARNING 01-04 22:26:49 [model.py:1487] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
Running batch:   0% Completed | 0/16 [00:00<?, ?req/s]
Running batch: 100% Completed | 16/16 [00:00<00:00, 23.38req/s]

[rank0]:[W104 22:26:50.227751251 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

========== Qwen2.5-0.5B-INT8 ==========
Time: 2026-01-04 22:26:52
Model: /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8
Output: /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Qwen2.5-0.5B-INT8.json
Command: CUDA_VISIBLE_DEVICES=0 VLLM_LOGGING_LEVEL=WARNING vllm run-batch         --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8         --served-model-name model         --input-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_prompts.jsonl         --output-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Qwen2.5-0.5B-INT8.json         --max-model-len 512         --gpu-memory-utilization 0.8         --override-generation-config '{"max_new_tokens": 64, "temperature": 0.0}'                  --disable-log-stats

(EngineCore_DP0 pid=1048797) Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1048797) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  8.82it/s]
(EngineCore_DP0 pid=1048797) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  8.81it/s]
(EngineCore_DP0 pid=1048797) 
(EngineCore_DP0 pid=1048797) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 27.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:00, 29.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:00, 29.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:00, 29.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:00<00:00, 28.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 16.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:01<00:00, 18.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:01<00:00, 20.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 22.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:01<00:00, 24.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 23.79it/s]
(EngineCore_DP0 pid=1048797) Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 31.44it/s]Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:00, 32.06it/s]Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:00<00:00, 32.36it/s]Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 18.91it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 23.41it/s]
WARNING 01-04 22:27:27 [model.py:1487] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
Running batch:   0% Completed | 0/16 [00:00<?, ?req/s]
Running batch: 100% Completed | 16/16 [00:00<00:00, 26.74req/s]

[rank0]:[W104 22:27:28.146460109 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

========== Qwen2.5-1.5B-INT8 ==========
Time: 2026-01-04 22:27:30
Model: /root/vllmbench/checkpoints/Qwen2.5-1.5B-INT8
Output: /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Qwen2.5-1.5B-INT8.json
Command: CUDA_VISIBLE_DEVICES=0 VLLM_LOGGING_LEVEL=WARNING vllm run-batch         --model /root/vllmbench/checkpoints/Qwen2.5-1.5B-INT8         --served-model-name model         --input-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_prompts.jsonl         --output-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Qwen2.5-1.5B-INT8.json         --max-model-len 512         --gpu-memory-utilization 0.8         --override-generation-config '{"max_new_tokens": 64, "temperature": 0.0}'                  --disable-log-stats

(EngineCore_DP0 pid=1049483) Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1049483) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.24it/s]
(EngineCore_DP0 pid=1049483) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.24it/s]
(EngineCore_DP0 pid=1049483) 
(EngineCore_DP0 pid=1049483) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 29.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:00, 32.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:00, 33.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 34.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:00<00:00, 33.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:00<00:00, 34.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:00<00:00, 34.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:00<00:00, 35.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 34.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 34.30it/s]
(EngineCore_DP0 pid=1049483) Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:04,  3.66it/s]Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 15.10it/s]Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 21.44it/s]Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 26.11it/s]Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 29.72it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 24.26it/s]
WARNING 01-04 22:28:15 [model.py:1487] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
Running batch:   0% Completed | 0/16 [00:00<?, ?req/s]
Running batch: 100% Completed | 16/16 [00:00<00:00, 18.51req/s]

[rank0]:[W104 22:28:16.364753036 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

========== Qwen2.5-14B-INT8 ==========
Time: 2026-01-04 22:28:18
Model: /root/vllmbench/checkpoints/Qwen2.5-14B-INT8
Output: /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Qwen2.5-14B-INT8.json
Command: CUDA_VISIBLE_DEVICES=0 VLLM_LOGGING_LEVEL=WARNING vllm run-batch         --model /root/vllmbench/checkpoints/Qwen2.5-14B-INT8         --served-model-name model         --input-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_prompts.jsonl         --output-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Qwen2.5-14B-INT8.json         --max-model-len 512         --gpu-memory-utilization 0.8         --override-generation-config '{"max_new_tokens": 64, "temperature": 0.0}'                  --disable-log-stats

(EngineCore_DP0 pid=1050431) Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1050431) Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.88it/s]
(EngineCore_DP0 pid=1050431) Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.49it/s]
(EngineCore_DP0 pid=1050431) Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.10it/s]
(EngineCore_DP0 pid=1050431) Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.79it/s]
(EngineCore_DP0 pid=1050431) Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.91it/s]
(EngineCore_DP0 pid=1050431) 
(EngineCore_DP0 pid=1050431) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:01, 18.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:03,  8.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:02, 10.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:02, 13.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:00<00:01, 14.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:01, 15.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:01, 16.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:01<00:01, 17.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:01<00:00, 18.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:01<00:00, 19.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:01<00:00, 12.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:01<00:00, 13.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:01<00:00, 15.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:02<00:00, 16.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:02<00:00, 17.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:02<00:00, 15.48it/s]
(EngineCore_DP0 pid=1050431) Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:00, 18.59it/s]Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 20.37it/s]Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:00, 21.04it/s]Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:00<00:00, 12.89it/s]Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:00<00:00, 15.02it/s]Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 16.05it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00, 17.79it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00, 16.86it/s]
WARNING 01-04 22:29:20 [model.py:1487] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
Running batch:   0% Completed | 0/16 [00:00<?, ?req/s]
Running batch: 100% Completed | 16/16 [00:01<00:00, 11.73req/s]

[rank0]:[W104 22:29:21.481164334 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

========== Qwen2.5-3B-INT8 ==========
Time: 2026-01-04 22:29:24
Model: /root/vllmbench/checkpoints/Qwen2.5-3B-INT8
Output: /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Qwen2.5-3B-INT8.json
Command: CUDA_VISIBLE_DEVICES=0 VLLM_LOGGING_LEVEL=WARNING vllm run-batch         --model /root/vllmbench/checkpoints/Qwen2.5-3B-INT8         --served-model-name model         --input-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_prompts.jsonl         --output-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Qwen2.5-3B-INT8.json         --max-model-len 512         --gpu-memory-utilization 0.8         --override-generation-config '{"max_new_tokens": 64, "temperature": 0.0}'                  --disable-log-stats

(EngineCore_DP0 pid=1051630) Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1051630) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.43it/s]
(EngineCore_DP0 pid=1051630) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.43it/s]
(EngineCore_DP0 pid=1051630) 
(EngineCore_DP0 pid=1051630) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 28.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:01, 28.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:01, 14.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:01, 17.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:01, 19.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 21.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:00<00:00, 23.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:01<00:00, 24.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:01<00:00, 25.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:01<00:00, 26.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:01<00:00, 27.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 23.39it/s]
(EngineCore_DP0 pid=1051630) Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:04,  3.74it/s]Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01, 12.27it/s]Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 17.61it/s]Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 21.21it/s]Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 23.66it/s]Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 25.28it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 21.15it/s]
WARNING 01-04 22:30:12 [model.py:1487] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
Running batch:   0% Completed | 0/16 [00:00<?, ?req/s]
Running batch: 100% Completed | 16/16 [00:00<00:00, 19.81req/s]

[rank0]:[W104 22:30:13.312514572 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

========== Qwen2.5-7B-INT8 ==========
Time: 2026-01-04 22:30:16
Model: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
Output: /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Qwen2.5-7B-INT8.json
Command: CUDA_VISIBLE_DEVICES=0 VLLM_LOGGING_LEVEL=WARNING vllm run-batch         --model /root/vllmbench/checkpoints/Qwen2.5-7B-INT8         --served-model-name model         --input-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_prompts.jsonl         --output-file /root/vllmbench/slidesparse/tools/accuracy_quickbench_results/A100_cc80/20260104_222534/Qwen2.5-7B-INT8.json         --max-model-len 512         --gpu-memory-utilization 0.8         --override-generation-config '{"max_new_tokens": 64, "temperature": 0.0}'                  --disable-log-stats

(EngineCore_DP0 pid=1052683) Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1052683) Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.87it/s]
(EngineCore_DP0 pid=1052683) Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.80it/s]
(EngineCore_DP0 pid=1052683) Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.81it/s]
(EngineCore_DP0 pid=1052683) 
(EngineCore_DP0 pid=1052683) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:00, 31.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:00, 31.74it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 31.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:00<00:00, 31.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 33.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 19.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:01<00:00, 22.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:01<00:00, 25.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 26.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 26.63it/s]
(EngineCore_DP0 pid=1052683) Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:00, 29.43it/s]Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 31.81it/s]Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:00<00:00, 32.90it/s]Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:00<00:00, 33.56it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 34.31it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 33.48it/s]
WARNING 01-04 22:31:04 [model.py:1487] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
Running batch:   0% Completed | 0/16 [00:00<?, ?req/s]
Running batch: 100% Completed | 16/16 [00:00<00:00, 17.22req/s]

[rank0]:[W104 22:31:05.483154873 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
