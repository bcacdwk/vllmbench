
==============================================
SlideSparse vLLM Throughput Benchmark Log (DECODE)
==============================================
Start Time: 2026-01-12 22:15:46
Test Mode: decode
M List: [1, 2, 4, 8, 16]
GPU: RTX5080, CC: cc120
==============================================

========== M=1 ==========
Time: 2026-01-12 22:15:46
Params: prompt_len=16, output_len=512, num_prompts=1, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 512 --num-prompts 1 --max-num-seqs 1 --max-model-len 656 --max-num-batched-tokens 656 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_bench_results/decode/RTX5080_cc120_py312_cu129_x86_64/json/Llama3.2-1B-FP8_M1.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 0.40 requests/s, 211.40 total tokens/s, 204.99 output tokens/s
Total num prompt tokens:  16
Total num output tokens:  512

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=471170) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=471170) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.35it/s]
(EngineCore_DP0 pid=471170) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.34it/s]
(EngineCore_DP0 pid=471170) 
(EngineCore_DP0 pid=471170) 2026-01-12 22:16:00,252 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=471170) 2026-01-12 22:16:00,255 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=471170) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 114.94it/s]
(EngineCore_DP0 pid=471170) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.98it/s]

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 1/1 [00:00<00:00, 798.31it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 6.42 toks/s, output: 205.28 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 6.42 toks/s, output: 205.28 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 6.42 toks/s, output: 205.28 toks/s]
[rank0]:[W112 22:16:03.755738942 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2 ==========
Time: 2026-01-12 22:16:05
Params: prompt_len=16, output_len=512, num_prompts=2, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 512 --num-prompts 2 --max-num-seqs 2 --max-model-len 656 --max-num-batched-tokens 1312 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_bench_results/decode/RTX5080_cc120_py312_cu129_x86_64/json/Llama3.2-1B-FP8_M2.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 0.79 requests/s, 417.18 total tokens/s, 404.54 output tokens/s
Total num prompt tokens:  32
Total num output tokens:  1024

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=471652) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=471652) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.29it/s]
(EngineCore_DP0 pid=471652) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.28it/s]
(EngineCore_DP0 pid=471652) 
(EngineCore_DP0 pid=471652) 2026-01-12 22:16:19,206 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=471652) 2026-01-12 22:16:19,208 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=471652) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 85.97it/s]
(EngineCore_DP0 pid=471652) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  8.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.27it/s]

Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 2/2 [00:00<00:00, 976.10it/s]

Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  50%|█████     | 1/2 [00:02<00:02,  2.52s/it, est. speed input: 6.34 toks/s, output: 203.01 toks/s]
Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  2.52s/it, est. speed input: 12.66 toks/s, output: 405.22 toks/s]
Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it, est. speed input: 12.66 toks/s, output: 405.22 toks/s]
[rank0]:[W112 22:16:22.736538447 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4 ==========
Time: 2026-01-12 22:16:24
Params: prompt_len=16, output_len=512, num_prompts=4, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 512 --num-prompts 4 --max-num-seqs 4 --max-model-len 656 --max-num-batched-tokens 2624 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_bench_results/decode/RTX5080_cc120_py312_cu129_x86_64/json/Llama3.2-1B-FP8_M4.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 1.57 requests/s, 828.83 total tokens/s, 803.71 output tokens/s
Total num prompt tokens:  64
Total num output tokens:  2048

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=472024) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=472024) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.30it/s]
(EngineCore_DP0 pid=472024) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.29it/s]
(EngineCore_DP0 pid=472024) 
(EngineCore_DP0 pid=472024) 2026-01-12 22:16:40,105 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=472024) 2026-01-12 22:16:40,108 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=472024) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 94.33it/s]
(EngineCore_DP0 pid=472024) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  8.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 21.74it/s]

Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 4/4 [00:00<00:00, 1249.33it/s]

Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  25%|██▌       | 1/4 [00:02<00:07,  2.54s/it, est. speed input: 6.30 toks/s, output: 201.74 toks/s]
Processed prompts: 100%|██████████| 4/4 [00:02<00:00,  2.54s/it, est. speed input: 25.17 toks/s, output: 805.40 toks/s]
Processed prompts: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s, est. speed input: 25.17 toks/s, output: 805.40 toks/s]
[rank0]:[W112 22:16:43.749452343 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8 ==========
Time: 2026-01-12 22:16:45
Params: prompt_len=16, output_len=512, num_prompts=8, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 512 --num-prompts 8 --max-num-seqs 8 --max-model-len 656 --max-num-batched-tokens 5248 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_bench_results/decode/RTX5080_cc120_py312_cu129_x86_64/json/Llama3.2-1B-FP8_M8.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 3.08 requests/s, 1627.48 total tokens/s, 1578.16 output tokens/s
Total num prompt tokens:  128
Total num output tokens:  4096

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=472402) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=472402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.27it/s]
(EngineCore_DP0 pid=472402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.26it/s]
(EngineCore_DP0 pid=472402) 
(EngineCore_DP0 pid=472402) 2026-01-12 22:17:01,233 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=472402) 2026-01-12 22:17:01,236 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=472402) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 94.92it/s]
(EngineCore_DP0 pid=472402) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 26.99it/s]

Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 8/8 [00:00<00:00, 1484.12it/s]

Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 1/8 [00:02<00:18,  2.58s/it, est. speed input: 6.20 toks/s, output: 198.26 toks/s]
Processed prompts: 100%|██████████| 8/8 [00:02<00:00,  2.58s/it, est. speed input: 49.46 toks/s, output: 1582.78 toks/s]
Processed prompts: 100%|██████████| 8/8 [00:02<00:00,  3.09it/s, est. speed input: 49.46 toks/s, output: 1582.78 toks/s]
[rank0]:[W112 22:17:05.939801069 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-12 22:17:06
Params: prompt_len=16, output_len=512, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 512 --num-prompts 16 --max-num-seqs 16 --max-model-len 656 --max-num-batched-tokens 10496 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_bench_results/decode/RTX5080_cc120_py312_cu129_x86_64/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 6.00 requests/s, 3170.30 total tokens/s, 3074.23 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  8192

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=472773) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=472773) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.23it/s]
(EngineCore_DP0 pid=472773) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.22it/s]
(EngineCore_DP0 pid=472773) 
(EngineCore_DP0 pid=472773) 2026-01-12 22:17:20,483 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=472773) 2026-01-12 22:17:20,487 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=472773) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 102.02it/s]
(EngineCore_DP0 pid=472773) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  8.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 32.19it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 4112.31it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:02<00:39,  2.65s/it, est. speed input: 6.03 toks/s, output: 192.89 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:02<00:00,  2.65s/it, est. speed input: 96.24 toks/s, output: 3079.74 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:02<00:00,  6.01it/s, est. speed input: 96.24 toks/s, output: 3079.74 toks/s]
[rank0]:[W112 22:17:24.208240250 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

