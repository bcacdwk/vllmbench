
==============================================
SlideSparse vLLM Throughput Benchmark Log (PREFILL)
==============================================
Start Time: 2026-01-12 22:13:38
Test Mode: prefill
M List: [16, 32, 64, 128, 256]
GPU: RTX5080, CC: cc120
==============================================

========== M=16 ==========
Time: 2026-01-12 22:13:38
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_bench_results/prefill/RTX5080_cc120_py312_cu129_x86_64/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 210.10 requests/s, 3571.65 total tokens/s, 210.10 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=468757) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=468757) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.31it/s]
(EngineCore_DP0 pid=468757) 
(EngineCore_DP0 pid=468757) 2026-01-12 22:13:54,733 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=468757) 2026-01-12 22:13:54,737 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=468757) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 85.88it/s]
(EngineCore_DP0 pid=468757) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.87it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 5104.01it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:00, 166.22it/s, est. speed input: 2659.67 toks/s, output: 166.22 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:00, 204.93it/s, est. speed input: 3186.63 toks/s, output: 199.16 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 216.07it/s, est. speed input: 3346.56 toks/s, output: 209.16 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:00<00:00, 222.36it/s, est. speed input: 3438.32 toks/s, output: 214.89 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:00<00:00, 225.62it/s, est. speed input: 3491.42 toks/s, output: 218.21 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 225.62it/s, est. speed input: 3516.58 toks/s, output: 219.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 219.76it/s, est. speed input: 3516.58 toks/s, output: 219.78 toks/s]
[rank0]:[W112 22:13:56.343613185 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32 ==========
Time: 2026-01-12 22:13:57
Params: prompt_len=32, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 32 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 161 --max-num-batched-tokens 161 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_bench_results/prefill/RTX5080_cc120_py312_cu129_x86_64/json/Qwen2.5-0.5B-FP8_M32.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 208.26 requests/s, 6872.62 total tokens/s, 208.26 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=469172) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=469172) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.86it/s]
(EngineCore_DP0 pid=469172) 
(EngineCore_DP0 pid=469172) 2026-01-12 22:14:13,875 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=469172) 2026-01-12 22:14:13,879 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=469172) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 88.12it/s]
(EngineCore_DP0 pid=469172) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3708.24it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:00, 175.43it/s, est. speed input: 5614.42 toks/s, output: 175.44 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:00, 208.71it/s, est. speed input: 6519.92 toks/s, output: 203.74 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:00<00:00, 219.10it/s, est. speed input: 6816.44 toks/s, output: 213.00 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:00<00:00, 223.66it/s, est. speed input: 6957.96 toks/s, output: 217.43 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:00<00:00, 226.17it/s, est. speed input: 7042.63 toks/s, output: 220.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 226.17it/s, est. speed input: 7082.94 toks/s, output: 221.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 221.28it/s, est. speed input: 7082.94 toks/s, output: 221.33 toks/s]
[rank0]:[W112 22:14:15.516313992 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-12 22:14:16
Params: prompt_len=64, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 64 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 193 --max-num-batched-tokens 193 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_bench_results/prefill/RTX5080_cc120_py312_cu129_x86_64/json/Qwen2.5-0.5B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 207.55 requests/s, 13490.88 total tokens/s, 207.55 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=469562) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=469562) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.19it/s]
(EngineCore_DP0 pid=469562) 
(EngineCore_DP0 pid=469562) 2026-01-12 22:14:34,876 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=469562) 2026-01-12 22:14:34,881 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=469562) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 86.14it/s]
(EngineCore_DP0 pid=469562) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2967.68it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:00, 189.06it/s, est. speed input: 12101.59 toks/s, output: 189.07 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:00, 214.10it/s, est. speed input: 13466.81 toks/s, output: 210.41 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:00<00:00, 222.06it/s, est. speed input: 13919.10 toks/s, output: 217.48 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:00<00:00, 225.53it/s, est. speed input: 14134.09 toks/s, output: 220.84 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:00<00:00, 227.72it/s, est. speed input: 14272.51 toks/s, output: 223.00 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 227.72it/s, est. speed input: 14327.03 toks/s, output: 223.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 223.83it/s, est. speed input: 14327.03 toks/s, output: 223.86 toks/s]
[rank0]:[W112 22:14:36.559130953 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-12 22:14:37
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_bench_results/prefill/RTX5080_cc120_py312_cu129_x86_64/json/Qwen2.5-0.5B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 201.86 requests/s, 26039.51 total tokens/s, 201.86 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=469962) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=469962) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.08it/s]
(EngineCore_DP0 pid=469962) 
(EngineCore_DP0 pid=469962) 2026-01-12 22:14:56,024 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=469962) 2026-01-12 22:14:56,028 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=469962) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 86.28it/s]
(EngineCore_DP0 pid=469962) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2143.27it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:00, 219.05it/s, est. speed input: 28039.43 toks/s, output: 219.05 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:00, 221.54it/s, est. speed input: 28310.40 toks/s, output: 221.17 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:00<00:00, 222.59it/s, est. speed input: 28424.65 toks/s, output: 222.07 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:00<00:00, 223.46it/s, est. speed input: 28512.08 toks/s, output: 222.75 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:00<00:00, 223.92it/s, est. speed input: 28562.83 toks/s, output: 223.15 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 223.92it/s, est. speed input: 28611.05 toks/s, output: 223.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 223.49it/s, est. speed input: 28611.05 toks/s, output: 223.52 toks/s]
[rank0]:[W112 22:14:57.721854293 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-12 22:14:59
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 385 --max-num-batched-tokens 385 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_bench_results/prefill/RTX5080_cc120_py312_cu129_x86_64/json/Qwen2.5-0.5B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 187.71 requests/s, 48240.99 total tokens/s, 187.71 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:78: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=470394) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=470394) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.11it/s]
(EngineCore_DP0 pid=470394) 
(EngineCore_DP0 pid=470394) 2026-01-12 22:15:14,793 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=470394) 2026-01-12 22:15:14,798 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=470394) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 84.87it/s]
(EngineCore_DP0 pid=470394) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1472.73it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:00, 252.39it/s, est. speed input: 64617.82 toks/s, output: 252.40 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:00, 223.92it/s, est. speed input: 58311.96 toks/s, output: 227.77 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:00<00:00, 217.12it/s, est. speed input: 56713.15 toks/s, output: 221.53 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:00<00:00, 213.65it/s, est. speed input: 55884.71 toks/s, output: 218.30 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:00<00:00, 211.66it/s, est. speed input: 55376.69 toks/s, output: 216.31 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 211.66it/s, est. speed input: 55239.25 toks/s, output: 215.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 215.74it/s, est. speed input: 55239.25 toks/s, output: 215.78 toks/s]
[rank0]:[W112 22:15:16.473997071 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

