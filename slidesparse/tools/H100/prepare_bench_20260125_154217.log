======================================================================
SlideSparse Prepare Benchmark Log
Started: 2026-01-25 15:42:17
======================================================================

Hardware:
  GPU: NVIDIA H100 PCIe (cc90)
  Python: py312
  CUDA: cu129
  Arch: x86_64

[INFO] 日志文件: /root/vllmbench/slidesparse/tools/prepare_bench_20260125_154217.log
[INFO] 跳过 Task 1: 模型下载

======================================================================
TASK 2: 模型转换 (SlideSparse)
Started: 2026-01-25 15:42:17
======================================================================


------------------------------------------------------------
  转换: llama3.2-1b-int8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-1b-int8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

======================================================================
Processing: Llama3.2-1B-INT8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=4, N=2, expand=1.000)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4

[INFO] Copying non-weight files...
[INFO]   Copied: config.json, .gitattributes, README.md, tokenizer.json, recipe.yaml...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 8192] -> [2048, 8192]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [8192, 2048] -> [8192, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2048, 2048] -> [2048, 2048]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [512, 2048] -> [512, 2048]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 112 layers
[INFO] Skipped: 147 layers
[INFO] Time: 29.52s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4/conversion_report.json
[SUCCESS] llama3.2-1b-int8 2_4 转换完成 (35.9s)

------------------------------------------------------------
  转换: llama3.2-1b-int8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-1b-int8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

======================================================================
Processing: Llama3.2-1B-INT8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=6, N=3, expand=1.333)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6

[INFO] Copying non-weight files...
[INFO]   Copied: config.json, .gitattributes, README.md, tokenizer.json, recipe.yaml...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2048, 8192], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 8192] -> [2048, 10944]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[8192, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [8192, 2048] -> [8192, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2048, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2048, 2048] -> [2048, 2752]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[512, 2048], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [512, 2048] -> [512, 2752]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 112 layers
[INFO] Skipped: 147 layers
[INFO] Time: 19.64s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6/conversion_report.json
[SUCCESS] llama3.2-1b-int8 2_6 转换完成 (25.9s)

------------------------------------------------------------
  转换: llama3.2-1b-int8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-1b-int8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-1b-int8
[INFO] Use --list to see available models
[ERROR] llama3.2-1b-int8 2_8 转换失败

------------------------------------------------------------
  转换: llama3.2-1b-int8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-1b-int8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-1b-int8
[INFO] Use --list to see available models
[ERROR] llama3.2-1b-int8 2_10 转换失败

------------------------------------------------------------
  转换: llama3.2-1b-fp8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-1b-fp8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-1b-fp8
[INFO] Use --list to see available models
[ERROR] llama3.2-1b-fp8 2_4 转换失败

------------------------------------------------------------
  转换: llama3.2-1b-fp8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-1b-fp8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-1b-fp8
[INFO] Use --list to see available models
[ERROR] llama3.2-1b-fp8 2_6 转换失败

------------------------------------------------------------
  转换: llama3.2-1b-fp8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-1b-fp8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-1b-fp8
[INFO] Use --list to see available models
[ERROR] llama3.2-1b-fp8 2_8 转换失败

------------------------------------------------------------
  转换: llama3.2-1b-fp8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-1b-fp8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-1b-fp8
[INFO] Use --list to see available models
[ERROR] llama3.2-1b-fp8 2_10 转换失败

------------------------------------------------------------
  转换: llama3.2-3b-int8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-3b-int8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-3b-int8
[INFO] Use --list to see available models
[ERROR] llama3.2-3b-int8 2_4 转换失败

------------------------------------------------------------
  转换: llama3.2-3b-int8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-3b-int8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-3b-int8
[INFO] Use --list to see available models
[ERROR] llama3.2-3b-int8 2_6 转换失败

------------------------------------------------------------
  转换: llama3.2-3b-int8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-3b-int8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-3b-int8
[INFO] Use --list to see available models
[ERROR] llama3.2-3b-int8 2_8 转换失败

------------------------------------------------------------
  转换: llama3.2-3b-int8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-3b-int8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-3b-int8
[INFO] Use --list to see available models
[ERROR] llama3.2-3b-int8 2_10 转换失败

------------------------------------------------------------
  转换: llama3.2-3b-fp8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-3b-fp8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-3b-fp8
[INFO] Use --list to see available models
[ERROR] llama3.2-3b-fp8 2_4 转换失败

------------------------------------------------------------
  转换: llama3.2-3b-fp8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-3b-fp8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-3b-fp8
[INFO] Use --list to see available models
[ERROR] llama3.2-3b-fp8 2_6 转换失败

------------------------------------------------------------
  转换: llama3.2-3b-fp8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-3b-fp8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-3b-fp8
[INFO] Use --list to see available models
[ERROR] llama3.2-3b-fp8 2_8 转换失败

------------------------------------------------------------
  转换: llama3.2-3b-fp8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model llama3.2-3b-fp8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: llama3.2-3b-fp8
[INFO] Use --list to see available models
[ERROR] llama3.2-3b-fp8 2_10 转换失败

------------------------------------------------------------
  转换: qwen2.5-7b-int8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-7b-int8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-7b-int8
[INFO] Use --list to see available models
[ERROR] qwen2.5-7b-int8 2_4 转换失败

------------------------------------------------------------
  转换: qwen2.5-7b-int8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-7b-int8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-7b-int8
[INFO] Use --list to see available models
[ERROR] qwen2.5-7b-int8 2_6 转换失败

------------------------------------------------------------
  转换: qwen2.5-7b-int8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-7b-int8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-7b-int8
[INFO] Use --list to see available models
[ERROR] qwen2.5-7b-int8 2_8 转换失败

------------------------------------------------------------
  转换: qwen2.5-7b-int8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-7b-int8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-7b-int8
[INFO] Use --list to see available models
[ERROR] qwen2.5-7b-int8 2_10 转换失败

------------------------------------------------------------
  转换: qwen2.5-7b-fp8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-7b-fp8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-7b-fp8
[INFO] Use --list to see available models
[ERROR] qwen2.5-7b-fp8 2_4 转换失败

------------------------------------------------------------
  转换: qwen2.5-7b-fp8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-7b-fp8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-7b-fp8
[INFO] Use --list to see available models
[ERROR] qwen2.5-7b-fp8 2_6 转换失败

------------------------------------------------------------
  转换: qwen2.5-7b-fp8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-7b-fp8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-7b-fp8
[INFO] Use --list to see available models
[ERROR] qwen2.5-7b-fp8 2_8 转换失败

------------------------------------------------------------
  转换: qwen2.5-7b-fp8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-7b-fp8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-7b-fp8
[INFO] Use --list to see available models
[ERROR] qwen2.5-7b-fp8 2_10 转换失败

------------------------------------------------------------
  转换: qwen2.5-14b-int8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-14b-int8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-14b-int8
[INFO] Use --list to see available models
[ERROR] qwen2.5-14b-int8 2_4 转换失败

------------------------------------------------------------
  转换: qwen2.5-14b-int8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-14b-int8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-14b-int8
[INFO] Use --list to see available models
[ERROR] qwen2.5-14b-int8 2_6 转换失败

------------------------------------------------------------
  转换: qwen2.5-14b-int8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-14b-int8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-14b-int8
[INFO] Use --list to see available models
[ERROR] qwen2.5-14b-int8 2_8 转换失败

------------------------------------------------------------
  转换: qwen2.5-14b-int8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-14b-int8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-14b-int8
[INFO] Use --list to see available models
[ERROR] qwen2.5-14b-int8 2_10 转换失败

------------------------------------------------------------
  转换: qwen2.5-14b-fp8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-14b-fp8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-14b-fp8
[INFO] Use --list to see available models
[ERROR] qwen2.5-14b-fp8 2_4 转换失败

------------------------------------------------------------
  转换: qwen2.5-14b-fp8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-14b-fp8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-14b-fp8
[INFO] Use --list to see available models
[ERROR] qwen2.5-14b-fp8 2_6 转换失败

------------------------------------------------------------
  转换: qwen2.5-14b-fp8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-14b-fp8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-14b-fp8
[INFO] Use --list to see available models
[ERROR] qwen2.5-14b-fp8 2_8 转换失败

------------------------------------------------------------
  转换: qwen2.5-14b-fp8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model qwen2.5-14b-fp8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

[ERROR] Model not found: qwen2.5-14b-fp8
[INFO] Use --list to see available models
[ERROR] qwen2.5-14b-fp8 2_10 转换失败

[INFO] 转换统计: 成功 2, 失败 30

----------------------------------------------------------------------
TASK 2: 模型转换 (SlideSparse) - FAILED
Duration: 242.2 seconds (4.0 minutes)
----------------------------------------------------------------------


======================================================================
TASK 3: 离线粗调优 (cuBLAS + quant_only)
Started: 2026-01-25 15:46:20
======================================================================

[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/offline_autotune_algsearch.py --model Llama3.2-1B,Llama3.2-3B,Qwen2.5-7B,Qwen2.5-14B --dtype all --m_list 256,1024,4096,16384,32768 --Lmax 10 --warmup 10 --repeat 50 --kernels 1,0,0,0,1

[ERROR] 未找到模型 'Llama3.2-1B'（base name: 'Llama3.2-1B'）

----------------------------------------------------------------------
TASK 3: 离线粗调优 (cuBLAS + quant_only) - FAILED
Duration: 5.8 seconds (0.1 minutes)
----------------------------------------------------------------------


======================================================================
TASK 4: 离线细调优 (cuSPARSE + Triton)
Started: 2026-01-25 15:46:25
======================================================================

[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/offline_autotune_algsearch.py --model Llama3.2-1B,Llama3.2-3B,Qwen2.5-7B,Qwen2.5-14B --dtype all --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768,65536 --Lmax 10 --warmup 25 --repeat 100 --kernels 0,1,1,1,0

[ERROR] 未找到模型 'Llama3.2-1B'（base name: 'Llama3.2-1B'）

----------------------------------------------------------------------
TASK 4: 离线细调优 (cuSPARSE + Triton) - FAILED
Duration: 5.7 seconds (0.1 minutes)
----------------------------------------------------------------------


======================================================================
TASK 5: 简单端到端 Benchmark
Started: 2026-01-25 15:46:31
======================================================================


------------------------------------------------------------
  Benchmark: llama3.2-1b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model llama3.2-1b-int8 --backend all --stage all --sparsity 2_4,2_6,2_10 --M quick


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA H100 PCIe                          ││
│ GPU (short):      H100                                      │
│ Memory:           79.2 GB                                    │
│ CC:               cc90 (Hopper)                              ││
│ SM Code:          sm_90                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.105.08                                │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['llama3.2-1b-int8']
  Backends:         ['cutlass', 'cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_10']
  Stages:           ['prefill', 'decode']
  M_prefill:        [16, 128, 256]
  M_decode:         [16, 128, 256]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260125_154637.log

[WARNING] Dense checkpoint 不存在，跳过: Llama3.2-1B-INT8
[WARNING] Dense checkpoint 不存在，跳过: Llama3.2-1B-INT8
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-INT8 (2_4)
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-INT8 (2_6)
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-INT8 (2_10)


============================================================
  Benchmark 完成!
============================================================


总计: 0 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260125_154637.log
[SUCCESS] llama3.2-1b-int8 Benchmark 完成 (7.2s)

------------------------------------------------------------
  Benchmark: llama3.2-1b-fp8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model llama3.2-1b-fp8 --backend all --stage all --sparsity 2_4,2_6,2_10 --M quick


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA H100 PCIe                          ││
│ GPU (short):      H100                                      │
│ Memory:           79.2 GB                                    │
│ CC:               cc90 (Hopper)                              ││
│ SM Code:          sm_90                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.105.08                                │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['llama3.2-1b-fp8']
  Backends:         ['cutlass', 'cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_10']
  Stages:           ['prefill', 'decode']
  M_prefill:        [16, 128, 256]
  M_decode:         [16, 128, 256]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260125_154645.log

[WARNING] Dense checkpoint 不存在，跳过: Llama3.2-1B-FP8
[WARNING] Dense checkpoint 不存在，跳过: Llama3.2-1B-FP8
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-FP8 (2_4)
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-FP8 (2_6)
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-FP8 (2_10)


============================================================
  Benchmark 完成!
============================================================


总计: 0 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260125_154645.log
[SUCCESS] llama3.2-1b-fp8 Benchmark 完成 (7.5s)

[INFO] Benchmark 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 5: 简单端到端 Benchmark - SUCCESS
Duration: 14.7 seconds (0.2 minutes)
----------------------------------------------------------------------


======================================================================
TASK 6: 完整 Prefill Benchmark
Started: 2026-01-25 15:46:46
======================================================================


------------------------------------------------------------
  Prefill Benchmark: llama3.2-1b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model llama3.2-1b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768,65536


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA H100 PCIe                          ││
│ GPU (short):      H100                                      │
│ Memory:           79.2 GB                                    │
│ CC:               cc90 (Hopper)                              ││
│ SM Code:          sm_90                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.105.08                                │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['llama3.2-1b-int8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260125_154652.log

[WARNING] Dense checkpoint 不存在，跳过: Llama3.2-1B-INT8
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-INT8 (2_4)
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-INT8 (2_6)
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-INT8 (2_8)
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-INT8 (2_10)


============================================================
  Benchmark 完成!
============================================================


总计: 0 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260125_154652.log
[SUCCESS] llama3.2-1b-int8 Prefill 完成 (7.0s)

------------------------------------------------------------
  Prefill Benchmark: llama3.2-1b-fp8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model llama3.2-1b-fp8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768,65536


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA H100 PCIe                          ││
│ GPU (short):      H100                                      │
│ Memory:           79.2 GB                                    │
│ CC:               cc90 (Hopper)                              ││
│ SM Code:          sm_90                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.105.08                                │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['llama3.2-1b-fp8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260125_154659.log

[WARNING] Dense checkpoint 不存在，跳过: Llama3.2-1B-FP8
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-FP8 (2_4)
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-FP8 (2_6)
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-FP8 (2_8)
[WARNING] Sparse checkpoint 不存在，跳过: Llama3.2-1B-FP8 (2_10)


============================================================
  Benchmark 完成!
============================================================


总计: 0 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260125_154659.log
[SUCCESS] llama3.2-1b-fp8 Prefill 完成 (7.3s)

------------------------------------------------------------
  Prefill Benchmark: llama3.2-3b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model llama3.2-3b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768,65536


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA H100 PCIe                          ││
│ GPU (short):      H100                                      │
│ Memory:           79.2 GB                                    │
│ CC:               cc90 (Hopper)                              ││
│ SM Code:          sm_90                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.105.08                                │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['llama3.2-3b-int8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260125_154707.log


============================================================
  Llama3.2-3B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:47:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=142988) WARNING 01-25 15:48:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 29.62 requests/s, 15195.10 total tokens/s, 29.62 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-25 15:47:14] INFO font_manager.py:1639: generated new fontManager
[2026-01-25 15:47:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:47:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:47:23] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:23] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:47:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:23] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:23] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:47:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:23] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:23] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:23] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:47:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:23] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:47:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:47:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:47:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:47:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:47:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:47:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:47:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:47:31] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:31] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:47:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:31] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:31] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:47:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:31] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:31] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:31] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:47:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:31] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:47:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:47:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:47:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:47:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:47:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:33] INFO gemm_wrapper.py:85: Auto-building cublaslt GEMM library from /root/vllmbench/slidesparse/csrc/cublaslt_gemm/build_cublaslt.py...
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:46] INFO gemm_wrapper.py:95: cublaslt GEMM library build completed
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:46] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=142988) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=142988) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.54it/s]
(EngineCore_DP0 pid=142988) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.54it/s]
(EngineCore_DP0 pid=142988) 
(EngineCore_DP0 pid=142988) [2026-01-25 15:48:17] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=142988) 2026-01-25 15:48:24,759 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=142988) 2026-01-25 15:48:24,786 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=142988) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=142988) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.46it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  42%|████▏     | 54/128 [00:00<00:00, 539.78it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 714.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:49,  2.55it/s, est. speed input: 1306.36 toks/s, output: 2.55 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:10, 11.96it/s, est. speed input: 5015.76 toks/s, output: 9.80 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:06, 18.56it/s, est. speed input: 7336.56 toks/s, output: 14.33 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 23.18it/s, est. speed input: 8922.80 toks/s, output: 17.43 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:04, 26.40it/s, est. speed input: 10075.06 toks/s, output: 19.68 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 28.65it/s, est. speed input: 10950.47 toks/s, output: 21.39 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:03, 30.26it/s, est. speed input: 11642.18 toks/s, output: 22.74 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 31.38it/s, est. speed input: 12198.86 toks/s, output: 23.83 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 32.16it/s, est. speed input: 12657.83 toks/s, output: 24.72 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 32.66it/s, est. speed input: 13036.78 toks/s, output: 25.46 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 33.08it/s, est. speed input: 13365.95 toks/s, output: 26.10 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 33.35it/s, est. speed input: 13646.84 toks/s, output: 26.65 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 33.50it/s, est. speed input: 13887.73 toks/s, output: 27.12 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 33.63it/s, est. speed input: 14101.47 toks/s, output: 27.54 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:02, 33.74it/s, est. speed input: 14291.61 toks/s, output: 27.91 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:01, 33.79it/s, est. speed input: 14459.64 toks/s, output: 28.24 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 33.87it/s, est. speed input: 14613.25 toks/s, output: 28.54 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 33.92it/s, est. speed input: 14750.78 toks/s, output: 28.81 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 33.99it/s, est. speed input: 14878.54 toks/s, output: 29.06 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 34.00it/s, est. speed input: 14992.37 toks/s, output: 29.28 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 33.96it/s, est. speed input: 15093.35 toks/s, output: 29.48 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 33.94it/s, est. speed input: 15186.41 toks/s, output: 29.66 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 33.83it/s, est. speed input: 15266.64 toks/s, output: 29.82 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 33.86it/s, est. speed input: 15346.59 toks/s, output: 29.97 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 33.88it/s, est. speed input: 15420.38 toks/s, output: 30.12 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 33.89it/s, est. speed input: 15489.34 toks/s, output: 30.25 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 33.86it/s, est. speed input: 15551.17 toks/s, output: 30.37 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 33.84it/s, est. speed input: 15609.02 toks/s, output: 30.49 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 33.82it/s, est. speed input: 15662.91 toks/s, output: 30.59 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 33.82it/s, est. speed input: 15714.56 toks/s, output: 30.69 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 33.47it/s, est. speed input: 15746.19 toks/s, output: 30.75 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 33.61it/s, est. speed input: 15793.73 toks/s, output: 30.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 33.61it/s, est. speed input: 15827.55 toks/s, output: 30.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 30.91it/s, est. speed input: 15827.55 toks/s, output: 30.91 toks/s]
[rank0]:[W125 15:48:32.720247585 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 88.0s

测试结果:
  Requests/s:   29.62
  Tokens/s:     15195.10
  Total Reqs:   128
  Elapsed:      4.32s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     15165.48

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:48:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=145389) WARNING 01-25 15:48:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.33 requests/s, 32114.95 total tokens/s, 31.33 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-25 15:48:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:48:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:48:43] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:43] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:48:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:43] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:43] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:48:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:43] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:43] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:43] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:48:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:43] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:48:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:48:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:48:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:48:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:48:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:48:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:48:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:48:50] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:50] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:48:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:50] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:50] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:48:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:50] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:50] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:50] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:48:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:50] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:48:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:48:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:48:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:48:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:48:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=145389) [2026-01-25 15:48:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=145389) [2026-01-25 15:48:51] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=145389) [2026-01-25 15:48:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=145389) [2026-01-25 15:48:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=145389) [2026-01-25 15:48:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=145389) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=145389) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=145389) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=145389) 
(EngineCore_DP0 pid=145389) [2026-01-25 15:49:05] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=145389) 2026-01-25 15:49:11,296 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=145389) 2026-01-25 15:49:11,355 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=145389) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.17it/s]
(EngineCore_DP0 pid=145389) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.74it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|███       | 39/128 [00:00<00:00, 382.44it/s]
Adding requests:  72%|███████▏  | 92/128 [00:00<00:00, 460.35it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 464.63it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 60.54it/s, est. speed input: 61997.78 toks/s, output: 60.54 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 40.28it/s, est. speed input: 43428.63 toks/s, output: 42.41 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 37.04it/s, est. speed input: 40267.80 toks/s, output: 39.32 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 35.69it/s, est. speed input: 38942.09 toks/s, output: 38.03 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 34.79it/s, est. speed input: 38056.76 toks/s, output: 37.16 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 34.23it/s, est. speed input: 37442.24 toks/s, output: 36.56 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 33.85it/s, est. speed input: 36988.58 toks/s, output: 36.12 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 33.56it/s, est. speed input: 36618.01 toks/s, output: 35.76 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 33.34it/s, est. speed input: 36317.35 toks/s, output: 35.47 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 33.15it/s, est. speed input: 36061.47 toks/s, output: 35.22 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 33.03it/s, est. speed input: 35848.49 toks/s, output: 35.01 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:02, 32.92it/s, est. speed input: 35664.26 toks/s, output: 34.83 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:02, 32.85it/s, est. speed input: 35506.95 toks/s, output: 34.67 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 32.83it/s, est. speed input: 35377.21 toks/s, output: 34.55 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 32.86it/s, est. speed input: 35273.79 toks/s, output: 34.45 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 32.85it/s, est. speed input: 35175.63 toks/s, output: 34.35 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 32.81it/s, est. speed input: 35083.06 toks/s, output: 34.26 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 32.80it/s, est. speed input: 35001.86 toks/s, output: 34.18 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 32.77it/s, est. speed input: 34926.65 toks/s, output: 34.11 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 32.82it/s, est. speed input: 34868.82 toks/s, output: 34.05 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:01, 32.84it/s, est. speed input: 34814.59 toks/s, output: 34.00 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:01, 32.83it/s, est. speed input: 34761.32 toks/s, output: 33.95 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 32.79it/s, est. speed input: 34708.21 toks/s, output: 33.89 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:03<00:00, 32.79it/s, est. speed input: 34663.09 toks/s, output: 33.85 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 32.81it/s, est. speed input: 34623.15 toks/s, output: 33.81 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 32.79it/s, est. speed input: 34583.24 toks/s, output: 33.77 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 32.80it/s, est. speed input: 34548.45 toks/s, output: 33.74 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 32.85it/s, est. speed input: 34521.40 toks/s, output: 33.71 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 32.78it/s, est. speed input: 34483.12 toks/s, output: 33.67 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 32.56it/s, est. speed input: 34428.71 toks/s, output: 33.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 32.56it/s, est. speed input: 34417.28 toks/s, output: 33.61 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.61it/s, est. speed input: 34417.28 toks/s, output: 33.61 toks/s]
[rank0]:[W125 15:49:17.386827969 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.0s

测试结果:
  Requests/s:   31.33
  Tokens/s:     32114.95
  Total Reqs:   128
  Elapsed:      4.09s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     32083.62

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:49:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=146681) WARNING 01-25 15:49:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.12 requests/s, 61621.97 total tokens/s, 60.12 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-25 15:49:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:49:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:49:27] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:27] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:49:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:27] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:27] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:49:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:27] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:27] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:27] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:49:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:27] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:49:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:49:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:49:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:49:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:49:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:49:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:49:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:49:33] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:33] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:49:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:33] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:33] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:49:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:33] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:33] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:33] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:49:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:33] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:49:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:49:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:49:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:49:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:49:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:35] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=146681) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=146681) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=146681) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=146681) 
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:48] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=146681) 2026-01-25 15:49:53,178 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=146681) 2026-01-25 15:49:53,230 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=146681) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.72it/s]
(EngineCore_DP0 pid=146681) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.33it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   9%|▉         | 24/256 [00:00<00:00, 237.96it/s]
Adding requests:  28%|██▊       | 72/256 [00:00<00:00, 377.37it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 439.75it/s]
Adding requests:  68%|██████▊   | 174/256 [00:00<00:00, 461.84it/s]
Adding requests:  88%|████████▊ | 226/256 [00:00<00:00, 480.66it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 455.03it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:00<00:00, 279.42it/s, est. speed input: 286174.35 toks/s, output: 279.43 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:00<00:02, 95.79it/s, est. speed input: 109624.64 toks/s, output: 107.05 toks/s] 
Processed prompts:  30%|██▉       | 76/256 [00:00<00:02, 82.33it/s, est. speed input: 95487.52 toks/s, output: 93.25 toks/s]  
Processed prompts:  34%|███▍      | 88/256 [00:01<00:02, 75.95it/s, est. speed input: 89249.95 toks/s, output: 87.16 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:01<00:02, 72.38it/s, est. speed input: 85801.15 toks/s, output: 83.79 toks/s]
Processed prompts:  42%|████▏     | 107/256 [00:01<00:02, 71.80it/s, est. speed input: 84391.64 toks/s, output: 82.41 toks/s]
Processed prompts:  45%|████▍     | 115/256 [00:01<00:02, 69.55it/s, est. speed input: 82590.46 toks/s, output: 80.65 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:01<00:01, 67.78it/s, est. speed input: 81090.16 toks/s, output: 79.19 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:01<00:01, 64.29it/s, est. speed input: 79212.99 toks/s, output: 77.35 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 63.89it/s, est. speed input: 78166.42 toks/s, output: 76.33 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:01<00:01, 63.41it/s, est. speed input: 77208.77 toks/s, output: 75.40 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:02<00:01, 63.03it/s, est. speed input: 76359.60 toks/s, output: 74.57 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 62.83it/s, est. speed input: 75626.26 toks/s, output: 73.85 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:02<00:01, 62.74it/s, est. speed input: 74988.29 toks/s, output: 73.23 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:02<00:01, 62.76it/s, est. speed input: 74431.70 toks/s, output: 72.69 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:02<00:01, 62.79it/s, est. speed input: 73934.81 toks/s, output: 72.20 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:02<00:00, 62.78it/s, est. speed input: 73478.14 toks/s, output: 71.76 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:02<00:00, 62.78it/s, est. speed input: 73064.68 toks/s, output: 71.35 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:02<00:00, 62.69it/s, est. speed input: 72672.24 toks/s, output: 70.97 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:03<00:00, 62.55it/s, est. speed input: 72299.33 toks/s, output: 70.60 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:03<00:00, 62.45it/s, est. speed input: 71955.45 toks/s, output: 70.27 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:03<00:00, 62.44it/s, est. speed input: 71647.49 toks/s, output: 69.97 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:03<00:00, 62.53it/s, est. speed input: 71375.11 toks/s, output: 69.70 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:03<00:00, 62.58it/s, est. speed input: 71120.32 toks/s, output: 69.45 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 62.58it/s, est. speed input: 70970.43 toks/s, output: 69.31 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 69.30it/s, est. speed input: 70970.43 toks/s, output: 69.31 toks/s]
[rank0]:[W125 15:49:59.617218434 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.8s

测试结果:
  Requests/s:   60.12
  Tokens/s:     61621.97
  Total Reqs:   256
  Elapsed:      4.26s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     61561.85

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:50:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=147878) WARNING 01-25 15:50:27 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.54 requests/s, 64105.01 total tokens/s, 62.54 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-25 15:50:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:50:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:50:11] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:11] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:50:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:11] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:11] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:50:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:11] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:11] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:11] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:50:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:11] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:50:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:50:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:50:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:50:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:50:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:50:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:50:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:50:18] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:18] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:50:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:18] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:18] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:50:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:18] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:18] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:18] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:50:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:18] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:50:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:50:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:50:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:50:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:50:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:20] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=147878) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=147878) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=147878) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=147878) 
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:33] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=147878) 2026-01-25 15:50:38,393 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=147878) 2026-01-25 15:50:38,421 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=147878) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:01,  2.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  6.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  5.79it/s]
(EngineCore_DP0 pid=147878) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 14.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.76it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 30/512 [00:00<00:01, 298.66it/s]
Adding requests:  16%|█▌        | 83/512 [00:00<00:00, 431.51it/s]
Adding requests:  26%|██▋       | 135/512 [00:00<00:00, 468.20it/s]
Adding requests:  36%|███▌      | 185/512 [00:00<00:00, 478.37it/s]
Adding requests:  46%|████▋     | 238/512 [00:00<00:00, 493.82it/s]
Adding requests:  56%|█████▋    | 289/512 [00:00<00:00, 496.98it/s]
Adding requests:  66%|██████▋   | 340/512 [00:00<00:00, 498.98it/s]
Adding requests:  77%|███████▋  | 393/512 [00:00<00:00, 505.68it/s]
Adding requests:  87%|████████▋ | 445/512 [00:00<00:00, 506.99it/s]
Adding requests:  97%|█████████▋| 497/512 [00:01<00:00, 508.03it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 490.15it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:00<00:01, 292.38it/s, est. speed input: 299443.45 toks/s, output: 292.40 toks/s]
Processed prompts:  14%|█▍        | 72/512 [00:00<00:03, 114.72it/s, est. speed input: 131456.35 toks/s, output: 128.37 toks/s]
Processed prompts:  17%|█▋        | 89/512 [00:00<00:04, 97.20it/s, est. speed input: 113763.84 toks/s, output: 111.10 toks/s] 
Processed prompts:  20%|█▉        | 102/512 [00:01<00:05, 80.83it/s, est. speed input: 99729.25 toks/s, output: 97.39 toks/s] 
Processed prompts:  22%|██▏       | 112/512 [00:01<00:04, 81.29it/s, est. speed input: 98216.02 toks/s, output: 95.91 toks/s]
Processed prompts:  24%|██▎       | 121/512 [00:01<00:04, 79.98it/s, est. speed input: 96272.30 toks/s, output: 94.02 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:01<00:05, 70.31it/s, est. speed input: 90799.84 toks/s, output: 88.67 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:01<00:05, 69.50it/s, est. speed input: 89115.67 toks/s, output: 87.02 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:01<00:05, 68.92it/s, est. speed input: 87696.73 toks/s, output: 85.64 toks/s]
Processed prompts:  30%|███       | 154/512 [00:01<00:05, 68.20it/s, est. speed input: 86382.96 toks/s, output: 84.36 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:01<00:05, 67.47it/s, est. speed input: 85177.45 toks/s, output: 83.18 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:05, 66.96it/s, est. speed input: 84122.13 toks/s, output: 82.15 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:02<00:04, 66.82it/s, est. speed input: 83239.04 toks/s, output: 81.29 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:02<00:04, 66.90it/s, est. speed input: 82488.42 toks/s, output: 80.55 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:02<00:04, 66.98it/s, est. speed input: 81816.64 toks/s, output: 79.90 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:02<00:04, 66.98it/s, est. speed input: 81197.31 toks/s, output: 79.29 toks/s]
Processed prompts:  41%|████      | 210/512 [00:02<00:04, 67.06it/s, est. speed input: 80645.74 toks/s, output: 78.75 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:02<00:04, 66.85it/s, est. speed input: 80097.12 toks/s, output: 78.22 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:02<00:04, 66.55it/s, est. speed input: 79568.97 toks/s, output: 77.70 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:03<00:04, 66.33it/s, est. speed input: 79080.30 toks/s, output: 77.23 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:03<00:04, 66.19it/s, est. speed input: 78632.11 toks/s, output: 76.79 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:03<00:03, 66.34it/s, est. speed input: 78253.23 toks/s, output: 76.42 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:03<00:03, 66.56it/s, est. speed input: 77916.85 toks/s, output: 76.09 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:03<00:03, 66.80it/s, est. speed input: 77614.19 toks/s, output: 75.79 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:03<00:03, 66.84it/s, est. speed input: 77316.11 toks/s, output: 75.50 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:03<00:03, 66.74it/s, est. speed input: 77019.89 toks/s, output: 75.21 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:03<00:03, 66.60it/s, est. speed input: 76734.05 toks/s, output: 74.93 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:03<00:03, 66.39it/s, est. speed input: 76453.14 toks/s, output: 74.66 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:04<00:03, 66.25it/s, est. speed input: 76189.55 toks/s, output: 74.40 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:04<00:02, 66.31it/s, est. speed input: 75957.96 toks/s, output: 74.18 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:04<00:02, 66.51it/s, est. speed input: 75755.04 toks/s, output: 73.98 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:04<00:02, 66.66it/s, est. speed input: 75564.66 toks/s, output: 73.79 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:04<00:02, 66.78it/s, est. speed input: 75385.49 toks/s, output: 73.62 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:04<00:02, 68.88it/s, est. speed input: 75402.15 toks/s, output: 73.63 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:04<00:02, 68.15it/s, est. speed input: 75220.69 toks/s, output: 73.46 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:04<00:02, 67.50it/s, est. speed input: 75033.69 toks/s, output: 73.27 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:05<00:02, 66.96it/s, est. speed input: 74848.07 toks/s, output: 73.09 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:05<00:02, 66.64it/s, est. speed input: 74675.50 toks/s, output: 72.92 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:05<00:01, 66.59it/s, est. speed input: 74526.03 toks/s, output: 72.78 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:05<00:01, 66.64it/s, est. speed input: 74389.38 toks/s, output: 72.65 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:05<00:01, 66.61it/s, est. speed input: 74253.65 toks/s, output: 72.51 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:05<00:01, 66.57it/s, est. speed input: 74123.20 toks/s, output: 72.39 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:05<00:01, 66.58it/s, est. speed input: 73999.81 toks/s, output: 72.26 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:05<00:01, 66.54it/s, est. speed input: 73878.26 toks/s, output: 72.15 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:06<00:01, 66.43it/s, est. speed input: 73755.69 toks/s, output: 72.03 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:06<00:01, 66.34it/s, est. speed input: 73636.81 toks/s, output: 71.91 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:06<00:00, 68.46it/s, est. speed input: 73673.16 toks/s, output: 71.95 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:06<00:00, 68.00it/s, est. speed input: 73577.19 toks/s, output: 71.85 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:06<00:00, 67.72it/s, est. speed input: 73487.71 toks/s, output: 71.76 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:06<00:00, 67.53it/s, est. speed input: 73400.99 toks/s, output: 71.68 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:06<00:00, 67.27it/s, est. speed input: 73309.42 toks/s, output: 71.59 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:06<00:00, 67.04it/s, est. speed input: 73218.41 toks/s, output: 71.50 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:06<00:00, 66.91it/s, est. speed input: 73131.80 toks/s, output: 71.42 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:07<00:00, 66.75it/s, est. speed input: 73044.12 toks/s, output: 71.33 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 66.75it/s, est. speed input: 73429.43 toks/s, output: 71.71 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 71.71it/s, est. speed input: 73429.43 toks/s, output: 71.71 toks/s]
[rank0]:[W125 15:50:49.339986401 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.5s

测试结果:
  Requests/s:   62.54
  Tokens/s:     64105.01
  Total Reqs:   512
  Elapsed:      8.19s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     64042.47

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:51:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=149123) WARNING 01-25 15:51:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 69.44 requests/s, 71177.80 total tokens/s, 69.44 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-25 15:51:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:51:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:51:03] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:03] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:51:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:03] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:03] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:51:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:03] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:03] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:03] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:51:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:03] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:51:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:51:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:51:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:51:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:51:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:51:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:51:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:51:10] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:10] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:51:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:10] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:10] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:51:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:10] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:10] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:10] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:51:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:10] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:51:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:51:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:51:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:51:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:51:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=149123) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=149123) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=149123) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=149123) 
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:25] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=149123) 2026-01-25 15:51:30,818 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=149123) 2026-01-25 15:51:30,853 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=149123) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:03,  1.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:01<00:01,  1.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:01<00:00,  3.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  3.18it/s]
(EngineCore_DP0 pid=149123) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 13.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.47it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 29/1024 [00:00<00:03, 289.12it/s]
Adding requests:   8%|▊         | 82/1024 [00:00<00:02, 428.49it/s]
Adding requests:  13%|█▎        | 134/1024 [00:00<00:01, 467.37it/s]
Adding requests:  18%|█▊        | 184/1024 [00:00<00:01, 477.98it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:01, 492.35it/s]
Adding requests:  28%|██▊       | 287/1024 [00:00<00:01, 497.29it/s]
Adding requests:  33%|███▎      | 338/1024 [00:00<00:01, 498.66it/s]
Adding requests:  38%|███▊      | 391/1024 [00:00<00:01, 507.52it/s]
Adding requests:  43%|████▎     | 442/1024 [00:00<00:01, 506.80it/s]
Adding requests:  48%|████▊     | 494/1024 [00:01<00:01, 509.55it/s]
Adding requests:  53%|█████▎    | 545/1024 [00:01<00:00, 501.86it/s]
Adding requests:  58%|█████▊    | 598/1024 [00:01<00:00, 509.97it/s]
Adding requests:  63%|██████▎   | 650/1024 [00:01<00:00, 507.18it/s]
Adding requests:  69%|██████▉   | 705/1024 [00:01<00:00, 516.84it/s]
Adding requests:  74%|███████▍  | 757/1024 [00:01<00:00, 516.14it/s]
Adding requests:  79%|███████▉  | 809/1024 [00:01<00:00, 510.60it/s]
Adding requests:  84%|████████▍ | 861/1024 [00:01<00:00, 509.74it/s]
Adding requests:  89%|████████▉ | 914/1024 [00:01<00:00, 514.84it/s]
Adding requests:  94%|█████████▍| 967/1024 [00:01<00:00, 519.21it/s]
Adding requests: 100%|█████████▉| 1020/1024 [00:02<00:00, 519.96it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 502.61it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:00<00:00, 1206.18it/s, est. speed input: 1235474.89 toks/s, output: 1206.27 toks/s]
Processed prompts:  25%|██▌       | 259/1024 [00:01<00:06, 121.03it/s, est. speed input: 144753.71 toks/s, output: 141.36 toks/s]   
Processed prompts:  31%|███       | 313/1024 [00:02<00:06, 105.78it/s, est. speed input: 126981.77 toks/s, output: 124.01 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:03<00:07, 91.95it/s, est. speed input: 114723.67 toks/s, output: 112.03 toks/s] 
Processed prompts:  36%|███▌      | 369/1024 [00:03<00:07, 93.11it/s, est. speed input: 113864.06 toks/s, output: 111.19 toks/s]
Processed prompts:  38%|███▊      | 387/1024 [00:03<00:07, 83.43it/s, est. speed input: 108198.61 toks/s, output: 105.66 toks/s]
Processed prompts:  39%|███▉      | 401/1024 [00:03<00:07, 87.41it/s, est. speed input: 108727.25 toks/s, output: 106.18 toks/s]
Processed prompts:  41%|████      | 415/1024 [00:04<00:07, 81.41it/s, est. speed input: 106058.24 toks/s, output: 103.57 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:04<00:08, 73.20it/s, est. speed input: 102949.68 toks/s, output: 100.54 toks/s]
Processed prompts:  42%|████▏     | 435/1024 [00:04<00:07, 73.93it/s, est. speed input: 102342.92 toks/s, output: 99.94 toks/s] 
Processed prompts:  43%|████▎     | 444/1024 [00:04<00:07, 74.65it/s, est. speed input: 101762.88 toks/s, output: 99.38 toks/s]
Processed prompts:  44%|████▍     | 453/1024 [00:04<00:07, 76.86it/s, est. speed input: 101467.65 toks/s, output: 99.09 toks/s]
Processed prompts:  45%|████▌     | 462/1024 [00:04<00:07, 77.33it/s, est. speed input: 100966.04 toks/s, output: 98.60 toks/s]
Processed prompts:  46%|████▌     | 471/1024 [00:04<00:07, 77.77it/s, est. speed input: 100494.23 toks/s, output: 98.14 toks/s]
Processed prompts:  47%|████▋     | 480/1024 [00:04<00:06, 77.91it/s, est. speed input: 100019.19 toks/s, output: 97.67 toks/s]
Processed prompts:  48%|████▊     | 489/1024 [00:05<00:06, 77.86it/s, est. speed input: 99549.14 toks/s, output: 97.22 toks/s] 
Processed prompts:  49%|████▊     | 497/1024 [00:05<00:06, 75.57it/s, est. speed input: 98917.94 toks/s, output: 96.60 toks/s]
Processed prompts:  49%|████▉     | 505/1024 [00:05<00:07, 73.86it/s, est. speed input: 98312.28 toks/s, output: 96.01 toks/s]
Processed prompts:  50%|█████     | 513/1024 [00:05<00:07, 72.47it/s, est. speed input: 97718.48 toks/s, output: 95.43 toks/s]
Processed prompts:  51%|█████     | 521/1024 [00:05<00:07, 71.49it/s, est. speed input: 97150.61 toks/s, output: 94.87 toks/s]
Processed prompts:  52%|█████▏    | 529/1024 [00:05<00:06, 71.06it/s, est. speed input: 96631.36 toks/s, output: 94.37 toks/s]
Processed prompts:  52%|█████▏    | 537/1024 [00:05<00:06, 70.84it/s, est. speed input: 96141.47 toks/s, output: 93.89 toks/s]
Processed prompts:  53%|█████▎    | 545/1024 [00:05<00:06, 70.54it/s, est. speed input: 95657.40 toks/s, output: 93.42 toks/s]
Processed prompts:  54%|█████▍    | 553/1024 [00:05<00:06, 70.21it/s, est. speed input: 95181.78 toks/s, output: 92.95 toks/s]
Processed prompts:  55%|█████▍    | 561/1024 [00:06<00:06, 70.02it/s, est. speed input: 94727.85 toks/s, output: 92.51 toks/s]
Processed prompts:  56%|█████▌    | 569/1024 [00:06<00:06, 69.76it/s, est. speed input: 94280.21 toks/s, output: 92.07 toks/s]
Processed prompts:  56%|█████▋    | 576/1024 [00:06<00:06, 67.15it/s, est. speed input: 93700.92 toks/s, output: 91.50 toks/s]
Processed prompts:  57%|█████▋    | 583/1024 [00:06<00:06, 65.12it/s, est. speed input: 93124.99 toks/s, output: 90.94 toks/s]
Processed prompts:  58%|█████▊    | 590/1024 [00:06<00:06, 63.87it/s, est. speed input: 92584.40 toks/s, output: 90.41 toks/s]
Processed prompts:  58%|█████▊    | 597/1024 [00:06<00:06, 63.03it/s, est. speed input: 92065.63 toks/s, output: 89.91 toks/s]
Processed prompts:  59%|█████▉    | 604/1024 [00:06<00:06, 62.43it/s, est. speed input: 91563.67 toks/s, output: 89.42 toks/s]
Processed prompts:  60%|█████▉    | 611/1024 [00:06<00:06, 61.94it/s, est. speed input: 91072.50 toks/s, output: 88.94 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:06<00:06, 61.64it/s, est. speed input: 90601.06 toks/s, output: 88.48 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:07<00:06, 64.04it/s, est. speed input: 90288.78 toks/s, output: 88.17 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:07<00:05, 65.62it/s, est. speed input: 89979.44 toks/s, output: 87.87 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:07<00:05, 66.70it/s, est. speed input: 89677.97 toks/s, output: 87.58 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:07<00:05, 67.52it/s, est. speed input: 89390.57 toks/s, output: 87.30 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:07<00:05, 68.19it/s, est. speed input: 89118.52 toks/s, output: 87.03 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:07<00:05, 68.63it/s, est. speed input: 88851.93 toks/s, output: 86.77 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:07<00:05, 68.82it/s, est. speed input: 88586.18 toks/s, output: 86.51 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:07<00:04, 69.10it/s, est. speed input: 88337.61 toks/s, output: 86.27 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:08<00:04, 69.33it/s, est. speed input: 88097.57 toks/s, output: 86.03 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:08<00:04, 69.44it/s, est. speed input: 87861.46 toks/s, output: 85.80 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:08<00:04, 69.26it/s, est. speed input: 87617.20 toks/s, output: 85.56 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:08<00:04, 69.35it/s, est. speed input: 87391.76 toks/s, output: 85.34 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:08<00:04, 69.50it/s, est. speed input: 87177.33 toks/s, output: 85.13 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:08<00:04, 69.59it/s, est. speed input: 86967.87 toks/s, output: 84.93 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:08<00:04, 69.50it/s, est. speed input: 86755.85 toks/s, output: 84.72 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:08<00:03, 69.61it/s, est. speed input: 86558.17 toks/s, output: 84.53 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:08<00:03, 69.60it/s, est. speed input: 86361.38 toks/s, output: 84.34 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:09<00:03, 69.61it/s, est. speed input: 86170.45 toks/s, output: 84.15 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:09<00:03, 69.45it/s, est. speed input: 85975.60 toks/s, output: 83.96 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:09<00:03, 69.44it/s, est. speed input: 85790.56 toks/s, output: 83.78 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:09<00:03, 69.43it/s, est. speed input: 85610.25 toks/s, output: 83.60 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:09<00:03, 69.55it/s, est. speed input: 85440.55 toks/s, output: 83.44 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:09<00:03, 69.35it/s, est. speed input: 85260.62 toks/s, output: 83.26 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:09<00:03, 69.46it/s, est. speed input: 85097.26 toks/s, output: 83.10 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:09<00:02, 69.54it/s, est. speed input: 84938.61 toks/s, output: 82.95 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:09<00:02, 69.58it/s, est. speed input: 84781.81 toks/s, output: 82.79 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:10<00:02, 69.48it/s, est. speed input: 84622.76 toks/s, output: 82.64 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:10<00:02, 69.53it/s, est. speed input: 84473.27 toks/s, output: 82.49 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:10<00:02, 69.38it/s, est. speed input: 84318.23 toks/s, output: 82.34 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:10<00:02, 69.40it/s, est. speed input: 84172.67 toks/s, output: 82.20 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:10<00:02, 69.40it/s, est. speed input: 84029.62 toks/s, output: 82.06 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:10<00:02, 69.41it/s, est. speed input: 83889.99 toks/s, output: 81.92 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:10<00:02, 69.49it/s, est. speed input: 83756.44 toks/s, output: 81.79 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:10<00:01, 69.49it/s, est. speed input: 83623.43 toks/s, output: 81.66 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:11<00:01, 69.36it/s, est. speed input: 83487.61 toks/s, output: 81.53 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:11<00:01, 69.24it/s, est. speed input: 83353.56 toks/s, output: 81.40 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:11<00:01, 69.41it/s, est. speed input: 83232.58 toks/s, output: 81.28 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:11<00:01, 69.46it/s, est. speed input: 83111.10 toks/s, output: 81.16 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:11<00:01, 69.40it/s, est. speed input: 82988.55 toks/s, output: 81.04 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:11<00:01, 71.37it/s, est. speed input: 82945.78 toks/s, output: 81.00 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:11<00:01, 70.82it/s, est. speed input: 82830.62 toks/s, output: 80.89 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:11<00:00, 70.39it/s, est. speed input: 82715.88 toks/s, output: 80.78 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:11<00:00, 70.19it/s, est. speed input: 82607.00 toks/s, output: 80.67 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:12<00:00, 69.96it/s, est. speed input: 82496.66 toks/s, output: 80.56 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:12<00:00, 69.73it/s, est. speed input: 82386.01 toks/s, output: 80.45 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:12<00:00, 71.54it/s, est. speed input: 82348.55 toks/s, output: 80.42 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:12<00:00, 70.89it/s, est. speed input: 82243.58 toks/s, output: 80.32 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:12<00:00, 70.37it/s, est. speed input: 82138.15 toks/s, output: 80.21 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:12<00:00, 70.22it/s, est. speed input: 82041.99 toks/s, output: 80.12 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:12<00:00, 72.87it/s, est. speed input: 82040.32 toks/s, output: 80.12 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 72.87it/s, est. speed input: 82521.30 toks/s, output: 80.59 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 80.59it/s, est. speed input: 82521.30 toks/s, output: 80.59 toks/s]
[rank0]:[W125 15:51:49.402640532 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 59.6s

测试结果:
  Requests/s:   69.44
  Tokens/s:     71177.80
  Total Reqs:   1024
  Elapsed:      14.75s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     71108.36

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:52:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=150656) WARNING 01-25 15:52:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 71.66 requests/s, 73454.51 total tokens/s, 71.66 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-25 15:52:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:52:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:52:06] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:06] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:52:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:06] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:06] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:52:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:06] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:06] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:06] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:52:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:06] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:52:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:52:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:52:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:52:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:52:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:52:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:52:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:52:14] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:14] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:52:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:14] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:14] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:52:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:14] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:14] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:14] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:52:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:14] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:52:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:52:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:52:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:52:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:52:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:15] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=150656) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=150656) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=150656) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=150656) 
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:28] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=150656) 2026-01-25 15:52:33,440 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=150656) 2026-01-25 15:52:33,475 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=150656) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  6.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:01,  3.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:01<00:00,  3.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  5.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.54it/s]
(EngineCore_DP0 pid=150656) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 17.38it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.20it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 32/2048 [00:00<00:06, 318.51it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 432.08it/s]
Adding requests:   6%|▋         | 133/2048 [00:00<00:04, 457.97it/s]
Adding requests:   9%|▉         | 182/2048 [00:00<00:03, 468.47it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:03, 488.56it/s]
Adding requests:  14%|█▍        | 287/2048 [00:00<00:03, 495.17it/s]
Adding requests:  17%|█▋        | 338/2048 [00:00<00:03, 498.22it/s]
Adding requests:  19%|█▉        | 391/2048 [00:00<00:03, 506.52it/s]
Adding requests:  22%|██▏       | 442/2048 [00:00<00:03, 507.46it/s]
Adding requests:  24%|██▍       | 494/2048 [00:01<00:03, 509.71it/s]
Adding requests:  27%|██▋       | 545/2048 [00:01<00:02, 502.51it/s]
Adding requests:  29%|██▉       | 598/2048 [00:01<00:02, 509.50it/s]
Adding requests:  32%|███▏      | 650/2048 [00:01<00:02, 512.64it/s]
Adding requests:  34%|███▍      | 704/2048 [00:01<00:02, 519.53it/s]
Adding requests:  37%|███▋      | 756/2048 [00:01<00:02, 515.50it/s]
Adding requests:  39%|███▉      | 808/2048 [00:01<00:02, 509.73it/s]
Adding requests:  42%|████▏     | 859/2048 [00:01<00:02, 506.95it/s]
Adding requests:  45%|████▍     | 912/2048 [00:01<00:02, 512.98it/s]
Adding requests:  47%|████▋     | 964/2048 [00:01<00:02, 504.95it/s]
Adding requests:  50%|████▉     | 1017/2048 [00:02<00:02, 510.17it/s]
Adding requests:  52%|█████▏    | 1069/2048 [00:02<00:01, 509.75it/s]
Adding requests:  55%|█████▍    | 1120/2048 [00:02<00:01, 505.97it/s]
Adding requests:  57%|█████▋    | 1174/2048 [00:02<00:01, 513.63it/s]
Adding requests:  60%|█████▉    | 1228/2048 [00:02<00:01, 519.83it/s]
Adding requests:  62%|██████▎   | 1280/2048 [00:02<00:01, 514.86it/s]
Adding requests:  65%|██████▌   | 1333/2048 [00:02<00:01, 518.46it/s]
Adding requests:  68%|██████▊   | 1386/2048 [00:02<00:01, 519.52it/s]
Adding requests:  70%|███████   | 1438/2048 [00:02<00:01, 518.90it/s]
Adding requests:  73%|███████▎  | 1492/2048 [00:02<00:01, 523.44it/s]
Adding requests:  75%|███████▌  | 1545/2048 [00:03<00:00, 524.19it/s]
Adding requests:  78%|███████▊  | 1599/2048 [00:03<00:00, 528.70it/s]
Adding requests:  81%|████████  | 1652/2048 [00:03<00:00, 527.00it/s]
Adding requests:  83%|████████▎ | 1705/2048 [00:03<00:00, 523.60it/s]
Adding requests:  86%|████████▌ | 1758/2048 [00:03<00:00, 522.26it/s]
Adding requests:  88%|████████▊ | 1811/2048 [00:03<00:00, 520.49it/s]
Adding requests:  91%|█████████ | 1864/2048 [00:03<00:00, 516.79it/s]
Adding requests:  94%|█████████▎| 1917/2048 [00:03<00:00, 519.51it/s]
Adding requests:  96%|█████████▌| 1969/2048 [00:03<00:00, 517.66it/s]
Adding requests:  99%|█████████▉| 2023/2048 [00:03<00:00, 521.86it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 510.12it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:00<00:00, 2421.15it/s, est. speed input: 2479773.27 toks/s, output: 2421.30 toks/s]
Processed prompts:  25%|██▌       | 517/2048 [00:03<00:11, 127.88it/s, est. speed input: 154170.97 toks/s, output: 150.56 toks/s]   
Processed prompts:  30%|███       | 621/2048 [00:04<00:12, 109.81it/s, est. speed input: 133236.82 toks/s, output: 130.11 toks/s]
Processed prompts:  33%|███▎      | 682/2048 [00:05<00:13, 99.20it/s, est. speed input: 123257.01 toks/s, output: 120.37 toks/s] 
Processed prompts:  35%|███▌      | 722/2048 [00:06<00:14, 90.75it/s, est. speed input: 116714.16 toks/s, output: 113.98 toks/s]
Processed prompts:  37%|███▋      | 750/2048 [00:06<00:13, 93.94it/s, est. speed input: 117097.34 toks/s, output: 114.35 toks/s]
Processed prompts:  38%|███▊      | 773/2048 [00:07<00:14, 85.06it/s, est. speed input: 113002.44 toks/s, output: 110.35 toks/s]
Processed prompts:  39%|███▊      | 790/2048 [00:07<00:14, 83.89it/s, est. speed input: 111916.74 toks/s, output: 109.29 toks/s]
Processed prompts:  39%|███▉      | 804/2048 [00:07<00:15, 80.56it/s, est. speed input: 110486.67 toks/s, output: 107.90 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:07<00:15, 77.40it/s, est. speed input: 109164.91 toks/s, output: 106.61 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:07<00:15, 76.19it/s, est. speed input: 108149.10 toks/s, output: 105.61 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:08<00:15, 75.10it/s, est. speed input: 107184.15 toks/s, output: 104.67 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:08<00:15, 74.23it/s, est. speed input: 106274.95 toks/s, output: 103.78 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:08<00:15, 73.54it/s, est. speed input: 105415.41 toks/s, output: 102.94 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:08<00:15, 73.09it/s, est. speed input: 104609.69 toks/s, output: 102.16 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:09<00:15, 72.77it/s, est. speed input: 103845.34 toks/s, output: 101.41 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:09<00:15, 73.61it/s, est. speed input: 103246.98 toks/s, output: 100.83 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:09<00:15, 72.96it/s, est. speed input: 102532.03 toks/s, output: 100.13 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:09<00:14, 72.66it/s, est. speed input: 101868.16 toks/s, output: 99.48 toks/s] 
Processed prompts:  48%|████▊     | 978/2048 [00:09<00:14, 73.67it/s, est. speed input: 101361.72 toks/s, output: 98.99 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:10<00:14, 73.03it/s, est. speed input: 100740.34 toks/s, output: 98.38 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:10<00:14, 72.58it/s, est. speed input: 100144.68 toks/s, output: 97.80 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:10<00:14, 72.32it/s, est. speed input: 99579.77 toks/s, output: 97.25 toks/s] 
Processed prompts:  51%|█████     | 1042/2048 [00:10<00:13, 72.24it/s, est. speed input: 99047.90 toks/s, output: 96.73 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:10<00:13, 72.10it/s, est. speed input: 98529.86 toks/s, output: 96.22 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:11<00:13, 71.88it/s, est. speed input: 98021.77 toks/s, output: 95.72 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:11<00:13, 71.70it/s, est. speed input: 97530.56 toks/s, output: 95.24 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:11<00:13, 71.61it/s, est. speed input: 97062.07 toks/s, output: 94.79 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:11<00:12, 71.69it/s, est. speed input: 96623.20 toks/s, output: 94.36 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:12<00:12, 71.75it/s, est. speed input: 96200.36 toks/s, output: 93.95 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:12<00:12, 72.86it/s, est. speed input: 95877.40 toks/s, output: 93.63 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:12<00:12, 72.40it/s, est. speed input: 95469.97 toks/s, output: 93.23 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:12<00:11, 72.12it/s, est. speed input: 95079.43 toks/s, output: 92.85 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:12<00:11, 72.06it/s, est. speed input: 94712.48 toks/s, output: 92.49 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:13<00:11, 71.88it/s, est. speed input: 94347.55 toks/s, output: 92.14 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:13<00:11, 71.69it/s, est. speed input: 93990.55 toks/s, output: 91.79 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:13<00:11, 71.57it/s, est. speed input: 93646.06 toks/s, output: 91.45 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:13<00:10, 71.70it/s, est. speed input: 93327.28 toks/s, output: 91.14 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:14<00:10, 71.76it/s, est. speed input: 93016.91 toks/s, output: 90.84 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:14<00:10, 71.80it/s, est. speed input: 92715.64 toks/s, output: 90.54 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:14<00:10, 71.73it/s, est. speed input: 92417.43 toks/s, output: 90.25 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:14<00:10, 71.70it/s, est. speed input: 92129.44 toks/s, output: 89.97 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:15<00:09, 71.76it/s, est. speed input: 91855.13 toks/s, output: 89.70 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:15<00:09, 71.78it/s, est. speed input: 91587.42 toks/s, output: 89.44 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:15<00:09, 71.81it/s, est. speed input: 91327.99 toks/s, output: 89.19 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:15<00:09, 71.79it/s, est. speed input: 91073.79 toks/s, output: 88.94 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:15<00:08, 71.78it/s, est. speed input: 90826.96 toks/s, output: 88.70 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:16<00:08, 71.76it/s, est. speed input: 90586.55 toks/s, output: 88.46 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:16<00:08, 71.68it/s, est. speed input: 90348.64 toks/s, output: 88.23 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:16<00:08, 71.67it/s, est. speed input: 90119.68 toks/s, output: 88.01 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:16<00:08, 71.72it/s, est. speed input: 89899.89 toks/s, output: 87.79 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:17<00:07, 71.80it/s, est. speed input: 89688.54 toks/s, output: 87.59 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:17<00:07, 71.82it/s, est. speed input: 89480.34 toks/s, output: 87.38 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:17<00:07, 71.71it/s, est. speed input: 89271.45 toks/s, output: 87.18 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:17<00:07, 71.72it/s, est. speed input: 89072.09 toks/s, output: 86.98 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:17<00:06, 71.74it/s, est. speed input: 88878.01 toks/s, output: 86.79 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:18<00:06, 71.79it/s, est. speed input: 88691.21 toks/s, output: 86.61 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:18<00:06, 71.76it/s, est. speed input: 88505.60 toks/s, output: 86.43 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:18<00:06, 71.74it/s, est. speed input: 88323.78 toks/s, output: 86.25 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:18<00:05, 71.85it/s, est. speed input: 88152.86 toks/s, output: 86.09 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:19<00:05, 71.75it/s, est. speed input: 87977.26 toks/s, output: 85.92 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:19<00:05, 71.69it/s, est. speed input: 87806.36 toks/s, output: 85.75 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:19<00:05, 71.67it/s, est. speed input: 87640.37 toks/s, output: 85.59 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:19<00:05, 71.67it/s, est. speed input: 87478.60 toks/s, output: 85.43 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:19<00:04, 71.78it/s, est. speed input: 87325.82 toks/s, output: 85.28 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:20<00:04, 71.76it/s, est. speed input: 87171.70 toks/s, output: 85.13 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:20<00:04, 71.71it/s, est. speed input: 87019.59 toks/s, output: 84.98 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:20<00:04, 71.70it/s, est. speed input: 86871.67 toks/s, output: 84.84 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:20<00:03, 71.69it/s, est. speed input: 86727.24 toks/s, output: 84.69 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:21<00:03, 71.73it/s, est. speed input: 86587.36 toks/s, output: 84.56 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:21<00:03, 71.68it/s, est. speed input: 86447.60 toks/s, output: 84.42 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:21<00:03, 71.72it/s, est. speed input: 86313.34 toks/s, output: 84.29 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:21<00:03, 71.72it/s, est. speed input: 86181.04 toks/s, output: 84.16 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:21<00:02, 71.70it/s, est. speed input: 86050.53 toks/s, output: 84.03 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:22<00:02, 71.65it/s, est. speed input: 85921.48 toks/s, output: 83.91 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:22<00:02, 72.79it/s, est. speed input: 85840.82 toks/s, output: 83.83 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:22<00:02, 72.47it/s, est. speed input: 85718.41 toks/s, output: 83.71 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:22<00:01, 72.22it/s, est. speed input: 85597.39 toks/s, output: 83.59 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:23<00:01, 72.03it/s, est. speed input: 85478.02 toks/s, output: 83.47 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:23<00:01, 71.88it/s, est. speed input: 85360.25 toks/s, output: 83.36 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:23<00:01, 73.08it/s, est. speed input: 85293.12 toks/s, output: 83.29 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:23<00:01, 72.61it/s, est. speed input: 85179.59 toks/s, output: 83.18 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:23<00:00, 72.32it/s, est. speed input: 85069.19 toks/s, output: 83.08 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:24<00:00, 72.07it/s, est. speed input: 84959.24 toks/s, output: 82.97 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:24<00:00, 71.93it/s, est. speed input: 84852.38 toks/s, output: 82.86 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:24<00:00, 73.36it/s, est. speed input: 84800.92 toks/s, output: 82.81 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:24<00:00, 73.36it/s, est. speed input: 85383.67 toks/s, output: 83.38 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:24<00:00, 83.38it/s, est. speed input: 85383.67 toks/s, output: 83.38 toks/s]
[rank0]:[W125 15:53:05.681615448 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 76.8s

测试结果:
  Requests/s:   71.66
  Tokens/s:     73454.51
  Total Reqs:   2048
  Elapsed:      28.58s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     73382.84

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:53:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=152507) WARNING 01-25 15:53:48 [backends.py:609] Failed to read file <frozen os>
Throughput: 73.08 requests/s, 74907.21 total tokens/s, 73.08 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 15:53:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:53:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:53:32] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:32] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:53:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:32] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:32] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:53:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:32] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:32] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:32] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:53:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:32] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:53:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:53:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:53:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:53:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:53:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:53:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:53:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:53:39] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:39] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:53:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:39] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:39] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:53:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:39] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:39] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:39] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:53:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:39] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:53:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:53:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:53:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:53:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:53:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:41] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=152507) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=152507) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.58it/s]
(EngineCore_DP0 pid=152507) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=152507) 
(EngineCore_DP0 pid=152507) [rank0]:W0125 15:53:53.740000 152507 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=152507) [rank0]:W0125 15:53:54.958000 152507 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:55] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=152507) [rank0]:W0125 15:53:56.519000 152507 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=152507) [rank0]:W0125 15:53:56.645000 152507 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=152507) 2026-01-25 15:54:02,038 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=152507) 2026-01-25 15:54:02,155 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=152507) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  5.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00, 10.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:01,  4.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:01<00:01,  4.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:01<00:00,  5.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.21it/s]
(EngineCore_DP0 pid=152507) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.64it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.49it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.65it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 34/4096 [00:00<00:12, 336.08it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 440.06it/s]
Adding requests:   3%|▎         | 137/4096 [00:00<00:08, 469.94it/s]
Adding requests:   5%|▍         | 187/4096 [00:00<00:08, 481.00it/s]
Adding requests:   6%|▌         | 239/4096 [00:00<00:07, 491.78it/s]
Adding requests:   7%|▋         | 290/4096 [00:00<00:07, 496.18it/s]
Adding requests:   8%|▊         | 340/4096 [00:00<00:07, 497.36it/s]
Adding requests:  10%|▉         | 393/4096 [00:00<00:07, 505.14it/s]
Adding requests:  11%|█         | 444/4096 [00:00<00:07, 505.01it/s]
Adding requests:  12%|█▏        | 495/4096 [00:01<00:07, 506.45it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 497.81it/s]
Adding requests:  15%|█▍        | 599/4096 [00:01<00:06, 505.20it/s]
Adding requests:  16%|█▌        | 651/4096 [00:01<00:06, 509.52it/s]
Adding requests:  17%|█▋        | 705/4096 [00:01<00:06, 516.15it/s]
Adding requests:  18%|█▊        | 757/4096 [00:01<00:06, 515.53it/s]
Adding requests:  20%|█▉        | 809/4096 [00:01<00:06, 508.14it/s]
Adding requests:  21%|██        | 860/4096 [00:01<00:06, 506.98it/s]
Adding requests:  22%|██▏       | 913/4096 [00:01<00:06, 512.38it/s]
Adding requests:  24%|██▎       | 966/4096 [00:01<00:06, 515.66it/s]
Adding requests:  25%|██▍       | 1019/4096 [00:02<00:05, 517.64it/s]
Adding requests:  26%|██▌       | 1071/4096 [00:02<00:05, 514.20it/s]
Adding requests:  27%|██▋       | 1123/4096 [00:02<00:05, 513.51it/s]
Adding requests:  29%|██▊       | 1177/4096 [00:02<00:05, 519.45it/s]
Adding requests:  30%|███       | 1229/4096 [00:02<00:05, 510.01it/s]
Adding requests:  31%|███▏      | 1281/4096 [00:02<00:05, 507.70it/s]
Adding requests:  33%|███▎      | 1335/4096 [00:02<00:05, 515.74it/s]
Adding requests:  34%|███▍      | 1387/4096 [00:02<00:05, 516.35it/s]
Adding requests:  35%|███▌      | 1440/4096 [00:02<00:05, 517.46it/s]
Adding requests:  36%|███▋      | 1494/4096 [00:02<00:04, 521.29it/s]
Adding requests:  38%|███▊      | 1547/4096 [00:03<00:04, 523.01it/s]
Adding requests:  39%|███▉      | 1601/4096 [00:03<00:04, 527.78it/s]
Adding requests:  40%|████      | 1654/4096 [00:03<00:04, 524.42it/s]
Adding requests:  42%|████▏     | 1707/4096 [00:03<00:04, 520.91it/s]
Adding requests:  43%|████▎     | 1760/4096 [00:03<00:04, 520.86it/s]
Adding requests:  44%|████▍     | 1813/4096 [00:03<00:04, 521.22it/s]
Adding requests:  46%|████▌     | 1866/4096 [00:03<00:04, 517.34it/s]
Adding requests:  47%|████▋     | 1919/4096 [00:03<00:04, 519.02it/s]
Adding requests:  48%|████▊     | 1971/4096 [00:03<00:04, 516.66it/s]
Adding requests:  49%|████▉     | 2024/4096 [00:03<00:03, 520.13it/s]
Adding requests:  51%|█████     | 2077/4096 [00:04<00:03, 521.88it/s]
Adding requests:  52%|█████▏    | 2130/4096 [00:04<00:03, 516.68it/s]
Adding requests:  53%|█████▎    | 2182/4096 [00:04<00:03, 509.46it/s]
Adding requests:  55%|█████▍    | 2236/4096 [00:04<00:03, 515.95it/s]
Adding requests:  56%|█████▌    | 2288/4096 [00:04<00:03, 515.08it/s]
Adding requests:  57%|█████▋    | 2340/4096 [00:04<00:03, 515.60it/s]
Adding requests:  58%|█████▊    | 2393/4096 [00:04<00:03, 517.88it/s]
Adding requests:  60%|█████▉    | 2445/4096 [00:04<00:03, 504.07it/s]
Adding requests:  61%|██████    | 2498/4096 [00:04<00:03, 509.49it/s]
Adding requests:  62%|██████▏   | 2550/4096 [00:04<00:03, 509.25it/s]
Adding requests:  64%|██████▎   | 2602/4096 [00:05<00:02, 510.92it/s]
Adding requests:  65%|██████▍   | 2655/4096 [00:05<00:02, 514.99it/s]
Adding requests:  66%|██████▌   | 2707/4096 [00:05<00:02, 511.12it/s]
Adding requests:  67%|██████▋   | 2759/4096 [00:05<00:02, 511.23it/s]
Adding requests:  69%|██████▊   | 2811/4096 [00:05<00:02, 509.39it/s]
Adding requests:  70%|██████▉   | 2863/4096 [00:05<00:02, 509.78it/s]
Adding requests:  71%|███████   | 2915/4096 [00:05<00:02, 512.29it/s]
Adding requests:  72%|███████▏  | 2967/4096 [00:05<00:02, 507.91it/s]
Adding requests:  74%|███████▎  | 3019/4096 [00:05<00:02, 510.31it/s]
Adding requests:  75%|███████▍  | 3071/4096 [00:06<00:02, 511.08it/s]
Adding requests:  76%|███████▌  | 3123/4096 [00:06<00:01, 512.88it/s]
Adding requests:  78%|███████▊  | 3175/4096 [00:06<00:01, 512.49it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:06<00:01, 511.54it/s]
Adding requests:  80%|████████  | 3280/4096 [00:06<00:01, 515.01it/s]
Adding requests:  81%|████████▏ | 3332/4096 [00:06<00:01, 512.72it/s]
Adding requests:  83%|████████▎ | 3385/4096 [00:06<00:01, 515.19it/s]
Adding requests:  84%|████████▍ | 3437/4096 [00:06<00:01, 515.88it/s]
Adding requests:  85%|████████▌ | 3489/4096 [00:06<00:01, 505.95it/s]
Adding requests:  86%|████████▋ | 3540/4096 [00:06<00:01, 505.28it/s]
Adding requests:  88%|████████▊ | 3591/4096 [00:07<00:00, 506.42it/s]
Adding requests:  89%|████████▉ | 3642/4096 [00:07<00:00, 502.36it/s]
Adding requests:  90%|█████████ | 3694/4096 [00:07<00:00, 507.01it/s]
Adding requests:  91%|█████████▏| 3745/4096 [00:07<00:00, 505.72it/s]
Adding requests:  93%|█████████▎| 3796/4096 [00:07<00:00, 499.78it/s]
Adding requests:  94%|█████████▍| 3848/4096 [00:07<00:00, 505.21it/s]
Adding requests:  95%|█████████▌| 3900/4096 [00:07<00:00, 508.23it/s]
Adding requests:  96%|█████████▋| 3952/4096 [00:07<00:00, 510.06it/s]
Adding requests:  98%|█████████▊| 4004/4096 [00:07<00:00, 509.77it/s]
Adding requests:  99%|█████████▉| 4055/4096 [00:07<00:00, 507.18it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 509.94it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 566/4096 [00:00<00:01, 3036.90it/s, est. speed input: 3110164.84 toks/s, output: 3037.01 toks/s]
Processed prompts:  21%|██        | 870/4096 [00:04<00:18, 172.01it/s, est. speed input: 215885.29 toks/s, output: 210.82 toks/s]   
Processed prompts:  24%|██▍       | 1000/4096 [00:05<00:22, 135.40it/s, est. speed input: 174824.12 toks/s, output: 170.73 toks/s]
Processed prompts:  26%|██▌       | 1074/4096 [00:06<00:24, 124.54it/s, est. speed input: 163386.39 toks/s, output: 159.56 toks/s]
Processed prompts:  27%|██▋       | 1123/4096 [00:07<00:27, 108.53it/s, est. speed input: 151233.89 toks/s, output: 147.69 toks/s]
Processed prompts:  28%|██▊       | 1156/4096 [00:08<00:28, 104.18it/s, est. speed input: 147426.05 toks/s, output: 143.97 toks/s]
Processed prompts:  29%|██▉       | 1181/4096 [00:08<00:30, 96.09it/s, est. speed input: 142803.29 toks/s, output: 139.46 toks/s] 
Processed prompts:  29%|██▉       | 1206/4096 [00:08<00:32, 88.48it/s, est. speed input: 138673.91 toks/s, output: 135.42 toks/s]
Processed prompts:  30%|███       | 1238/4096 [00:09<00:33, 85.05it/s, est. speed input: 135661.11 toks/s, output: 132.48 toks/s]
Processed prompts:  31%|███       | 1270/4096 [00:09<00:34, 82.21it/s, est. speed input: 132942.16 toks/s, output: 129.83 toks/s]
Processed prompts:  32%|███▏      | 1302/4096 [00:10<00:34, 79.97it/s, est. speed input: 130474.34 toks/s, output: 127.42 toks/s]
Processed prompts:  33%|███▎      | 1334/4096 [00:10<00:35, 78.23it/s, est. speed input: 128210.79 toks/s, output: 125.21 toks/s]
Processed prompts:  33%|███▎      | 1366/4096 [00:11<00:35, 76.91it/s, est. speed input: 126127.14 toks/s, output: 123.17 toks/s]
Processed prompts:  34%|███▍      | 1398/4096 [00:11<00:35, 75.83it/s, est. speed input: 124180.61 toks/s, output: 121.27 toks/s]
Processed prompts:  35%|███▍      | 1430/4096 [00:11<00:35, 75.08it/s, est. speed input: 122383.20 toks/s, output: 119.51 toks/s]
Processed prompts:  36%|███▌      | 1462/4096 [00:12<00:35, 74.49it/s, est. speed input: 120703.92 toks/s, output: 117.87 toks/s]
Processed prompts:  36%|███▋      | 1494/4096 [00:12<00:35, 74.13it/s, est. speed input: 119148.84 toks/s, output: 116.36 toks/s]
Processed prompts:  37%|███▋      | 1526/4096 [00:13<00:34, 73.80it/s, est. speed input: 117683.38 toks/s, output: 114.92 toks/s]
Processed prompts:  38%|███▊      | 1558/4096 [00:13<00:34, 73.66it/s, est. speed input: 116329.28 toks/s, output: 113.60 toks/s]
Processed prompts:  39%|███▉      | 1590/4096 [00:14<00:34, 73.51it/s, est. speed input: 115049.83 toks/s, output: 112.35 toks/s]
Processed prompts:  40%|███▉      | 1622/4096 [00:14<00:33, 73.41it/s, est. speed input: 113845.72 toks/s, output: 111.18 toks/s]
Processed prompts:  40%|████      | 1654/4096 [00:15<00:33, 73.36it/s, est. speed input: 112717.37 toks/s, output: 110.08 toks/s]
Processed prompts:  41%|████      | 1686/4096 [00:15<00:32, 73.26it/s, est. speed input: 111641.54 toks/s, output: 109.02 toks/s]
Processed prompts:  42%|████▏     | 1718/4096 [00:15<00:32, 73.17it/s, est. speed input: 110623.98 toks/s, output: 108.03 toks/s]
Processed prompts:  43%|████▎     | 1750/4096 [00:16<00:32, 73.14it/s, est. speed input: 109664.48 toks/s, output: 107.09 toks/s]
Processed prompts:  44%|████▎     | 1782/4096 [00:16<00:31, 73.04it/s, est. speed input: 108744.00 toks/s, output: 106.20 toks/s]
Processed prompts:  44%|████▍     | 1814/4096 [00:17<00:31, 73.08it/s, est. speed input: 107885.74 toks/s, output: 105.36 toks/s]
Processed prompts:  45%|████▌     | 1846/4096 [00:17<00:30, 73.06it/s, est. speed input: 107063.31 toks/s, output: 104.55 toks/s]
Processed prompts:  46%|████▌     | 1878/4096 [00:18<00:30, 73.70it/s, est. speed input: 106357.13 toks/s, output: 103.86 toks/s]
Processed prompts:  47%|████▋     | 1910/4096 [00:18<00:29, 73.50it/s, est. speed input: 105610.12 toks/s, output: 103.13 toks/s]
Processed prompts:  47%|████▋     | 1942/4096 [00:18<00:29, 73.95it/s, est. speed input: 104962.57 toks/s, output: 102.50 toks/s]
Processed prompts:  48%|████▊     | 1974/4096 [00:19<00:28, 73.64it/s, est. speed input: 104277.16 toks/s, output: 101.83 toks/s]
Processed prompts:  49%|████▉     | 2006/4096 [00:19<00:28, 73.35it/s, est. speed input: 103614.11 toks/s, output: 101.19 toks/s]
Processed prompts:  50%|████▉     | 2038/4096 [00:20<00:28, 73.18it/s, est. speed input: 102982.68 toks/s, output: 100.57 toks/s]
Processed prompts:  51%|█████     | 2070/4096 [00:20<00:27, 73.00it/s, est. speed input: 102372.64 toks/s, output: 99.97 toks/s] 
Processed prompts:  51%|█████▏    | 2102/4096 [00:21<00:27, 72.96it/s, est. speed input: 101796.41 toks/s, output: 99.41 toks/s]
Processed prompts:  52%|█████▏    | 2134/4096 [00:21<00:26, 72.88it/s, est. speed input: 101238.22 toks/s, output: 98.87 toks/s]
Processed prompts:  53%|█████▎    | 2166/4096 [00:22<00:26, 72.90it/s, est. speed input: 100709.89 toks/s, output: 98.35 toks/s]
Processed prompts:  54%|█████▎    | 2198/4096 [00:22<00:26, 72.88it/s, est. speed input: 100199.05 toks/s, output: 97.85 toks/s]
Processed prompts:  54%|█████▍    | 2230/4096 [00:22<00:25, 74.19it/s, est. speed input: 99821.18 toks/s, output: 97.48 toks/s] 
Processed prompts:  55%|█████▌    | 2262/4096 [00:23<00:24, 73.84it/s, est. speed input: 99350.96 toks/s, output: 97.02 toks/s]
Processed prompts:  56%|█████▌    | 2294/4096 [00:23<00:24, 74.20it/s, est. speed input: 98946.93 toks/s, output: 96.63 toks/s]
Processed prompts:  57%|█████▋    | 2326/4096 [00:24<00:23, 74.47it/s, est. speed input: 98558.45 toks/s, output: 96.25 toks/s]
Processed prompts:  58%|█████▊    | 2358/4096 [00:24<00:23, 73.92it/s, est. speed input: 98125.86 toks/s, output: 95.83 toks/s]
Processed prompts:  58%|█████▊    | 2390/4096 [00:25<00:23, 73.61it/s, est. speed input: 97714.29 toks/s, output: 95.42 toks/s]
Processed prompts:  59%|█████▉    | 2422/4096 [00:25<00:22, 73.36it/s, est. speed input: 97314.77 toks/s, output: 95.03 toks/s]
Processed prompts:  60%|█████▉    | 2454/4096 [00:25<00:22, 73.25it/s, est. speed input: 96932.91 toks/s, output: 94.66 toks/s]
Processed prompts:  61%|██████    | 2486/4096 [00:26<00:21, 73.70it/s, est. speed input: 96602.34 toks/s, output: 94.34 toks/s]
Processed prompts:  61%|██████▏   | 2518/4096 [00:26<00:21, 73.52it/s, est. speed input: 96246.95 toks/s, output: 93.99 toks/s]
Processed prompts:  62%|██████▏   | 2550/4096 [00:27<00:21, 73.29it/s, est. speed input: 95895.42 toks/s, output: 93.65 toks/s]
Processed prompts:  63%|██████▎   | 2582/4096 [00:27<00:20, 73.72it/s, est. speed input: 95595.93 toks/s, output: 93.36 toks/s]
Processed prompts:  64%|██████▍   | 2614/4096 [00:28<00:20, 73.53it/s, est. speed input: 95272.80 toks/s, output: 93.04 toks/s]
Processed prompts:  65%|██████▍   | 2646/4096 [00:28<00:19, 73.38it/s, est. speed input: 94957.86 toks/s, output: 92.73 toks/s]
Processed prompts:  65%|██████▌   | 2678/4096 [00:28<00:19, 73.29it/s, est. speed input: 94653.76 toks/s, output: 92.44 toks/s]
Processed prompts:  66%|██████▌   | 2710/4096 [00:29<00:18, 73.15it/s, est. speed input: 94353.77 toks/s, output: 92.14 toks/s]
Processed prompts:  67%|██████▋   | 2742/4096 [00:29<00:18, 73.11it/s, est. speed input: 94066.50 toks/s, output: 91.86 toks/s]
Processed prompts:  68%|██████▊   | 2774/4096 [00:30<00:18, 73.02it/s, est. speed input: 93783.50 toks/s, output: 91.59 toks/s]
Processed prompts:  69%|██████▊   | 2806/4096 [00:30<00:17, 72.96it/s, est. speed input: 93508.32 toks/s, output: 91.32 toks/s]
Processed prompts:  69%|██████▉   | 2838/4096 [00:31<00:17, 72.94it/s, est. speed input: 93242.97 toks/s, output: 91.06 toks/s]
Processed prompts:  70%|███████   | 2870/4096 [00:31<00:16, 72.83it/s, est. speed input: 92978.63 toks/s, output: 90.80 toks/s]
Processed prompts:  71%|███████   | 2902/4096 [00:32<00:16, 72.90it/s, est. speed input: 92730.81 toks/s, output: 90.56 toks/s]
Processed prompts:  72%|███████▏  | 2934/4096 [00:32<00:15, 72.81it/s, est. speed input: 92481.36 toks/s, output: 90.31 toks/s]
Processed prompts:  72%|███████▏  | 2966/4096 [00:32<00:15, 72.89it/s, est. speed input: 92246.34 toks/s, output: 90.08 toks/s]
Processed prompts:  73%|███████▎  | 2998/4096 [00:33<00:15, 72.82it/s, est. speed input: 92010.81 toks/s, output: 89.85 toks/s]
Processed prompts:  74%|███████▍  | 3030/4096 [00:33<00:14, 72.80it/s, est. speed input: 91783.27 toks/s, output: 89.63 toks/s]
Processed prompts:  75%|███████▍  | 3062/4096 [00:34<00:14, 72.71it/s, est. speed input: 91556.85 toks/s, output: 89.41 toks/s]
Processed prompts:  76%|███████▌  | 3094/4096 [00:34<00:13, 72.73it/s, est. speed input: 91341.23 toks/s, output: 89.20 toks/s]
Processed prompts:  76%|███████▋  | 3126/4096 [00:35<00:13, 73.35it/s, est. speed input: 91161.77 toks/s, output: 89.03 toks/s]
Processed prompts:  77%|███████▋  | 3158/4096 [00:35<00:12, 73.18it/s, est. speed input: 90956.33 toks/s, output: 88.82 toks/s]
Processed prompts:  78%|███████▊  | 3190/4096 [00:35<00:12, 73.10it/s, est. speed input: 90757.74 toks/s, output: 88.63 toks/s]
Processed prompts:  79%|███████▊  | 3222/4096 [00:36<00:11, 72.94it/s, est. speed input: 90558.60 toks/s, output: 88.44 toks/s]
Processed prompts:  79%|███████▉  | 3254/4096 [00:36<00:11, 72.92it/s, est. speed input: 90368.82 toks/s, output: 88.25 toks/s]
Processed prompts:  80%|████████  | 3286/4096 [00:37<00:11, 72.87it/s, est. speed input: 90181.82 toks/s, output: 88.07 toks/s]
Processed prompts:  81%|████████  | 3318/4096 [00:37<00:10, 72.82it/s, est. speed input: 89998.53 toks/s, output: 87.89 toks/s]
Processed prompts:  82%|████████▏ | 3350/4096 [00:38<00:10, 72.78it/s, est. speed input: 89819.01 toks/s, output: 87.71 toks/s]
Processed prompts:  83%|████████▎ | 3382/4096 [00:38<00:09, 72.81it/s, est. speed input: 89646.16 toks/s, output: 87.54 toks/s]
Processed prompts:  83%|████████▎ | 3414/4096 [00:39<00:09, 72.72it/s, est. speed input: 89472.34 toks/s, output: 87.38 toks/s]
Processed prompts:  84%|████████▍ | 3446/4096 [00:39<00:08, 72.75it/s, est. speed input: 89306.68 toks/s, output: 87.21 toks/s]
Processed prompts:  85%|████████▍ | 3478/4096 [00:39<00:08, 72.71it/s, est. speed input: 89142.01 toks/s, output: 87.05 toks/s]
Processed prompts:  86%|████████▌ | 3510/4096 [00:40<00:08, 72.76it/s, est. speed input: 88984.06 toks/s, output: 86.90 toks/s]
Processed prompts:  86%|████████▋ | 3542/4096 [00:40<00:07, 72.75it/s, est. speed input: 88827.43 toks/s, output: 86.75 toks/s]
Processed prompts:  87%|████████▋ | 3574/4096 [00:41<00:07, 72.69it/s, est. speed input: 88672.21 toks/s, output: 86.59 toks/s]
Processed prompts:  88%|████████▊ | 3606/4096 [00:41<00:06, 72.68it/s, est. speed input: 88521.61 toks/s, output: 86.45 toks/s]
Processed prompts:  89%|████████▉ | 3638/4096 [00:42<00:06, 72.72it/s, est. speed input: 88376.16 toks/s, output: 86.30 toks/s]
Processed prompts:  90%|████████▉ | 3670/4096 [00:42<00:05, 72.74it/s, est. speed input: 88233.08 toks/s, output: 86.17 toks/s]
Processed prompts:  90%|█████████ | 3702/4096 [00:43<00:05, 72.72it/s, est. speed input: 88091.60 toks/s, output: 86.03 toks/s]
Processed prompts:  91%|█████████ | 3734/4096 [00:43<00:04, 73.29it/s, est. speed input: 87976.81 toks/s, output: 85.91 toks/s]
Processed prompts:  92%|█████████▏| 3766/4096 [00:43<00:04, 73.08it/s, est. speed input: 87839.73 toks/s, output: 85.78 toks/s]
Processed prompts:  93%|█████████▎| 3798/4096 [00:44<00:04, 72.92it/s, est. speed input: 87705.28 toks/s, output: 85.65 toks/s]
Processed prompts:  94%|█████████▎| 3830/4096 [00:44<00:03, 72.83it/s, est. speed input: 87573.70 toks/s, output: 85.52 toks/s]
Processed prompts:  94%|█████████▍| 3862/4096 [00:45<00:03, 72.76it/s, est. speed input: 87444.64 toks/s, output: 85.40 toks/s]
Processed prompts:  95%|█████████▌| 3894/4096 [00:45<00:02, 72.64it/s, est. speed input: 87315.63 toks/s, output: 85.27 toks/s]
Processed prompts:  96%|█████████▌| 3926/4096 [00:46<00:02, 72.62it/s, est. speed input: 87191.07 toks/s, output: 85.15 toks/s]
Processed prompts:  97%|█████████▋| 3958/4096 [00:46<00:01, 72.63it/s, est. speed input: 87070.05 toks/s, output: 85.03 toks/s]
Processed prompts:  97%|█████████▋| 3990/4096 [00:46<00:01, 72.64it/s, est. speed input: 86951.36 toks/s, output: 84.91 toks/s]
Processed prompts:  98%|█████████▊| 4022/4096 [00:47<00:01, 73.25it/s, est. speed input: 86856.98 toks/s, output: 84.82 toks/s]
Processed prompts:  99%|█████████▉| 4054/4096 [00:47<00:00, 73.17it/s, est. speed input: 86745.81 toks/s, output: 84.71 toks/s]
Processed prompts: 100%|█████████▉| 4086/4096 [00:48<00:00, 90.70it/s, est. speed input: 87147.26 toks/s, output: 85.10 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:48<00:00, 90.70it/s, est. speed input: 87359.54 toks/s, output: 85.31 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:48<00:00, 85.31it/s, est. speed input: 87359.54 toks/s, output: 85.31 toks/s]
[rank0]:[W125 15:55:02.049774221 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 116.1s

测试结果:
  Requests/s:   73.08
  Tokens/s:     74907.21
  Total Reqs:   4096
  Elapsed:      56.05s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     74834.13

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:55:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=155178) WARNING 01-25 15:56:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 73.09 requests/s, 74915.41 total tokens/s, 73.09 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-25 15:55:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:55:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:55:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:44] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:55:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:44] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:55:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:44] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:55:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:55:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:55:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:55:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:55:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:55:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:55:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:55:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:55:51] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:51] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:55:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:51] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:51] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:55:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:51] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:51] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:51] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:55:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:51] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:55:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:55:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:55:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:55:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:55:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=155178) [2026-01-25 15:55:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=155178) [2026-01-25 15:55:53] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=155178) [2026-01-25 15:55:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=155178) [2026-01-25 15:55:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=155178) [2026-01-25 15:55:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=155178) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=155178) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=155178) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=155178) 
(EngineCore_DP0 pid=155178) [rank0]:W0125 15:56:06.097000 155178 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=155178) [rank0]:W0125 15:56:06.400000 155178 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=155178) [2026-01-25 15:56:06] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=155178) [rank0]:W0125 15:56:07.539000 155178 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=155178) [rank0]:W0125 15:56:07.665000 155178 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=155178) 2026-01-25 15:56:12,038 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=155178) 2026-01-25 15:56:12,074 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=155178) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:15,  1.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:04,  3.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:02,  6.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:00, 10.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  9.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  5.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  5.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  7.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.74it/s]
(EngineCore_DP0 pid=155178) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 17.19it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.92it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 18.28it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 18.63it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 18.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 16.07it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/8192 [00:00<00:20, 404.67it/s]
Adding requests:   1%|          | 93/8192 [00:00<00:17, 468.58it/s]
Adding requests:   2%|▏         | 144/8192 [00:00<00:16, 483.12it/s]
Adding requests:   2%|▏         | 194/8192 [00:00<00:16, 486.69it/s]
Adding requests:   3%|▎         | 246/8192 [00:00<00:16, 495.67it/s]
Adding requests:   4%|▎         | 296/8192 [00:00<00:15, 496.82it/s]
Adding requests:   4%|▍         | 346/8192 [00:00<00:15, 494.08it/s]
Adding requests:   5%|▍         | 398/8192 [00:00<00:15, 499.60it/s]
Adding requests:   5%|▌         | 448/8192 [00:00<00:15, 498.89it/s]
Adding requests:   6%|▌         | 498/8192 [00:01<00:15, 497.88it/s]
Adding requests:   7%|▋         | 548/8192 [00:01<00:15, 490.26it/s]
Adding requests:   7%|▋         | 600/8192 [00:01<00:15, 494.77it/s]
Adding requests:   8%|▊         | 652/8192 [00:01<00:15, 501.47it/s]
Adding requests:   9%|▊         | 703/8192 [00:01<00:14, 502.22it/s]
Adding requests:   9%|▉         | 754/8192 [00:01<00:14, 500.72it/s]
Adding requests:  10%|▉         | 805/8192 [00:01<00:14, 494.12it/s]
Adding requests:  10%|█         | 855/8192 [00:01<00:14, 493.92it/s]
Adding requests:  11%|█         | 907/8192 [00:01<00:14, 501.02it/s]
Adding requests:  12%|█▏        | 958/8192 [00:01<00:14, 503.45it/s]
Adding requests:  12%|█▏        | 1009/8192 [00:02<00:14, 504.97it/s]
Adding requests:  13%|█▎        | 1060/8192 [00:02<00:14, 506.24it/s]
Adding requests:  14%|█▎        | 1111/8192 [00:02<00:14, 502.16it/s]
Adding requests:  14%|█▍        | 1163/8192 [00:02<00:13, 505.15it/s]
Adding requests:  15%|█▍        | 1216/8192 [00:02<00:13, 511.54it/s]
Adding requests:  15%|█▌        | 1268/8192 [00:02<00:13, 507.80it/s]
Adding requests:  16%|█▌        | 1319/8192 [00:02<00:13, 508.43it/s]
Adding requests:  17%|█▋        | 1371/8192 [00:02<00:13, 510.14it/s]
Adding requests:  17%|█▋        | 1423/8192 [00:02<00:13, 512.84it/s]
Adding requests:  18%|█▊        | 1475/8192 [00:02<00:13, 513.68it/s]
Adding requests:  19%|█▊        | 1528/8192 [00:03<00:12, 516.12it/s]
Adding requests:  19%|█▉        | 1581/8192 [00:03<00:12, 517.63it/s]
Adding requests:  20%|█▉        | 1635/8192 [00:03<00:12, 522.26it/s]
Adding requests:  21%|██        | 1688/8192 [00:03<00:12, 515.40it/s]
Adding requests:  21%|██▏       | 1741/8192 [00:03<00:12, 517.63it/s]
Adding requests:  22%|██▏       | 1793/8192 [00:03<00:12, 515.58it/s]
Adding requests:  23%|██▎       | 1845/8192 [00:03<00:12, 516.84it/s]
Adding requests:  23%|██▎       | 1897/8192 [00:03<00:12, 501.62it/s]
Adding requests:  24%|██▍       | 1949/8192 [00:03<00:12, 504.51it/s]
Adding requests:  24%|██▍       | 2001/8192 [00:03<00:12, 507.02it/s]
Adding requests:  25%|██▌       | 2053/8192 [00:04<00:12, 509.94it/s]
Adding requests:  26%|██▌       | 2105/8192 [00:04<00:11, 511.93it/s]
Adding requests:  26%|██▋       | 2157/8192 [00:04<00:11, 503.48it/s]
Adding requests:  27%|██▋       | 2208/8192 [00:04<00:11, 500.48it/s]
Adding requests:  28%|██▊       | 2260/8192 [00:04<00:11, 505.62it/s]
Adding requests:  28%|██▊       | 2312/8192 [00:04<00:11, 509.05it/s]
Adding requests:  29%|██▉       | 2363/8192 [00:04<00:11, 506.96it/s]
Adding requests:  29%|██▉       | 2414/8192 [00:04<00:11, 506.99it/s]
Adding requests:  30%|███       | 2466/8192 [00:04<00:11, 508.50it/s]
Adding requests:  31%|███       | 2517/8192 [00:04<00:11, 507.96it/s]
Adding requests:  31%|███▏      | 2570/8192 [00:05<00:10, 514.01it/s]
Adding requests:  32%|███▏      | 2622/8192 [00:05<00:10, 513.66it/s]
Adding requests:  33%|███▎      | 2674/8192 [00:05<00:10, 514.78it/s]
Adding requests:  33%|███▎      | 2726/8192 [00:05<00:10, 512.80it/s]
Adding requests:  34%|███▍      | 2778/8192 [00:05<00:10, 510.05it/s]
Adding requests:  35%|███▍      | 2830/8192 [00:05<00:10, 505.36it/s]
Adding requests:  35%|███▌      | 2882/8192 [00:05<00:10, 507.61it/s]
Adding requests:  36%|███▌      | 2933/8192 [00:05<00:10, 505.16it/s]
Adding requests:  36%|███▋      | 2984/8192 [00:05<00:10, 505.80it/s]
Adding requests:  37%|███▋      | 3035/8192 [00:06<00:10, 504.57it/s]
Adding requests:  38%|███▊      | 3086/8192 [00:06<00:10, 503.07it/s]
Adding requests:  38%|███▊      | 3137/8192 [00:06<00:10, 504.39it/s]
Adding requests:  39%|███▉      | 3188/8192 [00:06<00:10, 494.59it/s]
Adding requests:  40%|███▉      | 3240/8192 [00:06<00:09, 500.62it/s]
Adding requests:  40%|████      | 3292/8192 [00:06<00:09, 504.62it/s]
Adding requests:  41%|████      | 3344/8192 [00:06<00:09, 506.69it/s]
Adding requests:  41%|████▏     | 3395/8192 [00:06<00:09, 507.56it/s]
Adding requests:  42%|████▏     | 3446/8192 [00:06<00:09, 506.79it/s]
Adding requests:  43%|████▎     | 3497/8192 [00:06<00:09, 503.95it/s]
Adding requests:  43%|████▎     | 3548/8192 [00:07<00:09, 505.01it/s]
Adding requests:  44%|████▍     | 3599/8192 [00:07<00:09, 504.37it/s]
Adding requests:  45%|████▍     | 3650/8192 [00:07<00:09, 501.07it/s]
Adding requests:  45%|████▌     | 3702/8192 [00:07<00:08, 504.26it/s]
Adding requests:  46%|████▌     | 3753/8192 [00:07<00:08, 502.23it/s]
Adding requests:  46%|████▋     | 3806/8192 [00:07<00:08, 509.16it/s]
Adding requests:  47%|████▋     | 3859/8192 [00:07<00:08, 515.28it/s]
Adding requests:  48%|████▊     | 3911/8192 [00:07<00:08, 509.85it/s]
Adding requests:  48%|████▊     | 3963/8192 [00:07<00:08, 512.42it/s]
Adding requests:  49%|████▉     | 4015/8192 [00:07<00:08, 509.65it/s]
Adding requests:  50%|████▉     | 4066/8192 [00:08<00:08, 506.46it/s]
Adding requests:  50%|█████     | 4118/8192 [00:08<00:08, 509.09it/s]
Adding requests:  51%|█████     | 4171/8192 [00:08<00:07, 513.10it/s]
Adding requests:  52%|█████▏    | 4223/8192 [00:08<00:07, 514.91it/s]
Adding requests:  52%|█████▏    | 4275/8192 [00:08<00:07, 514.97it/s]
Adding requests:  53%|█████▎    | 4327/8192 [00:08<00:07, 515.23it/s]
Adding requests:  53%|█████▎    | 4381/8192 [00:08<00:07, 520.46it/s]
Adding requests:  54%|█████▍    | 4434/8192 [00:08<00:07, 517.09it/s]
Adding requests:  55%|█████▍    | 4486/8192 [00:08<00:07, 514.29it/s]
Adding requests:  55%|█████▌    | 4538/8192 [00:08<00:07, 500.97it/s]
Adding requests:  56%|█████▌    | 4589/8192 [00:09<00:07, 501.71it/s]
Adding requests:  57%|█████▋    | 4643/8192 [00:09<00:06, 511.16it/s]
Adding requests:  57%|█████▋    | 4695/8192 [00:09<00:06, 506.88it/s]
Adding requests:  58%|█████▊    | 4748/8192 [00:09<00:06, 512.02it/s]
Adding requests:  59%|█████▊    | 4800/8192 [00:09<00:06, 509.57it/s]
Adding requests:  59%|█████▉    | 4852/8192 [00:09<00:06, 511.61it/s]
Adding requests:  60%|█████▉    | 4904/8192 [00:09<00:06, 507.49it/s]
Adding requests:  61%|██████    | 4957/8192 [00:09<00:06, 512.17it/s]
Adding requests:  61%|██████    | 5009/8192 [00:09<00:06, 512.56it/s]
Adding requests:  62%|██████▏   | 5061/8192 [00:09<00:06, 513.99it/s]
Adding requests:  62%|██████▏   | 5114/8192 [00:10<00:05, 516.81it/s]
Adding requests:  63%|██████▎   | 5166/8192 [00:10<00:05, 514.84it/s]
Adding requests:  64%|██████▎   | 5218/8192 [00:10<00:05, 512.95it/s]
Adding requests:  64%|██████▍   | 5270/8192 [00:10<00:05, 507.53it/s]
Adding requests:  65%|██████▍   | 5322/8192 [00:10<00:05, 509.14it/s]
Adding requests:  66%|██████▌   | 5373/8192 [00:10<00:05, 508.64it/s]
Adding requests:  66%|██████▌   | 5425/8192 [00:10<00:05, 509.55it/s]
Adding requests:  67%|██████▋   | 5476/8192 [00:10<00:05, 504.44it/s]
Adding requests:  67%|██████▋   | 5527/8192 [00:10<00:05, 501.02it/s]
Adding requests:  68%|██████▊   | 5578/8192 [00:11<00:05, 501.27it/s]
Adding requests:  69%|██████▊   | 5629/8192 [00:11<00:05, 498.66it/s]
Adding requests:  69%|██████▉   | 5679/8192 [00:11<00:05, 498.14it/s]
Adding requests:  70%|██████▉   | 5731/8192 [00:11<00:04, 503.95it/s]
Adding requests:  71%|███████   | 5783/8192 [00:11<00:04, 507.72it/s]
Adding requests:  71%|███████   | 5834/8192 [00:11<00:04, 505.16it/s]
Adding requests:  72%|███████▏  | 5885/8192 [00:11<00:04, 500.73it/s]
Adding requests:  72%|███████▏  | 5937/8192 [00:11<00:04, 504.87it/s]
Adding requests:  73%|███████▎  | 5990/8192 [00:11<00:04, 510.89it/s]
Adding requests:  74%|███████▍  | 6043/8192 [00:11<00:04, 514.82it/s]
Adding requests:  74%|███████▍  | 6095/8192 [00:12<00:04, 514.32it/s]
Adding requests:  75%|███████▌  | 6148/8192 [00:12<00:03, 516.66it/s]
Adding requests:  76%|███████▌  | 6201/8192 [00:12<00:03, 520.05it/s]
Adding requests:  76%|███████▋  | 6255/8192 [00:12<00:03, 525.76it/s]
Adding requests:  77%|███████▋  | 6309/8192 [00:12<00:03, 527.78it/s]
Adding requests:  78%|███████▊  | 6363/8192 [00:12<00:03, 530.03it/s]
Adding requests:  78%|███████▊  | 6417/8192 [00:12<00:03, 529.41it/s]
Adding requests:  79%|███████▉  | 6471/8192 [00:12<00:03, 530.88it/s]
Adding requests:  80%|███████▉  | 6526/8192 [00:12<00:03, 534.86it/s]
Adding requests:  80%|████████  | 6580/8192 [00:12<00:03, 530.24it/s]
Adding requests:  81%|████████  | 6634/8192 [00:13<00:02, 526.06it/s]
Adding requests:  82%|████████▏ | 6687/8192 [00:13<00:02, 525.04it/s]
Adding requests:  82%|████████▏ | 6740/8192 [00:13<00:02, 525.41it/s]
Adding requests:  83%|████████▎ | 6794/8192 [00:13<00:02, 528.17it/s]
Adding requests:  84%|████████▎ | 6847/8192 [00:13<00:02, 525.75it/s]
Adding requests:  84%|████████▍ | 6901/8192 [00:13<00:02, 529.11it/s]
Adding requests:  85%|████████▍ | 6956/8192 [00:13<00:02, 532.85it/s]
Adding requests:  86%|████████▌ | 7010/8192 [00:13<00:02, 525.73it/s]
Adding requests:  86%|████████▌ | 7063/8192 [00:13<00:02, 523.91it/s]
Adding requests:  87%|████████▋ | 7116/8192 [00:13<00:02, 524.76it/s]
Adding requests:  88%|████████▊ | 7169/8192 [00:14<00:01, 520.31it/s]
Adding requests:  88%|████████▊ | 7222/8192 [00:14<00:01, 520.59it/s]
Adding requests:  89%|████████▉ | 7275/8192 [00:14<00:01, 508.14it/s]
Adding requests:  89%|████████▉ | 7329/8192 [00:14<00:01, 515.99it/s]
Adding requests:  90%|█████████ | 7381/8192 [00:14<00:01, 515.26it/s]
Adding requests:  91%|█████████ | 7437/8192 [00:14<00:01, 526.34it/s]
Adding requests:  91%|█████████▏| 7490/8192 [00:14<00:01, 525.33it/s]
Adding requests:  92%|█████████▏| 7543/8192 [00:14<00:01, 525.77it/s]
Adding requests:  93%|█████████▎| 7596/8192 [00:14<00:01, 521.68it/s]
Adding requests:  93%|█████████▎| 7650/8192 [00:14<00:01, 524.98it/s]
Adding requests:  94%|█████████▍| 7703/8192 [00:15<00:00, 526.17it/s]
Adding requests:  95%|█████████▍| 7756/8192 [00:15<00:00, 522.16it/s]
Adding requests:  95%|█████████▌| 7809/8192 [00:15<00:00, 516.91it/s]
Adding requests:  96%|█████████▌| 7863/8192 [00:15<00:00, 522.65it/s]
Adding requests:  97%|█████████▋| 7916/8192 [00:15<00:00, 517.64it/s]
Adding requests:  97%|█████████▋| 7968/8192 [00:15<00:00, 516.75it/s]
Adding requests:  98%|█████████▊| 8020/8192 [00:15<00:00, 512.57it/s]
Adding requests:  99%|█████████▊| 8073/8192 [00:15<00:00, 516.14it/s]
Adding requests:  99%|█████████▉| 8126/8192 [00:15<00:00, 520.04it/s]
Adding requests: 100%|█████████▉| 8179/8192 [00:16<00:00, 519.20it/s]
Adding requests: 100%|██████████| 8192/8192 [00:16<00:00, 510.92it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▎        | 1118/8192 [00:00<00:01, 3831.69it/s, est. speed input: 3923972.86 toks/s, output: 3831.79 toks/s]
Processed prompts:  18%|█▊        | 1502/8192 [00:05<00:31, 215.27it/s, est. speed input: 279308.55 toks/s, output: 272.76 toks/s]   
Processed prompts:  20%|██        | 1665/8192 [00:07<00:36, 176.97it/s, est. speed input: 235203.03 toks/s, output: 229.69 toks/s]
Processed prompts:  21%|██▏       | 1758/8192 [00:08<00:46, 138.67it/s, est. speed input: 200250.03 toks/s, output: 195.56 toks/s]
Processed prompts:  22%|██▏       | 1822/8192 [00:09<00:50, 127.19it/s, est. speed input: 189461.51 toks/s, output: 185.02 toks/s]
Processed prompts:  23%|██▎       | 1886/8192 [00:10<00:54, 116.11it/s, est. speed input: 180174.15 toks/s, output: 175.95 toks/s]
Processed prompts:  24%|██▍       | 1950/8192 [00:11<00:58, 106.72it/s, est. speed input: 172457.77 toks/s, output: 168.42 toks/s]
Processed prompts:  25%|██▍       | 2014/8192 [00:12<01:02, 98.56it/s, est. speed input: 165662.33 toks/s, output: 161.78 toks/s] 
Processed prompts:  25%|██▌       | 2078/8192 [00:13<01:06, 92.03it/s, est. speed input: 159747.37 toks/s, output: 156.00 toks/s]
Processed prompts:  26%|██▌       | 2142/8192 [00:14<01:09, 86.99it/s, est. speed input: 154553.51 toks/s, output: 150.93 toks/s]
Processed prompts:  27%|██▋       | 2206/8192 [00:15<01:11, 83.78it/s, est. speed input: 150175.21 toks/s, output: 146.66 toks/s]
Processed prompts:  28%|██▊       | 2270/8192 [00:15<01:12, 81.17it/s, est. speed input: 146188.69 toks/s, output: 142.76 toks/s]
Processed prompts:  28%|██▊       | 2334/8192 [00:16<01:13, 79.28it/s, est. speed input: 142615.63 toks/s, output: 139.27 toks/s]
Processed prompts:  29%|██▉       | 2398/8192 [00:17<01:14, 77.56it/s, est. speed input: 139285.35 toks/s, output: 136.02 toks/s]
Processed prompts:  30%|███       | 2462/8192 [00:18<01:14, 76.67it/s, est. speed input: 136360.26 toks/s, output: 133.16 toks/s]
Processed prompts:  31%|███       | 2526/8192 [00:19<01:14, 75.67it/s, est. speed input: 133596.51 toks/s, output: 130.47 toks/s]
Processed prompts:  32%|███▏      | 2590/8192 [00:20<01:14, 75.33it/s, est. speed input: 131163.00 toks/s, output: 128.09 toks/s]
Processed prompts:  32%|███▏      | 2654/8192 [00:21<01:14, 74.71it/s, est. speed input: 128839.45 toks/s, output: 125.82 toks/s]
Processed prompts:  33%|███▎      | 2718/8192 [00:21<01:13, 74.32it/s, est. speed input: 126708.23 toks/s, output: 123.74 toks/s]
Processed prompts:  34%|███▍      | 2782/8192 [00:22<01:13, 74.02it/s, est. speed input: 124735.15 toks/s, output: 121.81 toks/s]
Processed prompts:  35%|███▍      | 2846/8192 [00:23<01:12, 73.77it/s, est. speed input: 122899.22 toks/s, output: 120.02 toks/s]
Processed prompts:  36%|███▌      | 2910/8192 [00:24<01:11, 73.64it/s, est. speed input: 121201.62 toks/s, output: 118.36 toks/s]
Processed prompts:  36%|███▋      | 2974/8192 [00:25<01:10, 73.55it/s, est. speed input: 119621.16 toks/s, output: 116.82 toks/s]
Processed prompts:  37%|███▋      | 3038/8192 [00:26<01:10, 73.44it/s, est. speed input: 118138.62 toks/s, output: 115.37 toks/s]
Processed prompts:  38%|███▊      | 3102/8192 [00:27<01:09, 73.70it/s, est. speed input: 116806.85 toks/s, output: 114.07 toks/s]
Processed prompts:  39%|███▊      | 3166/8192 [00:28<01:08, 73.56it/s, est. speed input: 115505.65 toks/s, output: 112.80 toks/s]
Processed prompts:  39%|███▉      | 3230/8192 [00:28<01:07, 73.49it/s, est. speed input: 114286.35 toks/s, output: 111.61 toks/s]
Processed prompts:  40%|████      | 3294/8192 [00:29<01:06, 73.33it/s, est. speed input: 113121.57 toks/s, output: 110.47 toks/s]
Processed prompts:  41%|████      | 3358/8192 [00:30<01:05, 73.25it/s, est. speed input: 112028.62 toks/s, output: 109.40 toks/s]
Processed prompts:  42%|████▏     | 3422/8192 [00:31<01:05, 73.24it/s, est. speed input: 111002.02 toks/s, output: 108.40 toks/s]
Processed prompts:  43%|████▎     | 3486/8192 [00:32<01:04, 73.21it/s, est. speed input: 110028.60 toks/s, output: 107.45 toks/s]
Processed prompts:  43%|████▎     | 3550/8192 [00:33<01:03, 73.22it/s, est. speed input: 109109.70 toks/s, output: 106.55 toks/s]
Processed prompts:  44%|████▍     | 3614/8192 [00:34<01:02, 73.15it/s, est. speed input: 108229.00 toks/s, output: 105.69 toks/s]
Processed prompts:  45%|████▍     | 3678/8192 [00:35<01:01, 73.17it/s, est. speed input: 107400.15 toks/s, output: 104.88 toks/s]
Processed prompts:  46%|████▌     | 3742/8192 [00:35<01:00, 73.41it/s, est. speed input: 106638.19 toks/s, output: 104.14 toks/s]
Processed prompts:  46%|████▋     | 3806/8192 [00:36<00:59, 73.33it/s, est. speed input: 105883.35 toks/s, output: 103.40 toks/s]
Processed prompts:  47%|████▋     | 3870/8192 [00:37<00:59, 73.23it/s, est. speed input: 105159.00 toks/s, output: 102.69 toks/s]
Processed prompts:  48%|████▊     | 3934/8192 [00:38<00:58, 73.21it/s, est. speed input: 104473.58 toks/s, output: 102.02 toks/s]
Processed prompts:  49%|████▉     | 3998/8192 [00:39<00:57, 73.46it/s, est. speed input: 103845.25 toks/s, output: 101.41 toks/s]
Processed prompts:  50%|████▉     | 4062/8192 [00:40<00:56, 73.36it/s, est. speed input: 103215.74 toks/s, output: 100.80 toks/s]
Processed prompts:  50%|█████     | 4126/8192 [00:41<00:55, 73.26it/s, est. speed input: 102610.66 toks/s, output: 100.21 toks/s]
Processed prompts:  51%|█████     | 4190/8192 [00:42<00:54, 73.53it/s, est. speed input: 102063.23 toks/s, output: 99.67 toks/s] 
Processed prompts:  52%|█████▏    | 4254/8192 [00:42<00:53, 73.65it/s, est. speed input: 101531.98 toks/s, output: 99.15 toks/s]
Processed prompts:  53%|█████▎    | 4318/8192 [00:43<00:52, 73.79it/s, est. speed input: 101025.66 toks/s, output: 98.66 toks/s]
Processed prompts:  53%|█████▎    | 4382/8192 [00:44<00:51, 73.54it/s, est. speed input: 100508.82 toks/s, output: 98.15 toks/s]
Processed prompts:  54%|█████▍    | 4446/8192 [00:45<00:51, 73.36it/s, est. speed input: 100011.49 toks/s, output: 97.67 toks/s]
Processed prompts:  55%|█████▌    | 4510/8192 [00:46<00:50, 73.24it/s, est. speed input: 99533.52 toks/s, output: 97.20 toks/s] 
Processed prompts:  56%|█████▌    | 4574/8192 [00:47<00:49, 73.12it/s, est. speed input: 99069.33 toks/s, output: 96.75 toks/s]
Processed prompts:  57%|█████▋    | 4638/8192 [00:48<00:48, 73.04it/s, est. speed input: 98623.41 toks/s, output: 96.31 toks/s]
Processed prompts:  57%|█████▋    | 4702/8192 [00:49<00:47, 73.05it/s, est. speed input: 98198.35 toks/s, output: 95.90 toks/s]
Processed prompts:  58%|█████▊    | 4766/8192 [00:49<00:46, 73.31it/s, est. speed input: 97807.71 toks/s, output: 95.52 toks/s]
Processed prompts:  59%|█████▉    | 4830/8192 [00:50<00:45, 73.47it/s, est. speed input: 97428.94 toks/s, output: 95.15 toks/s]
Processed prompts:  60%|█████▉    | 4894/8192 [00:51<00:45, 73.26it/s, est. speed input: 97038.98 toks/s, output: 94.76 toks/s]
Processed prompts:  61%|██████    | 4958/8192 [00:52<00:43, 73.56it/s, est. speed input: 96694.33 toks/s, output: 94.43 toks/s]
Processed prompts:  61%|██████▏   | 5022/8192 [00:53<00:43, 73.29it/s, est. speed input: 96326.58 toks/s, output: 94.07 toks/s]
Processed prompts:  62%|██████▏   | 5086/8192 [00:54<00:42, 73.13it/s, est. speed input: 95973.20 toks/s, output: 93.72 toks/s]
Processed prompts:  63%|██████▎   | 5150/8192 [00:55<00:41, 73.11it/s, est. speed input: 95636.88 toks/s, output: 93.40 toks/s]
Processed prompts:  64%|██████▎   | 5214/8192 [00:56<00:40, 72.99it/s, est. speed input: 95304.01 toks/s, output: 93.07 toks/s]
Processed prompts:  64%|██████▍   | 5278/8192 [00:56<00:39, 72.93it/s, est. speed input: 94983.16 toks/s, output: 92.76 toks/s]
Processed prompts:  65%|██████▌   | 5342/8192 [00:57<00:39, 72.90it/s, est. speed input: 94672.70 toks/s, output: 92.45 toks/s]
Processed prompts:  66%|██████▌   | 5406/8192 [00:58<00:38, 72.87it/s, est. speed input: 94371.14 toks/s, output: 92.16 toks/s]
Processed prompts:  67%|██████▋   | 5470/8192 [00:59<00:37, 72.83it/s, est. speed input: 94077.09 toks/s, output: 91.87 toks/s]
Processed prompts:  68%|██████▊   | 5534/8192 [01:00<00:36, 73.10it/s, est. speed input: 93810.43 toks/s, output: 91.61 toks/s]
Processed prompts:  68%|██████▊   | 5598/8192 [01:01<00:35, 72.99it/s, est. speed input: 93533.04 toks/s, output: 91.34 toks/s]
Processed prompts:  69%|██████▉   | 5662/8192 [01:02<00:34, 72.92it/s, est. speed input: 93263.87 toks/s, output: 91.08 toks/s]
Processed prompts:  70%|██████▉   | 5726/8192 [01:03<00:33, 72.88it/s, est. speed input: 93002.73 toks/s, output: 90.82 toks/s]
Processed prompts:  71%|███████   | 5790/8192 [01:03<00:32, 72.84it/s, est. speed input: 92747.96 toks/s, output: 90.57 toks/s]
Processed prompts:  71%|███████▏  | 5854/8192 [01:04<00:32, 72.80it/s, est. speed input: 92499.37 toks/s, output: 90.33 toks/s]
Processed prompts:  72%|███████▏  | 5918/8192 [01:05<00:31, 72.76it/s, est. speed input: 92257.08 toks/s, output: 90.09 toks/s]
Processed prompts:  73%|███████▎  | 5982/8192 [01:06<00:30, 72.75it/s, est. speed input: 92021.79 toks/s, output: 89.86 toks/s]
Processed prompts:  74%|███████▍  | 6046/8192 [01:07<00:29, 72.74it/s, est. speed input: 91792.42 toks/s, output: 89.64 toks/s]
Processed prompts:  75%|███████▍  | 6110/8192 [01:08<00:28, 72.71it/s, est. speed input: 91568.18 toks/s, output: 89.42 toks/s]
Processed prompts:  75%|███████▌  | 6174/8192 [01:09<00:27, 72.72it/s, est. speed input: 91350.79 toks/s, output: 89.21 toks/s]
Processed prompts:  76%|███████▌  | 6238/8192 [01:10<00:26, 72.72it/s, est. speed input: 91138.83 toks/s, output: 89.00 toks/s]
Processed prompts:  77%|███████▋  | 6302/8192 [01:10<00:25, 72.69it/s, est. speed input: 90930.76 toks/s, output: 88.80 toks/s]
Processed prompts:  78%|███████▊  | 6366/8192 [01:11<00:25, 72.69it/s, est. speed input: 90728.51 toks/s, output: 88.60 toks/s]
Processed prompts:  78%|███████▊  | 6430/8192 [01:12<00:24, 72.67it/s, est. speed input: 90530.07 toks/s, output: 88.41 toks/s]
Processed prompts:  79%|███████▉  | 6494/8192 [01:13<00:23, 72.64it/s, est. speed input: 90336.04 toks/s, output: 88.22 toks/s]
Processed prompts:  80%|████████  | 6558/8192 [01:14<00:22, 72.98it/s, est. speed input: 90163.82 toks/s, output: 88.05 toks/s]
Processed prompts:  81%|████████  | 6622/8192 [01:15<00:21, 73.19it/s, est. speed input: 89994.43 toks/s, output: 87.89 toks/s]
Processed prompts:  82%|████████▏ | 6686/8192 [01:16<00:20, 72.99it/s, est. speed input: 89812.48 toks/s, output: 87.71 toks/s]
Processed prompts:  82%|████████▏ | 6750/8192 [01:17<00:19, 72.90it/s, est. speed input: 89636.83 toks/s, output: 87.54 toks/s]
Processed prompts:  83%|████████▎ | 6814/8192 [01:17<00:18, 72.82it/s, est. speed input: 89464.54 toks/s, output: 87.37 toks/s]
Processed prompts:  84%|████████▍ | 6878/8192 [01:18<00:18, 72.75it/s, est. speed input: 89295.38 toks/s, output: 87.20 toks/s]
Processed prompts:  85%|████████▍ | 6942/8192 [01:19<00:17, 72.70it/s, est. speed input: 89129.88 toks/s, output: 87.04 toks/s]
Processed prompts:  86%|████████▌ | 7006/8192 [01:20<00:16, 72.68it/s, est. speed input: 88968.51 toks/s, output: 86.88 toks/s]
Processed prompts:  86%|████████▋ | 7070/8192 [01:21<00:15, 72.65it/s, est. speed input: 88810.27 toks/s, output: 86.73 toks/s]
Processed prompts:  87%|████████▋ | 7134/8192 [01:22<00:14, 72.95it/s, est. speed input: 88668.98 toks/s, output: 86.59 toks/s]
Processed prompts:  88%|████████▊ | 7198/8192 [01:23<00:13, 72.81it/s, est. speed input: 88515.92 toks/s, output: 86.44 toks/s]
Processed prompts:  89%|████████▊ | 7262/8192 [01:24<00:12, 73.06it/s, est. speed input: 88380.62 toks/s, output: 86.31 toks/s]
Processed prompts:  89%|████████▉ | 7326/8192 [01:25<00:11, 72.95it/s, est. speed input: 88236.28 toks/s, output: 86.17 toks/s]
Processed prompts:  90%|█████████ | 7390/8192 [01:25<00:10, 73.04it/s, est. speed input: 88101.58 toks/s, output: 86.04 toks/s]
Processed prompts:  91%|█████████ | 7454/8192 [01:26<00:10, 72.92it/s, est. speed input: 87962.45 toks/s, output: 85.90 toks/s]
Processed prompts:  92%|█████████▏| 7518/8192 [01:27<00:09, 72.84it/s, est. speed input: 87825.90 toks/s, output: 85.77 toks/s]
Processed prompts:  93%|█████████▎| 7582/8192 [01:28<00:08, 72.84it/s, est. speed input: 87694.52 toks/s, output: 85.64 toks/s]
Processed prompts:  93%|█████████▎| 7646/8192 [01:29<00:07, 72.73it/s, est. speed input: 87561.51 toks/s, output: 85.51 toks/s]
Processed prompts:  94%|█████████▍| 7710/8192 [01:30<00:06, 72.65it/s, est. speed input: 87430.97 toks/s, output: 85.38 toks/s]
Processed prompts:  95%|█████████▍| 7774/8192 [01:31<00:05, 72.67it/s, est. speed input: 87305.70 toks/s, output: 85.26 toks/s]
Processed prompts:  96%|█████████▌| 7838/8192 [01:32<00:04, 72.58it/s, est. speed input: 87178.98 toks/s, output: 85.14 toks/s]
Processed prompts:  96%|█████████▋| 7902/8192 [01:32<00:03, 72.56it/s, est. speed input: 87056.39 toks/s, output: 85.02 toks/s]
Processed prompts:  97%|█████████▋| 7966/8192 [01:33<00:03, 72.58it/s, est. speed input: 86937.10 toks/s, output: 84.90 toks/s]
Processed prompts:  98%|█████████▊| 8030/8192 [01:34<00:02, 72.57it/s, est. speed input: 86819.28 toks/s, output: 84.78 toks/s]
Processed prompts:  99%|█████████▉| 8094/8192 [01:35<00:01, 72.99it/s, est. speed input: 86719.26 toks/s, output: 84.69 toks/s]
Processed prompts: 100%|█████████▉| 8158/8192 [01:36<00:00, 84.80it/s, est. speed input: 86977.49 toks/s, output: 84.94 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:36<00:00, 84.80it/s, est. speed input: 87339.45 toks/s, output: 85.29 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:36<00:00, 85.29it/s, est. speed input: 87339.45 toks/s, output: 85.29 toks/s]
[rank0]:[W125 15:58:10.111310054 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 188.3s

测试结果:
  Requests/s:   73.09
  Tokens/s:     74915.41
  Total Reqs:   8192
  Elapsed:      112.08s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     74842.32


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/Llama3.2-3B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,29.6201,15195.0955,4.3214
1024,1024,1,128,128,31.3317,32114.9524,4.0853
2048,1024,2,256,128,60.1190,61621.9682,4.2582
4096,1024,4,512,128,62.5415,64105.0102,8.1866
8192,1024,8,1024,128,69.4418,71177.7969,14.7462
16384,1024,16,2048,128,71.6629,73454.5054,28.5782
32768,1024,32,4096,128,73.0802,74907.2134,56.0480
65536,1024,64,8192,128,73.0882,74915.4089,112.0838

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败

============================================================
  Llama3.2-3B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:58:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=157791) [INFO] Compress extension not found, building...
(EngineCore_DP0 pid=157791) ============================================================
(EngineCore_DP0 pid=157791) cuSPARSELt Compress Extension Builder
(EngineCore_DP0 pid=157791) ============================================================
(EngineCore_DP0 pid=157791) Extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64
(EngineCore_DP0 pid=157791) Source: cusparselt_compress.cu
(EngineCore_DP0 pid=157791) Build dir: /root/vllmbench/slidesparse/weight_convert/build
(EngineCore_DP0 pid=157791) ------------------------------------------------------------
(EngineCore_DP0 pid=157791) GPU: H100 (NVIDIA H100 PCIe)
(EngineCore_DP0 pid=157791) CC: cc90 (sm_90)
(EngineCore_DP0 pid=157791) Python: py312
(EngineCore_DP0 pid=157791) CUDA: cu129
(EngineCore_DP0 pid=157791) Arch: x86_64
(EngineCore_DP0 pid=157791) ============================================================
(EngineCore_DP0 pid=157791) 🔨 Building cusparselt_compress_H100_cc90_py312_cu129_x86_64...
(EngineCore_DP0 pid=157791) Command: /usr/local/cuda/bin/nvcc -std=c++17 -O3 -Xcompiler -fPIC --shared -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=sm_121 -I /usr/local/cuda/include /root/vllmbench/slidesparse/weight_convert/cusparselt_compress.cu -L/usr/lib/x86_64-linux-gnu -lcusparseLt -lcusparse -lcuda -o /root/vllmbench/slidesparse/weight_convert/build/cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=157791) ✓ Built: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=157791) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=157791) WARNING 01-25 15:58:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.86 requests/s, 17369.61 total tokens/s, 33.86 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-25 15:58:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:58:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:58:19] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:19] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:58:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:19] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:19] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:58:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:19] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:19] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:19] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:58:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:19] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:58:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:58:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:58:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:58:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:58:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:58:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:58:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:58:26] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:26] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:58:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:26] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:26] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:58:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:26] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:26] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:26] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:58:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:26] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:58:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:58:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:58:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:58:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:58:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:28] INFO gemm_wrapper.py:85: Auto-building cusparselt GEMM library from /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py...
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO gemm_wrapper.py:95: cusparselt GEMM library build completed
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=157791) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=157791) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=157791) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=157791) 
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=157791) [2026-01-25 15:59:00] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=157791) 2026-01-25 15:59:06,369 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=157791) 2026-01-25 15:59:06,418 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=157791) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=157791) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.84it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  42%|████▏     | 54/128 [00:00<00:00, 534.55it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 708.59it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.48it/s, est. speed input: 2295.89 toks/s, output: 4.48 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:07, 17.24it/s, est. speed input: 7538.72 toks/s, output: 14.72 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 24.52it/s, est. speed input: 10331.32 toks/s, output: 20.18 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 28.93it/s, est. speed input: 12047.12 toks/s, output: 23.53 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 31.75it/s, est. speed input: 13207.99 toks/s, output: 25.80 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.56it/s, est. speed input: 14038.98 toks/s, output: 27.42 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 34.82it/s, est. speed input: 14674.73 toks/s, output: 28.66 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 35.70it/s, est. speed input: 15176.52 toks/s, output: 29.64 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 36.29it/s, est. speed input: 15577.21 toks/s, output: 30.42 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 36.70it/s, est. speed input: 15906.63 toks/s, output: 31.07 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 37.01it/s, est. speed input: 16184.97 toks/s, output: 31.61 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 37.23it/s, est. speed input: 16422.84 toks/s, output: 32.07 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 37.33it/s, est. speed input: 16620.99 toks/s, output: 32.46 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 37.47it/s, est. speed input: 16799.81 toks/s, output: 32.81 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 37.58it/s, est. speed input: 16957.80 toks/s, output: 33.12 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 37.60it/s, est. speed input: 17092.30 toks/s, output: 33.38 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 37.63it/s, est. speed input: 17214.23 toks/s, output: 33.62 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 37.69it/s, est. speed input: 17325.31 toks/s, output: 33.84 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 37.67it/s, est. speed input: 17421.63 toks/s, output: 34.03 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 37.68it/s, est. speed input: 17509.97 toks/s, output: 34.20 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 37.74it/s, est. speed input: 17594.80 toks/s, output: 34.36 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 37.78it/s, est. speed input: 17671.30 toks/s, output: 34.51 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 37.84it/s, est. speed input: 17744.47 toks/s, output: 34.66 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 37.90it/s, est. speed input: 17812.33 toks/s, output: 34.79 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 37.91it/s, est. speed input: 17873.38 toks/s, output: 34.91 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 37.89it/s, est. speed input: 17928.64 toks/s, output: 35.02 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 37.87it/s, est. speed input: 17979.52 toks/s, output: 35.12 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 37.86it/s, est. speed input: 18026.93 toks/s, output: 35.21 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 37.84it/s, est. speed input: 18070.89 toks/s, output: 35.29 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 37.85it/s, est. speed input: 18113.14 toks/s, output: 35.38 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 37.83it/s, est. speed input: 18151.25 toks/s, output: 35.45 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 37.82it/s, est. speed input: 18187.46 toks/s, output: 35.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.82it/s, est. speed input: 18214.03 toks/s, output: 35.57 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.57it/s, est. speed input: 18214.03 toks/s, output: 35.57 toks/s]
[rank0]:[W125 15:59:13.519654195 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 63.0s

测试结果:
  Requests/s:   33.86
  Tokens/s:     17369.61
  Total Reqs:   128
  Elapsed:      3.78s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     17335.75

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:59:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=159648) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=159648) WARNING 01-25 15:59:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.21 requests/s, 36092.39 total tokens/s, 35.21 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-25 15:59:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:59:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:59:22] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:22] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:59:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:22] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:22] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:59:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:22] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:22] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:22] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:59:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:22] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:59:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:59:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:59:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:59:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:59:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:59:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:59:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:59:29] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:29] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:59:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:29] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:29] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:59:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:29] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:29] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:29] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:59:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:29] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:59:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:59:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:59:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:59:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:59:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=159648) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=159648) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.58it/s]
(EngineCore_DP0 pid=159648) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.58it/s]
(EngineCore_DP0 pid=159648) 
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:44] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=159648) 2026-01-25 15:59:50,133 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=159648) 2026-01-25 15:59:50,182 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=159648) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 17.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 17.62it/s]
(EngineCore_DP0 pid=159648) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.44it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 265.02it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 418.81it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 431.98it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 72.46it/s, est. speed input: 74210.81 toks/s, output: 72.46 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 47.23it/s, est. speed input: 51203.93 toks/s, output: 50.00 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 42.85it/s, est. speed input: 46949.10 toks/s, output: 45.85 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 40.92it/s, est. speed input: 45066.56 toks/s, output: 44.01 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 39.70it/s, est. speed input: 43848.99 toks/s, output: 42.82 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:02, 38.92it/s, est. speed input: 42999.67 toks/s, output: 41.99 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 38.48it/s, est. speed input: 42488.37 toks/s, output: 41.49 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 38.05it/s, est. speed input: 42034.71 toks/s, output: 41.05 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 37.70it/s, est. speed input: 41652.61 toks/s, output: 40.68 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 37.53it/s, est. speed input: 41356.93 toks/s, output: 40.39 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 37.40it/s, est. speed input: 41106.04 toks/s, output: 40.14 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 37.32it/s, est. speed input: 40892.28 toks/s, output: 39.93 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 37.23it/s, est. speed input: 40697.89 toks/s, output: 39.74 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 37.19it/s, est. speed input: 40532.78 toks/s, output: 39.58 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 37.17it/s, est. speed input: 40387.93 toks/s, output: 39.44 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 37.17it/s, est. speed input: 40262.13 toks/s, output: 39.32 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 37.13it/s, est. speed input: 40142.33 toks/s, output: 39.20 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 37.10it/s, est. speed input: 40033.42 toks/s, output: 39.09 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 37.11it/s, est. speed input: 39937.82 toks/s, output: 39.00 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 37.16it/s, est. speed input: 39860.09 toks/s, output: 38.93 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 37.22it/s, est. speed input: 39791.52 toks/s, output: 38.86 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 37.18it/s, est. speed input: 39717.93 toks/s, output: 38.79 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 37.20it/s, est. speed input: 39656.13 toks/s, output: 38.73 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 36.74it/s, est. speed input: 39533.87 toks/s, output: 38.61 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 36.75it/s, est. speed input: 39465.69 toks/s, output: 38.54 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 36.76it/s, est. speed input: 39401.19 toks/s, output: 38.48 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 36.81it/s, est. speed input: 39346.70 toks/s, output: 38.42 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 36.87it/s, est. speed input: 39299.47 toks/s, output: 38.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.87it/s, est. speed input: 39277.76 toks/s, output: 38.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.35it/s, est. speed input: 39277.76 toks/s, output: 38.36 toks/s]
[rank0]:[W125 15:59:55.977038661 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.3s

测试结果:
  Requests/s:   35.21
  Tokens/s:     36092.39
  Total Reqs:   128
  Elapsed:      3.64s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     36057.18

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:00:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=160820) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=160820) WARNING 01-25 16:00:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 65.90 requests/s, 67545.63 total tokens/s, 65.90 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-25 16:00:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:00:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:05] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:05] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:00:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:05] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:05] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:00:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:05] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:05] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:05] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:00:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:05] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:00:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:00:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:00:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:00:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:00:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:00:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:12] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:12] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:00:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:12] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:12] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:00:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:12] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:12] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:12] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:00:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:12] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:00:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:00:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:00:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:00:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=160820) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=160820) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=160820) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=160820) 
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:26] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=160820) 2026-01-25 16:00:31,347 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=160820) 2026-01-25 16:00:31,379 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=160820) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.78it/s]
(EngineCore_DP0 pid=160820) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  8.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.04it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  10%|▉         | 25/256 [00:00<00:00, 244.63it/s]
Adding requests:  29%|██▊       | 73/256 [00:00<00:00, 379.51it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 437.64it/s]
Adding requests:  68%|██████▊   | 174/256 [00:00<00:00, 459.91it/s]
Adding requests:  87%|████████▋ | 222/256 [00:00<00:00, 466.58it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 449.02it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:00<00:01, 221.24it/s, est. speed input: 226589.56 toks/s, output: 221.25 toks/s]
Processed prompts:  18%|█▊        | 47/256 [00:00<00:01, 106.34it/s, est. speed input: 118310.20 toks/s, output: 115.53 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:00<00:02, 92.60it/s, est. speed input: 104659.97 toks/s, output: 102.20 toks/s] 
Processed prompts:  28%|██▊       | 72/256 [00:00<00:02, 84.16it/s, est. speed input: 97102.28 toks/s, output: 94.82 toks/s]  
Processed prompts:  32%|███▏      | 82/256 [00:00<00:02, 81.14it/s, est. speed input: 93819.97 toks/s, output: 91.62 toks/s]
Processed prompts:  36%|███▌      | 91/256 [00:01<00:02, 81.58it/s, est. speed input: 92852.18 toks/s, output: 90.68 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:01<00:02, 76.94it/s, est. speed input: 89901.10 toks/s, output: 87.79 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:01<00:01, 75.82it/s, est. speed input: 88536.94 toks/s, output: 86.46 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:01<00:01, 74.65it/s, est. speed input: 87288.37 toks/s, output: 85.24 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:01<00:01, 73.78it/s, est. speed input: 86224.09 toks/s, output: 84.20 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:01<00:01, 73.19it/s, est. speed input: 85323.21 toks/s, output: 83.32 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:01<00:01, 72.78it/s, est. speed input: 84547.94 toks/s, output: 82.56 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:01<00:01, 72.46it/s, est. speed input: 83859.49 toks/s, output: 81.89 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:01<00:01, 72.21it/s, est. speed input: 83245.67 toks/s, output: 81.29 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:02<00:01, 72.06it/s, est. speed input: 82706.24 toks/s, output: 80.77 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:02<00:01, 72.02it/s, est. speed input: 82235.32 toks/s, output: 80.31 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 71.94it/s, est. speed input: 81802.93 toks/s, output: 79.88 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 71.89it/s, est. speed input: 81410.66 toks/s, output: 79.50 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 71.82it/s, est. speed input: 81047.96 toks/s, output: 79.15 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 71.77it/s, est. speed input: 80717.28 toks/s, output: 78.82 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 71.74it/s, est. speed input: 80414.15 toks/s, output: 78.53 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 71.68it/s, est. speed input: 80129.83 toks/s, output: 78.25 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 71.74it/s, est. speed input: 79880.45 toks/s, output: 78.01 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:03<00:00, 71.72it/s, est. speed input: 79642.40 toks/s, output: 77.77 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:03<00:00, 71.75it/s, est. speed input: 79425.05 toks/s, output: 77.56 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:03<00:00, 71.65it/s, est. speed input: 79209.66 toks/s, output: 77.35 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 71.65it/s, est. speed input: 79127.52 toks/s, output: 77.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 77.27it/s, est. speed input: 79127.52 toks/s, output: 77.27 toks/s]
[rank0]:[W125 16:00:37.430297344 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   65.90
  Tokens/s:     67545.63
  Total Reqs:   256
  Elapsed:      3.88s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     67479.74

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:00:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=161973) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=161973) WARNING 01-25 16:01:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 80.62 requests/s, 82633.65 total tokens/s, 80.62 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-25 16:00:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:00:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:48] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:48] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:00:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:48] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:48] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:00:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:48] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:48] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:48] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:00:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:48] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:00:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:00:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:00:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:00:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:00:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:00:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:54] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:54] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:00:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:54] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:54] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:00:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:54] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:54] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:54] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:00:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:54] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:00:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:00:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:00:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:00:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=161973) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=161973) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=161973) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=161973) 
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=161973) [2026-01-25 16:01:09] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=161973) 2026-01-25 16:01:15,326 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=161973) 2026-01-25 16:01:15,364 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=161973) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  6.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 12.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.00it/s]
(EngineCore_DP0 pid=161973) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 18.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 19.37it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 30/512 [00:00<00:01, 296.94it/s]
Adding requests:  16%|█▌        | 82/512 [00:00<00:01, 426.32it/s]
Adding requests:  26%|██▌       | 134/512 [00:00<00:00, 465.57it/s]
Adding requests:  36%|███▌      | 184/512 [00:00<00:00, 477.22it/s]
Adding requests:  46%|████▌     | 236/512 [00:00<00:00, 492.13it/s]
Adding requests:  56%|█████▌    | 287/512 [00:00<00:00, 496.63it/s]
Adding requests:  66%|██████▌   | 338/512 [00:00<00:00, 498.64it/s]
Adding requests:  76%|███████▋  | 391/512 [00:00<00:00, 506.65it/s]
Adding requests:  86%|████████▋ | 442/512 [00:00<00:00, 507.47it/s]
Adding requests:  96%|█████████▋| 494/512 [00:01<00:00, 509.32it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 489.66it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:00<00:00, 561.50it/s, est. speed input: 575066.19 toks/s, output: 561.53 toks/s]
Processed prompts:  26%|██▌       | 131/512 [00:00<00:02, 144.11it/s, est. speed input: 168838.58 toks/s, output: 164.88 toks/s]
Processed prompts:  31%|███▏      | 160/512 [00:01<00:02, 121.53it/s, est. speed input: 144825.85 toks/s, output: 141.43 toks/s]
Processed prompts:  35%|███▌      | 180/512 [00:01<00:02, 111.31it/s, est. speed input: 134874.04 toks/s, output: 131.71 toks/s]
Processed prompts:  38%|███▊      | 196/512 [00:01<00:03, 104.86it/s, est. speed input: 129034.86 toks/s, output: 126.01 toks/s]
Processed prompts:  41%|████      | 209/512 [00:01<00:02, 101.78it/s, est. speed input: 125922.77 toks/s, output: 122.97 toks/s]
Processed prompts:  43%|████▎     | 221/512 [00:01<00:02, 97.48it/s, est. speed input: 122728.58 toks/s, output: 119.85 toks/s] 
Processed prompts:  45%|████▌     | 232/512 [00:01<00:03, 92.17it/s, est. speed input: 119468.77 toks/s, output: 116.67 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:02<00:03, 86.50it/s, est. speed input: 116294.04 toks/s, output: 113.57 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:02<00:03, 85.99it/s, est. speed input: 114444.94 toks/s, output: 111.76 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:02<00:02, 85.69it/s, est. speed input: 112833.10 toks/s, output: 110.19 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:02<00:02, 85.39it/s, est. speed input: 111382.35 toks/s, output: 108.77 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:02<00:02, 84.93it/s, est. speed input: 110026.07 toks/s, output: 107.45 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:02<00:02, 84.34it/s, est. speed input: 108750.70 toks/s, output: 106.20 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:02<00:02, 83.72it/s, est. speed input: 107553.24 toks/s, output: 105.03 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:03<00:02, 83.90it/s, est. speed input: 106590.16 toks/s, output: 104.09 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:03<00:02, 84.10it/s, est. speed input: 105724.02 toks/s, output: 103.25 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:03<00:01, 85.57it/s, est. speed input: 105158.74 toks/s, output: 102.69 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:03<00:01, 85.38it/s, est. speed input: 104435.37 toks/s, output: 101.99 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:03<00:01, 84.98it/s, est. speed input: 103725.41 toks/s, output: 101.29 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:03<00:01, 84.44it/s, est. speed input: 103029.22 toks/s, output: 100.61 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:03<00:01, 84.30it/s, est. speed input: 102416.52 toks/s, output: 100.02 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:04<00:01, 84.36it/s, est. speed input: 101868.94 toks/s, output: 99.48 toks/s] 
Processed prompts:  82%|████████▏ | 422/512 [00:04<00:01, 83.99it/s, est. speed input: 101303.97 toks/s, output: 98.93 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:04<00:00, 84.22it/s, est. speed input: 100837.07 toks/s, output: 98.47 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:04<00:00, 84.34it/s, est. speed input: 100395.02 toks/s, output: 98.04 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:04<00:00, 85.80it/s, est. speed input: 100141.74 toks/s, output: 97.79 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:04<00:00, 84.76it/s, est. speed input: 99667.69 toks/s, output: 97.33 toks/s] 
Processed prompts:  94%|█████████▍| 482/512 [00:04<00:00, 84.25it/s, est. speed input: 99243.66 toks/s, output: 96.92 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:05<00:00, 83.85it/s, est. speed input: 98838.48 toks/s, output: 96.52 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:05<00:00, 83.34it/s, est. speed input: 98430.54 toks/s, output: 96.12 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 83.34it/s, est. speed input: 98860.40 toks/s, output: 96.54 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 96.54it/s, est. speed input: 98860.40 toks/s, output: 96.54 toks/s]
[rank0]:[W125 16:01:24.022361961 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.6s

测试结果:
  Requests/s:   80.62
  Tokens/s:     82633.65
  Total Reqs:   512
  Elapsed:      6.35s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     82553.03

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:01:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=163192) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=163192) WARNING 01-25 16:01:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 87.04 requests/s, 89217.44 total tokens/s, 87.04 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-25 16:01:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:01:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:01:36] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:36] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:01:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:36] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:36] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:01:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:36] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:36] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:36] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:01:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:36] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:01:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:01:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:01:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:01:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:01:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:01:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:01:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:01:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:44] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:01:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:44] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:01:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:44] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:01:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:01:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:01:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:01:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:01:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:01:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=163192) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=163192) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=163192) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=163192) 
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:58] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=163192) 2026-01-25 16:02:03,574 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=163192) 2026-01-25 16:02:03,598 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=163192) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:03,  1.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:01<00:01,  2.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:01<00:00,  3.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  5.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  3.80it/s]
(EngineCore_DP0 pid=163192) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 19.53it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 25/1024 [00:00<00:04, 246.32it/s]
Adding requests:   7%|▋         | 72/1024 [00:00<00:02, 374.68it/s]
Adding requests:  12%|█▏        | 120/1024 [00:00<00:02, 420.02it/s]
Adding requests:  16%|█▋        | 167/1024 [00:00<00:01, 437.09it/s]
Adding requests:  21%|██        | 214/1024 [00:00<00:01, 447.35it/s]
Adding requests:  26%|██▌       | 263/1024 [00:00<00:01, 459.95it/s]
Adding requests:  30%|███       | 311/1024 [00:00<00:01, 463.43it/s]
Adding requests:  35%|███▍      | 358/1024 [00:00<00:01, 464.95it/s]
Adding requests:  40%|███▉      | 405/1024 [00:00<00:01, 466.03it/s]
Adding requests:  44%|████▍     | 453/1024 [00:01<00:01, 467.87it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 468.32it/s]
Adding requests:  53%|█████▎    | 547/1024 [00:01<00:01, 459.97it/s]
Adding requests:  58%|█████▊    | 596/1024 [00:01<00:00, 467.72it/s]
Adding requests:  63%|██████▎   | 645/1024 [00:01<00:00, 473.07it/s]
Adding requests:  68%|██████▊   | 695/1024 [00:01<00:00, 479.00it/s]
Adding requests:  73%|███████▎  | 743/1024 [00:01<00:00, 478.43it/s]
Adding requests:  77%|███████▋  | 791/1024 [00:01<00:00, 478.50it/s]
Adding requests:  82%|████████▏ | 839/1024 [00:01<00:00, 466.44it/s]
Adding requests:  87%|████████▋ | 889/1024 [00:01<00:00, 474.79it/s]
Adding requests:  92%|█████████▏| 938/1024 [00:02<00:00, 477.48it/s]
Adding requests:  96%|█████████▋| 987/1024 [00:02<00:00, 480.51it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 463.34it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:00<00:00, 1263.75it/s, est. speed input: 1294285.79 toks/s, output: 1263.81 toks/s]
Processed prompts:  31%|███▏      | 321/1024 [00:01<00:03, 177.85it/s, est. speed input: 215725.18 toks/s, output: 210.67 toks/s]   
Processed prompts:  37%|███▋      | 379/1024 [00:02<00:04, 137.57it/s, est. speed input: 172633.05 toks/s, output: 168.59 toks/s]
Processed prompts:  40%|████      | 414/1024 [00:02<00:04, 128.02it/s, est. speed input: 162216.26 toks/s, output: 158.41 toks/s]
Processed prompts:  43%|████▎     | 439/1024 [00:02<00:04, 120.60it/s, est. speed input: 155582.15 toks/s, output: 151.94 toks/s]
Processed prompts:  45%|████▍     | 459/1024 [00:03<00:05, 110.87it/s, est. speed input: 148954.89 toks/s, output: 145.46 toks/s]
Processed prompts:  46%|████▋     | 475/1024 [00:03<00:05, 106.77it/s, est. speed input: 145688.84 toks/s, output: 142.27 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:03<00:05, 101.87it/s, est. speed input: 142494.78 toks/s, output: 139.15 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:03<00:05, 98.62it/s, est. speed input: 139860.04 toks/s, output: 136.58 toks/s] 
Processed prompts:  51%|█████     | 522/1024 [00:03<00:05, 95.81it/s, est. speed input: 137456.75 toks/s, output: 134.23 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:04<00:05, 93.54it/s, est. speed input: 135270.05 toks/s, output: 132.10 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:04<00:05, 91.85it/s, est. speed input: 133287.18 toks/s, output: 130.16 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:04<00:05, 90.65it/s, est. speed input: 131487.81 toks/s, output: 128.40 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:04<00:04, 89.58it/s, est. speed input: 129794.39 toks/s, output: 126.75 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:04<00:04, 88.85it/s, est. speed input: 128239.95 toks/s, output: 125.23 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:04<00:04, 88.35it/s, est. speed input: 126804.13 toks/s, output: 123.83 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:05<00:04, 88.05it/s, est. speed input: 125480.19 toks/s, output: 122.54 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:05<00:04, 87.89it/s, est. speed input: 124255.84 toks/s, output: 121.34 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:05<00:04, 87.69it/s, est. speed input: 123098.11 toks/s, output: 120.21 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:05<00:03, 87.53it/s, est. speed input: 122013.04 toks/s, output: 119.15 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:05<00:03, 87.28it/s, est. speed input: 120975.16 toks/s, output: 118.14 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:06<00:03, 87.06it/s, est. speed input: 119993.20 toks/s, output: 117.18 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:06<00:03, 87.22it/s, est. speed input: 119110.91 toks/s, output: 116.32 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:06<00:03, 87.27it/s, est. speed input: 118271.06 toks/s, output: 115.50 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:06<00:03, 87.16it/s, est. speed input: 117459.85 toks/s, output: 114.71 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:06<00:02, 87.11it/s, est. speed input: 116695.60 toks/s, output: 113.96 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:07<00:02, 87.04it/s, est. speed input: 115967.42 toks/s, output: 113.25 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:07<00:02, 86.97it/s, est. speed input: 115273.85 toks/s, output: 112.57 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:07<00:02, 87.14it/s, est. speed input: 114638.21 toks/s, output: 111.95 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:07<00:02, 87.01it/s, est. speed input: 114007.71 toks/s, output: 111.34 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:07<00:01, 87.20it/s, est. speed input: 113436.27 toks/s, output: 110.78 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:07<00:01, 87.11it/s, est. speed input: 112869.26 toks/s, output: 110.22 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:08<00:01, 87.11it/s, est. speed input: 112332.53 toks/s, output: 109.70 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:08<00:01, 87.12it/s, est. speed input: 111821.75 toks/s, output: 109.20 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:08<00:01, 87.05it/s, est. speed input: 111325.34 toks/s, output: 108.72 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:08<00:00, 88.51it/s, est. speed input: 110984.52 toks/s, output: 108.38 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:08<00:00, 88.15it/s, est. speed input: 110537.16 toks/s, output: 107.95 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:09<00:00, 87.74it/s, est. speed input: 110094.58 toks/s, output: 107.51 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:09<00:00, 89.06it/s, est. speed input: 109801.29 toks/s, output: 107.23 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:09<00:00, 88.50it/s, est. speed input: 109400.36 toks/s, output: 106.84 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:09<00:00, 89.65it/s, est. speed input: 109133.28 toks/s, output: 106.58 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 89.65it/s, est. speed input: 109772.13 toks/s, output: 107.20 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 107.20it/s, est. speed input: 109772.13 toks/s, output: 107.20 toks/s]
[rank0]:[W125 16:02:18.688068259 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.6s

测试结果:
  Requests/s:   87.04
  Tokens/s:     89217.44
  Total Reqs:   1024
  Elapsed:      11.76s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     89130.40

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:02:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=164555) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=164555) WARNING 01-25 16:02:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 89.46 requests/s, 91697.40 total tokens/s, 89.46 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-25 16:02:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:02:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:02:35] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:35] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:02:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:35] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:35] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:02:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:35] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:35] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:35] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:02:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:35] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:02:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:02:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:02:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:02:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:02:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:02:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:02:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:02:42] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:42] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:02:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:42] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:42] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:02:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:42] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:42] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:42] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:02:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:42] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:02:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:02:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:02:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:02:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:02:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=164555) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=164555) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=164555) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=164555) 
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:57] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=164555) 2026-01-25 16:03:01,892 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=164555) 2026-01-25 16:03:01,921 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=164555) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  6.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:01<00:00,  3.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  5.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.71it/s]
(EngineCore_DP0 pid=164555) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 17.70it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 11.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 12.77it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 33/2048 [00:00<00:06, 329.75it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 441.12it/s]
Adding requests:   7%|▋         | 136/2048 [00:00<00:04, 469.41it/s]
Adding requests:   9%|▉         | 185/2048 [00:00<00:03, 477.10it/s]
Adding requests:  12%|█▏        | 237/2048 [00:00<00:03, 490.37it/s]
Adding requests:  14%|█▍        | 288/2048 [00:00<00:03, 494.17it/s]
Adding requests:  17%|█▋        | 338/2048 [00:00<00:03, 495.96it/s]
Adding requests:  19%|█▉        | 390/2048 [00:00<00:03, 501.39it/s]
Adding requests:  22%|██▏       | 441/2048 [00:00<00:03, 502.24it/s]
Adding requests:  24%|██▍       | 492/2048 [00:01<00:03, 502.86it/s]
Adding requests:  27%|██▋       | 543/2048 [00:01<00:03, 495.18it/s]
Adding requests:  29%|██▉       | 596/2048 [00:01<00:02, 501.64it/s]
Adding requests:  32%|███▏      | 648/2048 [00:01<00:02, 506.79it/s]
Adding requests:  34%|███▍      | 702/2048 [00:01<00:02, 513.55it/s]
Adding requests:  37%|███▋      | 754/2048 [00:01<00:02, 504.61it/s]
Adding requests:  39%|███▉      | 805/2048 [00:01<00:02, 498.50it/s]
Adding requests:  42%|████▏     | 855/2048 [00:01<00:02, 495.18it/s]
Adding requests:  44%|████▍     | 908/2048 [00:01<00:02, 504.10it/s]
Adding requests:  47%|████▋     | 960/2048 [00:01<00:02, 507.31it/s]
Adding requests:  49%|████▉     | 1013/2048 [00:02<00:02, 512.21it/s]
Adding requests:  52%|█████▏    | 1065/2048 [00:02<00:01, 513.97it/s]
Adding requests:  55%|█████▍    | 1117/2048 [00:02<00:01, 506.86it/s]
Adding requests:  57%|█████▋    | 1170/2048 [00:02<00:01, 512.78it/s]
Adding requests:  60%|█████▉    | 1224/2048 [00:02<00:01, 519.03it/s]
Adding requests:  62%|██████▏   | 1276/2048 [00:02<00:01, 509.56it/s]
Adding requests:  65%|██████▍   | 1329/2048 [00:02<00:01, 513.81it/s]
Adding requests:  67%|██████▋   | 1382/2048 [00:02<00:01, 515.77it/s]
Adding requests:  70%|███████   | 1434/2048 [00:02<00:01, 516.66it/s]
Adding requests:  73%|███████▎  | 1487/2048 [00:02<00:01, 519.29it/s]
Adding requests:  75%|███████▌  | 1540/2048 [00:03<00:00, 520.87it/s]
Adding requests:  78%|███████▊  | 1594/2048 [00:03<00:00, 525.64it/s]
Adding requests:  80%|████████  | 1647/2048 [00:03<00:00, 525.95it/s]
Adding requests:  83%|████████▎ | 1700/2048 [00:03<00:00, 516.99it/s]
Adding requests:  86%|████████▌ | 1753/2048 [00:03<00:00, 517.99it/s]
Adding requests:  88%|████████▊ | 1805/2048 [00:03<00:00, 514.59it/s]
Adding requests:  91%|█████████ | 1858/2048 [00:03<00:00, 516.89it/s]
Adding requests:  93%|█████████▎| 1910/2048 [00:03<00:00, 502.06it/s]
Adding requests:  96%|█████████▌| 1962/2048 [00:03<00:00, 506.97it/s]
Adding requests:  98%|█████████▊| 2015/2048 [00:03<00:00, 511.21it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 506.04it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:00<00:00, 2524.61it/s, est. speed input: 2585644.48 toks/s, output: 2524.75 toks/s]
Processed prompts:  30%|██▉       | 607/2048 [00:02<00:07, 181.03it/s, est. speed input: 221320.85 toks/s, output: 216.13 toks/s]   
Processed prompts:  35%|███▌      | 717/2048 [00:04<00:09, 144.59it/s, est. speed input: 180964.19 toks/s, output: 176.72 toks/s]
Processed prompts:  38%|███▊      | 781/2048 [00:04<00:09, 131.26it/s, est. speed input: 167613.83 toks/s, output: 163.68 toks/s]
Processed prompts:  40%|████      | 823/2048 [00:05<00:10, 120.22it/s, est. speed input: 158711.22 toks/s, output: 154.99 toks/s]
Processed prompts:  42%|████▏     | 853/2048 [00:05<00:10, 114.31it/s, est. speed input: 154149.40 toks/s, output: 150.54 toks/s]
Processed prompts:  43%|████▎     | 876/2048 [00:05<00:10, 115.87it/s, est. speed input: 153486.13 toks/s, output: 149.89 toks/s]
Processed prompts:  44%|████▍     | 896/2048 [00:06<00:09, 115.42it/s, est. speed input: 152349.72 toks/s, output: 148.78 toks/s]
Processed prompts:  45%|████▍     | 913/2048 [00:06<00:10, 112.18it/s, est. speed input: 150754.19 toks/s, output: 147.22 toks/s]
Processed prompts:  45%|████▌     | 928/2048 [00:06<00:10, 106.97it/s, est. speed input: 148932.58 toks/s, output: 145.44 toks/s]
Processed prompts:  46%|████▌     | 941/2048 [00:06<00:10, 101.14it/s, est. speed input: 147147.98 toks/s, output: 143.70 toks/s]
Processed prompts:  47%|████▋     | 953/2048 [00:06<00:11, 93.27it/s, est. speed input: 145061.77 toks/s, output: 141.66 toks/s] 
Processed prompts:  47%|████▋     | 963/2048 [00:06<00:12, 84.02it/s, est. speed input: 142791.37 toks/s, output: 139.44 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:07<00:12, 85.30it/s, est. speed input: 141566.58 toks/s, output: 138.25 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:07<00:12, 86.30it/s, est. speed input: 140313.52 toks/s, output: 137.02 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:07<00:11, 87.15it/s, est. speed input: 139135.30 toks/s, output: 135.87 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:07<00:11, 87.79it/s, est. speed input: 138015.63 toks/s, output: 134.78 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:07<00:11, 88.40it/s, est. speed input: 136965.86 toks/s, output: 133.76 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:07<00:11, 88.72it/s, est. speed input: 135949.14 toks/s, output: 132.76 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:08<00:10, 88.70it/s, est. speed input: 134949.02 toks/s, output: 131.79 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:08<00:10, 88.74it/s, est. speed input: 133997.92 toks/s, output: 130.86 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:08<00:10, 88.91it/s, est. speed input: 133102.01 toks/s, output: 129.98 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:08<00:10, 89.06it/s, est. speed input: 132246.14 toks/s, output: 129.15 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:08<00:10, 89.19it/s, est. speed input: 131427.91 toks/s, output: 128.35 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:09<00:09, 90.64it/s, est. speed input: 130770.79 toks/s, output: 127.71 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:09<00:09, 90.08it/s, est. speed input: 129992.38 toks/s, output: 126.95 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:09<00:09, 89.95it/s, est. speed input: 129266.84 toks/s, output: 126.24 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:09<00:09, 89.80it/s, est. speed input: 128562.78 toks/s, output: 125.55 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:09<00:09, 89.85it/s, est. speed input: 127898.64 toks/s, output: 124.90 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:09<00:09, 89.64it/s, est. speed input: 127237.41 toks/s, output: 124.25 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:10<00:08, 89.40it/s, est. speed input: 126591.02 toks/s, output: 123.62 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:10<00:08, 89.27it/s, est. speed input: 125971.19 toks/s, output: 123.02 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:10<00:08, 89.32it/s, est. speed input: 125383.37 toks/s, output: 122.44 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:10<00:08, 89.36it/s, est. speed input: 124815.85 toks/s, output: 121.89 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:10<00:08, 89.23it/s, est. speed input: 124255.49 toks/s, output: 121.34 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:11<00:08, 89.07it/s, est. speed input: 123707.60 toks/s, output: 120.81 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:11<00:07, 89.02it/s, est. speed input: 123181.87 toks/s, output: 120.29 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:11<00:07, 89.02it/s, est. speed input: 122675.89 toks/s, output: 119.80 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:11<00:07, 89.16it/s, est. speed input: 122195.83 toks/s, output: 119.33 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:11<00:07, 89.21it/s, est. speed input: 121726.11 toks/s, output: 118.87 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:11<00:07, 89.11it/s, est. speed input: 121261.92 toks/s, output: 118.42 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:12<00:06, 89.20it/s, est. speed input: 120821.95 toks/s, output: 117.99 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:12<00:06, 89.25it/s, est. speed input: 120393.71 toks/s, output: 117.57 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:12<00:06, 89.36it/s, est. speed input: 119983.35 toks/s, output: 117.17 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:12<00:06, 89.47it/s, est. speed input: 119586.90 toks/s, output: 116.78 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:12<00:06, 89.54it/s, est. speed input: 119199.77 toks/s, output: 116.41 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:12<00:06, 89.50it/s, est. speed input: 118818.82 toks/s, output: 116.03 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:13<00:05, 89.55it/s, est. speed input: 118452.77 toks/s, output: 115.68 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:13<00:05, 89.34it/s, est. speed input: 118081.95 toks/s, output: 115.31 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:13<00:05, 89.37it/s, est. speed input: 117731.13 toks/s, output: 114.97 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:13<00:05, 89.36it/s, est. speed input: 117388.11 toks/s, output: 114.64 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:13<00:05, 90.75it/s, est. speed input: 117131.37 toks/s, output: 114.39 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:14<00:04, 90.39it/s, est. speed input: 116807.66 toks/s, output: 114.07 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:14<00:04, 90.04it/s, est. speed input: 116487.33 toks/s, output: 113.76 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:14<00:04, 89.90it/s, est. speed input: 116179.88 toks/s, output: 113.46 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:14<00:04, 89.90it/s, est. speed input: 115885.61 toks/s, output: 113.17 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:14<00:04, 89.71it/s, est. speed input: 115588.46 toks/s, output: 112.88 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:14<00:04, 89.59it/s, est. speed input: 115298.72 toks/s, output: 112.60 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:15<00:03, 89.45it/s, est. speed input: 115013.64 toks/s, output: 112.32 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:15<00:03, 89.39it/s, est. speed input: 114736.91 toks/s, output: 112.05 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:15<00:03, 89.34it/s, est. speed input: 114466.00 toks/s, output: 111.78 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:15<00:03, 89.29it/s, est. speed input: 114200.66 toks/s, output: 111.52 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:15<00:03, 89.37it/s, est. speed input: 113946.83 toks/s, output: 111.28 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:16<00:03, 89.27it/s, est. speed input: 113691.02 toks/s, output: 111.03 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:16<00:02, 89.20it/s, est. speed input: 113441.23 toks/s, output: 110.78 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:16<00:02, 89.21it/s, est. speed input: 113199.54 toks/s, output: 110.55 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:16<00:02, 89.17it/s, est. speed input: 112960.96 toks/s, output: 110.31 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:16<00:02, 89.30it/s, est. speed input: 112734.82 toks/s, output: 110.09 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:16<00:02, 89.19it/s, est. speed input: 112504.13 toks/s, output: 109.87 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:17<00:01, 90.77it/s, est. speed input: 112350.37 toks/s, output: 109.72 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:17<00:01, 90.43it/s, est. speed input: 112138.03 toks/s, output: 109.51 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:17<00:01, 90.25it/s, est. speed input: 111932.30 toks/s, output: 109.31 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:17<00:01, 89.95it/s, est. speed input: 111723.13 toks/s, output: 109.10 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:17<00:01, 89.81it/s, est. speed input: 111521.46 toks/s, output: 108.91 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:17<00:01, 91.15it/s, est. speed input: 111381.70 toks/s, output: 108.77 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:18<00:00, 90.77it/s, est. speed input: 111192.19 toks/s, output: 108.59 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:18<00:00, 90.55it/s, est. speed input: 111007.93 toks/s, output: 108.41 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:18<00:00, 90.25it/s, est. speed input: 110821.89 toks/s, output: 108.22 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:18<00:00, 90.04it/s, est. speed input: 110638.60 toks/s, output: 108.05 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:18<00:00, 92.02it/s, est. speed input: 110539.49 toks/s, output: 107.95 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 92.02it/s, est. speed input: 111296.28 toks/s, output: 108.69 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 108.69it/s, est. speed input: 111296.28 toks/s, output: 108.69 toks/s]
[rank0]:[W125 16:03:28.277395840 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.7s

测试结果:
  Requests/s:   89.46
  Tokens/s:     91697.40
  Total Reqs:   2048
  Elapsed:      22.89s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     91607.94

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:03:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=166168) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=166168) WARNING 01-25 16:04:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 90.41 requests/s, 92670.95 total tokens/s, 90.41 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 16:03:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:03:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:03:53] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:03:53] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:03:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:03:53] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:03:53] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:03:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:03:53] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:03:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:03:53] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:03:53] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:03:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:03:53] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:03:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:03:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:03:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:03:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:03:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:03:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:03:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:04:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:04:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:04:00] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:04:00] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:04:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:04:00] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:04:00] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:04:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:04:00] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:04:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:04:00] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:04:00] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:04:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:04:00] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:04:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:04:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:04:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:04:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:04:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:04:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:04:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=166168) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=166168) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=166168) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=166168) 
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=166168) [rank0]:W0125 16:04:15.357000 166168 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=166168) [rank0]:W0125 16:04:15.441000 166168 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:15] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=166168) [rank0]:W0125 16:04:16.372000 166168 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=166168) [rank0]:W0125 16:04:16.493000 166168 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=166168) 2026-01-25 16:04:20,439 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=166168) 2026-01-25 16:04:20,466 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=166168) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 16.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 11.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 11.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  5.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.79it/s]
(EngineCore_DP0 pid=166168) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  6.49it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.10it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 11.91it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 10.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 10.62it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 34/4096 [00:00<00:12, 336.49it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 441.27it/s]
Adding requests:   3%|▎         | 137/4096 [00:00<00:08, 470.86it/s]
Adding requests:   5%|▍         | 188/4096 [00:00<00:08, 475.97it/s]
Adding requests:   6%|▌         | 240/4096 [00:00<00:07, 488.86it/s]
Adding requests:   7%|▋         | 292/4096 [00:00<00:07, 496.70it/s]
Adding requests:   8%|▊         | 343/4096 [00:00<00:07, 498.72it/s]
Adding requests:  10%|▉         | 396/4096 [00:00<00:07, 508.39it/s]
Adding requests:  11%|█         | 448/4096 [00:00<00:07, 509.97it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:07, 508.34it/s]
Adding requests:  13%|█▎        | 551/4096 [00:01<00:07, 504.79it/s]
Adding requests:  15%|█▍        | 603/4096 [00:01<00:06, 507.98it/s]
Adding requests:  16%|█▌        | 657/4096 [00:01<00:06, 516.38it/s]
Adding requests:  17%|█▋        | 711/4096 [00:01<00:06, 522.57it/s]
Adding requests:  19%|█▊        | 764/4096 [00:01<00:06, 520.45it/s]
Adding requests:  20%|█▉        | 817/4096 [00:01<00:06, 511.09it/s]
Adding requests:  21%|██        | 869/4096 [00:01<00:06, 511.89it/s]
Adding requests:  23%|██▎       | 922/4096 [00:01<00:06, 517.00it/s]
Adding requests:  24%|██▍       | 975/4096 [00:01<00:06, 519.34it/s]
Adding requests:  25%|██▌       | 1028/4096 [00:02<00:05, 521.87it/s]
Adding requests:  26%|██▋       | 1081/4096 [00:02<00:05, 519.61it/s]
Adding requests:  28%|██▊       | 1133/4096 [00:02<00:05, 517.20it/s]
Adding requests:  29%|██▉       | 1185/4096 [00:02<00:05, 511.95it/s]
Adding requests:  30%|███       | 1238/4096 [00:02<00:05, 515.87it/s]
Adding requests:  31%|███▏      | 1290/4096 [00:02<00:05, 514.26it/s]
Adding requests:  33%|███▎      | 1344/4096 [00:02<00:05, 521.56it/s]
Adding requests:  34%|███▍      | 1398/4096 [00:02<00:05, 526.28it/s]
Adding requests:  35%|███▌      | 1451/4096 [00:02<00:05, 525.29it/s]
Adding requests:  37%|███▋      | 1506/4096 [00:02<00:04, 528.73it/s]
Adding requests:  38%|███▊      | 1560/4096 [00:03<00:04, 530.69it/s]
Adding requests:  39%|███▉      | 1615/4096 [00:03<00:04, 533.62it/s]
Adding requests:  41%|████      | 1669/4096 [00:03<00:04, 529.30it/s]
Adding requests:  42%|████▏     | 1723/4096 [00:03<00:04, 531.87it/s]
Adding requests:  43%|████▎     | 1777/4096 [00:03<00:04, 528.17it/s]
Adding requests:  45%|████▍     | 1830/4096 [00:03<00:04, 527.63it/s]
Adding requests:  46%|████▌     | 1883/4096 [00:03<00:04, 527.33it/s]
Adding requests:  47%|████▋     | 1936/4096 [00:03<00:04, 526.52it/s]
Adding requests:  49%|████▊     | 1989/4096 [00:03<00:04, 524.73it/s]
Adding requests:  50%|████▉     | 2043/4096 [00:03<00:03, 527.45it/s]
Adding requests:  51%|█████     | 2097/4096 [00:04<00:03, 529.25it/s]
Adding requests:  52%|█████▏    | 2150/4096 [00:04<00:03, 523.54it/s]
Adding requests:  54%|█████▍    | 2203/4096 [00:04<00:03, 518.60it/s]
Adding requests:  55%|█████▌    | 2258/4096 [00:04<00:03, 525.89it/s]
Adding requests:  56%|█████▋    | 2311/4096 [00:04<00:03, 523.08it/s]
Adding requests:  58%|█████▊    | 2364/4096 [00:04<00:03, 519.77it/s]
Adding requests:  59%|█████▉    | 2416/4096 [00:04<00:03, 505.78it/s]
Adding requests:  60%|██████    | 2468/4096 [00:04<00:03, 509.51it/s]
Adding requests:  62%|██████▏   | 2520/4096 [00:04<00:03, 512.54it/s]
Adding requests:  63%|██████▎   | 2575/4096 [00:04<00:02, 520.75it/s]
Adding requests:  64%|██████▍   | 2628/4096 [00:05<00:02, 522.13it/s]
Adding requests:  65%|██████▌   | 2682/4096 [00:05<00:02, 525.61it/s]
Adding requests:  67%|██████▋   | 2735/4096 [00:05<00:02, 521.37it/s]
Adding requests:  68%|██████▊   | 2788/4096 [00:05<00:02, 522.32it/s]
Adding requests:  69%|██████▉   | 2841/4096 [00:05<00:02, 521.10it/s]
Adding requests:  71%|███████   | 2895/4096 [00:05<00:02, 525.41it/s]
Adding requests:  72%|███████▏  | 2948/4096 [00:05<00:02, 519.49it/s]
Adding requests:  73%|███████▎  | 3001/4096 [00:05<00:02, 520.94it/s]
Adding requests:  75%|███████▍  | 3054/4096 [00:05<00:01, 522.21it/s]
Adding requests:  76%|███████▌  | 3107/4096 [00:06<00:01, 519.27it/s]
Adding requests:  77%|███████▋  | 3160/4096 [00:06<00:01, 521.28it/s]
Adding requests:  78%|███████▊  | 3213/4096 [00:06<00:01, 521.73it/s]
Adding requests:  80%|███████▉  | 3267/4096 [00:06<00:01, 525.68it/s]
Adding requests:  81%|████████  | 3320/4096 [00:06<00:01, 525.93it/s]
Adding requests:  82%|████████▏ | 3373/4096 [00:06<00:01, 526.41it/s]
Adding requests:  84%|████████▎ | 3427/4096 [00:06<00:01, 528.46it/s]
Adding requests:  85%|████████▍ | 3480/4096 [00:06<00:01, 518.52it/s]
Adding requests:  86%|████████▌ | 3532/4096 [00:06<00:01, 518.45it/s]
Adding requests:  88%|████████▊ | 3585/4096 [00:06<00:00, 519.47it/s]
Adding requests:  89%|████████▉ | 3637/4096 [00:07<00:00, 519.53it/s]
Adding requests:  90%|█████████ | 3690/4096 [00:07<00:00, 521.87it/s]
Adding requests:  91%|█████████▏| 3743/4096 [00:07<00:00, 506.05it/s]
Adding requests:  93%|█████████▎| 3799/4096 [00:07<00:00, 518.77it/s]
Adding requests:  94%|█████████▍| 3853/4096 [00:07<00:00, 523.10it/s]
Adding requests:  95%|█████████▌| 3906/4096 [00:07<00:00, 523.85it/s]
Adding requests:  97%|█████████▋| 3960/4096 [00:07<00:00, 526.52it/s]
Adding requests:  98%|█████████▊| 4013/4096 [00:07<00:00, 526.92it/s]
Adding requests:  99%|█████████▉| 4066/4096 [00:07<00:00, 520.73it/s]
Adding requests: 100%|██████████| 4096/4096 [00:07<00:00, 517.84it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 691/4096 [00:00<00:00, 5921.57it/s, est. speed input: 6064937.13 toks/s, output: 5921.92 toks/s]
Processed prompts:  31%|███▏      | 1284/4096 [00:06<00:16, 167.60it/s, est. speed input: 203561.43 toks/s, output: 198.79 toks/s]  
Processed prompts:  37%|███▋      | 1534/4096 [00:09<00:18, 136.78it/s, est. speed input: 168903.18 toks/s, output: 164.94 toks/s]
Processed prompts:  41%|████      | 1674/4096 [00:10<00:18, 128.51it/s, est. speed input: 159898.84 toks/s, output: 156.15 toks/s]
Processed prompts:  43%|████▎     | 1763/4096 [00:11<00:19, 120.02it/s, est. speed input: 153167.18 toks/s, output: 149.58 toks/s]
Processed prompts:  45%|████▍     | 1823/4096 [00:12<00:19, 114.63it/s, est. speed input: 149385.72 toks/s, output: 145.88 toks/s]
Processed prompts:  46%|████▌     | 1866/4096 [00:12<00:19, 115.27it/s, est. speed input: 148687.11 toks/s, output: 145.20 toks/s]
Processed prompts:  46%|████▋     | 1899/4096 [00:13<00:19, 112.97it/s, est. speed input: 147375.79 toks/s, output: 143.92 toks/s]
Processed prompts:  47%|████▋     | 1925/4096 [00:13<00:20, 107.10it/s, est. speed input: 145497.04 toks/s, output: 142.09 toks/s]
Processed prompts:  47%|████▋     | 1945/4096 [00:13<00:21, 98.70it/s, est. speed input: 143370.39 toks/s, output: 140.01 toks/s] 
Processed prompts:  48%|████▊     | 1971/4096 [00:14<00:22, 93.65it/s, est. speed input: 141686.38 toks/s, output: 138.37 toks/s]
Processed prompts:  49%|████▉     | 2003/4096 [00:14<00:22, 93.02it/s, est. speed input: 140510.25 toks/s, output: 137.22 toks/s]
Processed prompts:  50%|████▉     | 2035/4096 [00:14<00:22, 92.29it/s, est. speed input: 139360.78 toks/s, output: 136.09 toks/s]
Processed prompts:  50%|█████     | 2067/4096 [00:15<00:22, 91.79it/s, est. speed input: 138277.80 toks/s, output: 135.04 toks/s]
Processed prompts:  51%|█████     | 2099/4096 [00:15<00:21, 91.52it/s, est. speed input: 137257.64 toks/s, output: 134.04 toks/s]
Processed prompts:  52%|█████▏    | 2131/4096 [00:16<00:21, 91.10it/s, est. speed input: 136257.02 toks/s, output: 133.06 toks/s]
Processed prompts:  53%|█████▎    | 2163/4096 [00:16<00:21, 90.84it/s, est. speed input: 135304.63 toks/s, output: 132.13 toks/s]
Processed prompts:  54%|█████▎    | 2195/4096 [00:16<00:20, 90.78it/s, est. speed input: 134408.07 toks/s, output: 131.26 toks/s]
Processed prompts:  54%|█████▍    | 2227/4096 [00:17<00:20, 92.24it/s, est. speed input: 133701.82 toks/s, output: 130.57 toks/s]
Processed prompts:  55%|█████▌    | 2259/4096 [00:17<00:20, 91.63it/s, est. speed input: 132859.33 toks/s, output: 129.75 toks/s]
Processed prompts:  56%|█████▌    | 2291/4096 [00:17<00:19, 92.04it/s, est. speed input: 132131.56 toks/s, output: 129.03 toks/s]
Processed prompts:  57%|█████▋    | 2323/4096 [00:18<00:19, 92.20it/s, est. speed input: 131418.59 toks/s, output: 128.34 toks/s]
Processed prompts:  57%|█████▋    | 2355/4096 [00:18<00:18, 91.65it/s, est. speed input: 130672.34 toks/s, output: 127.61 toks/s]
Processed prompts:  58%|█████▊    | 2387/4096 [00:18<00:18, 91.15it/s, est. speed input: 129944.23 toks/s, output: 126.90 toks/s]
Processed prompts:  59%|█████▉    | 2419/4096 [00:19<00:18, 90.93it/s, est. speed input: 129254.22 toks/s, output: 126.22 toks/s]
Processed prompts:  60%|█████▉    | 2451/4096 [00:19<00:18, 90.79it/s, est. speed input: 128590.95 toks/s, output: 125.58 toks/s]
Processed prompts:  61%|██████    | 2483/4096 [00:19<00:17, 91.39it/s, est. speed input: 128008.88 toks/s, output: 125.01 toks/s]
Processed prompts:  61%|██████▏   | 2515/4096 [00:20<00:17, 91.04it/s, est. speed input: 127383.83 toks/s, output: 124.40 toks/s]
Processed prompts:  62%|██████▏   | 2547/4096 [00:20<00:17, 90.88it/s, est. speed input: 126787.43 toks/s, output: 123.82 toks/s]
Processed prompts:  63%|██████▎   | 2579/4096 [00:20<00:16, 91.37it/s, est. speed input: 126257.78 toks/s, output: 123.30 toks/s]
Processed prompts:  64%|██████▎   | 2611/4096 [00:21<00:16, 91.09it/s, est. speed input: 125698.27 toks/s, output: 122.75 toks/s]
Processed prompts:  65%|██████▍   | 2643/4096 [00:21<00:15, 90.91it/s, est. speed input: 125158.23 toks/s, output: 122.22 toks/s]
Processed prompts:  65%|██████▌   | 2675/4096 [00:21<00:15, 90.74it/s, est. speed input: 124632.24 toks/s, output: 121.71 toks/s]
Processed prompts:  66%|██████▌   | 2707/4096 [00:22<00:15, 90.51it/s, est. speed input: 124114.47 toks/s, output: 121.21 toks/s]
Processed prompts:  67%|██████▋   | 2739/4096 [00:22<00:15, 90.40it/s, est. speed input: 123617.16 toks/s, output: 120.72 toks/s]
Processed prompts:  68%|██████▊   | 2771/4096 [00:23<00:14, 90.42it/s, est. speed input: 123141.10 toks/s, output: 120.25 toks/s]
Processed prompts:  68%|██████▊   | 2803/4096 [00:23<00:14, 90.37it/s, est. speed input: 122676.09 toks/s, output: 119.80 toks/s]
Processed prompts:  69%|██████▉   | 2835/4096 [00:23<00:13, 90.26it/s, est. speed input: 122218.87 toks/s, output: 119.35 toks/s]
Processed prompts:  70%|██████▉   | 2867/4096 [00:24<00:13, 90.20it/s, est. speed input: 121776.65 toks/s, output: 118.92 toks/s]
Processed prompts:  71%|███████   | 2899/4096 [00:24<00:13, 90.18it/s, est. speed input: 121348.95 toks/s, output: 118.50 toks/s]
Processed prompts:  72%|███████▏  | 2931/4096 [00:24<00:12, 90.20it/s, est. speed input: 120935.37 toks/s, output: 118.10 toks/s]
Processed prompts:  72%|███████▏  | 2963/4096 [00:25<00:12, 90.17it/s, est. speed input: 120531.26 toks/s, output: 117.71 toks/s]
Processed prompts:  73%|███████▎  | 2995/4096 [00:25<00:12, 90.11it/s, est. speed input: 120135.67 toks/s, output: 117.32 toks/s]
Processed prompts:  74%|███████▍  | 3027/4096 [00:25<00:11, 90.21it/s, est. speed input: 119759.07 toks/s, output: 116.95 toks/s]
Processed prompts:  75%|███████▍  | 3059/4096 [00:26<00:11, 90.20it/s, est. speed input: 119388.12 toks/s, output: 116.59 toks/s]
Processed prompts:  75%|███████▌  | 3091/4096 [00:26<00:11, 90.08it/s, est. speed input: 119020.49 toks/s, output: 116.23 toks/s]
Processed prompts:  76%|███████▌  | 3123/4096 [00:26<00:10, 90.84it/s, est. speed input: 118711.03 toks/s, output: 115.93 toks/s]
Processed prompts:  77%|███████▋  | 3155/4096 [00:27<00:10, 90.60it/s, est. speed input: 118365.94 toks/s, output: 115.59 toks/s]
Processed prompts:  78%|███████▊  | 3187/4096 [00:27<00:10, 90.29it/s, est. speed input: 118021.92 toks/s, output: 115.26 toks/s]
Processed prompts:  79%|███████▊  | 3219/4096 [00:28<00:09, 90.20it/s, est. speed input: 117693.20 toks/s, output: 114.93 toks/s]
Processed prompts:  79%|███████▉  | 3251/4096 [00:28<00:09, 90.32it/s, est. speed input: 117382.87 toks/s, output: 114.63 toks/s]
Processed prompts:  80%|████████  | 3283/4096 [00:28<00:09, 90.09it/s, est. speed input: 117063.50 toks/s, output: 114.32 toks/s]
Processed prompts:  81%|████████  | 3315/4096 [00:29<00:08, 90.10it/s, est. speed input: 116760.73 toks/s, output: 114.02 toks/s]
Processed prompts:  82%|████████▏ | 3347/4096 [00:29<00:08, 90.14it/s, est. speed input: 116467.45 toks/s, output: 113.74 toks/s]
Processed prompts:  82%|████████▏ | 3379/4096 [00:29<00:07, 90.03it/s, est. speed input: 116173.86 toks/s, output: 113.45 toks/s]
Processed prompts:  83%|████████▎ | 3411/4096 [00:30<00:07, 90.13it/s, est. speed input: 115895.96 toks/s, output: 113.18 toks/s]
Processed prompts:  84%|████████▍ | 3443/4096 [00:30<00:07, 90.09it/s, est. speed input: 115618.96 toks/s, output: 112.91 toks/s]
Processed prompts:  85%|████████▍ | 3475/4096 [00:30<00:06, 89.90it/s, est. speed input: 115340.89 toks/s, output: 112.64 toks/s]
Processed prompts:  86%|████████▌ | 3507/4096 [00:31<00:06, 90.04it/s, est. speed input: 115082.02 toks/s, output: 112.38 toks/s]
Processed prompts:  86%|████████▋ | 3539/4096 [00:31<00:06, 90.04it/s, est. speed input: 114824.23 toks/s, output: 112.13 toks/s]
Processed prompts:  87%|████████▋ | 3571/4096 [00:31<00:05, 89.93it/s, est. speed input: 114566.96 toks/s, output: 111.88 toks/s]
Processed prompts:  88%|████████▊ | 3603/4096 [00:32<00:05, 90.09it/s, est. speed input: 114326.80 toks/s, output: 111.65 toks/s]
Processed prompts:  89%|████████▊ | 3635/4096 [00:32<00:05, 89.97it/s, est. speed input: 114080.76 toks/s, output: 111.41 toks/s]
Processed prompts:  90%|████████▉ | 3667/4096 [00:32<00:04, 90.01it/s, est. speed input: 113845.81 toks/s, output: 111.18 toks/s]
Processed prompts:  90%|█████████ | 3699/4096 [00:33<00:04, 90.12it/s, est. speed input: 113619.81 toks/s, output: 110.96 toks/s]
Processed prompts:  91%|█████████ | 3731/4096 [00:33<00:04, 90.70it/s, est. speed input: 113420.30 toks/s, output: 110.76 toks/s]
Processed prompts:  92%|█████████▏| 3763/4096 [00:34<00:03, 90.55it/s, est. speed input: 113201.03 toks/s, output: 110.55 toks/s]
Processed prompts:  93%|█████████▎| 3795/4096 [00:34<00:03, 90.35it/s, est. speed input: 112981.97 toks/s, output: 110.33 toks/s]
Processed prompts:  93%|█████████▎| 3827/4096 [00:34<00:02, 89.97it/s, est. speed input: 112757.40 toks/s, output: 110.11 toks/s]
Processed prompts:  94%|█████████▍| 3859/4096 [00:35<00:02, 89.98it/s, est. speed input: 112548.80 toks/s, output: 109.91 toks/s]
Processed prompts:  95%|█████████▍| 3891/4096 [00:35<00:02, 90.06it/s, est. speed input: 112347.35 toks/s, output: 109.71 toks/s]
Processed prompts:  96%|█████████▌| 3923/4096 [00:35<00:01, 89.93it/s, est. speed input: 112142.47 toks/s, output: 109.51 toks/s]
Processed prompts:  97%|█████████▋| 3955/4096 [00:36<00:01, 90.09it/s, est. speed input: 111951.48 toks/s, output: 109.33 toks/s]
Processed prompts:  97%|█████████▋| 3987/4096 [00:36<00:01, 89.92it/s, est. speed input: 111753.45 toks/s, output: 109.13 toks/s]
Processed prompts:  98%|█████████▊| 4019/4096 [00:36<00:00, 90.59it/s, est. speed input: 111589.99 toks/s, output: 108.97 toks/s]
Processed prompts:  99%|█████████▉| 4051/4096 [00:37<00:00, 90.60it/s, est. speed input: 111411.93 toks/s, output: 108.80 toks/s]
Processed prompts: 100%|█████████▉| 4083/4096 [00:37<00:00, 108.67it/s, est. speed input: 111819.04 toks/s, output: 109.20 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:37<00:00, 108.67it/s, est. speed input: 112173.46 toks/s, output: 109.54 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:37<00:00, 109.54it/s, est. speed input: 112173.46 toks/s, output: 109.54 toks/s]
[rank0]:[W125 16:05:09.758097722 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 101.7s

测试结果:
  Requests/s:   90.41
  Tokens/s:     92670.95
  Total Reqs:   4096
  Elapsed:      45.30s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     92580.54

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:05:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=168312) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=168312) WARNING 01-25 16:06:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 90.46 requests/s, 92717.30 total tokens/s, 90.46 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-25 16:05:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:05:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:05:52] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:52] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:05:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:52] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:52] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:05:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:52] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:52] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:52] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:05:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:52] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:05:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:05:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:05:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:05:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:05:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:05:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:05:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:05:59] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:59] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:05:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:59] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:59] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:05:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:59] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:59] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:59] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:05:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:59] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:05:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:05:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:05:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:05:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:05:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=168312) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=168312) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=168312) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=168312) 
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=168312) [rank0]:W0125 16:06:14.372000 168312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=168312) [rank0]:W0125 16:06:14.454000 168312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:14] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=168312) [rank0]:W0125 16:06:15.694000 168312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=168312) [rank0]:W0125 16:06:15.815000 168312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=168312) 2026-01-25 16:06:20,530 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=168312) 2026-01-25 16:06:20,595 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=168312) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:03,  4.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  5.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:03,  4.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:03,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:02,  5.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  5.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00, 10.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 12.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 13.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00, 15.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.81it/s]
(EngineCore_DP0 pid=168312) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:02,  4.04it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:02,  4.24it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:02,  3.12it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:01<00:01,  5.53it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:01<00:00,  8.17it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  8.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  7.44it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   1%|          | 50/8192 [00:00<00:16, 494.04it/s]
Adding requests:   1%|          | 102/8192 [00:00<00:16, 503.90it/s]
Adding requests:   2%|▏         | 153/8192 [00:00<00:16, 498.88it/s]
Adding requests:   2%|▏         | 204/8192 [00:00<00:15, 499.75it/s]
Adding requests:   3%|▎         | 257/8192 [00:00<00:15, 507.36it/s]
Adding requests:   4%|▍         | 308/8192 [00:00<00:15, 502.70it/s]
Adding requests:   4%|▍         | 359/8192 [00:00<00:15, 504.11it/s]
Adding requests:   5%|▌         | 411/8192 [00:00<00:15, 506.90it/s]
Adding requests:   6%|▌         | 462/8192 [00:00<00:15, 507.40it/s]
Adding requests:   6%|▋         | 513/8192 [00:01<00:15, 501.84it/s]
Adding requests:   7%|▋         | 564/8192 [00:01<00:15, 501.07it/s]
Adding requests:   8%|▊         | 615/8192 [00:01<00:15, 503.42it/s]
Adding requests:   8%|▊         | 668/8192 [00:01<00:14, 511.34it/s]
Adding requests:   9%|▉         | 722/8192 [00:01<00:14, 517.22it/s]
Adding requests:   9%|▉         | 774/8192 [00:01<00:14, 513.60it/s]
Adding requests:  10%|█         | 826/8192 [00:01<00:14, 496.64it/s]
Adding requests:  11%|█         | 878/8192 [00:01<00:14, 502.70it/s]
Adding requests:  11%|█▏        | 931/8192 [00:01<00:14, 510.20it/s]
Adding requests:  12%|█▏        | 983/8192 [00:01<00:14, 512.85it/s]
Adding requests:  13%|█▎        | 1036/8192 [00:02<00:13, 516.95it/s]
Adding requests:  13%|█▎        | 1088/8192 [00:02<00:13, 513.79it/s]
Adding requests:  14%|█▍        | 1140/8192 [00:02<00:13, 511.62it/s]
Adding requests:  15%|█▍        | 1196/8192 [00:02<00:13, 523.19it/s]
Adding requests:  15%|█▌        | 1249/8192 [00:02<00:13, 522.26it/s]
Adding requests:  16%|█▌        | 1302/8192 [00:02<00:13, 518.62it/s]
Adding requests:  17%|█▋        | 1356/8192 [00:02<00:13, 521.96it/s]
Adding requests:  17%|█▋        | 1410/8192 [00:02<00:12, 526.38it/s]
Adding requests:  18%|█▊        | 1463/8192 [00:02<00:12, 524.84it/s]
Adding requests:  19%|█▊        | 1516/8192 [00:02<00:12, 524.54it/s]
Adding requests:  19%|█▉        | 1569/8192 [00:03<00:12, 525.05it/s]
Adding requests:  20%|█▉        | 1623/8192 [00:03<00:12, 528.99it/s]
Adding requests:  20%|██        | 1676/8192 [00:03<00:12, 521.78it/s]
Adding requests:  21%|██        | 1730/8192 [00:03<00:12, 524.30it/s]
Adding requests:  22%|██▏       | 1783/8192 [00:03<00:12, 519.16it/s]
Adding requests:  22%|██▏       | 1836/8192 [00:03<00:12, 520.73it/s]
Adding requests:  23%|██▎       | 1889/8192 [00:03<00:12, 520.65it/s]
Adding requests:  24%|██▎       | 1942/8192 [00:03<00:12, 519.26it/s]
Adding requests:  24%|██▍       | 1994/8192 [00:03<00:12, 503.09it/s]
Adding requests:  25%|██▌       | 2048/8192 [00:03<00:11, 512.28it/s]
Adding requests:  26%|██▌       | 2101/8192 [00:04<00:11, 517.40it/s]
Adding requests:  26%|██▋       | 2153/8192 [00:04<00:11, 513.79it/s]
Adding requests:  27%|██▋       | 2205/8192 [00:04<00:11, 510.61it/s]
Adding requests:  28%|██▊       | 2260/8192 [00:04<00:11, 517.41it/s]
Adding requests:  28%|██▊       | 2314/8192 [00:04<00:11, 521.65it/s]
Adding requests:  29%|██▉       | 2367/8192 [00:04<00:11, 518.24it/s]
Adding requests:  30%|██▉       | 2420/8192 [00:04<00:11, 519.86it/s]
Adding requests:  30%|███       | 2473/8192 [00:04<00:11, 519.62it/s]
Adding requests:  31%|███       | 2525/8192 [00:04<00:10, 517.83it/s]
Adding requests:  31%|███▏      | 2580/8192 [00:05<00:10, 525.28it/s]
Adding requests:  32%|███▏      | 2633/8192 [00:05<00:10, 521.01it/s]
Adding requests:  33%|███▎      | 2686/8192 [00:05<00:10, 521.99it/s]
Adding requests:  33%|███▎      | 2739/8192 [00:05<00:10, 517.31it/s]
Adding requests:  34%|███▍      | 2791/8192 [00:05<00:10, 515.35it/s]
Adding requests:  35%|███▍      | 2844/8192 [00:05<00:10, 517.27it/s]
Adding requests:  35%|███▌      | 2897/8192 [00:05<00:10, 520.97it/s]
Adding requests:  36%|███▌      | 2950/8192 [00:05<00:10, 515.82it/s]
Adding requests:  37%|███▋      | 3003/8192 [00:05<00:10, 517.41it/s]
Adding requests:  37%|███▋      | 3056/8192 [00:05<00:09, 518.74it/s]
Adding requests:  38%|███▊      | 3108/8192 [00:06<00:09, 514.49it/s]
Adding requests:  39%|███▊      | 3160/8192 [00:06<00:09, 515.76it/s]
Adding requests:  39%|███▉      | 3212/8192 [00:06<00:09, 516.30it/s]
Adding requests:  40%|███▉      | 3265/8192 [00:06<00:09, 520.21it/s]
Adding requests:  41%|████      | 3318/8192 [00:06<00:09, 508.76it/s]
Adding requests:  41%|████      | 3371/8192 [00:06<00:09, 514.25it/s]
Adding requests:  42%|████▏     | 3425/8192 [00:06<00:09, 518.87it/s]
Adding requests:  42%|████▏     | 3477/8192 [00:06<00:09, 506.81it/s]
Adding requests:  43%|████▎     | 3528/8192 [00:06<00:09, 507.06it/s]
Adding requests:  44%|████▎     | 3580/8192 [00:06<00:09, 509.84it/s]
Adding requests:  44%|████▍     | 3632/8192 [00:07<00:08, 510.43it/s]
Adding requests:  45%|████▍     | 3685/8192 [00:07<00:08, 513.83it/s]
Adding requests:  46%|████▌     | 3737/8192 [00:07<00:08, 512.60it/s]
Adding requests:  46%|████▋     | 3792/8192 [00:07<00:08, 522.53it/s]
Adding requests:  47%|████▋     | 3846/8192 [00:07<00:08, 525.23it/s]
Adding requests:  48%|████▊     | 3899/8192 [00:07<00:08, 523.06it/s]
Adding requests:  48%|████▊     | 3952/8192 [00:07<00:08, 522.65it/s]
Adding requests:  49%|████▉     | 4005/8192 [00:07<00:08, 521.76it/s]
Adding requests:  50%|████▉     | 4058/8192 [00:07<00:07, 518.55it/s]
Adding requests:  50%|█████     | 4111/8192 [00:07<00:07, 519.27it/s]
Adding requests:  51%|█████     | 4164/8192 [00:08<00:07, 521.17it/s]
Adding requests:  51%|█████▏    | 4217/8192 [00:08<00:07, 522.27it/s]
Adding requests:  52%|█████▏    | 4270/8192 [00:08<00:07, 521.95it/s]
Adding requests:  53%|█████▎    | 4323/8192 [00:08<00:07, 524.08it/s]
Adding requests:  53%|█████▎    | 4377/8192 [00:08<00:07, 528.66it/s]
Adding requests:  54%|█████▍    | 4430/8192 [00:08<00:07, 525.49it/s]
Adding requests:  55%|█████▍    | 4483/8192 [00:08<00:07, 522.49it/s]
Adding requests:  55%|█████▌    | 4536/8192 [00:08<00:07, 518.70it/s]
Adding requests:  56%|█████▌    | 4589/8192 [00:08<00:06, 519.91it/s]
Adding requests:  57%|█████▋    | 4642/8192 [00:08<00:06, 513.45it/s]
Adding requests:  57%|█████▋    | 4694/8192 [00:09<00:06, 511.07it/s]
Adding requests:  58%|█████▊    | 4747/8192 [00:09<00:06, 516.11it/s]
Adding requests:  59%|█████▊    | 4799/8192 [00:09<00:06, 515.20it/s]
Adding requests:  59%|█████▉    | 4851/8192 [00:09<00:06, 516.16it/s]
Adding requests:  60%|█████▉    | 4903/8192 [00:09<00:06, 512.52it/s]
Adding requests:  60%|██████    | 4956/8192 [00:09<00:06, 516.99it/s]
Adding requests:  61%|██████    | 5008/8192 [00:09<00:06, 517.68it/s]
Adding requests:  62%|██████▏   | 5062/8192 [00:09<00:05, 521.69it/s]
Adding requests:  62%|██████▏   | 5116/8192 [00:09<00:05, 526.50it/s]
Adding requests:  63%|██████▎   | 5169/8192 [00:10<00:05, 525.83it/s]
Adding requests:  64%|██████▎   | 5222/8192 [00:10<00:05, 523.06it/s]
Adding requests:  64%|██████▍   | 5275/8192 [00:10<00:05, 518.25it/s]
Adding requests:  65%|██████▌   | 5329/8192 [00:10<00:05, 523.76it/s]
Adding requests:  66%|██████▌   | 5382/8192 [00:10<00:05, 523.24it/s]
Adding requests:  66%|██████▋   | 5435/8192 [00:10<00:05, 524.00it/s]
Adding requests:  67%|██████▋   | 5488/8192 [00:10<00:05, 517.13it/s]
Adding requests:  68%|██████▊   | 5540/8192 [00:10<00:05, 516.29it/s]
Adding requests:  68%|██████▊   | 5592/8192 [00:10<00:05, 516.59it/s]
Adding requests:  69%|██████▉   | 5644/8192 [00:10<00:04, 517.05it/s]
Adding requests:  70%|██████▉   | 5696/8192 [00:11<00:04, 515.20it/s]
Adding requests:  70%|███████   | 5749/8192 [00:11<00:04, 517.12it/s]
Adding requests:  71%|███████   | 5802/8192 [00:11<00:04, 518.19it/s]
Adding requests:  71%|███████▏  | 5854/8192 [00:11<00:04, 516.47it/s]
Adding requests:  72%|███████▏  | 5908/8192 [00:11<00:04, 522.59it/s]
Adding requests:  73%|███████▎  | 5961/8192 [00:11<00:04, 521.71it/s]
Adding requests:  73%|███████▎  | 6014/8192 [00:11<00:04, 513.21it/s]
Adding requests:  74%|███████▍  | 6067/8192 [00:11<00:04, 517.04it/s]
Adding requests:  75%|███████▍  | 6119/8192 [00:11<00:04, 513.63it/s]
Adding requests:  75%|███████▌  | 6171/8192 [00:11<00:03, 512.79it/s]
Adding requests:  76%|███████▌  | 6225/8192 [00:12<00:03, 519.12it/s]
Adding requests:  77%|███████▋  | 6279/8192 [00:12<00:03, 522.94it/s]
Adding requests:  77%|███████▋  | 6333/8192 [00:12<00:03, 526.39it/s]
Adding requests:  78%|███████▊  | 6386/8192 [00:12<00:03, 527.44it/s]
Adding requests:  79%|███████▊  | 6441/8192 [00:12<00:03, 532.23it/s]
Adding requests:  79%|███████▉  | 6496/8192 [00:12<00:03, 535.10it/s]
Adding requests:  80%|███████▉  | 6550/8192 [00:12<00:03, 534.16it/s]
Adding requests:  81%|████████  | 6604/8192 [00:12<00:02, 531.62it/s]
Adding requests:  81%|████████▏ | 6658/8192 [00:12<00:02, 532.23it/s]
Adding requests:  82%|████████▏ | 6712/8192 [00:12<00:02, 529.25it/s]
Adding requests:  83%|████████▎ | 6765/8192 [00:13<00:02, 527.42it/s]
Adding requests:  83%|████████▎ | 6819/8192 [00:13<00:02, 531.07it/s]
Adding requests:  84%|████████▍ | 6874/8192 [00:13<00:02, 534.05it/s]
Adding requests:  85%|████████▍ | 6928/8192 [00:13<00:02, 535.61it/s]
Adding requests:  85%|████████▌ | 6982/8192 [00:13<00:02, 532.21it/s]
Adding requests:  86%|████████▌ | 7036/8192 [00:13<00:02, 529.94it/s]
Adding requests:  87%|████████▋ | 7090/8192 [00:13<00:02, 528.05it/s]
Adding requests:  87%|████████▋ | 7143/8192 [00:13<00:01, 526.96it/s]
Adding requests:  88%|████████▊ | 7196/8192 [00:13<00:01, 525.27it/s]
Adding requests:  89%|████████▊ | 7250/8192 [00:13<00:01, 528.04it/s]
Adding requests:  89%|████████▉ | 7304/8192 [00:14<00:01, 529.57it/s]
Adding requests:  90%|████████▉ | 7357/8192 [00:14<00:01, 512.70it/s]
Adding requests:  90%|█████████ | 7412/8192 [00:14<00:01, 522.31it/s]
Adding requests:  91%|█████████ | 7467/8192 [00:14<00:01, 527.75it/s]
Adding requests:  92%|█████████▏| 7521/8192 [00:14<00:01, 529.12it/s]
Adding requests:  92%|█████████▏| 7574/8192 [00:14<00:01, 527.48it/s]
Adding requests:  93%|█████████▎| 7627/8192 [00:14<00:01, 524.95it/s]
Adding requests:  94%|█████████▍| 7682/8192 [00:14<00:00, 531.26it/s]
Adding requests:  94%|█████████▍| 7736/8192 [00:14<00:00, 529.68it/s]
Adding requests:  95%|█████████▌| 7789/8192 [00:15<00:00, 524.04it/s]
Adding requests:  96%|█████████▌| 7843/8192 [00:15<00:00, 527.61it/s]
Adding requests:  96%|█████████▋| 7896/8192 [00:15<00:00, 526.94it/s]
Adding requests:  97%|█████████▋| 7949/8192 [00:15<00:00, 520.78it/s]
Adding requests:  98%|█████████▊| 8002/8192 [00:15<00:00, 519.49it/s]
Adding requests:  98%|█████████▊| 8054/8192 [00:15<00:00, 517.91it/s]
Adding requests:  99%|█████████▉| 8108/8192 [00:15<00:00, 521.90it/s]
Adding requests: 100%|█████████▉| 8161/8192 [00:15<00:00, 523.64it/s]
Adding requests: 100%|██████████| 8192/8192 [00:15<00:00, 519.33it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 1386/8192 [00:00<00:01, 6567.46it/s, est. speed input: 6725831.18 toks/s, output: 6567.68 toks/s]
Processed prompts:  25%|██▍       | 2043/8192 [00:07<00:27, 227.10it/s, est. speed input: 289420.23 toks/s, output: 282.64 toks/s]   
Processed prompts:  28%|██▊       | 2319/8192 [00:10<00:32, 181.60it/s, est. speed input: 237350.83 toks/s, output: 231.79 toks/s]
Processed prompts:  30%|███       | 2474/8192 [00:12<00:37, 151.61it/s, est. speed input: 209267.73 toks/s, output: 204.36 toks/s]
Processed prompts:  31%|███▏      | 2571/8192 [00:12<00:37, 150.23it/s, est. speed input: 205676.33 toks/s, output: 200.86 toks/s]
Processed prompts:  32%|███▏      | 2638/8192 [00:13<00:39, 142.25it/s, est. speed input: 200002.07 toks/s, output: 195.31 toks/s]
Processed prompts:  33%|███▎      | 2686/8192 [00:14<00:42, 129.53it/s, est. speed input: 193511.97 toks/s, output: 188.98 toks/s]
Processed prompts:  33%|███▎      | 2730/8192 [00:14<00:46, 116.35it/s, est. speed input: 187377.09 toks/s, output: 182.98 toks/s]
Processed prompts:  34%|███▍      | 2794/8192 [00:15<00:48, 110.68it/s, est. speed input: 183090.99 toks/s, output: 178.80 toks/s]
Processed prompts:  35%|███▍      | 2858/8192 [00:16<00:50, 105.90it/s, est. speed input: 179185.31 toks/s, output: 174.99 toks/s]
Processed prompts:  36%|███▌      | 2922/8192 [00:17<00:51, 102.01it/s, est. speed input: 175600.68 toks/s, output: 171.48 toks/s]
Processed prompts:  36%|███▋      | 2986/8192 [00:17<00:52, 98.93it/s, est. speed input: 172293.02 toks/s, output: 168.25 toks/s] 
Processed prompts:  37%|███▋      | 3050/8192 [00:18<00:53, 96.62it/s, est. speed input: 169245.97 toks/s, output: 165.28 toks/s]
Processed prompts:  38%|███▊      | 3114/8192 [00:19<00:53, 95.34it/s, est. speed input: 166521.66 toks/s, output: 162.62 toks/s]
Processed prompts:  39%|███▉      | 3178/8192 [00:19<00:53, 93.93it/s, est. speed input: 163888.02 toks/s, output: 160.05 toks/s]
Processed prompts:  40%|███▉      | 3242/8192 [00:20<00:53, 92.91it/s, est. speed input: 161435.50 toks/s, output: 157.65 toks/s]
Processed prompts:  40%|████      | 3306/8192 [00:21<00:53, 92.16it/s, est. speed input: 159140.48 toks/s, output: 155.41 toks/s]
Processed prompts:  41%|████      | 3370/8192 [00:21<00:52, 91.66it/s, est. speed input: 157000.14 toks/s, output: 153.32 toks/s]
Processed prompts:  42%|████▏     | 3434/8192 [00:22<00:52, 91.29it/s, est. speed input: 154990.32 toks/s, output: 151.36 toks/s]
Processed prompts:  43%|████▎     | 3498/8192 [00:23<00:51, 91.04it/s, est. speed input: 153105.08 toks/s, output: 149.52 toks/s]
Processed prompts:  43%|████▎     | 3562/8192 [00:24<00:50, 90.82it/s, est. speed input: 151321.13 toks/s, output: 147.77 toks/s]
Processed prompts:  44%|████▍     | 3626/8192 [00:24<00:50, 90.61it/s, est. speed input: 149631.96 toks/s, output: 146.12 toks/s]
Processed prompts:  45%|████▌     | 3690/8192 [00:25<00:49, 91.02it/s, est. speed input: 148119.98 toks/s, output: 144.65 toks/s]
Processed prompts:  46%|████▌     | 3754/8192 [00:26<00:48, 90.76it/s, est. speed input: 146609.35 toks/s, output: 143.17 toks/s]
Processed prompts:  47%|████▋     | 3818/8192 [00:26<00:48, 90.58it/s, est. speed input: 145178.80 toks/s, output: 141.78 toks/s]
Processed prompts:  47%|████▋     | 3882/8192 [00:27<00:47, 90.47it/s, est. speed input: 143823.94 toks/s, output: 140.45 toks/s]
Processed prompts:  48%|████▊     | 3946/8192 [00:28<00:46, 90.37it/s, est. speed input: 142532.68 toks/s, output: 139.19 toks/s]
Processed prompts:  49%|████▉     | 4010/8192 [00:29<00:46, 90.81it/s, est. speed input: 141370.09 toks/s, output: 138.06 toks/s]
Processed prompts:  50%|████▉     | 4074/8192 [00:29<00:45, 90.63it/s, est. speed input: 140201.61 toks/s, output: 136.92 toks/s]
Processed prompts:  51%|█████     | 4138/8192 [00:30<00:44, 90.52it/s, est. speed input: 139089.79 toks/s, output: 135.83 toks/s]
Processed prompts:  51%|█████▏    | 4202/8192 [00:31<00:43, 90.81it/s, est. speed input: 138071.30 toks/s, output: 134.84 toks/s]
Processed prompts:  52%|█████▏    | 4266/8192 [00:31<00:43, 91.06it/s, est. speed input: 137102.38 toks/s, output: 133.89 toks/s]
Processed prompts:  53%|█████▎    | 4330/8192 [00:32<00:42, 91.22it/s, est. speed input: 136172.96 toks/s, output: 132.98 toks/s]
Processed prompts:  54%|█████▎    | 4394/8192 [00:33<00:41, 90.92it/s, est. speed input: 135239.18 toks/s, output: 132.07 toks/s]
Processed prompts:  54%|█████▍    | 4458/8192 [00:33<00:41, 90.65it/s, est. speed input: 134339.05 toks/s, output: 131.19 toks/s]
Processed prompts:  55%|█████▌    | 4522/8192 [00:34<00:40, 90.51it/s, est. speed input: 133480.12 toks/s, output: 130.35 toks/s]
Processed prompts:  56%|█████▌    | 4586/8192 [00:35<00:39, 90.43it/s, est. speed input: 132657.16 toks/s, output: 129.55 toks/s]
Processed prompts:  57%|█████▋    | 4650/8192 [00:36<00:39, 90.27it/s, est. speed input: 131856.58 toks/s, output: 128.77 toks/s]
Processed prompts:  58%|█████▊    | 4714/8192 [00:36<00:38, 90.23it/s, est. speed input: 131093.74 toks/s, output: 128.02 toks/s]
Processed prompts:  58%|█████▊    | 4778/8192 [00:37<00:37, 90.65it/s, est. speed input: 130400.71 toks/s, output: 127.34 toks/s]
Processed prompts:  59%|█████▉    | 4842/8192 [00:38<00:36, 90.82it/s, est. speed input: 129721.78 toks/s, output: 126.68 toks/s]
Processed prompts:  60%|█████▉    | 4906/8192 [00:38<00:36, 90.58it/s, est. speed input: 129036.13 toks/s, output: 126.01 toks/s]
Processed prompts:  61%|██████    | 4970/8192 [00:39<00:35, 90.94it/s, est. speed input: 128419.56 toks/s, output: 125.41 toks/s]
Processed prompts:  61%|██████▏   | 5034/8192 [00:40<00:34, 90.63it/s, est. speed input: 127778.71 toks/s, output: 124.78 toks/s]
Processed prompts:  62%|██████▏   | 5098/8192 [00:41<00:34, 90.52it/s, est. speed input: 127167.91 toks/s, output: 124.19 toks/s]
Processed prompts:  63%|██████▎   | 5162/8192 [00:41<00:33, 90.37it/s, est. speed input: 126571.96 toks/s, output: 123.61 toks/s]
Processed prompts:  64%|██████▍   | 5226/8192 [00:42<00:32, 90.21it/s, est. speed input: 125992.13 toks/s, output: 123.04 toks/s]
Processed prompts:  65%|██████▍   | 5290/8192 [00:43<00:32, 90.09it/s, est. speed input: 125430.70 toks/s, output: 122.49 toks/s]
Processed prompts:  65%|██████▌   | 5354/8192 [00:43<00:31, 90.07it/s, est. speed input: 124892.15 toks/s, output: 121.96 toks/s]
Processed prompts:  66%|██████▌   | 5418/8192 [00:44<00:30, 90.01it/s, est. speed input: 124367.72 toks/s, output: 121.45 toks/s]
Processed prompts:  67%|██████▋   | 5482/8192 [00:45<00:30, 89.83it/s, est. speed input: 123849.88 toks/s, output: 120.95 toks/s]
Processed prompts:  68%|██████▊   | 5546/8192 [00:46<00:29, 90.31it/s, est. speed input: 123390.40 toks/s, output: 120.50 toks/s]
Processed prompts:  68%|██████▊   | 5610/8192 [00:46<00:28, 90.23it/s, est. speed input: 122916.12 toks/s, output: 120.04 toks/s]
Processed prompts:  69%|██████▉   | 5674/8192 [00:47<00:27, 90.07it/s, est. speed input: 122449.27 toks/s, output: 119.58 toks/s]
Processed prompts:  70%|███████   | 5738/8192 [00:48<00:27, 90.02it/s, est. speed input: 121999.97 toks/s, output: 119.14 toks/s]
Processed prompts:  71%|███████   | 5802/8192 [00:48<00:26, 89.86it/s, est. speed input: 121555.33 toks/s, output: 118.71 toks/s]
Processed prompts:  72%|███████▏  | 5866/8192 [00:49<00:25, 89.93it/s, est. speed input: 121135.55 toks/s, output: 118.30 toks/s]
Processed prompts:  72%|███████▏  | 5930/8192 [00:50<00:25, 89.70it/s, est. speed input: 120710.24 toks/s, output: 117.88 toks/s]
Processed prompts:  73%|███████▎  | 5994/8192 [00:51<00:24, 89.70it/s, est. speed input: 120306.82 toks/s, output: 117.49 toks/s]
Processed prompts:  74%|███████▍  | 6058/8192 [00:51<00:23, 89.73it/s, est. speed input: 119916.29 toks/s, output: 117.11 toks/s]
Processed prompts:  75%|███████▍  | 6122/8192 [00:52<00:23, 89.70it/s, est. speed input: 119532.78 toks/s, output: 116.73 toks/s]
Processed prompts:  76%|███████▌  | 6186/8192 [00:53<00:22, 89.66it/s, est. speed input: 119159.22 toks/s, output: 116.37 toks/s]
Processed prompts:  76%|███████▋  | 6250/8192 [00:53<00:21, 89.66it/s, est. speed input: 118796.87 toks/s, output: 116.01 toks/s]
Processed prompts:  77%|███████▋  | 6314/8192 [00:54<00:20, 89.71it/s, est. speed input: 118446.56 toks/s, output: 115.67 toks/s]
Processed prompts:  78%|███████▊  | 6378/8192 [00:55<00:20, 89.67it/s, est. speed input: 118101.49 toks/s, output: 115.33 toks/s]
Processed prompts:  79%|███████▊  | 6442/8192 [00:56<00:19, 89.59it/s, est. speed input: 117762.35 toks/s, output: 115.00 toks/s]
Processed prompts:  79%|███████▉  | 6506/8192 [00:56<00:18, 89.59it/s, est. speed input: 117434.70 toks/s, output: 114.68 toks/s]
Processed prompts:  80%|████████  | 6570/8192 [00:57<00:18, 90.04it/s, est. speed input: 117139.24 toks/s, output: 114.39 toks/s]
Processed prompts:  81%|████████  | 6634/8192 [00:58<00:17, 90.23it/s, est. speed input: 116844.45 toks/s, output: 114.11 toks/s]
Processed prompts:  82%|████████▏ | 6698/8192 [00:58<00:16, 90.00it/s, est. speed input: 116538.02 toks/s, output: 113.81 toks/s]
Processed prompts:  83%|████████▎ | 6762/8192 [00:59<00:15, 89.94it/s, est. speed input: 116243.54 toks/s, output: 113.52 toks/s]
Processed prompts:  83%|████████▎ | 6826/8192 [01:00<00:15, 89.89it/s, est. speed input: 115956.04 toks/s, output: 113.24 toks/s]
Processed prompts:  84%|████████▍ | 6890/8192 [01:00<00:14, 89.69it/s, est. speed input: 115666.90 toks/s, output: 112.96 toks/s]
Processed prompts:  85%|████████▍ | 6954/8192 [01:01<00:13, 89.69it/s, est. speed input: 115391.55 toks/s, output: 112.69 toks/s]
Processed prompts:  86%|████████▌ | 7018/8192 [01:02<00:13, 89.65it/s, est. speed input: 115120.14 toks/s, output: 112.42 toks/s]
Processed prompts:  86%|████████▋ | 7082/8192 [01:03<00:12, 89.62it/s, est. speed input: 114855.17 toks/s, output: 112.16 toks/s]
Processed prompts:  87%|████████▋ | 7146/8192 [01:03<00:11, 90.02it/s, est. speed input: 114615.75 toks/s, output: 111.93 toks/s]
Processed prompts:  88%|████████▊ | 7210/8192 [01:04<00:10, 89.87it/s, est. speed input: 114361.97 toks/s, output: 111.68 toks/s]
Processed prompts:  89%|████████▉ | 7274/8192 [01:05<00:10, 90.18it/s, est. speed input: 114132.31 toks/s, output: 111.46 toks/s]
Processed prompts:  90%|████████▉ | 7338/8192 [01:05<00:09, 89.93it/s, est. speed input: 113886.45 toks/s, output: 111.22 toks/s]
Processed prompts:  90%|█████████ | 7402/8192 [01:06<00:08, 90.29it/s, est. speed input: 113670.27 toks/s, output: 111.01 toks/s]
Processed prompts:  91%|█████████ | 7466/8192 [01:07<00:08, 90.06it/s, est. speed input: 113436.87 toks/s, output: 110.78 toks/s]
Processed prompts:  92%|█████████▏| 7530/8192 [01:08<00:07, 89.79it/s, est. speed input: 113203.68 toks/s, output: 110.55 toks/s]
Processed prompts:  93%|█████████▎| 7594/8192 [01:08<00:06, 89.76it/s, est. speed input: 112982.04 toks/s, output: 110.33 toks/s]
Processed prompts:  93%|█████████▎| 7658/8192 [01:09<00:05, 89.69it/s, est. speed input: 112763.07 toks/s, output: 110.12 toks/s]
Processed prompts:  94%|█████████▍| 7722/8192 [01:10<00:05, 89.65it/s, est. speed input: 112549.01 toks/s, output: 109.91 toks/s]
Processed prompts:  95%|█████████▌| 7786/8192 [01:10<00:04, 89.54it/s, est. speed input: 112335.72 toks/s, output: 109.70 toks/s]
Processed prompts:  96%|█████████▌| 7850/8192 [01:11<00:03, 89.60it/s, est. speed input: 112132.31 toks/s, output: 109.50 toks/s]
Processed prompts:  97%|█████████▋| 7914/8192 [01:12<00:03, 89.55it/s, est. speed input: 111929.34 toks/s, output: 109.31 toks/s]
Processed prompts:  97%|█████████▋| 7978/8192 [01:13<00:02, 89.51it/s, est. speed input: 111729.80 toks/s, output: 109.11 toks/s]
Processed prompts:  98%|█████████▊| 8042/8192 [01:13<00:01, 89.39it/s, est. speed input: 111530.66 toks/s, output: 108.92 toks/s]
Processed prompts:  99%|█████████▉| 8106/8192 [01:14<00:00, 90.03it/s, est. speed input: 111363.89 toks/s, output: 108.75 toks/s]
Processed prompts: 100%|█████████▉| 8170/8192 [01:14<00:00, 111.83it/s, est. speed input: 111869.55 toks/s, output: 109.25 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:14<00:00, 111.83it/s, est. speed input: 112169.69 toks/s, output: 109.54 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:14<00:00, 109.54it/s, est. speed input: 112169.69 toks/s, output: 109.54 toks/s]
[rank0]:[W125 16:07:56.840251659 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

======================================================================
收到中断信号 (signal 2)
======================================================================
[INFO] 状态已保存: /root/vllmbench/slidesparse/tools/prepare_bench_20260125_154217_status.json
