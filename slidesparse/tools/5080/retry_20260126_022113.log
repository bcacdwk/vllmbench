
======================================================================
Test: qwen2.5-7b-int8 | cublaslt | prefill | M=32768
Attempt: 1/3
GPU Mem Util: 0.50
Time: 2026-01-26 02:21:50
Duration: 35.8s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cublaslt --M 32768 --gpu-mem 0.5 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022117.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuBLASLt [INT32 output]                         â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:21:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=834366)[0;0m WARNING 01-26 02:21:45 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/tmp/torchinductor_root/kd/ckdstq2kdp6ahdss75n3ubb4qomtfqyvtza4562h56rn5447pvuf.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m ERROR 01-26 02:21:47 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1018.94 MiB is free. Including non-PyTorch memory, this process has 14.45 GiB memory in use. Of the allocated memory 13.67 GiB is allocated by PyTorch, and 422.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:21:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:21:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:21:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:21:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:21:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:21:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:21:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:21:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:21:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:21:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:21:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:21:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:21:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:21:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:21:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:21:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:21:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=834366)[0;0m [2026-01-26 02:21:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=834366)[0;0m [2026-01-26 02:21:40] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=834366)[0;0m [2026-01-26 02:21:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=834366)[0;0m [2026-01-26 02:21:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=834366)[0;0m [2026-01-26 02:21:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
[0;36m(EngineCore_DP0 pid=834366)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=834366)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=834366)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.44it/s]
[0;36m(EngineCore_DP0 pid=834366)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=834366)[0;0m 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
[0;36m(EngineCore_DP0 pid=834366)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=834366)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=834366)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=834366)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=834366)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=834366)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=834366)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=834366)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=834366)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=834366)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=834366)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=834366)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=834366)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=834366)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=834366)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=834366)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=834366)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=834366)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=834366)[0;0m     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=834366)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=834366)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=834366)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=834366)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=834366)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=834366)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/tmp/torchinductor_root/kd/ckdstq2kdp6ahdss75n3ubb4qomtfqyvtza4562h56rn5447pvuf.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=834366)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=834366)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=834366)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=834366)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=834366)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=834366)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=834366)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1018.94 MiB is free. Including non-PyTorch memory, this process has 14.45 GiB memory in use. Of the allocated memory 13.67 GiB is allocated by PyTorch, and 422.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:21:48.189721678 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022117.log


======================================================================
Test: qwen2.5-7b-int8 | cublaslt | prefill | M=32768
Attempt: 2/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:22:32
Duration: 35.8s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cublaslt --M 32768 --gpu-mem 0.45 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022159.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuBLASLt [INT32 output]                         â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:22:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=835353)[0;0m WARNING 01-26 02:22:27 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/tmp/torchinductor_root/kd/ckdstq2kdp6ahdss75n3ubb4qomtfqyvtza4562h56rn5447pvuf.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m ERROR 01-26 02:22:29 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1018.94 MiB is free. Including non-PyTorch memory, this process has 14.45 GiB memory in use. Of the allocated memory 13.67 GiB is allocated by PyTorch, and 422.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:22:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:22:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:22:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:22:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:22:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:22:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:22:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:22:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:22:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:22:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:22:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:22:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:22:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:22:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:22:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:22:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=835353)[0;0m [2026-01-26 02:22:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=835353)[0;0m [2026-01-26 02:22:22] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=835353)[0;0m [2026-01-26 02:22:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=835353)[0;0m [2026-01-26 02:22:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=835353)[0;0m [2026-01-26 02:22:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
[0;36m(EngineCore_DP0 pid=835353)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=835353)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=835353)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
[0;36m(EngineCore_DP0 pid=835353)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.50it/s]
[0;36m(EngineCore_DP0 pid=835353)[0;0m 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
[0;36m(EngineCore_DP0 pid=835353)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=835353)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=835353)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=835353)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=835353)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=835353)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=835353)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=835353)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=835353)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=835353)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=835353)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=835353)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=835353)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=835353)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=835353)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=835353)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=835353)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=835353)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=835353)[0;0m     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=835353)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=835353)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=835353)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=835353)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=835353)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=835353)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/tmp/torchinductor_root/kd/ckdstq2kdp6ahdss75n3ubb4qomtfqyvtza4562h56rn5447pvuf.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=835353)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=835353)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=835353)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=835353)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=835353)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=835353)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=835353)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1018.94 MiB is free. Including non-PyTorch memory, this process has 14.45 GiB memory in use. Of the allocated memory 13.67 GiB is allocated by PyTorch, and 422.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:22:30.818828990 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022159.log


======================================================================
Test: qwen2.5-7b-int8 | cublaslt | prefill | M=32768
Attempt: 3/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:23:13
Duration: 35.4s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cublaslt --M 32768 --gpu-mem 0.4 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022240.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuBLASLt [INT32 output]                         â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:22:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=836339)[0;0m WARNING 01-26 02:23:08 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/tmp/torchinductor_root/kd/ckdstq2kdp6ahdss75n3ubb4qomtfqyvtza4562h56rn5447pvuf.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m ERROR 01-26 02:23:10 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1018.94 MiB is free. Including non-PyTorch memory, this process has 14.45 GiB memory in use. Of the allocated memory 13.67 GiB is allocated by PyTorch, and 422.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:22:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:22:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:22:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:22:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:22:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:22:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:22:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:22:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:22:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:23:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:23:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:23:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:23:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:23:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:23:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:23:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:23:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=836339)[0;0m [2026-01-26 02:23:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=836339)[0;0m [2026-01-26 02:23:03] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=836339)[0;0m [2026-01-26 02:23:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=836339)[0;0m [2026-01-26 02:23:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=836339)[0;0m [2026-01-26 02:23:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
[0;36m(EngineCore_DP0 pid=836339)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=836339)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=836339)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
[0;36m(EngineCore_DP0 pid=836339)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=836339)[0;0m 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
[0;36m(EngineCore_DP0 pid=836339)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=836339)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=836339)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=836339)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=836339)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=836339)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=836339)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=836339)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=836339)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=836339)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=836339)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=836339)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=836339)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=836339)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=836339)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=836339)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=836339)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=836339)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=836339)[0;0m     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=836339)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=836339)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=836339)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=836339)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=836339)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=836339)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/tmp/torchinductor_root/kd/ckdstq2kdp6ahdss75n3ubb4qomtfqyvtza4562h56rn5447pvuf.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=836339)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=836339)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=836339)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=836339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=836339)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=836339)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=836339)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1018.94 MiB is free. Including non-PyTorch memory, this process has 14.45 GiB memory in use. Of the allocated memory 13.67 GiB is allocated by PyTorch, and 422.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:23:11.145318371 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022240.log


======================================================================
Test: qwen2.5-7b-int8 | cublaslt | prefill | M=65536
Attempt: 1/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:24:15
Duration: 51.2s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cublaslt --M 65536 --gpu-mem 0.45 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022327.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuBLASLt [INT32 output]                         â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:23:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=837599)[0;0m WARNING 01-26 02:24:09 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=837599)[0;0m ERROR 01-26 02:24:12 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.84 GiB is free. Including non-PyTorch memory, this process has 11.61 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 902.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:23:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:23:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:23:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:23:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:23:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:23:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:23:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:23:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:23:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:24:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:24:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:24:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:24:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:24:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:24:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:24:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:24:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=837599)[0;0m [2026-01-26 02:24:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=837599)[0;0m [2026-01-26 02:24:04] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=837599)[0;0m [2026-01-26 02:24:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=837599)[0;0m [2026-01-26 02:24:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=837599)[0;0m [2026-01-26 02:24:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
[0;36m(EngineCore_DP0 pid=837599)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=837599)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=837599)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
[0;36m(EngineCore_DP0 pid=837599)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=837599)[0;0m 
[0;36m(EngineCore_DP0 pid=837599)[0;0m [2026-01-26 02:24:11] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=4608, K=3584), falling back to default heuristic
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
[0;36m(EngineCore_DP0 pid=837599)[0;0m [rank0]:W0126 02:24:12.459000 837599 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=837599)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=837599)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=837599)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=837599)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=837599)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=837599)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=837599)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=837599)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=837599)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=837599)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=837599)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=837599)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=837599)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=837599)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=837599)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=837599)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=837599)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=837599)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=837599)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=837599)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=837599)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=837599)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=837599)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=837599)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=837599)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=837599)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=837599)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=837599)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=837599)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=837599)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=837599)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=837599)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=837599)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=837599)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=837599)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=837599)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=837599)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.84 GiB is free. Including non-PyTorch memory, this process has 11.61 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 902.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:24:13.849614292 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022327.log


======================================================================
Test: qwen2.5-7b-int8 | cublaslt | prefill | M=65536
Attempt: 2/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:25:12
Duration: 51.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cublaslt --M 65536 --gpu-mem 0.4 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022424.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuBLASLt [INT32 output]                         â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:24:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=838793)[0;0m WARNING 01-26 02:25:06 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=838793)[0;0m ERROR 01-26 02:25:09 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.84 GiB is free. Including non-PyTorch memory, this process has 11.61 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 902.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:24:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:24:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:24:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:24:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:24:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:24:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:24:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:24:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:24:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:25:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:25:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:25:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:25:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=838793)[0;0m [2026-01-26 02:25:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=838793)[0;0m [2026-01-26 02:25:01] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=838793)[0;0m [2026-01-26 02:25:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=838793)[0;0m [2026-01-26 02:25:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=838793)[0;0m [2026-01-26 02:25:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
[0;36m(EngineCore_DP0 pid=838793)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=838793)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=838793)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
[0;36m(EngineCore_DP0 pid=838793)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.50it/s]
[0;36m(EngineCore_DP0 pid=838793)[0;0m 
[0;36m(EngineCore_DP0 pid=838793)[0;0m [2026-01-26 02:25:08] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=4608, K=3584), falling back to default heuristic
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
[0;36m(EngineCore_DP0 pid=838793)[0;0m [rank0]:W0126 02:25:09.443000 838793 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=838793)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=838793)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=838793)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=838793)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=838793)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=838793)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=838793)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=838793)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=838793)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=838793)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=838793)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=838793)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=838793)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=838793)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=838793)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=838793)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=838793)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=838793)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=838793)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=838793)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=838793)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=838793)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=838793)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=838793)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=838793)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=838793)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=838793)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=838793)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=838793)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=838793)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=838793)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=838793)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=838793)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=838793)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=838793)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=838793)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=838793)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.84 GiB is free. Including non-PyTorch memory, this process has 11.61 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 902.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:25:10.857727125 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022424.log


======================================================================
Test: qwen2.5-7b-int8 | cublaslt | prefill | M=65536
Attempt: 3/3
GPU Mem Util: 0.35
Time: 2026-01-26 02:26:09
Duration: 51.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cublaslt --M 65536 --gpu-mem 0.35000000000000003 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.35000000000000003

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022521.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuBLASLt [INT32 output]                         â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:25:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=839994)[0;0m WARNING 01-26 02:26:03 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=839994)[0;0m ERROR 01-26 02:26:06 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.84 GiB is free. Including non-PyTorch memory, this process has 11.61 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 902.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:25:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:25:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:25:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:25:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:25:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:25:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:25:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:25:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:25:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=839994)[0;0m [2026-01-26 02:25:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=839994)[0;0m [2026-01-26 02:25:58] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=839994)[0;0m [2026-01-26 02:25:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=839994)[0;0m [2026-01-26 02:25:58] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=839994)[0;0m [2026-01-26 02:25:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
[0;36m(EngineCore_DP0 pid=839994)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=839994)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=839994)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
[0;36m(EngineCore_DP0 pid=839994)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.50it/s]
[0;36m(EngineCore_DP0 pid=839994)[0;0m 
[0;36m(EngineCore_DP0 pid=839994)[0;0m [2026-01-26 02:26:05] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=4608, K=3584), falling back to default heuristic
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
[0;36m(EngineCore_DP0 pid=839994)[0;0m [rank0]:W0126 02:26:06.473000 839994 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=839994)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=839994)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=839994)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=839994)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=839994)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=839994)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=839994)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=839994)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=839994)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=839994)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=839994)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=839994)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=839994)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=839994)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=839994)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=839994)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=839994)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=839994)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=839994)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=839994)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=839994)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=839994)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=839994)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=839994)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=839994)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=839994)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=839994)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=839994)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=839994)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=839994)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=839994)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=839994)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=839994)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=839994)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=839994)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=839994)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=839994)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.84 GiB is free. Including non-PyTorch memory, this process has 11.61 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 902.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:26:07.897654748 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022521.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_4) | prefill | M=32768
Attempt: 1/3
GPU Mem Util: 0.50
Time: 2026-01-26 02:26:56
Duration: 36.2s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.5 --gpu-id 1 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022623.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:26:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=841067)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=841067)[0;0m WARNING 01-26 02:26:51 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/tmp/torchinductor_root/uh/cuhjk66tpkacyjr3tunftqid3hw564ybbxhlbtqhxdd2sqoz7wog.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m ERROR 01-26 02:26:53 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 13.92 GiB memory in use. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:26:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:26:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:26:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:26:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:26:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:26:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:26:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:26:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:26:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:26:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:26:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:26:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:26:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:26:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:26:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:26:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:26:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=841067)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=841067)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=841067)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.43it/s]
[0;36m(EngineCore_DP0 pid=841067)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.48it/s]
[0;36m(EngineCore_DP0 pid=841067)[0;0m 
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
[0;36m(EngineCore_DP0 pid=841067)[0;0m [2026-01-26 02:26:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
[0;36m(EngineCore_DP0 pid=841067)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=841067)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=841067)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=841067)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=841067)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=841067)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=841067)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=841067)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=841067)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=841067)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=841067)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=841067)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=841067)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=841067)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=841067)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=841067)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=841067)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=841067)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=841067)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=841067)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=841067)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=841067)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=841067)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=841067)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=841067)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/tmp/torchinductor_root/uh/cuhjk66tpkacyjr3tunftqid3hw564ybbxhlbtqhxdd2sqoz7wog.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=841067)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=841067)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=841067)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=841067)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=841067)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=841067)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=841067)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 13.92 GiB memory in use. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:26:54.107903890 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022623.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_4) | prefill | M=32768
Attempt: 2/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:27:38
Duration: 36.6s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022705.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:27:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=842063)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=842063)[0;0m WARNING 01-26 02:27:33 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/tmp/torchinductor_root/uh/cuhjk66tpkacyjr3tunftqid3hw564ybbxhlbtqhxdd2sqoz7wog.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m ERROR 01-26 02:27:36 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 13.92 GiB memory in use. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:27:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:27:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:27:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:27:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:27:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:27:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:27:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:27:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:27:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:27:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:27:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:27:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:27:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:27:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:27:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:27:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:27:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=842063)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=842063)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=842063)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.44it/s]
[0;36m(EngineCore_DP0 pid=842063)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=842063)[0;0m 
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
[0;36m(EngineCore_DP0 pid=842063)[0;0m [2026-01-26 02:27:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
[0;36m(EngineCore_DP0 pid=842063)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=842063)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=842063)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=842063)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=842063)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=842063)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=842063)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=842063)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=842063)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=842063)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=842063)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=842063)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=842063)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=842063)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=842063)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=842063)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=842063)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=842063)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=842063)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=842063)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=842063)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=842063)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=842063)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=842063)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=842063)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/tmp/torchinductor_root/uh/cuhjk66tpkacyjr3tunftqid3hw564ybbxhlbtqhxdd2sqoz7wog.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=842063)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=842063)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=842063)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=842063)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=842063)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=842063)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=842063)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 13.92 GiB memory in use. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:27:36.536848889 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022705.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_4) | prefill | M=32768
Attempt: 3/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:28:21
Duration: 36.4s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022747.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:28:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=843056)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=843056)[0;0m WARNING 01-26 02:28:16 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/tmp/torchinductor_root/uh/cuhjk66tpkacyjr3tunftqid3hw564ybbxhlbtqhxdd2sqoz7wog.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m ERROR 01-26 02:28:18 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 13.92 GiB memory in use. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:28:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:28:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:28:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:28:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:28:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:28:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:28:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:28:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:28:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:28:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:28:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:28:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:28:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:28:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:28:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:28:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:28:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=843056)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=843056)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=843056)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.44it/s]
[0;36m(EngineCore_DP0 pid=843056)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=843056)[0;0m 
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
[0;36m(EngineCore_DP0 pid=843056)[0;0m [2026-01-26 02:28:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
[0;36m(EngineCore_DP0 pid=843056)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=843056)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=843056)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=843056)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=843056)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=843056)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=843056)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=843056)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=843056)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=843056)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=843056)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=843056)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=843056)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=843056)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=843056)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=843056)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=843056)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=843056)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=843056)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=843056)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=843056)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=843056)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=843056)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=843056)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=843056)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/tmp/torchinductor_root/uh/cuhjk66tpkacyjr3tunftqid3hw564ybbxhlbtqhxdd2sqoz7wog.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=843056)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=843056)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=843056)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=843056)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=843056)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=843056)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=843056)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 13.92 GiB memory in use. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:28:18.678045391 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022747.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_4) | prefill | M=65536
Attempt: 1/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:29:23
Duration: 51.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022834.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:29:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=844327)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=844327)[0;0m WARNING 01-26 02:29:17 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=844327)[0;0m ERROR 01-26 02:29:20 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.43 GiB is free. Including non-PyTorch memory, this process has 11.01 GiB memory in use. Of the allocated memory 8.86 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:29:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:29:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:29:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:29:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:29:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:29:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:29:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:29:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:29:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:29:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:29:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:29:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:29:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:29:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:29:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:29:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:29:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=844327)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=844327)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=844327)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.44it/s]
[0;36m(EngineCore_DP0 pid=844327)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=844327)[0;0m 
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
[0;36m(EngineCore_DP0 pid=844327)[0;0m [2026-01-26 02:29:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
[0;36m(EngineCore_DP0 pid=844327)[0;0m [rank0]:W0126 02:29:20.317000 844327 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=844327)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=844327)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=844327)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=844327)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=844327)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=844327)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=844327)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=844327)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=844327)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=844327)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=844327)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=844327)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=844327)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=844327)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=844327)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=844327)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=844327)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=844327)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=844327)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=844327)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=844327)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=844327)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=844327)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=844327)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=844327)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=844327)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=844327)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=844327)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=844327)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=844327)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=844327)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=844327)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=844327)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=844327)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=844327)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=844327)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=844327)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.43 GiB is free. Including non-PyTorch memory, this process has 11.01 GiB memory in use. Of the allocated memory 8.86 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:29:20.717326229 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022834.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_4) | prefill | M=65536
Attempt: 2/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:30:19
Duration: 51.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022931.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:30:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=845519)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=845519)[0;0m WARNING 01-26 02:30:14 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=845519)[0;0m ERROR 01-26 02:30:17 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.43 GiB is free. Including non-PyTorch memory, this process has 11.01 GiB memory in use. Of the allocated memory 8.86 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:30:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:30:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:30:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:30:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:30:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:30:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:30:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:30:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:30:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:30:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:30:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:30:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:30:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:30:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:30:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:30:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:30:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=845519)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=845519)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=845519)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.44it/s]
[0;36m(EngineCore_DP0 pid=845519)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=845519)[0;0m 
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
[0;36m(EngineCore_DP0 pid=845519)[0;0m [2026-01-26 02:30:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
[0;36m(EngineCore_DP0 pid=845519)[0;0m [rank0]:W0126 02:30:17.088000 845519 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=845519)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=845519)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=845519)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=845519)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=845519)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=845519)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=845519)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=845519)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=845519)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=845519)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=845519)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=845519)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=845519)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=845519)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=845519)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=845519)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=845519)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=845519)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=845519)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=845519)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=845519)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=845519)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=845519)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=845519)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=845519)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=845519)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=845519)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=845519)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=845519)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=845519)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=845519)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=845519)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=845519)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=845519)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=845519)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=845519)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=845519)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.43 GiB is free. Including non-PyTorch memory, this process has 11.01 GiB memory in use. Of the allocated memory 8.86 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:30:17.487069560 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_022931.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_4) | prefill | M=65536
Attempt: 3/3
GPU Mem Util: 0.35
Time: 2026-01-26 02:31:16
Duration: 51.2s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.35000000000000003 --gpu-id 1 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.35000000000000003

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023028.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:31:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=846711)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=846711)[0;0m WARNING 01-26 02:31:11 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=846711)[0;0m ERROR 01-26 02:31:14 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.43 GiB is free. Including non-PyTorch memory, this process has 11.01 GiB memory in use. Of the allocated memory 8.86 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:31:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:31:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:31:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:31:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=846711)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=846711)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=846711)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.44it/s]
[0;36m(EngineCore_DP0 pid=846711)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=846711)[0;0m 
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
[0;36m(EngineCore_DP0 pid=846711)[0;0m [2026-01-26 02:31:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
[0;36m(EngineCore_DP0 pid=846711)[0;0m [rank0]:W0126 02:31:14.070000 846711 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=846711)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=846711)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=846711)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=846711)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=846711)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=846711)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=846711)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=846711)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=846711)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=846711)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=846711)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=846711)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=846711)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=846711)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=846711)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=846711)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=846711)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=846711)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=846711)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=846711)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=846711)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=846711)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=846711)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=846711)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=846711)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=846711)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=846711)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=846711)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=846711)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=846711)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=846711)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=846711)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=846711)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=846711)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=846711)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=846711)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=846711)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.43 GiB is free. Including non-PyTorch memory, this process has 11.01 GiB memory in use. Of the allocated memory 8.86 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:31:14.462215342 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023028.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=8192
Attempt: 1/3
GPU Mem Util: 0.70
Time: 2026-01-26 02:32:45
Duration: 78.0s
Success: True
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 8192 --gpu-mem 0.7 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [8192]
  M_decode:         [8192]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.7

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023131.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=8192
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 8192
â”‚   M_prefill     = 8192 (= 8 x 1024)
â”‚   M_decode      = 8
â”‚   batched_tokens = 8192 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 1024
â”‚   --max-num-seqs           = 8
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 8192
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:31:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=847658)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=847658)[0;0m WARNING 01-26 02:31:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 20.66 requests/s, 21175.56 total tokens/s, 20.66 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:31:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:31:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:31:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:31:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=847658)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=847658)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=847658)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
[0;36m(EngineCore_DP0 pid=847658)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
[0;36m(EngineCore_DP0 pid=847658)[0;0m 
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=847658)[0;0m [2026-01-26 02:31:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
[0;36m(EngineCore_DP0 pid=847658)[0;0m 2026-01-26 02:31:52,759 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=847658)[0;0m 2026-01-26 02:31:52,774 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=847658)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:00,  5.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 17.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 15.02it/s]
[0;36m(EngineCore_DP0 pid=847658)[0;0m 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 17.37it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   5%|â–         | 49/1024 [00:00<00:02, 485.31it/s]
Adding requests:  10%|â–ˆ         | 104/1024 [00:00<00:01, 517.96it/s]
Adding requests:  16%|â–ˆâ–Œ        | 162/1024 [00:00<00:01, 544.52it/s]
Adding requests:  22%|â–ˆâ–ˆâ–       | 222/1024 [00:00<00:01, 561.77it/s]
Adding requests:  27%|â–ˆâ–ˆâ–‹       | 279/1024 [00:00<00:01, 564.41it/s]
Adding requests:  33%|â–ˆâ–ˆâ–ˆâ–      | 339/1024 [00:00<00:01, 575.74it/s]
Adding requests:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 401/1024 [00:00<00:01, 589.23it/s]
Adding requests:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 460/1024 [00:00<00:00, 589.45it/s]
Adding requests:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 528/1024 [00:00<00:00, 615.21it/s]
Adding requests:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 590/1024 [00:01<00:00, 595.67it/s]
Adding requests:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 650/1024 [00:01<00:00, 592.64it/s]
Adding requests:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 711/1024 [00:01<00:00, 596.09it/s]
Adding requests:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 771/1024 [00:01<00:00, 581.80it/s]
Adding requests:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 831/1024 [00:01<00:00, 586.58it/s]
Adding requests:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 890/1024 [00:01<00:00, 584.63it/s]
Adding requests:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1024 [00:01<00:00, 579.61it/s]
Adding requests:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1009/1024 [00:01<00:00, 584.15it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:01<00:00, 580.69it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|â–         | 34/1024 [00:00<00:08, 120.44it/s, est. speed input: 123334.48 toks/s, output: 120.44 toks/s]
Processed prompts:   5%|â–         | 47/1024 [00:00<00:15, 63.20it/s, est. speed input: 72157.67 toks/s, output: 70.47 toks/s]   
Processed prompts:   5%|â–Œ         | 54/1024 [00:01<00:23, 41.35it/s, est. speed input: 52583.26 toks/s, output: 51.35 toks/s]
Processed prompts:   6%|â–Œ         | 59/1024 [00:01<00:32, 29.73it/s, est. speed input: 42059.15 toks/s, output: 41.07 toks/s]
Processed prompts:   6%|â–‹         | 66/1024 [00:01<00:37, 25.47it/s, est. speed input: 37103.56 toks/s, output: 36.23 toks/s]
Processed prompts:   7%|â–‹         | 74/1024 [00:02<00:39, 23.85it/s, est. speed input: 34342.11 toks/s, output: 33.54 toks/s]
Processed prompts:   8%|â–Š         | 82/1024 [00:02<00:41, 22.84it/s, est. speed input: 32404.19 toks/s, output: 31.64 toks/s]
Processed prompts:   9%|â–‰         | 90/1024 [00:02<00:42, 22.17it/s, est. speed input: 30961.15 toks/s, output: 30.24 toks/s]
Processed prompts:  10%|â–‰         | 98/1024 [00:03<00:42, 21.74it/s, est. speed input: 29852.95 toks/s, output: 29.15 toks/s]
Processed prompts:  10%|â–ˆ         | 106/1024 [00:03<00:42, 21.43it/s, est. speed input: 28969.36 toks/s, output: 28.29 toks/s]
Processed prompts:  11%|â–ˆ         | 114/1024 [00:04<00:42, 21.23it/s, est. speed input: 28250.41 toks/s, output: 27.59 toks/s]
Processed prompts:  12%|â–ˆâ–        | 122/1024 [00:04<00:42, 21.08it/s, est. speed input: 27652.91 toks/s, output: 27.00 toks/s]
Processed prompts:  13%|â–ˆâ–        | 130/1024 [00:04<00:42, 20.98it/s, est. speed input: 27150.07 toks/s, output: 26.51 toks/s]
Processed prompts:  13%|â–ˆâ–        | 138/1024 [00:05<00:42, 20.91it/s, est. speed input: 26718.17 toks/s, output: 26.09 toks/s]
Processed prompts:  14%|â–ˆâ–        | 146/1024 [00:05<00:42, 20.86it/s, est. speed input: 26345.45 toks/s, output: 25.73 toks/s]
Processed prompts:  15%|â–ˆâ–Œ        | 154/1024 [00:06<00:41, 20.82it/s, est. speed input: 26019.34 toks/s, output: 25.41 toks/s]
Processed prompts:  16%|â–ˆâ–Œ        | 162/1024 [00:06<00:41, 20.79it/s, est. speed input: 25732.18 toks/s, output: 25.13 toks/s]
Processed prompts:  17%|â–ˆâ–‹        | 170/1024 [00:06<00:41, 20.77it/s, est. speed input: 25478.20 toks/s, output: 24.88 toks/s]
Processed prompts:  17%|â–ˆâ–‹        | 178/1024 [00:07<00:40, 20.76it/s, est. speed input: 25251.17 toks/s, output: 24.66 toks/s]
Processed prompts:  18%|â–ˆâ–Š        | 186/1024 [00:07<00:40, 20.75it/s, est. speed input: 25046.60 toks/s, output: 24.46 toks/s]
Processed prompts:  19%|â–ˆâ–‰        | 194/1024 [00:07<00:40, 20.74it/s, est. speed input: 24861.77 toks/s, output: 24.28 toks/s]
Processed prompts:  20%|â–ˆâ–‰        | 202/1024 [00:08<00:39, 20.74it/s, est. speed input: 24693.86 toks/s, output: 24.12 toks/s]
Processed prompts:  21%|â–ˆâ–ˆ        | 210/1024 [00:08<00:39, 20.73it/s, est. speed input: 24541.26 toks/s, output: 23.97 toks/s]
Processed prompts:  21%|â–ˆâ–ˆâ–       | 218/1024 [00:09<00:38, 20.73it/s, est. speed input: 24401.24 toks/s, output: 23.83 toks/s]
Processed prompts:  22%|â–ˆâ–ˆâ–       | 226/1024 [00:09<00:38, 20.73it/s, est. speed input: 24272.40 toks/s, output: 23.70 toks/s]
Processed prompts:  23%|â–ˆâ–ˆâ–       | 234/1024 [00:09<00:38, 20.73it/s, est. speed input: 24153.61 toks/s, output: 23.59 toks/s]
Processed prompts:  24%|â–ˆâ–ˆâ–       | 242/1024 [00:10<00:37, 20.72it/s, est. speed input: 24042.71 toks/s, output: 23.48 toks/s]
Processed prompts:  24%|â–ˆâ–ˆâ–       | 250/1024 [00:10<00:37, 20.71it/s, est. speed input: 23939.67 toks/s, output: 23.38 toks/s]
Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 258/1024 [00:11<00:36, 20.71it/s, est. speed input: 23844.70 toks/s, output: 23.29 toks/s]
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 266/1024 [00:11<00:36, 20.72it/s, est. speed input: 23756.27 toks/s, output: 23.20 toks/s]
Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 274/1024 [00:11<00:36, 20.72it/s, est. speed input: 23673.65 toks/s, output: 23.12 toks/s]
Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 282/1024 [00:12<00:35, 20.71it/s, est. speed input: 23595.54 toks/s, output: 23.04 toks/s]
Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 290/1024 [00:12<00:35, 20.72it/s, est. speed input: 23522.76 toks/s, output: 22.97 toks/s]
Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 298/1024 [00:13<00:35, 20.72it/s, est. speed input: 23454.43 toks/s, output: 22.90 toks/s]
Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 306/1024 [00:13<00:34, 20.71it/s, est. speed input: 23388.59 toks/s, output: 22.84 toks/s]
Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 314/1024 [00:13<00:34, 20.71it/s, est. speed input: 23327.30 toks/s, output: 22.78 toks/s]
Processed prompts:  31%|â–ˆâ–ˆâ–ˆâ–      | 322/1024 [00:14<00:33, 20.70it/s, est. speed input: 23268.93 toks/s, output: 22.72 toks/s]
Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 330/1024 [00:14<00:33, 20.70it/s, est. speed input: 23213.66 toks/s, output: 22.67 toks/s]
Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–      | 338/1024 [00:14<00:33, 20.70it/s, est. speed input: 23161.72 toks/s, output: 22.62 toks/s]
Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 346/1024 [00:15<00:32, 20.70it/s, est. speed input: 23111.93 toks/s, output: 22.57 toks/s]
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–      | 354/1024 [00:15<00:32, 20.70it/s, est. speed input: 23065.06 toks/s, output: 22.52 toks/s]
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1024 [00:16<00:31, 20.70it/s, est. speed input: 23020.34 toks/s, output: 22.48 toks/s]
Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 370/1024 [00:16<00:31, 20.69it/s, est. speed input: 22976.85 toks/s, output: 22.44 toks/s]
Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 378/1024 [00:16<00:31, 20.70it/s, est. speed input: 22936.09 toks/s, output: 22.40 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1024 [00:17<00:30, 20.69it/s, est. speed input: 22896.59 toks/s, output: 22.36 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 394/1024 [00:17<00:30, 20.69it/s, est. speed input: 22859.14 toks/s, output: 22.32 toks/s]
Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 402/1024 [00:18<00:30, 20.69it/s, est. speed input: 22823.59 toks/s, output: 22.29 toks/s]
Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1024 [00:18<00:29, 20.69it/s, est. speed input: 22789.39 toks/s, output: 22.26 toks/s]
Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 418/1024 [00:18<00:29, 20.69it/s, est. speed input: 22756.44 toks/s, output: 22.22 toks/s]
Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 426/1024 [00:19<00:28, 20.69it/s, est. speed input: 22724.89 toks/s, output: 22.19 toks/s]
Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 434/1024 [00:19<00:28, 20.69it/s, est. speed input: 22694.15 toks/s, output: 22.16 toks/s]
Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1024 [00:19<00:28, 20.68it/s, est. speed input: 22664.52 toks/s, output: 22.13 toks/s]
Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 450/1024 [00:20<00:27, 20.68it/s, est. speed input: 22636.31 toks/s, output: 22.11 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 458/1024 [00:20<00:27, 20.68it/s, est. speed input: 22609.20 toks/s, output: 22.08 toks/s]
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 466/1024 [00:21<00:26, 20.68it/s, est. speed input: 22583.08 toks/s, output: 22.05 toks/s]
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1024 [00:21<00:26, 20.68it/s, est. speed input: 22557.60 toks/s, output: 22.03 toks/s]
Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 482/1024 [00:21<00:26, 20.68it/s, est. speed input: 22533.45 toks/s, output: 22.01 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 490/1024 [00:22<00:25, 20.68it/s, est. speed input: 22510.02 toks/s, output: 21.98 toks/s]
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 498/1024 [00:22<00:25, 20.68it/s, est. speed input: 22486.96 toks/s, output: 21.96 toks/s]
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 506/1024 [00:23<00:25, 20.68it/s, est. speed input: 22464.98 toks/s, output: 21.94 toks/s]
Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 514/1024 [00:23<00:24, 20.69it/s, est. speed input: 22444.14 toks/s, output: 21.92 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 522/1024 [00:23<00:24, 20.69it/s, est. speed input: 22423.61 toks/s, output: 21.90 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 530/1024 [00:24<00:23, 20.68it/s, est. speed input: 22403.70 toks/s, output: 21.88 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1024 [00:24<00:23, 20.69it/s, est. speed input: 22384.59 toks/s, output: 21.86 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1024 [00:24<00:23, 20.68it/s, est. speed input: 22365.91 toks/s, output: 21.84 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 554/1024 [00:25<00:22, 20.68it/s, est. speed input: 22347.61 toks/s, output: 21.82 toks/s]
Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 562/1024 [00:25<00:22, 20.68it/s, est. speed input: 22329.97 toks/s, output: 21.81 toks/s]
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 570/1024 [00:26<00:21, 20.68it/s, est. speed input: 22312.83 toks/s, output: 21.79 toks/s]
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 578/1024 [00:26<00:21, 20.68it/s, est. speed input: 22296.34 toks/s, output: 21.77 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 586/1024 [00:26<00:21, 20.68it/s, est. speed input: 22280.11 toks/s, output: 21.76 toks/s]
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 594/1024 [00:27<00:20, 20.67it/s, est. speed input: 22264.17 toks/s, output: 21.74 toks/s]
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 602/1024 [00:27<00:20, 20.67it/s, est. speed input: 22248.72 toks/s, output: 21.73 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 610/1024 [00:28<00:20, 20.67it/s, est. speed input: 22233.97 toks/s, output: 21.71 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 618/1024 [00:28<00:19, 20.67it/s, est. speed input: 22219.45 toks/s, output: 21.70 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 626/1024 [00:28<00:19, 20.67it/s, est. speed input: 22205.26 toks/s, output: 21.68 toks/s]
Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 634/1024 [00:29<00:18, 20.67it/s, est. speed input: 22191.56 toks/s, output: 21.67 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1024 [00:29<00:18, 20.67it/s, est. speed input: 22178.16 toks/s, output: 21.66 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 650/1024 [00:30<00:18, 20.67it/s, est. speed input: 22165.12 toks/s, output: 21.65 toks/s]
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 658/1024 [00:30<00:17, 20.67it/s, est. speed input: 22152.40 toks/s, output: 21.63 toks/s]
Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 666/1024 [00:30<00:17, 20.66it/s, est. speed input: 22139.77 toks/s, output: 21.62 toks/s]
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 674/1024 [00:31<00:16, 20.67it/s, est. speed input: 22127.74 toks/s, output: 21.61 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 682/1024 [00:31<00:16, 20.66it/s, est. speed input: 22115.79 toks/s, output: 21.60 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 690/1024 [00:31<00:16, 20.66it/s, est. speed input: 22104.03 toks/s, output: 21.59 toks/s]
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 698/1024 [00:32<00:15, 20.67it/s, est. speed input: 22092.90 toks/s, output: 21.58 toks/s]
Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 706/1024 [00:32<00:15, 20.67it/s, est. speed input: 22081.94 toks/s, output: 21.56 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 714/1024 [00:33<00:15, 20.66it/s, est. speed input: 22070.99 toks/s, output: 21.55 toks/s]
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 722/1024 [00:33<00:14, 20.66it/s, est. speed input: 22060.35 toks/s, output: 21.54 toks/s]
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 730/1024 [00:33<00:14, 20.66it/s, est. speed input: 22049.89 toks/s, output: 21.53 toks/s]
Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1024 [00:34<00:13, 20.66it/s, est. speed input: 22039.74 toks/s, output: 21.52 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1024 [00:34<00:13, 20.66it/s, est. speed input: 22029.84 toks/s, output: 21.51 toks/s]
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 754/1024 [00:35<00:13, 20.65it/s, est. speed input: 22019.99 toks/s, output: 21.50 toks/s]
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 762/1024 [00:35<00:12, 20.65it/s, est. speed input: 22010.41 toks/s, output: 21.49 toks/s]
Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 770/1024 [00:35<00:12, 20.65it/s, est. speed input: 22001.00 toks/s, output: 21.49 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 778/1024 [00:36<00:11, 20.65it/s, est. speed input: 21992.06 toks/s, output: 21.48 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 786/1024 [00:36<00:11, 20.66it/s, est. speed input: 21983.22 toks/s, output: 21.47 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 794/1024 [00:36<00:11, 20.66it/s, est. speed input: 21974.74 toks/s, output: 21.46 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 802/1024 [00:37<00:10, 20.66it/s, est. speed input: 21966.21 toks/s, output: 21.45 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 810/1024 [00:37<00:10, 20.66it/s, est. speed input: 21957.75 toks/s, output: 21.44 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 818/1024 [00:38<00:09, 20.66it/s, est. speed input: 21949.62 toks/s, output: 21.44 toks/s]
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 826/1024 [00:38<00:09, 20.66it/s, est. speed input: 21941.81 toks/s, output: 21.43 toks/s]
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 834/1024 [00:38<00:09, 20.66it/s, est. speed input: 21934.00 toks/s, output: 21.42 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1024 [00:39<00:08, 20.66it/s, est. speed input: 21926.38 toks/s, output: 21.41 toks/s]
Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 850/1024 [00:39<00:08, 20.66it/s, est. speed input: 21918.72 toks/s, output: 21.40 toks/s]
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 858/1024 [00:40<00:08, 20.66it/s, est. speed input: 21911.40 toks/s, output: 21.40 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 866/1024 [00:40<00:07, 20.65it/s, est. speed input: 21903.95 toks/s, output: 21.39 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 874/1024 [00:40<00:07, 20.65it/s, est. speed input: 21896.71 toks/s, output: 21.38 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 882/1024 [00:41<00:06, 20.65it/s, est. speed input: 21889.71 toks/s, output: 21.38 toks/s]
Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 890/1024 [00:41<00:06, 20.66it/s, est. speed input: 21882.92 toks/s, output: 21.37 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 898/1024 [00:42<00:06, 20.66it/s, est. speed input: 21876.17 toks/s, output: 21.36 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 906/1024 [00:42<00:05, 20.65it/s, est. speed input: 21869.40 toks/s, output: 21.36 toks/s]
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 914/1024 [00:42<00:05, 20.65it/s, est. speed input: 21862.80 toks/s, output: 21.35 toks/s]
Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 922/1024 [00:43<00:04, 20.65it/s, est. speed input: 21856.37 toks/s, output: 21.34 toks/s]
Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 930/1024 [00:43<00:04, 20.64it/s, est. speed input: 21849.76 toks/s, output: 21.34 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1024 [00:43<00:04, 20.64it/s, est. speed input: 21843.44 toks/s, output: 21.33 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1024 [00:44<00:03, 20.64it/s, est. speed input: 21837.20 toks/s, output: 21.33 toks/s]
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 954/1024 [00:44<00:03, 20.64it/s, est. speed input: 21831.23 toks/s, output: 21.32 toks/s]
Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 962/1024 [00:45<00:03, 20.65it/s, est. speed input: 21825.42 toks/s, output: 21.31 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 970/1024 [00:45<00:02, 20.64it/s, est. speed input: 21819.52 toks/s, output: 21.31 toks/s]
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 978/1024 [00:45<00:02, 20.64it/s, est. speed input: 21813.71 toks/s, output: 21.30 toks/s]
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 986/1024 [00:46<00:01, 20.64it/s, est. speed input: 21808.05 toks/s, output: 21.30 toks/s]
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 994/1024 [00:46<00:01, 20.63it/s, est. speed input: 21802.26 toks/s, output: 21.29 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1002/1024 [00:47<00:01, 20.63it/s, est. speed input: 21796.72 toks/s, output: 21.29 toks/s]
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1010/1024 [00:47<00:00, 20.63it/s, est. speed input: 21791.27 toks/s, output: 21.28 toks/s]
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1018/1024 [00:47<00:00, 21.41it/s, est. speed input: 21807.32 toks/s, output: 21.30 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:47<00:00, 21.41it/s, est. speed input: 21935.80 toks/s, output: 21.42 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:47<00:00, 21.42it/s, est. speed input: 21935.80 toks/s, output: 21.42 toks/s]
[rank0]:[W126 02:32:43.742980244 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[0;32m[SUCCESS][0m æµ‹è¯•å®Œæˆ! è€—æ—¶: 74.0s

[0;32mæµ‹è¯•ç»“æœ:[0m
  Requests/s:   20.66
  Tokens/s:     21175.56
  Total Reqs:   1024
  Elapsed:      49.57s

  [Prefill åˆ†æ]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     21154.90


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
8192,1024,8,1024,128,20.6591,21175.5617,49.5666

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 1 æˆåŠŸ, 0 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m1 æˆåŠŸ[0m, 0 å¤±è´¥
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023131.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=16384
Attempt: 1/3
GPU Mem Util: 0.60
Time: 2026-01-26 02:33:27
Duration: 30.9s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 16384 --gpu-mem 0.6 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [16384]
  M_decode:         [16384]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.6

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023259.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=16384
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 16384
â”‚   M_prefill     = 16384 (= 16 x 1024)
â”‚   M_decode      = 16
â”‚   batched_tokens = 16384 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 2048
â”‚   --max-num-seqs           = 16
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 16384
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:33:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=849361)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=849361)[0;0m WARNING 01-26 02:33:21 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=849361)[0;0m ERROR 01-26 02:33:24 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:33:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:33:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:33:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:33:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:33:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:33:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:33:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:33:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:33:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:33:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:33:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:33:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=849361)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=849361)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.48it/s]
[0;36m(EngineCore_DP0 pid=849361)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
[0;36m(EngineCore_DP0 pid=849361)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
[0;36m(EngineCore_DP0 pid=849361)[0;0m 
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=849361)[0;0m [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
[0;36m(EngineCore_DP0 pid=849361)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=849361)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=849361)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=849361)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=849361)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=849361)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=849361)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=849361)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=849361)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=849361)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=849361)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=849361)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=849361)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=849361)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=849361)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=849361)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=849361)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=849361)[0;0m     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=849361)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=849361)[0;0m   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=849361)[0;0m     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=849361)[0;0m   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=849361)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=849361)[0;0m ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 02:33:25.888069013 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=16384 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16384,1024,16,2048,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023259.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=32768
Attempt: 1/3
GPU Mem Util: 0.50
Time: 2026-01-26 02:34:16
Duration: 38.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.5 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023341.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:33:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=850457)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=850457)[0;0m WARNING 01-26 02:34:10 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     outs = compiled_fn(args)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m ERROR 01-26 02:34:13 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.51 GiB is free. Including non-PyTorch memory, this process has 13.93 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:33:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:33:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:33:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:33:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:33:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:33:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:34:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:34:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:34:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:34:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:34:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:34:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=850457)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=850457)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=850457)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
[0;36m(EngineCore_DP0 pid=850457)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
[0;36m(EngineCore_DP0 pid=850457)[0;0m 
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=850457)[0;0m [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
[0;36m(EngineCore_DP0 pid=850457)[0;0m [rank0]:W0126 02:34:13.481000 850457 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=850457)[0;0m [rank0]:W0126 02:34:13.576000 850457 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=850457)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=850457)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=850457)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=850457)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=850457)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=850457)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=850457)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=850457)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=850457)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=850457)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=850457)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=850457)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=850457)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=850457)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=850457)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=850457)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=850457)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=850457)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=850457)[0;0m     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=850457)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=850457)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=850457)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=850457)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=850457)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
[0;36m(EngineCore_DP0 pid=850457)[0;0m     outs = compiled_fn(args)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=850457)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=850457)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=850457)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=850457)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=850457)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=850457)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=850457)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=850457)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.51 GiB is free. Including non-PyTorch memory, this process has 13.93 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:34:14.933350716 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023341.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=32768
Attempt: 2/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:34:58
Duration: 36.5s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023425.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:34:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=851473)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=851473)[0;0m WARNING 01-26 02:34:53 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m ERROR 01-26 02:34:56 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 13.99 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:34:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:34:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:34:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:34:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:34:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:34:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:34:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:34:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:34:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:34:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:34:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:34:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=851473)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=851473)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.48it/s]
[0;36m(EngineCore_DP0 pid=851473)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
[0;36m(EngineCore_DP0 pid=851473)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
[0;36m(EngineCore_DP0 pid=851473)[0;0m 
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=851473)[0;0m [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
[0;36m(EngineCore_DP0 pid=851473)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=851473)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=851473)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=851473)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=851473)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=851473)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=851473)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=851473)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=851473)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=851473)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=851473)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=851473)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=851473)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=851473)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=851473)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=851473)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=851473)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=851473)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=851473)[0;0m     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=851473)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=851473)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=851473)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=851473)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=851473)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=851473)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=851473)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=851473)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=851473)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=851473)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=851473)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=851473)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=851473)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 13.99 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:34:56.237085113 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023425.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=32768
Attempt: 3/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:35:41
Duration: 36.6s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023507.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:35:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=852458)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=852458)[0;0m WARNING 01-26 02:35:36 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m ERROR 01-26 02:35:38 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 13.99 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:35:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:35:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:35:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:35:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:35:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:35:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:35:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:35:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:35:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:35:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:35:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:35:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:35:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:35:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:35:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:35:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=852458)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=852458)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=852458)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
[0;36m(EngineCore_DP0 pid=852458)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
[0;36m(EngineCore_DP0 pid=852458)[0;0m 
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=852458)[0;0m [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
[0;36m(EngineCore_DP0 pid=852458)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=852458)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=852458)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=852458)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=852458)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=852458)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=852458)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=852458)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=852458)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=852458)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=852458)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=852458)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=852458)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=852458)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=852458)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=852458)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=852458)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=852458)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "<eval_with_key>.58", line 325, in forward
[0;36m(EngineCore_DP0 pid=852458)[0;0m     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=852458)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=852458)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=852458)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=852458)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=852458)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=852458)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=852458)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=852458)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=852458)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=852458)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=852458)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=852458)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=852458)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 13.99 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:35:38.734150720 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023507.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=65536
Attempt: 1/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:36:43
Duration: 52.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023555.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:36:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=853736)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=853736)[0;0m WARNING 01-26 02:36:38 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=853736)[0;0m ERROR 01-26 02:36:41 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:36:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:36:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:36:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:36:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:36:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:36:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:36:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:36:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:36:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:36:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:36:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:36:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:36:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:36:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:36:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:36:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=853736)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=853736)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.48it/s]
[0;36m(EngineCore_DP0 pid=853736)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
[0;36m(EngineCore_DP0 pid=853736)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
[0;36m(EngineCore_DP0 pid=853736)[0;0m 
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=853736)[0;0m [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
[0;36m(EngineCore_DP0 pid=853736)[0;0m [rank0]:W0126 02:36:41.119000 853736 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=853736)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=853736)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=853736)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=853736)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=853736)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=853736)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=853736)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=853736)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=853736)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=853736)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=853736)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=853736)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=853736)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=853736)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=853736)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=853736)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=853736)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=853736)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=853736)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=853736)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=853736)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=853736)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=853736)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=853736)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=853736)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=853736)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=853736)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=853736)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=853736)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=853736)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=853736)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=853736)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=853736)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=853736)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=853736)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=853736)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=853736)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:36:41.524996581 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023555.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=65536
Attempt: 2/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:37:42
Duration: 52.4s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023652.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:37:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=854936)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=854936)[0;0m WARNING 01-26 02:37:36 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=854936)[0;0m ERROR 01-26 02:37:39 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:37:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:37:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:37:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:37:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:37:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:37:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:37:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:37:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:37:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:37:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:37:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:37:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:37:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:37:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:37:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:37:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=854936)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=854936)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.48it/s]
[0;36m(EngineCore_DP0 pid=854936)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
[0;36m(EngineCore_DP0 pid=854936)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
[0;36m(EngineCore_DP0 pid=854936)[0;0m 
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=854936)[0;0m [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
[0;36m(EngineCore_DP0 pid=854936)[0;0m [rank0]:W0126 02:37:39.307000 854936 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=854936)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=854936)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=854936)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=854936)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=854936)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=854936)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=854936)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=854936)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=854936)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=854936)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=854936)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=854936)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=854936)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=854936)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=854936)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=854936)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=854936)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=854936)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=854936)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=854936)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=854936)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=854936)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=854936)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=854936)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=854936)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=854936)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=854936)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=854936)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=854936)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=854936)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=854936)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=854936)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=854936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=854936)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=854936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=854936)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=854936)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:37:39.707966227 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023652.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=65536
Attempt: 3/3
GPU Mem Util: 0.35
Time: 2026-01-26 02:38:40
Duration: 52.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.35000000000000003 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.35000000000000003

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023751.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:38:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=856153)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=856153)[0;0m WARNING 01-26 02:38:34 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=856153)[0;0m ERROR 01-26 02:38:37 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:38:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:38:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:38:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:38:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:38:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:38:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:38:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:38:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:38:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:38:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:38:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:38:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=856153)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=856153)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=856153)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
[0;36m(EngineCore_DP0 pid=856153)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
[0;36m(EngineCore_DP0 pid=856153)[0;0m 
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=856153)[0;0m [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
[0;36m(EngineCore_DP0 pid=856153)[0;0m [rank0]:W0126 02:38:37.458000 856153 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=856153)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=856153)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=856153)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=856153)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=856153)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=856153)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=856153)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=856153)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=856153)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=856153)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=856153)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=856153)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=856153)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=856153)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=856153)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=856153)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=856153)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=856153)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=856153)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=856153)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=856153)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=856153)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=856153)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=856153)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=856153)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=856153)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=856153)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=856153)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=856153)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=856153)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=856153)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=856153)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=856153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=856153)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=856153)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=856153)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=856153)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:38:38.862140672 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023751.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=8192
Attempt: 1/3
GPU Mem Util: 0.70
Time: 2026-01-26 02:40:13
Duration: 82.7s
Success: True
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 8192 --gpu-mem 0.7 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [8192]
  M_decode:         [8192]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.7

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023854.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=8192
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 8192
â”‚   M_prefill     = 8192 (= 8 x 1024)
â”‚   M_decode      = 8
â”‚   batched_tokens = 8192 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 1024
â”‚   --max-num-seqs           = 8
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 8192
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:39:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=857104)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=857104)[0;0m WARNING 01-26 02:39:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 19.40 requests/s, 19880.98 total tokens/s, 19.40 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:39:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:39:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:39:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:39:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:39:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:39:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:39:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:39:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:39:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:39:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:39:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:39:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:39:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:39:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:39:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:39:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=857104)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=857104)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.55s/it]
[0;36m(EngineCore_DP0 pid=857104)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.76s/it]
[0;36m(EngineCore_DP0 pid=857104)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.73s/it]
[0;36m(EngineCore_DP0 pid=857104)[0;0m 
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=857104)[0;0m [2026-01-26 02:39:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=857104)[0;0m 2026-01-26 02:39:17,344 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=857104)[0;0m 2026-01-26 02:39:17,358 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=857104)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:00,  4.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 15.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 13.36it/s]
[0;36m(EngineCore_DP0 pid=857104)[0;0m 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  5.99it/s]
Capturing CUDA graphs (decode, FULL):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 12.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 12.25it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   5%|â–         | 50/1024 [00:00<00:01, 490.54it/s]
Adding requests:  10%|â–ˆ         | 105/1024 [00:00<00:01, 524.98it/s]
Adding requests:  16%|â–ˆâ–Œ        | 162/1024 [00:00<00:01, 543.48it/s]
Adding requests:  22%|â–ˆâ–ˆâ–       | 224/1024 [00:00<00:01, 568.26it/s]
Adding requests:  28%|â–ˆâ–ˆâ–Š       | 282/1024 [00:00<00:01, 570.89it/s]
Adding requests:  33%|â–ˆâ–ˆâ–ˆâ–      | 343/1024 [00:00<00:01, 583.40it/s]
Adding requests:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 405/1024 [00:00<00:01, 593.36it/s]
Adding requests:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 465/1024 [00:00<00:00, 585.25it/s]
Adding requests:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 530/1024 [00:00<00:00, 602.89it/s]
Adding requests:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 591/1024 [00:01<00:00, 593.64it/s]
Adding requests:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 651/1024 [00:01<00:00, 587.94it/s]
Adding requests:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 710/1024 [00:01<00:00, 588.54it/s]
Adding requests:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 769/1024 [00:01<00:00, 582.42it/s]
Adding requests:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 830/1024 [00:01<00:00, 589.29it/s]
Adding requests:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 890/1024 [00:01<00:00, 591.18it/s]
Adding requests:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 950/1024 [00:01<00:00, 575.32it/s]
Adding requests:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1008/1024 [00:01<00:00, 574.92it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:01<00:00, 578.41it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|â–         | 34/1024 [00:00<00:11, 89.39it/s, est. speed input: 91540.33 toks/s, output: 89.39 toks/s]
Processed prompts:   4%|â–         | 43/1024 [00:00<00:20, 48.51it/s, est. speed input: 55717.39 toks/s, output: 54.41 toks/s]
Processed prompts:   5%|â–         | 50/1024 [00:01<00:28, 33.92it/s, est. speed input: 42664.13 toks/s, output: 41.66 toks/s]
Processed prompts:   6%|â–Œ         | 58/1024 [00:01<00:34, 28.18it/s, est. speed input: 36893.02 toks/s, output: 36.03 toks/s]
Processed prompts:   6%|â–‹         | 66/1024 [00:02<00:38, 25.04it/s, est. speed input: 33459.13 toks/s, output: 32.67 toks/s]
Processed prompts:   7%|â–‹         | 74/1024 [00:02<00:41, 23.14it/s, est. speed input: 31181.46 toks/s, output: 30.45 toks/s]
Processed prompts:   8%|â–Š         | 82/1024 [00:02<00:42, 21.95it/s, est. speed input: 29563.97 toks/s, output: 28.87 toks/s]
Processed prompts:   9%|â–‰         | 90/1024 [00:03<00:44, 21.17it/s, est. speed input: 28351.78 toks/s, output: 27.69 toks/s]
Processed prompts:  10%|â–‰         | 98/1024 [00:03<00:44, 20.64it/s, est. speed input: 27410.59 toks/s, output: 26.77 toks/s]
Processed prompts:  10%|â–ˆ         | 106/1024 [00:04<00:45, 20.29it/s, est. speed input: 26659.18 toks/s, output: 26.03 toks/s]
Processed prompts:  11%|â–ˆ         | 114/1024 [00:04<00:45, 20.05it/s, est. speed input: 26046.97 toks/s, output: 25.44 toks/s]
Processed prompts:  12%|â–ˆâ–        | 122/1024 [00:04<00:45, 19.87it/s, est. speed input: 25534.43 toks/s, output: 24.94 toks/s]
Processed prompts:  13%|â–ˆâ–        | 130/1024 [00:05<00:45, 19.75it/s, est. speed input: 25100.17 toks/s, output: 24.51 toks/s]
Processed prompts:  13%|â–ˆâ–        | 138/1024 [00:05<00:45, 19.66it/s, est. speed input: 24728.16 toks/s, output: 24.15 toks/s]
Processed prompts:  14%|â–ˆâ–        | 146/1024 [00:06<00:44, 19.60it/s, est. speed input: 24406.55 toks/s, output: 23.83 toks/s]
Processed prompts:  15%|â–ˆâ–Œ        | 154/1024 [00:06<00:44, 19.56it/s, est. speed input: 24125.83 toks/s, output: 23.56 toks/s]
Processed prompts:  16%|â–ˆâ–Œ        | 162/1024 [00:06<00:44, 19.53it/s, est. speed input: 23876.70 toks/s, output: 23.32 toks/s]
Processed prompts:  17%|â–ˆâ–‹        | 170/1024 [00:07<00:43, 19.51it/s, est. speed input: 23656.19 toks/s, output: 23.10 toks/s]
Processed prompts:  17%|â–ˆâ–‹        | 178/1024 [00:07<00:43, 19.49it/s, est. speed input: 23458.52 toks/s, output: 22.91 toks/s]
Processed prompts:  18%|â–ˆâ–Š        | 186/1024 [00:08<00:43, 19.48it/s, est. speed input: 23281.04 toks/s, output: 22.74 toks/s]
Processed prompts:  19%|â–ˆâ–‰        | 194/1024 [00:08<00:42, 19.47it/s, est. speed input: 23119.55 toks/s, output: 22.58 toks/s]
Processed prompts:  20%|â–ˆâ–‰        | 202/1024 [00:09<00:42, 19.47it/s, est. speed input: 22973.54 toks/s, output: 22.44 toks/s]
Processed prompts:  21%|â–ˆâ–ˆ        | 210/1024 [00:09<00:41, 19.46it/s, est. speed input: 22839.54 toks/s, output: 22.30 toks/s]
Processed prompts:  21%|â–ˆâ–ˆâ–       | 218/1024 [00:09<00:41, 19.46it/s, est. speed input: 22717.15 toks/s, output: 22.18 toks/s]
Processed prompts:  22%|â–ˆâ–ˆâ–       | 226/1024 [00:10<00:41, 19.45it/s, est. speed input: 22604.22 toks/s, output: 22.07 toks/s]
Processed prompts:  23%|â–ˆâ–ˆâ–       | 234/1024 [00:10<00:40, 19.45it/s, est. speed input: 22500.23 toks/s, output: 21.97 toks/s]
Processed prompts:  24%|â–ˆâ–ˆâ–       | 242/1024 [00:11<00:40, 19.45it/s, est. speed input: 22404.17 toks/s, output: 21.88 toks/s]
Processed prompts:  24%|â–ˆâ–ˆâ–       | 250/1024 [00:11<00:39, 19.44it/s, est. speed input: 22314.10 toks/s, output: 21.79 toks/s]
Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 258/1024 [00:11<00:39, 19.45it/s, est. speed input: 22231.36 toks/s, output: 21.71 toks/s]
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 266/1024 [00:12<00:38, 19.45it/s, est. speed input: 22153.64 toks/s, output: 21.63 toks/s]
Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 274/1024 [00:12<00:38, 19.45it/s, est. speed input: 22081.03 toks/s, output: 21.56 toks/s]
Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 282/1024 [00:13<00:38, 19.44it/s, est. speed input: 22012.15 toks/s, output: 21.50 toks/s]
Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 290/1024 [00:13<00:37, 19.44it/s, est. speed input: 21947.80 toks/s, output: 21.43 toks/s]
Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 298/1024 [00:13<00:37, 19.43it/s, est. speed input: 21887.11 toks/s, output: 21.37 toks/s]
Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 306/1024 [00:14<00:36, 19.44it/s, est. speed input: 21830.65 toks/s, output: 21.32 toks/s]
Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 314/1024 [00:14<00:36, 19.44it/s, est. speed input: 21776.97 toks/s, output: 21.27 toks/s]
Processed prompts:  31%|â–ˆâ–ˆâ–ˆâ–      | 322/1024 [00:15<00:36, 19.44it/s, est. speed input: 21726.12 toks/s, output: 21.22 toks/s]
Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 330/1024 [00:15<00:35, 19.43it/s, est. speed input: 21677.66 toks/s, output: 21.17 toks/s]
Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–      | 338/1024 [00:16<00:35, 19.43it/s, est. speed input: 21631.74 toks/s, output: 21.12 toks/s]
Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 346/1024 [00:16<00:34, 19.43it/s, est. speed input: 21588.27 toks/s, output: 21.08 toks/s]
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–      | 354/1024 [00:16<00:34, 19.43it/s, est. speed input: 21547.05 toks/s, output: 21.04 toks/s]
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1024 [00:17<00:34, 19.43it/s, est. speed input: 21507.53 toks/s, output: 21.00 toks/s]
Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 370/1024 [00:17<00:33, 19.43it/s, est. speed input: 21469.66 toks/s, output: 20.97 toks/s]
Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 378/1024 [00:18<00:33, 19.43it/s, est. speed input: 21433.84 toks/s, output: 20.93 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1024 [00:18<00:32, 19.43it/s, est. speed input: 21399.53 toks/s, output: 20.90 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 394/1024 [00:18<00:32, 19.42it/s, est. speed input: 21366.40 toks/s, output: 20.87 toks/s]
Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 402/1024 [00:19<00:32, 19.42it/s, est. speed input: 21334.79 toks/s, output: 20.83 toks/s]
Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1024 [00:19<00:31, 19.42it/s, est. speed input: 21304.58 toks/s, output: 20.81 toks/s]
Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 418/1024 [00:20<00:31, 19.42it/s, est. speed input: 21275.36 toks/s, output: 20.78 toks/s]
Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 426/1024 [00:20<00:30, 19.42it/s, est. speed input: 21247.41 toks/s, output: 20.75 toks/s]
Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 434/1024 [00:20<00:30, 19.42it/s, est. speed input: 21220.43 toks/s, output: 20.72 toks/s]
Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1024 [00:21<00:29, 19.41it/s, est. speed input: 21194.17 toks/s, output: 20.70 toks/s]
Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 450/1024 [00:21<00:29, 19.41it/s, est. speed input: 21169.15 toks/s, output: 20.67 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 458/1024 [00:22<00:29, 19.41it/s, est. speed input: 21145.22 toks/s, output: 20.65 toks/s]
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 466/1024 [00:22<00:28, 19.41it/s, est. speed input: 21122.23 toks/s, output: 20.63 toks/s]
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1024 [00:23<00:28, 19.41it/s, est. speed input: 21100.07 toks/s, output: 20.61 toks/s]
Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 482/1024 [00:23<00:27, 19.42it/s, est. speed input: 21078.67 toks/s, output: 20.58 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 490/1024 [00:23<00:27, 19.42it/s, est. speed input: 21057.99 toks/s, output: 20.56 toks/s]
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 498/1024 [00:24<00:27, 19.41it/s, est. speed input: 21037.73 toks/s, output: 20.54 toks/s]
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 506/1024 [00:24<00:26, 19.41it/s, est. speed input: 21018.38 toks/s, output: 20.53 toks/s]
Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 514/1024 [00:25<00:26, 19.41it/s, est. speed input: 20999.53 toks/s, output: 20.51 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 522/1024 [00:25<00:25, 19.41it/s, est. speed input: 20981.57 toks/s, output: 20.49 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 530/1024 [00:25<00:25, 19.41it/s, est. speed input: 20963.84 toks/s, output: 20.47 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1024 [00:26<00:25, 19.41it/s, est. speed input: 20946.67 toks/s, output: 20.46 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1024 [00:26<00:24, 19.41it/s, est. speed input: 20930.15 toks/s, output: 20.44 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 554/1024 [00:27<00:24, 19.41it/s, est. speed input: 20914.07 toks/s, output: 20.42 toks/s]
Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 562/1024 [00:27<00:23, 19.41it/s, est. speed input: 20898.46 toks/s, output: 20.41 toks/s]
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 570/1024 [00:27<00:23, 19.40it/s, est. speed input: 20883.06 toks/s, output: 20.39 toks/s]
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 578/1024 [00:28<00:22, 19.40it/s, est. speed input: 20868.07 toks/s, output: 20.38 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 586/1024 [00:28<00:22, 19.40it/s, est. speed input: 20853.80 toks/s, output: 20.37 toks/s]
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 594/1024 [00:29<00:22, 19.39it/s, est. speed input: 20839.57 toks/s, output: 20.35 toks/s]
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 602/1024 [00:29<00:21, 19.39it/s, est. speed input: 20825.75 toks/s, output: 20.34 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 610/1024 [00:30<00:21, 19.40it/s, est. speed input: 20812.66 toks/s, output: 20.32 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 618/1024 [00:30<00:20, 19.40it/s, est. speed input: 20800.03 toks/s, output: 20.31 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 626/1024 [00:30<00:20, 19.40it/s, est. speed input: 20787.25 toks/s, output: 20.30 toks/s]
Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 634/1024 [00:31<00:20, 19.40it/s, est. speed input: 20775.13 toks/s, output: 20.29 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1024 [00:31<00:19, 19.39it/s, est. speed input: 20763.10 toks/s, output: 20.28 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 650/1024 [00:32<00:19, 19.39it/s, est. speed input: 20751.32 toks/s, output: 20.26 toks/s]
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 658/1024 [00:32<00:18, 19.39it/s, est. speed input: 20740.10 toks/s, output: 20.25 toks/s]
Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 666/1024 [00:32<00:18, 19.39it/s, est. speed input: 20728.76 toks/s, output: 20.24 toks/s]
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 674/1024 [00:33<00:18, 19.39it/s, est. speed input: 20718.10 toks/s, output: 20.23 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 682/1024 [00:33<00:17, 19.39it/s, est. speed input: 20707.66 toks/s, output: 20.22 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 690/1024 [00:34<00:17, 19.39it/s, est. speed input: 20697.27 toks/s, output: 20.21 toks/s]
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 698/1024 [00:34<00:16, 19.39it/s, est. speed input: 20687.13 toks/s, output: 20.20 toks/s]
Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 706/1024 [00:34<00:16, 19.39it/s, est. speed input: 20677.33 toks/s, output: 20.19 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 714/1024 [00:35<00:15, 19.38it/s, est. speed input: 20667.37 toks/s, output: 20.18 toks/s]
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 722/1024 [00:35<00:15, 19.38it/s, est. speed input: 20657.88 toks/s, output: 20.17 toks/s]
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 730/1024 [00:36<00:15, 19.38it/s, est. speed input: 20648.70 toks/s, output: 20.16 toks/s]
Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1024 [00:36<00:14, 19.38it/s, est. speed input: 20639.53 toks/s, output: 20.16 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1024 [00:37<00:14, 19.38it/s, est. speed input: 20630.75 toks/s, output: 20.15 toks/s]
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 754/1024 [00:37<00:13, 19.38it/s, est. speed input: 20621.97 toks/s, output: 20.14 toks/s]
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 762/1024 [00:37<00:13, 19.38it/s, est. speed input: 20613.42 toks/s, output: 20.13 toks/s]
Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 770/1024 [00:38<00:13, 19.38it/s, est. speed input: 20605.08 toks/s, output: 20.12 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 778/1024 [00:38<00:12, 19.38it/s, est. speed input: 20597.04 toks/s, output: 20.11 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 786/1024 [00:39<00:12, 19.38it/s, est. speed input: 20589.07 toks/s, output: 20.11 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 794/1024 [00:39<00:11, 19.38it/s, est. speed input: 20581.38 toks/s, output: 20.10 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 802/1024 [00:39<00:11, 19.38it/s, est. speed input: 20573.61 toks/s, output: 20.09 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 810/1024 [00:40<00:11, 19.37it/s, est. speed input: 20566.04 toks/s, output: 20.08 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 818/1024 [00:40<00:10, 19.37it/s, est. speed input: 20558.69 toks/s, output: 20.08 toks/s]
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 826/1024 [00:41<00:10, 19.38it/s, est. speed input: 20551.61 toks/s, output: 20.07 toks/s]
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 834/1024 [00:41<00:09, 19.37it/s, est. speed input: 20544.41 toks/s, output: 20.06 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1024 [00:41<00:09, 19.38it/s, est. speed input: 20537.57 toks/s, output: 20.06 toks/s]
Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 850/1024 [00:42<00:08, 19.37it/s, est. speed input: 20530.73 toks/s, output: 20.05 toks/s]
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 858/1024 [00:42<00:08, 19.36it/s, est. speed input: 20523.72 toks/s, output: 20.04 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 866/1024 [00:43<00:08, 19.37it/s, est. speed input: 20517.23 toks/s, output: 20.04 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 874/1024 [00:43<00:07, 19.37it/s, est. speed input: 20510.72 toks/s, output: 20.03 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 882/1024 [00:44<00:07, 19.37it/s, est. speed input: 20504.44 toks/s, output: 20.02 toks/s]
Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 890/1024 [00:44<00:06, 19.37it/s, est. speed input: 20498.22 toks/s, output: 20.02 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 898/1024 [00:44<00:06, 19.37it/s, est. speed input: 20492.13 toks/s, output: 20.01 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 906/1024 [00:45<00:06, 19.37it/s, est. speed input: 20486.15 toks/s, output: 20.01 toks/s]
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 914/1024 [00:45<00:05, 19.36it/s, est. speed input: 20480.00 toks/s, output: 20.00 toks/s]
Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 922/1024 [00:46<00:05, 19.36it/s, est. speed input: 20474.17 toks/s, output: 19.99 toks/s]
Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 930/1024 [00:46<00:04, 19.36it/s, est. speed input: 20468.32 toks/s, output: 19.99 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1024 [00:46<00:04, 19.36it/s, est. speed input: 20462.71 toks/s, output: 19.98 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1024 [00:47<00:04, 19.36it/s, est. speed input: 20457.21 toks/s, output: 19.98 toks/s]
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 954/1024 [00:47<00:03, 19.36it/s, est. speed input: 20451.77 toks/s, output: 19.97 toks/s]
Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 962/1024 [00:48<00:03, 19.36it/s, est. speed input: 20446.35 toks/s, output: 19.97 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 970/1024 [00:48<00:02, 19.36it/s, est. speed input: 20441.16 toks/s, output: 19.96 toks/s]
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 978/1024 [00:49<00:02, 19.36it/s, est. speed input: 20435.93 toks/s, output: 19.96 toks/s]
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 986/1024 [00:49<00:01, 19.36it/s, est. speed input: 20430.69 toks/s, output: 19.95 toks/s]
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 994/1024 [00:49<00:01, 19.35it/s, est. speed input: 20425.55 toks/s, output: 19.95 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1002/1024 [00:50<00:01, 19.35it/s, est. speed input: 20420.57 toks/s, output: 19.94 toks/s]
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1010/1024 [00:50<00:00, 19.35it/s, est. speed input: 20415.59 toks/s, output: 19.94 toks/s]
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1018/1024 [00:51<00:00, 20.08it/s, est. speed input: 20430.68 toks/s, output: 19.95 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:51<00:00, 20.08it/s, est. speed input: 20551.05 toks/s, output: 20.07 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:51<00:00, 20.07it/s, est. speed input: 20551.05 toks/s, output: 20.07 toks/s]
[rank0]:[W126 02:40:11.698203560 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[0;32m[SUCCESS][0m æµ‹è¯•å®Œæˆ! è€—æ—¶: 79.0s

[0;32mæµ‹è¯•ç»“æœ:[0m
  Requests/s:   19.40
  Tokens/s:     19880.98
  Total Reqs:   1024
  Elapsed:      52.79s

  [Prefill åˆ†æ]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     19861.58


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
8192,1024,8,1024,128,19.3961,19880.9798,52.7942

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 1 æˆåŠŸ, 0 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m1 æˆåŠŸ[0m, 0 å¤±è´¥
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023854.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=16384
Attempt: 1/3
GPU Mem Util: 0.60
Time: 2026-01-26 02:40:54
Duration: 29.9s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 16384 --gpu-mem 0.6 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [16384]
  M_decode:         [16384]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.6

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024028.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=16384
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 16384
â”‚   M_prefill     = 16384 (= 16 x 1024)
â”‚   M_decode      = 16
â”‚   batched_tokens = 16384 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 2048
â”‚   --max-num-seqs           = 16
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 16384
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:40:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=858883)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=858883)[0;0m WARNING 01-26 02:40:49 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/tmp/torchinductor_root/ym/cymejugwfo4ynt3g5vecn4t4bafgeplcxhhgirqpjux5ni6ricrj.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m ERROR 01-26 02:40:52 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 14.43 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 3.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:40:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:40:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:40:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:40:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:40:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:40:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:40:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:40:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:40:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:40:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:40:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:40:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:40:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:40:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:40:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:40:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=858883)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=858883)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=858883)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=858883)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[0;36m(EngineCore_DP0 pid=858883)[0;0m 
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=858883)[0;0m [2026-01-26 02:40:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=858883)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=858883)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=858883)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=858883)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=858883)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=858883)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=858883)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=858883)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=858883)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=858883)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=858883)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=858883)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=858883)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=858883)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=858883)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=858883)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=858883)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=858883)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=858883)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=858883)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=858883)[0;0m     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=858883)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=858883)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=858883)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=858883)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=858883)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=858883)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=858883)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/tmp/torchinductor_root/ym/cymejugwfo4ynt3g5vecn4t4bafgeplcxhhgirqpjux5ni6ricrj.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=858883)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=858883)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=858883)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=858883)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=858883)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=858883)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=858883)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 14.43 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 3.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:40:52.268046357 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=16384 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16384,1024,16,2048,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024028.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=16384
Attempt: 2/3
GPU Mem Util: 0.55
Time: 2026-01-26 02:41:30
Duration: 30.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 16384 --gpu-mem 0.5499999999999999 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [16384]
  M_decode:         [16384]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5499999999999999

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024103.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=16384
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 16384
â”‚   M_prefill     = 16384 (= 16 x 1024)
â”‚   M_decode      = 16
â”‚   batched_tokens = 16384 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 2048
â”‚   --max-num-seqs           = 16
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 16384
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:41:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=859781)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=859781)[0;0m WARNING 01-26 02:41:25 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/tmp/torchinductor_root/ym/cymejugwfo4ynt3g5vecn4t4bafgeplcxhhgirqpjux5ni6ricrj.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m ERROR 01-26 02:41:27 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 14.43 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 3.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:41:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:41:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:41:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:41:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:41:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:41:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:41:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:41:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:41:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:41:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:41:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:41:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=859781)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=859781)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=859781)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=859781)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[0;36m(EngineCore_DP0 pid=859781)[0;0m 
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=859781)[0;0m [2026-01-26 02:41:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=859781)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=859781)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=859781)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=859781)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=859781)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=859781)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=859781)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=859781)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=859781)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=859781)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=859781)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=859781)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=859781)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=859781)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=859781)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=859781)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=859781)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=859781)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=859781)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=859781)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=859781)[0;0m     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=859781)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=859781)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=859781)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=859781)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=859781)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=859781)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=859781)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/tmp/torchinductor_root/ym/cymejugwfo4ynt3g5vecn4t4bafgeplcxhhgirqpjux5ni6ricrj.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=859781)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=859781)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=859781)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=859781)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=859781)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=859781)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=859781)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 14.43 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 3.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:41:28.181566400 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=16384 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16384,1024,16,2048,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024103.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=16384
Attempt: 3/3
GPU Mem Util: 0.50
Time: 2026-01-26 02:42:06
Duration: 30.2s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 16384 --gpu-mem 0.49999999999999994 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [16384]
  M_decode:         [16384]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.49999999999999994

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024139.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=16384
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 16384
â”‚   M_prefill     = 16384 (= 16 x 1024)
â”‚   M_decode      = 16
â”‚   batched_tokens = 16384 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 2048
â”‚   --max-num-seqs           = 16
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 16384
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:41:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=860687)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=860687)[0;0m WARNING 01-26 02:42:01 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/tmp/torchinductor_root/ym/cymejugwfo4ynt3g5vecn4t4bafgeplcxhhgirqpjux5ni6ricrj.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m ERROR 01-26 02:42:03 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 14.43 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 3.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:41:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:41:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:41:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:41:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:41:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:41:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:41:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:41:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:41:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:41:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:41:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:41:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=860687)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=860687)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=860687)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=860687)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[0;36m(EngineCore_DP0 pid=860687)[0;0m 
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=860687)[0;0m [2026-01-26 02:41:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=860687)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=860687)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=860687)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=860687)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=860687)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=860687)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=860687)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=860687)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=860687)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=860687)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=860687)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=860687)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=860687)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=860687)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=860687)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=860687)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=860687)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=860687)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=860687)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=860687)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=860687)[0;0m     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=860687)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=860687)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=860687)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=860687)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=860687)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=860687)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=860687)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/tmp/torchinductor_root/ym/cymejugwfo4ynt3g5vecn4t4bafgeplcxhhgirqpjux5ni6ricrj.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=860687)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=860687)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=860687)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=860687)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=860687)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=860687)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=860687)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 14.43 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 3.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:42:04.200993798 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=16384 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16384,1024,16,2048,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024139.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=32768
Attempt: 1/3
GPU Mem Util: 0.50
Time: 2026-01-26 02:42:55
Duration: 38.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.5 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024220.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:42:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=861763)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=861763)[0;0m WARNING 01-26 02:42:49 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=861763)[0;0m ERROR 01-26 02:42:52 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 268.94 MiB is free. Including non-PyTorch memory, this process has 15.18 GiB memory in use. Of the allocated memory 12.35 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:42:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:42:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:42:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:42:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:42:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:42:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:42:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:42:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:42:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:42:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:42:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:42:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:42:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:42:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:42:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:42:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:42:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=861763)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=861763)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.37it/s]
[0;36m(EngineCore_DP0 pid=861763)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=861763)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[0;36m(EngineCore_DP0 pid=861763)[0;0m 
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=861763)[0;0m [2026-01-26 02:42:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=861763)[0;0m [rank0]:W0126 02:42:52.614000 861763 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=861763)[0;0m [rank0]:W0126 02:42:52.703000 861763 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=861763)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=861763)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=861763)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=861763)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=861763)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=861763)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=861763)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=861763)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=861763)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=861763)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=861763)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=861763)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=861763)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=861763)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=861763)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=861763)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=861763)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=861763)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=861763)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=861763)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=861763)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=861763)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=861763)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=861763)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=861763)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=861763)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=861763)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=861763)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=861763)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=861763)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=861763)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=861763)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=861763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=861763)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=861763)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=861763)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=861763)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 268.94 MiB is free. Including non-PyTorch memory, this process has 15.18 GiB memory in use. Of the allocated memory 12.35 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:42:53.015298101 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024220.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=32768
Attempt: 2/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:43:38
Duration: 37.6s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024304.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:43:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=862777)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=862777)[0;0m WARNING 01-26 02:43:32 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=862777)[0;0m ERROR 01-26 02:43:36 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 268.94 MiB is free. Including non-PyTorch memory, this process has 15.18 GiB memory in use. Of the allocated memory 12.35 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:43:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:43:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:43:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:43:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:43:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:43:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:43:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:43:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:43:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:43:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:43:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:43:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:43:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:43:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:43:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:43:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=862777)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=862777)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=862777)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=862777)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.09it/s]
[0;36m(EngineCore_DP0 pid=862777)[0;0m 
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=862777)[0;0m [2026-01-26 02:43:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=862777)[0;0m [rank0]:W0126 02:43:35.892000 862777 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=862777)[0;0m [rank0]:W0126 02:43:35.982000 862777 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=862777)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=862777)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=862777)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=862777)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=862777)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=862777)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=862777)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=862777)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=862777)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=862777)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=862777)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=862777)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=862777)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=862777)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=862777)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=862777)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=862777)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=862777)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=862777)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=862777)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=862777)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=862777)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=862777)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=862777)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=862777)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=862777)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=862777)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=862777)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=862777)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=862777)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=862777)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=862777)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=862777)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=862777)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=862777)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=862777)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=862777)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 268.94 MiB is free. Including non-PyTorch memory, this process has 15.18 GiB memory in use. Of the allocated memory 12.35 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:43:36.337879010 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024304.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=32768
Attempt: 3/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:44:22
Duration: 37.7s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024347.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:44:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=863787)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=863787)[0;0m WARNING 01-26 02:44:16 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=863787)[0;0m ERROR 01-26 02:44:19 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 268.94 MiB is free. Including non-PyTorch memory, this process has 15.18 GiB memory in use. Of the allocated memory 12.35 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:44:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:44:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:44:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:44:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:44:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:44:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:44:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:44:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:44:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:44:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:44:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:44:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:44:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:44:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:44:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:44:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=863787)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=863787)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=863787)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=863787)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.09it/s]
[0;36m(EngineCore_DP0 pid=863787)[0;0m 
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=863787)[0;0m [2026-01-26 02:44:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=863787)[0;0m [rank0]:W0126 02:44:19.485000 863787 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=863787)[0;0m [rank0]:W0126 02:44:19.574000 863787 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=863787)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=863787)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=863787)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=863787)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=863787)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=863787)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=863787)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=863787)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=863787)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=863787)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=863787)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=863787)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=863787)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=863787)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=863787)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=863787)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=863787)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=863787)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=863787)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=863787)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=863787)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=863787)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=863787)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=863787)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=863787)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=863787)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=863787)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=863787)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=863787)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=863787)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=863787)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=863787)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=863787)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=863787)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=863787)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=863787)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=863787)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 268.94 MiB is free. Including non-PyTorch memory, this process has 15.18 GiB memory in use. Of the allocated memory 12.35 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:44:20.913041988 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024347.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=65536
Attempt: 1/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:45:25
Duration: 52.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024436.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:45:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=865083)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=865083)[0;0m WARNING 01-26 02:45:19 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=865083)[0;0m ERROR 01-26 02:45:22 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.36 GiB is free. Including non-PyTorch memory, this process has 14.09 GiB memory in use. Of the allocated memory 11.13 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:45:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:45:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:45:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:45:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:45:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:45:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:45:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:45:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:45:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:45:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:45:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:45:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:45:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:45:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:45:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:45:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:45:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:12] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=865083)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=865083)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=865083)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=865083)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[0;36m(EngineCore_DP0 pid=865083)[0;0m 
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=865083)[0;0m [2026-01-26 02:45:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=865083)[0;0m [rank0]:W0126 02:45:22.327000 865083 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=865083)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=865083)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=865083)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=865083)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=865083)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=865083)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=865083)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=865083)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=865083)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=865083)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=865083)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=865083)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=865083)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=865083)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=865083)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=865083)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=865083)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=865083)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=865083)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=865083)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=865083)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=865083)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=865083)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=865083)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=865083)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=865083)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=865083)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=865083)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=865083)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=865083)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=865083)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=865083)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=865083)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=865083)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=865083)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=865083)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=865083)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.36 GiB is free. Including non-PyTorch memory, this process has 14.09 GiB memory in use. Of the allocated memory 11.13 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:45:22.725577040 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024436.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=65536
Attempt: 2/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:46:22
Duration: 51.9s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024533.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:46:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=866283)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=866283)[0;0m WARNING 01-26 02:46:17 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=866283)[0;0m ERROR 01-26 02:46:20 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.36 GiB is free. Including non-PyTorch memory, this process has 14.09 GiB memory in use. Of the allocated memory 11.13 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:46:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:46:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:46:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:46:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:46:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:46:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:46:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:46:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:46:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:46:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:46:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:46:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:46:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:46:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:46:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:46:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=866283)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=866283)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=866283)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=866283)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[0;36m(EngineCore_DP0 pid=866283)[0;0m 
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=866283)[0;0m [2026-01-26 02:46:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=866283)[0;0m [rank0]:W0126 02:46:19.943000 866283 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=866283)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=866283)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=866283)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=866283)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=866283)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=866283)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=866283)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=866283)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=866283)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=866283)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=866283)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=866283)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=866283)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=866283)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=866283)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=866283)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=866283)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=866283)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=866283)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=866283)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=866283)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=866283)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=866283)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=866283)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=866283)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=866283)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=866283)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=866283)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=866283)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=866283)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=866283)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=866283)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=866283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=866283)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=866283)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=866283)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=866283)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.36 GiB is free. Including non-PyTorch memory, this process has 14.09 GiB memory in use. Of the allocated memory 11.13 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:46:20.376750194 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024533.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=65536
Attempt: 3/3
GPU Mem Util: 0.35
Time: 2026-01-26 02:47:20
Duration: 51.8s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.35000000000000003 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.35000000000000003

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024631.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:47:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=867489)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=867489)[0;0m WARNING 01-26 02:47:14 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=867489)[0;0m ERROR 01-26 02:47:17 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.36 GiB is free. Including non-PyTorch memory, this process has 14.09 GiB memory in use. Of the allocated memory 11.13 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:47:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:47:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:47:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:47:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:47:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:47:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:47:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:47:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:47:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:47:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:47:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:47:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=867489)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=867489)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=867489)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=867489)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[0;36m(EngineCore_DP0 pid=867489)[0;0m 
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=867489)[0;0m [2026-01-26 02:47:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=867489)[0;0m [rank0]:W0126 02:47:17.616000 867489 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=867489)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=867489)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=867489)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=867489)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=867489)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=867489)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=867489)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=867489)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=867489)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=867489)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=867489)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=867489)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=867489)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=867489)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=867489)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=867489)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=867489)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=867489)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=867489)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=867489)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=867489)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=867489)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=867489)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=867489)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=867489)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=867489)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=867489)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=867489)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=867489)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=867489)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=867489)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=867489)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=867489)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=867489)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=867489)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=867489)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=867489)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.36 GiB is free. Including non-PyTorch memory, this process has 14.09 GiB memory in use. Of the allocated memory 11.13 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:47:18.035049241 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024631.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=8192
Attempt: 1/3
GPU Mem Util: 0.70
Time: 2026-01-26 02:48:00
Duration: 29.4s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 8192 --gpu-mem 0.7 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [8192]
  M_decode:         [8192]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.7

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024734.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=8192
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 8192
â”‚   M_prefill     = 8192 (= 8 x 1024)
â”‚   M_decode      = 8
â”‚   batched_tokens = 8192 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 1024
â”‚   --max-num-seqs           = 8
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 8192
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:47:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=868436)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=868436)[0;0m WARNING 01-26 02:47:54 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=868436)[0;0m ERROR 01-26 02:47:58 [core.py:866] ValueError: To serve at least one request with the models's max seq len (1025), (0.06 GiB KV cache is needed, which is larger than the available KV cache memory (0.00 GiB). Based on the available memory, the estimated maximum model length is 48. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:47:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:47:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:47:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:47:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:47:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:47:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:47:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:47:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:47:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:47:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:47:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:47:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=868436)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=868436)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.31s/it]
[0;36m(EngineCore_DP0 pid=868436)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  2.06s/it]
[0;36m(EngineCore_DP0 pid=868436)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.95s/it]
[0;36m(EngineCore_DP0 pid=868436)[0;0m 
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=868436)[0;0m [2026-01-26 02:47:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=868436)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=868436)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=868436)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=868436)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=868436)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=868436)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=868436)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=868436)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=868436)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=868436)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=868436)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=868436)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=868436)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=868436)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=868436)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=868436)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=868436)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=868436)[0;0m     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=868436)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=868436)[0;0m   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=868436)[0;0m     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=868436)[0;0m   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=868436)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=868436)[0;0m ValueError: To serve at least one request with the models's max seq len (1025), (0.06 GiB KV cache is needed, which is larger than the available KV cache memory (0.00 GiB). Based on the available memory, the estimated maximum model length is 48. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 02:47:58.276356205 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=8192 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
8192,1024,8,1024,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024734.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=16384
Attempt: 1/3
GPU Mem Util: 0.60
Time: 2026-01-26 02:48:41
Duration: 29.9s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 16384 --gpu-mem 0.6 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [16384]
  M_decode:         [16384]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.6

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024814.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=16384
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 16384
â”‚   M_prefill     = 16384 (= 16 x 1024)
â”‚   M_decode      = 16
â”‚   batched_tokens = 16384 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 2048
â”‚   --max-num-seqs           = 16
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 16384
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:48:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=869465)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=869465)[0;0m WARNING 01-26 02:48:36 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/tmp/torchinductor_root/42/c424begxyaqtjc3ez64touk2oxkoe2uah7ehpgcf2nvt677rsosm.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m ERROR 01-26 02:48:38 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 416.94 MiB is free. Including non-PyTorch memory, this process has 15.04 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 3.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:48:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:48:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:48:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:48:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:48:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:48:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:48:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:48:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:48:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:48:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:48:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:48:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:48:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:48:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:48:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:48:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=869465)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=869465)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.28it/s]
[0;36m(EngineCore_DP0 pid=869465)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
[0;36m(EngineCore_DP0 pid=869465)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=869465)[0;0m 
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=869465)[0;0m [2026-01-26 02:48:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=869465)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=869465)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=869465)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=869465)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=869465)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=869465)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=869465)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=869465)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=869465)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=869465)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=869465)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=869465)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=869465)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=869465)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=869465)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=869465)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=869465)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=869465)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=869465)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=869465)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=869465)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=869465)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=869465)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=869465)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=869465)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/tmp/torchinductor_root/42/c424begxyaqtjc3ez64touk2oxkoe2uah7ehpgcf2nvt677rsosm.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=869465)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=869465)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=869465)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=869465)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=869465)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=869465)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=869465)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 416.94 MiB is free. Including non-PyTorch memory, this process has 15.04 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 3.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:48:39.091992125 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=16384 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16384,1024,16,2048,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024814.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=16384
Attempt: 2/3
GPU Mem Util: 0.55
Time: 2026-01-26 02:49:17
Duration: 30.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 16384 --gpu-mem 0.5499999999999999 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [16384]
  M_decode:         [16384]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5499999999999999

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024850.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=16384
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 16384
â”‚   M_prefill     = 16384 (= 16 x 1024)
â”‚   M_decode      = 16
â”‚   batched_tokens = 16384 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 2048
â”‚   --max-num-seqs           = 16
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 16384
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:49:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=870356)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=870356)[0;0m WARNING 01-26 02:49:12 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/tmp/torchinductor_root/42/c424begxyaqtjc3ez64touk2oxkoe2uah7ehpgcf2nvt677rsosm.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m ERROR 01-26 02:49:14 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 416.94 MiB is free. Including non-PyTorch memory, this process has 15.04 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 3.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:49:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:49:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:49:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:49:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:49:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:49:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:49:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:49:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:49:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:49:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:49:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:49:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=870356)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=870356)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
[0;36m(EngineCore_DP0 pid=870356)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
[0;36m(EngineCore_DP0 pid=870356)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=870356)[0;0m 
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=870356)[0;0m [2026-01-26 02:49:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=870356)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=870356)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=870356)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=870356)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=870356)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=870356)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=870356)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=870356)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=870356)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=870356)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=870356)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=870356)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=870356)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=870356)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=870356)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=870356)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=870356)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=870356)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=870356)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=870356)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=870356)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=870356)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=870356)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=870356)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=870356)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/tmp/torchinductor_root/42/c424begxyaqtjc3ez64touk2oxkoe2uah7ehpgcf2nvt677rsosm.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=870356)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=870356)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=870356)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=870356)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=870356)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=870356)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=870356)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 416.94 MiB is free. Including non-PyTorch memory, this process has 15.04 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 3.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:49:15.855785284 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=16384 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16384,1024,16,2048,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024850.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=16384
Attempt: 3/3
GPU Mem Util: 0.50
Time: 2026-01-26 02:49:52
Duration: 29.9s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 16384 --gpu-mem 0.49999999999999994 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [16384]
  M_decode:         [16384]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.49999999999999994

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024925.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=16384
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 16384
â”‚   M_prefill     = 16384 (= 16 x 1024)
â”‚   M_decode      = 16
â”‚   batched_tokens = 16384 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 2048
â”‚   --max-num-seqs           = 16
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 16384
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:49:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=871255)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=871255)[0;0m WARNING 01-26 02:49:47 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/tmp/torchinductor_root/42/c424begxyaqtjc3ez64touk2oxkoe2uah7ehpgcf2nvt677rsosm.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m ERROR 01-26 02:49:50 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 416.94 MiB is free. Including non-PyTorch memory, this process has 15.04 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 3.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:49:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:49:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:49:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:49:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:49:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:49:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:49:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:49:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:49:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:49:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:49:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:49:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=871255)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=871255)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]
[0;36m(EngineCore_DP0 pid=871255)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
[0;36m(EngineCore_DP0 pid=871255)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=871255)[0;0m 
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=871255)[0;0m [2026-01-26 02:49:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=871255)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=871255)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=871255)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=871255)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=871255)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=871255)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=871255)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=871255)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=871255)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=871255)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=871255)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=871255)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=871255)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=871255)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=871255)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=871255)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=871255)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=871255)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=871255)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=871255)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=871255)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=871255)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=871255)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=871255)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=871255)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/tmp/torchinductor_root/42/c424begxyaqtjc3ez64touk2oxkoe2uah7ehpgcf2nvt677rsosm.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=871255)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
[0;36m(EngineCore_DP0 pid=871255)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=871255)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=871255)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=871255)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=871255)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=871255)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.47 GiB of which 416.94 MiB is free. Including non-PyTorch memory, this process has 15.04 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 3.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:49:50.563382324 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=16384 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16384,1024,16,2048,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024925.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=32768
Attempt: 1/3
GPU Mem Util: 0.50
Time: 2026-01-26 02:50:41
Duration: 38.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.5 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025006.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:50:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=872335)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=872335)[0;0m WARNING 01-26 02:50:36 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=872335)[0;0m ERROR 01-26 02:50:39 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.48 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:50:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:50:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:50:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:50:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:50:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:50:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:50:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:50:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:50:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:50:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:50:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:50:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:50:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:50:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:50:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:50:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=872335)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=872335)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
[0;36m(EngineCore_DP0 pid=872335)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
[0;36m(EngineCore_DP0 pid=872335)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=872335)[0;0m 
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=872335)[0;0m [2026-01-26 02:50:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=872335)[0;0m [rank0]:W0126 02:50:38.968000 872335 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=872335)[0;0m [rank0]:W0126 02:50:39.058000 872335 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=872335)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=872335)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=872335)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=872335)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=872335)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=872335)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=872335)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=872335)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=872335)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=872335)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=872335)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=872335)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=872335)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=872335)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=872335)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=872335)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=872335)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=872335)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=872335)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=872335)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=872335)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=872335)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=872335)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=872335)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=872335)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=872335)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=872335)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=872335)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=872335)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=872335)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=872335)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=872335)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=872335)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=872335)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=872335)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=872335)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=872335)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.48 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:50:39.389276690 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025006.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=32768
Attempt: 2/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:51:25
Duration: 37.8s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025050.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:51:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=873348)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=873348)[0;0m WARNING 01-26 02:51:19 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=873348)[0;0m ERROR 01-26 02:51:22 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.48 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:51:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:51:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:51:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:51:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:51:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:51:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:51:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:51:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:51:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:51:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:51:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:51:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=873348)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=873348)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]
[0;36m(EngineCore_DP0 pid=873348)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
[0;36m(EngineCore_DP0 pid=873348)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=873348)[0;0m 
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=873348)[0;0m [2026-01-26 02:51:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=873348)[0;0m [rank0]:W0126 02:51:22.654000 873348 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=873348)[0;0m [rank0]:W0126 02:51:22.741000 873348 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=873348)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=873348)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=873348)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=873348)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=873348)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=873348)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=873348)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=873348)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=873348)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=873348)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=873348)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=873348)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=873348)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=873348)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=873348)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=873348)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=873348)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=873348)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=873348)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=873348)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=873348)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=873348)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=873348)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=873348)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=873348)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=873348)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=873348)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=873348)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=873348)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=873348)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=873348)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=873348)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=873348)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=873348)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=873348)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=873348)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=873348)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.48 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:51:23.043644454 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025050.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=32768
Attempt: 3/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:52:08
Duration: 37.4s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025134.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:51:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=874349)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=874349)[0;0m WARNING 01-26 02:52:03 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=874349)[0;0m ERROR 01-26 02:52:06 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.48 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:51:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:51:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:51:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:51:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:51:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:51:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:51:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:51:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:51:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:51:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:51:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:51:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=874349)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=874349)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]
[0;36m(EngineCore_DP0 pid=874349)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
[0;36m(EngineCore_DP0 pid=874349)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=874349)[0;0m 
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=874349)[0;0m [2026-01-26 02:51:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=874349)[0;0m [rank0]:W0126 02:52:05.885000 874349 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=874349)[0;0m [rank0]:W0126 02:52:05.975000 874349 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=874349)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=874349)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=874349)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=874349)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=874349)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=874349)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=874349)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=874349)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=874349)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=874349)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=874349)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=874349)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=874349)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=874349)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=874349)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=874349)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=874349)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=874349)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=874349)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=874349)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=874349)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=874349)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=874349)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=874349)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=874349)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=874349)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=874349)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=874349)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=874349)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=874349)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=874349)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=874349)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=874349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=874349)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=874349)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=874349)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=874349)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.48 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:52:06.321741516 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025134.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=65536
Attempt: 1/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:53:11
Duration: 52.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025222.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:52:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=875627)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=875627)[0;0m WARNING 01-26 02:53:05 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=875627)[0;0m ERROR 01-26 02:53:08 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 768.94 MiB is free. Including non-PyTorch memory, this process has 14.70 GiB memory in use. Of the allocated memory 11.61 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:52:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:52:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:52:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:52:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:52:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:52:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:52:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:52:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:52:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:52:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:52:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:52:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:52:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:52:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:52:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:52:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:52:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:52:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=875627)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=875627)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
[0;36m(EngineCore_DP0 pid=875627)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
[0;36m(EngineCore_DP0 pid=875627)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=875627)[0;0m 
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:53:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:53:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:53:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:53:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:53:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:53:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:53:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=875627)[0;0m [2026-01-26 02:53:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=875627)[0;0m [rank0]:W0126 02:53:08.777000 875627 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=875627)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=875627)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=875627)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=875627)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=875627)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=875627)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=875627)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=875627)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=875627)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=875627)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=875627)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=875627)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=875627)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=875627)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=875627)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=875627)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=875627)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=875627)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=875627)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=875627)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=875627)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=875627)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=875627)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=875627)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=875627)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=875627)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=875627)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=875627)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=875627)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=875627)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=875627)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=875627)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=875627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=875627)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=875627)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=875627)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=875627)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 768.94 MiB is free. Including non-PyTorch memory, this process has 14.70 GiB memory in use. Of the allocated memory 11.61 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:53:09.180123151 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025222.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=65536
Attempt: 2/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:54:09
Duration: 52.2s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025320.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:53:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=876848)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=876848)[0;0m WARNING 01-26 02:54:03 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=876848)[0;0m ERROR 01-26 02:54:06 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 768.94 MiB is free. Including non-PyTorch memory, this process has 14.70 GiB memory in use. Of the allocated memory 11.61 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:53:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:53:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:53:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:53:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:53:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:53:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:53:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:53:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:53:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:53:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:53:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:53:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:53:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:53:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:53:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:53:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:57] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:57] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:57] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:57] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:57] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=876848)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=876848)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
[0;36m(EngineCore_DP0 pid=876848)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
[0;36m(EngineCore_DP0 pid=876848)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=876848)[0;0m 
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=876848)[0;0m [2026-01-26 02:53:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=876848)[0;0m [rank0]:W0126 02:54:06.701000 876848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=876848)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=876848)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=876848)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=876848)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=876848)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=876848)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=876848)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=876848)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=876848)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=876848)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=876848)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=876848)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=876848)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=876848)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=876848)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=876848)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=876848)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=876848)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=876848)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=876848)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=876848)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=876848)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=876848)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=876848)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=876848)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=876848)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=876848)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=876848)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=876848)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=876848)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=876848)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=876848)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=876848)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=876848)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=876848)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=876848)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=876848)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 768.94 MiB is free. Including non-PyTorch memory, this process has 14.70 GiB memory in use. Of the allocated memory 11.61 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:54:07.103345938 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025320.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=65536
Attempt: 3/3
GPU Mem Util: 0.35
Time: 2026-01-26 02:55:08
Duration: 52.6s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.35000000000000003 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.35000000000000003

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025418.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:54:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=878053)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=878053)[0;0m WARNING 01-26 02:55:02 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=878053)[0;0m ERROR 01-26 02:55:05 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 768.94 MiB is free. Including non-PyTorch memory, this process has 14.70 GiB memory in use. Of the allocated memory 11.61 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:54:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:54:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:54:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:54:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:54:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:54:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:54:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:54:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:54:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:54:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:54:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:54:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:54:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:54:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:54:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:54:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=878053)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=878053)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]
[0;36m(EngineCore_DP0 pid=878053)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
[0;36m(EngineCore_DP0 pid=878053)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=878053)[0;0m 
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=878053)[0;0m [2026-01-26 02:54:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=878053)[0;0m [rank0]:W0126 02:55:05.208000 878053 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=878053)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=878053)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=878053)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=878053)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=878053)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=878053)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=878053)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=878053)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=878053)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=878053)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=878053)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=878053)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=878053)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=878053)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=878053)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=878053)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=878053)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=878053)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=878053)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=878053)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=878053)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=878053)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=878053)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=878053)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=878053)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=878053)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=878053)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=878053)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=878053)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=878053)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=878053)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=878053)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=878053)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=878053)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=878053)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=878053)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=878053)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 768.94 MiB is free. Including non-PyTorch memory, this process has 14.70 GiB memory in use. Of the allocated memory 11.61 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:55:05.616555634 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025418.log


======================================================================
Test: qwen2.5-7b-fp8 | cublaslt | prefill | M=32768
Attempt: 1/3
GPU Mem Util: 0.50
Time: 2026-01-26 02:55:57
Duration: 39.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cublaslt --M 32768 --gpu-mem 0.5 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025521.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuBLASLt                                        â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:55:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=879138)[0;0m WARNING 01-26 02:55:52 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/tmp/torchinductor_root/ve/cvevelyvgsqnqqarjjyddla7lg73v34r5i2rmlrd622qgnvk74uw.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m ERROR 01-26 02:55:55 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 14.17 GiB memory in use. Of the allocated memory 11.36 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:55:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:55:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:55:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:55:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:55:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:55:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:55:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:55:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:55:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:55:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:55:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:55:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:55:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:55:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:55:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:55:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:55:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=879138)[0;0m [2026-01-26 02:55:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=879138)[0;0m [2026-01-26 02:55:44] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=879138)[0;0m [2026-01-26 02:55:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=879138)[0;0m [2026-01-26 02:55:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=879138)[0;0m [2026-01-26 02:55:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
[0;36m(EngineCore_DP0 pid=879138)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=879138)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.41s/it]
[0;36m(EngineCore_DP0 pid=879138)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.12s/it]
[0;36m(EngineCore_DP0 pid=879138)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.16s/it]
[0;36m(EngineCore_DP0 pid=879138)[0;0m 
[0;36m(EngineCore_DP0 pid=879138)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=879138)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=879138)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=879138)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=879138)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=879138)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=879138)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=879138)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=879138)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=879138)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=879138)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=879138)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=879138)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=879138)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=879138)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=879138)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=879138)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=879138)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=879138)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=879138)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=879138)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=879138)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=879138)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=879138)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=879138)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/tmp/torchinductor_root/ve/cvevelyvgsqnqqarjjyddla7lg73v34r5i2rmlrd622qgnvk74uw.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=879138)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=879138)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=879138)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=879138)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=879138)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=879138)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=879138)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 14.17 GiB memory in use. Of the allocated memory 11.36 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:55:55.401440510 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025521.log


======================================================================
Test: qwen2.5-7b-fp8 | cublaslt | prefill | M=32768
Attempt: 2/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:56:40
Duration: 36.3s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cublaslt --M 32768 --gpu-mem 0.45 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025606.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuBLASLt                                        â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:56:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=880166)[0;0m WARNING 01-26 02:56:35 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/tmp/torchinductor_root/ve/cvevelyvgsqnqqarjjyddla7lg73v34r5i2rmlrd622qgnvk74uw.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m ERROR 01-26 02:56:37 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 14.17 GiB memory in use. Of the allocated memory 11.36 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:56:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:56:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:56:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:56:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:56:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:56:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:56:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:56:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:56:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:56:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:56:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:56:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:56:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:56:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:56:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:56:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:56:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=880166)[0;0m [2026-01-26 02:56:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=880166)[0;0m [2026-01-26 02:56:29] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=880166)[0;0m [2026-01-26 02:56:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=880166)[0;0m [2026-01-26 02:56:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=880166)[0;0m [2026-01-26 02:56:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
[0;36m(EngineCore_DP0 pid=880166)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=880166)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.79it/s]
[0;36m(EngineCore_DP0 pid=880166)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.41it/s]
[0;36m(EngineCore_DP0 pid=880166)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]
[0;36m(EngineCore_DP0 pid=880166)[0;0m 
[0;36m(EngineCore_DP0 pid=880166)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=880166)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=880166)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=880166)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=880166)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=880166)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=880166)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=880166)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=880166)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=880166)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=880166)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=880166)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=880166)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=880166)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=880166)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=880166)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=880166)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=880166)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=880166)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=880166)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=880166)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=880166)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=880166)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=880166)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=880166)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/tmp/torchinductor_root/ve/cvevelyvgsqnqqarjjyddla7lg73v34r5i2rmlrd622qgnvk74uw.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=880166)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=880166)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=880166)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=880166)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=880166)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=880166)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=880166)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 14.17 GiB memory in use. Of the allocated memory 11.36 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:56:37.644087364 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025606.log


======================================================================
Test: qwen2.5-7b-fp8 | cublaslt | prefill | M=32768
Attempt: 3/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:57:22
Duration: 36.2s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cublaslt --M 32768 --gpu-mem 0.4 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025649.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuBLASLt                                        â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:57:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=881157)[0;0m WARNING 01-26 02:57:16 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/tmp/torchinductor_root/ve/cvevelyvgsqnqqarjjyddla7lg73v34r5i2rmlrd622qgnvk74uw.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m ERROR 01-26 02:57:19 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 14.17 GiB memory in use. Of the allocated memory 11.36 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:57:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:57:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:57:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:57:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:57:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:57:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:57:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:57:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:57:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:57:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:57:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:57:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:57:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:57:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:57:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:57:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:57:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=881157)[0;0m [2026-01-26 02:57:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=881157)[0;0m [2026-01-26 02:57:11] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=881157)[0;0m [2026-01-26 02:57:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=881157)[0;0m [2026-01-26 02:57:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=881157)[0;0m [2026-01-26 02:57:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
[0;36m(EngineCore_DP0 pid=881157)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=881157)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=881157)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
[0;36m(EngineCore_DP0 pid=881157)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.47it/s]
[0;36m(EngineCore_DP0 pid=881157)[0;0m 
[0;36m(EngineCore_DP0 pid=881157)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=881157)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=881157)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=881157)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=881157)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=881157)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=881157)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=881157)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=881157)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=881157)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=881157)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=881157)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=881157)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=881157)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=881157)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=881157)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=881157)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=881157)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=881157)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=881157)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=881157)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=881157)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=881157)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=881157)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=881157)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/tmp/torchinductor_root/ve/cvevelyvgsqnqqarjjyddla7lg73v34r5i2rmlrd622qgnvk74uw.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=881157)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=881157)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=881157)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=881157)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=881157)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=881157)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=881157)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 14.17 GiB memory in use. Of the allocated memory 11.36 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:57:19.655808592 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025649.log


======================================================================
Test: qwen2.5-7b-fp8 | cublaslt | prefill | M=65536
Attempt: 1/3
GPU Mem Util: 0.45
Time: 2026-01-26 02:58:24
Duration: 51.5s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cublaslt --M 65536 --gpu-mem 0.45 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025736.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuBLASLt                                        â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:58:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=882441)[0;0m WARNING 01-26 02:58:18 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=882441)[0;0m ERROR 01-26 02:58:21 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.96 GiB is free. Including non-PyTorch memory, this process has 11.48 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 774.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:58:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:58:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:58:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:58:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:58:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:58:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:58:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:58:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:58:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:58:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:58:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:58:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:58:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:58:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:58:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:58:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:58:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=882441)[0;0m [2026-01-26 02:58:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=882441)[0;0m [2026-01-26 02:58:13] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=882441)[0;0m [2026-01-26 02:58:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=882441)[0;0m [2026-01-26 02:58:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=882441)[0;0m [2026-01-26 02:58:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
[0;36m(EngineCore_DP0 pid=882441)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=882441)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=882441)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
[0;36m(EngineCore_DP0 pid=882441)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]
[0;36m(EngineCore_DP0 pid=882441)[0;0m 
[0;36m(EngineCore_DP0 pid=882441)[0;0m [2026-01-26 02:58:20] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=4608, K=3584), falling back to default heuristic
[0;36m(EngineCore_DP0 pid=882441)[0;0m [rank0]:W0126 02:58:21.518000 882441 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=882441)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=882441)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=882441)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=882441)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=882441)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=882441)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=882441)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=882441)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=882441)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=882441)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=882441)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=882441)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=882441)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=882441)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=882441)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=882441)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=882441)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=882441)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=882441)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=882441)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=882441)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=882441)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=882441)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=882441)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=882441)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=882441)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=882441)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=882441)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=882441)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=882441)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=882441)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=882441)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=882441)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=882441)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=882441)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=882441)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=882441)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.96 GiB is free. Including non-PyTorch memory, this process has 11.48 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 774.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:58:22.937397476 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025736.log


======================================================================
Test: qwen2.5-7b-fp8 | cublaslt | prefill | M=65536
Attempt: 2/3
GPU Mem Util: 0.40
Time: 2026-01-26 02:59:21
Duration: 51.2s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cublaslt --M 65536 --gpu-mem 0.4 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025833.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuBLASLt                                        â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:59:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=883637)[0;0m WARNING 01-26 02:59:15 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=883637)[0;0m ERROR 01-26 02:59:18 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.96 GiB is free. Including non-PyTorch memory, this process has 11.48 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 774.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:59:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:59:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:59:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:59:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:59:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:59:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:59:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:59:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:59:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:59:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:59:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 02:59:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 02:59:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:59:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:59:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:59:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:59:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=883637)[0;0m [2026-01-26 02:59:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=883637)[0;0m [2026-01-26 02:59:10] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=883637)[0;0m [2026-01-26 02:59:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=883637)[0;0m [2026-01-26 02:59:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=883637)[0;0m [2026-01-26 02:59:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
[0;36m(EngineCore_DP0 pid=883637)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=883637)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=883637)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
[0;36m(EngineCore_DP0 pid=883637)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.47it/s]
[0;36m(EngineCore_DP0 pid=883637)[0;0m 
[0;36m(EngineCore_DP0 pid=883637)[0;0m [2026-01-26 02:59:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=4608, K=3584), falling back to default heuristic
[0;36m(EngineCore_DP0 pid=883637)[0;0m [rank0]:W0126 02:59:18.675000 883637 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=883637)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=883637)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=883637)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=883637)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=883637)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=883637)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=883637)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=883637)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=883637)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=883637)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=883637)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=883637)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=883637)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=883637)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=883637)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=883637)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=883637)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=883637)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=883637)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=883637)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=883637)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=883637)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=883637)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=883637)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=883637)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=883637)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=883637)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=883637)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=883637)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=883637)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=883637)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=883637)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=883637)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=883637)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=883637)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=883637)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=883637)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.96 GiB is free. Including non-PyTorch memory, this process has 11.48 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 774.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:59:19.079534935 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025833.log


======================================================================
Test: qwen2.5-7b-fp8 | cublaslt | prefill | M=65536
Attempt: 3/3
GPU Mem Util: 0.35
Time: 2026-01-26 03:00:18
Duration: 51.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cublaslt --M 65536 --gpu-mem 0.35000000000000003 --gpu-id 1

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.35000000000000003

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025930.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuBLASLt                                        â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:00:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=884838)[0;0m WARNING 01-26 03:00:12 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=884838)[0;0m ERROR 01-26 03:00:15 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.96 GiB is free. Including non-PyTorch memory, this process has 11.48 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 774.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:00:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:00:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:00:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:00:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:00:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:00:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:00:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:00:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:00:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:00:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:00:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:00:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:00:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:00:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:00:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:00:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:00:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=884838)[0;0m [2026-01-26 03:00:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
[0;36m(EngineCore_DP0 pid=884838)[0;0m [2026-01-26 03:00:07] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=884838)[0;0m [2026-01-26 03:00:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
[0;36m(EngineCore_DP0 pid=884838)[0;0m [2026-01-26 03:00:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=884838)[0;0m [2026-01-26 03:00:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
[0;36m(EngineCore_DP0 pid=884838)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=884838)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.83it/s]
[0;36m(EngineCore_DP0 pid=884838)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
[0;36m(EngineCore_DP0 pid=884838)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.47it/s]
[0;36m(EngineCore_DP0 pid=884838)[0;0m 
[0;36m(EngineCore_DP0 pid=884838)[0;0m [2026-01-26 03:00:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=4608, K=3584), falling back to default heuristic
[0;36m(EngineCore_DP0 pid=884838)[0;0m [rank0]:W0126 03:00:15.609000 884838 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=884838)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=884838)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=884838)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=884838)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=884838)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=884838)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=884838)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=884838)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=884838)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=884838)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=884838)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=884838)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=884838)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=884838)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=884838)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=884838)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=884838)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=884838)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=884838)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=884838)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=884838)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=884838)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=884838)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=884838)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=884838)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=884838)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=884838)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=884838)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=884838)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=884838)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=884838)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=884838)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=884838)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=884838)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=884838)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=884838)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=884838)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.96 GiB is free. Including non-PyTorch memory, this process has 11.48 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 774.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:00:16.011117505 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025930.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_4) | prefill | M=65536
Attempt: 1/3
GPU Mem Util: 0.45
Time: 2026-01-26 03:01:26
Duration: 57.4s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030032.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:01:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=886090)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=886090)[0;0m WARNING 01-26 03:01:20 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=886090)[0;0m ERROR 01-26 03:01:23 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 13.89 GiB memory in use. Of the allocated memory 12.74 GiB is allocated by PyTorch, and 797.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:01:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:01:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:01:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:01:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:01:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:01:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:01:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:01:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:01:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:01:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:01:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:01:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:01:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:01:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:01:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:01:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:01:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:08] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:08] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:08] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:08] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:08] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=886090)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=886090)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.01it/s]
[0;36m(EngineCore_DP0 pid=886090)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  4.15s/it]
[0;36m(EngineCore_DP0 pid=886090)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.68s/it]
[0;36m(EngineCore_DP0 pid=886090)[0;0m 
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
[0;36m(EngineCore_DP0 pid=886090)[0;0m [2026-01-26 03:01:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
[0;36m(EngineCore_DP0 pid=886090)[0;0m [rank0]:W0126 03:01:23.708000 886090 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=886090)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=886090)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=886090)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=886090)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=886090)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=886090)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=886090)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=886090)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=886090)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=886090)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=886090)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=886090)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=886090)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=886090)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=886090)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=886090)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=886090)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=886090)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=886090)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=886090)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=886090)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=886090)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=886090)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=886090)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=886090)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=886090)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=886090)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=886090)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=886090)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=886090)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=886090)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=886090)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=886090)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=886090)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=886090)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=886090)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=886090)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 13.89 GiB memory in use. Of the allocated memory 12.74 GiB is allocated by PyTorch, and 797.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:01:24.121021728 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030032.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_4) | prefill | M=65536
Attempt: 2/3
GPU Mem Util: 0.40
Time: 2026-01-26 03:02:25
Duration: 52.9s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030135.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:02:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=887369)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=887369)[0;0m WARNING 01-26 03:02:19 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=887369)[0;0m ERROR 01-26 03:02:22 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 13.89 GiB memory in use. Of the allocated memory 12.74 GiB is allocated by PyTorch, and 797.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:02:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:02:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:02:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:02:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:02:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:02:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:02:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:02:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:02:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:02:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:02:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:02:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:02:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:02:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:02:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:02:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:02:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=887369)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=887369)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.05s/it]
[0;36m(EngineCore_DP0 pid=887369)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.17s/it]
[0;36m(EngineCore_DP0 pid=887369)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.15s/it]
[0;36m(EngineCore_DP0 pid=887369)[0;0m 
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
[0;36m(EngineCore_DP0 pid=887369)[0;0m [2026-01-26 03:02:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
[0;36m(EngineCore_DP0 pid=887369)[0;0m [rank0]:W0126 03:02:22.606000 887369 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=887369)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=887369)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=887369)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=887369)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=887369)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=887369)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=887369)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=887369)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=887369)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=887369)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=887369)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=887369)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=887369)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=887369)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=887369)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=887369)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=887369)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=887369)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=887369)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=887369)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=887369)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=887369)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=887369)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=887369)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=887369)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=887369)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=887369)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=887369)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=887369)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=887369)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=887369)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=887369)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=887369)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=887369)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=887369)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=887369)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=887369)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 13.89 GiB memory in use. Of the allocated memory 12.74 GiB is allocated by PyTorch, and 797.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:02:23.032579151 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030135.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_4) | prefill | M=65536
Attempt: 3/3
GPU Mem Util: 0.35
Time: 2026-01-26 03:03:22
Duration: 51.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.35000000000000003 --gpu-id 1 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.35000000000000003

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030234.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:03:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=888581)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=888581)[0;0m WARNING 01-26 03:03:16 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=888581)[0;0m ERROR 01-26 03:03:19 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 13.89 GiB memory in use. Of the allocated memory 12.74 GiB is allocated by PyTorch, and 797.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:03:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:03:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:03:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:03:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:03:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:03:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:03:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:03:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:03:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:03:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:03:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:03:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:03:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:03:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:03:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:03:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=888581)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=888581)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.79it/s]
[0;36m(EngineCore_DP0 pid=888581)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
[0;36m(EngineCore_DP0 pid=888581)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.47it/s]
[0;36m(EngineCore_DP0 pid=888581)[0;0m 
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
[0;36m(EngineCore_DP0 pid=888581)[0;0m [2026-01-26 03:03:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
[0;36m(EngineCore_DP0 pid=888581)[0;0m [rank0]:W0126 03:03:19.412000 888581 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=888581)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=888581)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=888581)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=888581)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=888581)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=888581)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=888581)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=888581)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=888581)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=888581)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=888581)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=888581)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=888581)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=888581)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=888581)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=888581)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=888581)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=888581)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=888581)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=888581)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=888581)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=888581)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=888581)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=888581)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=888581)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=888581)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=888581)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=888581)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=888581)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=888581)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=888581)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=888581)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=888581)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=888581)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=888581)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=888581)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=888581)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 13.89 GiB memory in use. Of the allocated memory 12.74 GiB is allocated by PyTorch, and 797.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:03:20.833754902 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030234.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_6) | prefill | M=32768
Attempt: 1/3
GPU Mem Util: 0.50
Time: 2026-01-26 03:04:11
Duration: 38.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.5 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030336.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:03:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=889659)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=889659)[0;0m WARNING 01-26 03:04:06 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/tmp/torchinductor_root/i7/ci754lmoai4utgnerfrsfesxagcrv4twj4ebssqaj7cvchhqcfv3.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m ERROR 01-26 03:04:08 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.16 GiB is free. Including non-PyTorch memory, this process has 13.29 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:03:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:03:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:03:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:03:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:03:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:03:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:03:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:03:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:03:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:03:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:03:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:03:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:03:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:03:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:03:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:03:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:03:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:03:58] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:03:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:03:58] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:03:58] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:03:58] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:03:58] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=889659)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=889659)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.19s/it]
[0;36m(EngineCore_DP0 pid=889659)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.48s/it]
[0;36m(EngineCore_DP0 pid=889659)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.44s/it]
[0;36m(EngineCore_DP0 pid=889659)[0;0m 
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:04:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:04:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:04:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:04:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:04:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:04:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:04:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=889659)[0;0m [2026-01-26 03:04:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
[0;36m(EngineCore_DP0 pid=889659)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=889659)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=889659)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=889659)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=889659)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=889659)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=889659)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=889659)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=889659)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=889659)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=889659)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=889659)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=889659)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=889659)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=889659)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=889659)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=889659)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=889659)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=889659)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=889659)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=889659)[0;0m     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=889659)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=889659)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=889659)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=889659)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=889659)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=889659)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=889659)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/tmp/torchinductor_root/i7/ci754lmoai4utgnerfrsfesxagcrv4twj4ebssqaj7cvchhqcfv3.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=889659)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=889659)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=889659)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=889659)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=889659)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=889659)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=889659)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.16 GiB is free. Including non-PyTorch memory, this process has 13.29 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:04:08.694787846 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030336.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_6) | prefill | M=32768
Attempt: 2/3
GPU Mem Util: 0.45
Time: 2026-01-26 03:04:53
Duration: 36.7s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030419.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:04:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=890671)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=890671)[0;0m WARNING 01-26 03:04:48 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/tmp/torchinductor_root/i7/ci754lmoai4utgnerfrsfesxagcrv4twj4ebssqaj7cvchhqcfv3.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m ERROR 01-26 03:04:51 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.16 GiB is free. Including non-PyTorch memory, this process has 13.29 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:04:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:04:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:04:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:04:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:04:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:04:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:04:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:04:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:04:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:04:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:04:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:04:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:04:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:04:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:04:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:04:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:04:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:42] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:42] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:42] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:42] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:42] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=890671)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=890671)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=890671)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
[0;36m(EngineCore_DP0 pid=890671)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]
[0;36m(EngineCore_DP0 pid=890671)[0;0m 
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=890671)[0;0m [2026-01-26 03:04:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
[0;36m(EngineCore_DP0 pid=890671)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=890671)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=890671)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=890671)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=890671)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=890671)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=890671)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=890671)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=890671)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=890671)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=890671)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=890671)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=890671)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=890671)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=890671)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=890671)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=890671)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=890671)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=890671)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=890671)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=890671)[0;0m     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=890671)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=890671)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=890671)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=890671)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=890671)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=890671)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=890671)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/tmp/torchinductor_root/i7/ci754lmoai4utgnerfrsfesxagcrv4twj4ebssqaj7cvchhqcfv3.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=890671)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=890671)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=890671)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=890671)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=890671)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=890671)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=890671)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.16 GiB is free. Including non-PyTorch memory, this process has 13.29 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:04:51.345822719 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030419.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_6) | prefill | M=32768
Attempt: 3/3
GPU Mem Util: 0.40
Time: 2026-01-26 03:05:36
Duration: 36.9s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030502.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:05:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=891651)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=891651)[0;0m WARNING 01-26 03:05:31 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/tmp/torchinductor_root/i7/ci754lmoai4utgnerfrsfesxagcrv4twj4ebssqaj7cvchhqcfv3.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m ERROR 01-26 03:05:33 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.16 GiB is free. Including non-PyTorch memory, this process has 13.29 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:05:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:05:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:05:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:05:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:05:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:05:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:05:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:05:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:05:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:05:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:05:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:05:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:05:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:05:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:05:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:05:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:05:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=891651)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=891651)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=891651)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
[0;36m(EngineCore_DP0 pid=891651)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]
[0;36m(EngineCore_DP0 pid=891651)[0;0m 
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=891651)[0;0m [2026-01-26 03:05:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
[0;36m(EngineCore_DP0 pid=891651)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=891651)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=891651)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=891651)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=891651)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=891651)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=891651)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=891651)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=891651)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=891651)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=891651)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=891651)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=891651)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=891651)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=891651)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=891651)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=891651)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=891651)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=891651)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=891651)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=891651)[0;0m     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=891651)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=891651)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=891651)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=891651)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=891651)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=891651)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=891651)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/tmp/torchinductor_root/i7/ci754lmoai4utgnerfrsfesxagcrv4twj4ebssqaj7cvchhqcfv3.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=891651)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=891651)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=891651)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=891651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=891651)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=891651)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=891651)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.16 GiB is free. Including non-PyTorch memory, this process has 13.29 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:05:34.833414165 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030502.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_6) | prefill | M=65536
Attempt: 1/3
GPU Mem Util: 0.45
Time: 2026-01-26 03:06:38
Duration: 51.6s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030550.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:06:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=892936)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=892936)[0;0m WARNING 01-26 03:06:33 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=892936)[0;0m ERROR 01-26 03:06:36 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 226.94 MiB is free. Including non-PyTorch memory, this process has 15.22 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 834.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:06:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:06:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:06:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:06:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:06:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:06:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:06:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:06:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:06:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:06:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:06:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:06:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:06:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:06:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:06:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:06:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:06:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=892936)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=892936)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=892936)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
[0;36m(EngineCore_DP0 pid=892936)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]
[0;36m(EngineCore_DP0 pid=892936)[0;0m 
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=892936)[0;0m [2026-01-26 03:06:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
[0;36m(EngineCore_DP0 pid=892936)[0;0m [rank0]:W0126 03:06:35.990000 892936 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=892936)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=892936)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=892936)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=892936)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=892936)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=892936)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=892936)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=892936)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=892936)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=892936)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=892936)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=892936)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=892936)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=892936)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=892936)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=892936)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=892936)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=892936)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=892936)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=892936)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=892936)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=892936)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=892936)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=892936)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=892936)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=892936)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=892936)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=892936)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=892936)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=892936)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=892936)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=892936)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=892936)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=892936)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=892936)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=892936)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=892936)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 226.94 MiB is free. Including non-PyTorch memory, this process has 15.22 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 834.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:06:36.428331170 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030550.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_6) | prefill | M=65536
Attempt: 2/3
GPU Mem Util: 0.40
Time: 2026-01-26 03:07:36
Duration: 51.7s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030647.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:07:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=894122)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=894122)[0;0m WARNING 01-26 03:07:30 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=894122)[0;0m ERROR 01-26 03:07:33 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 226.94 MiB is free. Including non-PyTorch memory, this process has 15.22 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 834.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:07:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:07:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:07:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:07:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:07:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:07:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:07:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:07:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:07:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:07:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:07:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:07:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:07:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:07:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:07:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:07:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:07:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:24] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=894122)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=894122)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=894122)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
[0;36m(EngineCore_DP0 pid=894122)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]
[0;36m(EngineCore_DP0 pid=894122)[0;0m 
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=894122)[0;0m [2026-01-26 03:07:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
[0;36m(EngineCore_DP0 pid=894122)[0;0m [rank0]:W0126 03:07:33.432000 894122 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=894122)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=894122)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=894122)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=894122)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=894122)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=894122)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=894122)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=894122)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=894122)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=894122)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=894122)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=894122)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=894122)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=894122)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=894122)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=894122)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=894122)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=894122)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=894122)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=894122)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=894122)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=894122)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=894122)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=894122)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=894122)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=894122)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=894122)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=894122)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=894122)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=894122)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=894122)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=894122)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=894122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=894122)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=894122)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=894122)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=894122)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 226.94 MiB is free. Including non-PyTorch memory, this process has 15.22 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 834.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:07:34.849921874 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030647.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_6) | prefill | M=65536
Attempt: 3/3
GPU Mem Util: 0.35
Time: 2026-01-26 03:08:34
Duration: 51.9s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.35000000000000003 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.35000000000000003

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030745.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:08:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=895334)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=895334)[0;0m WARNING 01-26 03:08:28 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=895334)[0;0m ERROR 01-26 03:08:31 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 226.94 MiB is free. Including non-PyTorch memory, this process has 15.22 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 834.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:08:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:08:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:08:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:08:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:08:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:08:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:08:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:08:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:08:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:08:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:08:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:08:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:08:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:08:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:08:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:08:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:08:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=895334)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=895334)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
[0;36m(EngineCore_DP0 pid=895334)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.17it/s]
[0;36m(EngineCore_DP0 pid=895334)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]
[0;36m(EngineCore_DP0 pid=895334)[0;0m 
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=895334)[0;0m [2026-01-26 03:08:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
[0;36m(EngineCore_DP0 pid=895334)[0;0m [rank0]:W0126 03:08:31.275000 895334 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=895334)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=895334)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=895334)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=895334)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=895334)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=895334)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=895334)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=895334)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=895334)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=895334)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=895334)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=895334)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=895334)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=895334)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=895334)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=895334)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=895334)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=895334)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=895334)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=895334)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=895334)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=895334)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=895334)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=895334)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=895334)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=895334)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=895334)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=895334)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=895334)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=895334)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=895334)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=895334)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=895334)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=895334)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=895334)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=895334)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=895334)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 226.94 MiB is free. Including non-PyTorch memory, this process has 15.22 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 834.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:08:31.695867092 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030745.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_8) | prefill | M=32768
Attempt: 1/3
GPU Mem Util: 0.50
Time: 2026-01-26 03:09:21
Duration: 36.7s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.5 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030848.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:09:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=896405)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=896405)[0;0m WARNING 01-26 03:09:16 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/tmp/torchinductor_root/mi/cmikorxfouzaz3f73b3mvq5h3fx4ue55ldwdrgqggoi7fsraalgy.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m ERROR 01-26 03:09:19 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.50 GiB is free. Including non-PyTorch memory, this process has 13.94 GiB memory in use. Of the allocated memory 10.99 GiB is allocated by PyTorch, and 2.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:09:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:09:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:09:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:09:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:09:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:09:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:09:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:09:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:09:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:09:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:09:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:09:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:09:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:09:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:09:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:09:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=896405)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=896405)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.14it/s]
[0;36m(EngineCore_DP0 pid=896405)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]
[0;36m(EngineCore_DP0 pid=896405)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.03s/it]
[0;36m(EngineCore_DP0 pid=896405)[0;0m 
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=896405)[0;0m [2026-01-26 03:09:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
[0;36m(EngineCore_DP0 pid=896405)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=896405)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=896405)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=896405)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=896405)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=896405)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=896405)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=896405)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=896405)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=896405)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=896405)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=896405)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=896405)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=896405)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=896405)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=896405)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=896405)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=896405)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=896405)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=896405)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=896405)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=896405)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=896405)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=896405)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=896405)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/tmp/torchinductor_root/mi/cmikorxfouzaz3f73b3mvq5h3fx4ue55ldwdrgqggoi7fsraalgy.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=896405)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=896405)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=896405)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=896405)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=896405)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=896405)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=896405)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.50 GiB is free. Including non-PyTorch memory, this process has 13.94 GiB memory in use. Of the allocated memory 10.99 GiB is allocated by PyTorch, and 2.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:09:19.383039803 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030848.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_8) | prefill | M=32768
Attempt: 2/3
GPU Mem Util: 0.45
Time: 2026-01-26 03:10:04
Duration: 37.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030931.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:09:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=897414)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=897414)[0;0m WARNING 01-26 03:09:59 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/tmp/torchinductor_root/mi/cmikorxfouzaz3f73b3mvq5h3fx4ue55ldwdrgqggoi7fsraalgy.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m ERROR 01-26 03:10:01 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.50 GiB is free. Including non-PyTorch memory, this process has 13.94 GiB memory in use. Of the allocated memory 10.99 GiB is allocated by PyTorch, and 2.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:09:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:09:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:09:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:09:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:09:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:09:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:09:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:09:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:09:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:09:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:09:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:09:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:09:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:09:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:09:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:09:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:09:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=897414)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=897414)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=897414)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=897414)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.09it/s]
[0;36m(EngineCore_DP0 pid=897414)[0;0m 
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=897414)[0;0m [2026-01-26 03:09:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
[0;36m(EngineCore_DP0 pid=897414)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=897414)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=897414)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=897414)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=897414)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=897414)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=897414)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=897414)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=897414)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=897414)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=897414)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=897414)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=897414)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=897414)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=897414)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=897414)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=897414)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=897414)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=897414)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=897414)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=897414)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=897414)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=897414)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=897414)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=897414)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/tmp/torchinductor_root/mi/cmikorxfouzaz3f73b3mvq5h3fx4ue55ldwdrgqggoi7fsraalgy.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=897414)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=897414)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=897414)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=897414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=897414)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=897414)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=897414)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.50 GiB is free. Including non-PyTorch memory, this process has 13.94 GiB memory in use. Of the allocated memory 10.99 GiB is allocated by PyTorch, and 2.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:10:02.234374739 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_030931.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_8) | prefill | M=32768
Attempt: 3/3
GPU Mem Util: 0.40
Time: 2026-01-26 03:10:46
Duration: 36.5s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031013.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:10:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=898400)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=898400)[0;0m WARNING 01-26 03:10:41 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/tmp/torchinductor_root/mi/cmikorxfouzaz3f73b3mvq5h3fx4ue55ldwdrgqggoi7fsraalgy.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m ERROR 01-26 03:10:44 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.50 GiB is free. Including non-PyTorch memory, this process has 13.94 GiB memory in use. Of the allocated memory 10.99 GiB is allocated by PyTorch, and 2.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:10:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:10:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:10:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:10:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:10:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:10:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:10:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:10:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:10:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:10:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:10:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:10:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:10:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:10:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:10:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:10:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:10:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=898400)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=898400)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=898400)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=898400)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.09it/s]
[0;36m(EngineCore_DP0 pid=898400)[0;0m 
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=898400)[0;0m [2026-01-26 03:10:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
[0;36m(EngineCore_DP0 pid=898400)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=898400)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=898400)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=898400)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=898400)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=898400)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=898400)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=898400)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=898400)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=898400)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=898400)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=898400)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=898400)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=898400)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=898400)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=898400)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=898400)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=898400)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=898400)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=898400)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=898400)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=898400)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=898400)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=898400)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=898400)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/tmp/torchinductor_root/mi/cmikorxfouzaz3f73b3mvq5h3fx4ue55ldwdrgqggoi7fsraalgy.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=898400)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=898400)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=898400)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=898400)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=898400)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=898400)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=898400)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.50 GiB is free. Including non-PyTorch memory, this process has 13.94 GiB memory in use. Of the allocated memory 10.99 GiB is allocated by PyTorch, and 2.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:10:44.627592672 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031013.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_8) | prefill | M=65536
Attempt: 1/3
GPU Mem Util: 0.45
Time: 2026-01-26 03:11:49
Duration: 51.9s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031100.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:11:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=899678)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=899678)[0;0m WARNING 01-26 03:11:43 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=899678)[0;0m ERROR 01-26 03:11:46 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.19 GiB is free. Including non-PyTorch memory, this process has 11.25 GiB memory in use. Of the allocated memory 9.99 GiB is allocated by PyTorch, and 909.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:11:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:11:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:11:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:11:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:11:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:11:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:11:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:11:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:11:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:11:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:11:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:11:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:11:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:11:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:11:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:11:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:11:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=899678)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=899678)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=899678)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=899678)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[0;36m(EngineCore_DP0 pid=899678)[0;0m 
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=899678)[0;0m [2026-01-26 03:11:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
[0;36m(EngineCore_DP0 pid=899678)[0;0m [rank0]:W0126 03:11:46.844000 899678 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=899678)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=899678)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=899678)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=899678)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=899678)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=899678)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=899678)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=899678)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=899678)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=899678)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=899678)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=899678)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=899678)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=899678)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=899678)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=899678)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=899678)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=899678)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=899678)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=899678)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=899678)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=899678)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=899678)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=899678)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=899678)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=899678)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=899678)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=899678)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=899678)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=899678)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=899678)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=899678)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=899678)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=899678)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=899678)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=899678)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=899678)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.19 GiB is free. Including non-PyTorch memory, this process has 11.25 GiB memory in use. Of the allocated memory 9.99 GiB is allocated by PyTorch, and 909.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:11:47.259582384 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031100.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_8) | prefill | M=65536
Attempt: 2/3
GPU Mem Util: 0.40
Time: 2026-01-26 03:12:47
Duration: 51.5s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031158.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:12:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=900877)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=900877)[0;0m WARNING 01-26 03:12:41 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=900877)[0;0m ERROR 01-26 03:12:44 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.19 GiB is free. Including non-PyTorch memory, this process has 11.25 GiB memory in use. Of the allocated memory 9.99 GiB is allocated by PyTorch, and 909.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:12:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:12:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:12:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:12:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:12:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:12:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:12:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:12:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:12:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:12:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:12:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:12:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:12:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:12:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:12:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:12:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:12:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=900877)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=900877)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=900877)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=900877)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.09it/s]
[0;36m(EngineCore_DP0 pid=900877)[0;0m 
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=900877)[0;0m [2026-01-26 03:12:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
[0;36m(EngineCore_DP0 pid=900877)[0;0m [rank0]:W0126 03:12:44.284000 900877 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=900877)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=900877)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=900877)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=900877)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=900877)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=900877)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=900877)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=900877)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=900877)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=900877)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=900877)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=900877)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=900877)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=900877)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=900877)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=900877)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=900877)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=900877)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=900877)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=900877)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=900877)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=900877)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=900877)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=900877)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=900877)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=900877)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=900877)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=900877)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=900877)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=900877)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=900877)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=900877)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=900877)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=900877)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=900877)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=900877)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=900877)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.19 GiB is free. Including non-PyTorch memory, this process has 11.25 GiB memory in use. Of the allocated memory 9.99 GiB is allocated by PyTorch, and 909.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:12:44.705675499 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031158.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_8) | prefill | M=65536
Attempt: 3/3
GPU Mem Util: 0.35
Time: 2026-01-26 03:13:44
Duration: 51.4s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.35000000000000003 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.35000000000000003

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031256.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:13:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=902084)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=902084)[0;0m WARNING 01-26 03:13:38 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=902084)[0;0m ERROR 01-26 03:13:41 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.19 GiB is free. Including non-PyTorch memory, this process has 11.25 GiB memory in use. Of the allocated memory 9.99 GiB is allocated by PyTorch, and 909.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:13:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:13:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:13:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:13:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:13:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:13:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:13:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:13:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:13:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:13:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:13:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:13:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:13:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:13:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:13:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:13:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:13:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:32] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:32] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:32] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:32] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:32] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=902084)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=902084)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]
[0;36m(EngineCore_DP0 pid=902084)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=902084)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.09it/s]
[0;36m(EngineCore_DP0 pid=902084)[0;0m 
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=902084)[0;0m [2026-01-26 03:13:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
[0;36m(EngineCore_DP0 pid=902084)[0;0m [rank0]:W0126 03:13:41.557000 902084 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=902084)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=902084)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=902084)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=902084)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=902084)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=902084)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=902084)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=902084)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=902084)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=902084)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=902084)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=902084)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=902084)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=902084)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=902084)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=902084)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=902084)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=902084)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=902084)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=902084)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=902084)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=902084)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=902084)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=902084)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=902084)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=902084)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=902084)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=902084)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=902084)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=902084)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=902084)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=902084)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=902084)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=902084)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=902084)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=902084)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=902084)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 4.19 GiB is free. Including non-PyTorch memory, this process has 11.25 GiB memory in use. Of the allocated memory 9.99 GiB is allocated by PyTorch, and 909.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:13:42.963481874 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031256.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_10) | prefill | M=32768
Attempt: 1/3
GPU Mem Util: 0.50
Time: 2026-01-26 03:14:31
Duration: 36.6s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.5 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.5

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031358.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:14:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=903163)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=903163)[0;0m WARNING 01-26 03:14:26 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/tmp/torchinductor_root/t2/ct2qy3oaoj5e2t6od4xospdnak2efdzitg652mcxfeawkkpjdseq.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m ERROR 01-26 03:14:29 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 14.33 GiB memory in use. Of the allocated memory 11.39 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:14:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:14:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:14:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:14:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:14:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:14:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:14:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:14:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:14:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:14:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:14:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:14:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:14:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:14:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:14:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:14:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=903163)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=903163)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]
[0;36m(EngineCore_DP0 pid=903163)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01s/it]
[0;36m(EngineCore_DP0 pid=903163)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
[0;36m(EngineCore_DP0 pid=903163)[0;0m 
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=903163)[0;0m [2026-01-26 03:14:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
[0;36m(EngineCore_DP0 pid=903163)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=903163)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=903163)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=903163)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=903163)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=903163)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=903163)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=903163)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=903163)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=903163)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=903163)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=903163)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=903163)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=903163)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=903163)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=903163)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=903163)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=903163)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=903163)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=903163)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=903163)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=903163)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=903163)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=903163)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=903163)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/tmp/torchinductor_root/t2/ct2qy3oaoj5e2t6od4xospdnak2efdzitg652mcxfeawkkpjdseq.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=903163)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=903163)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=903163)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=903163)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=903163)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=903163)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=903163)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 14.33 GiB memory in use. Of the allocated memory 11.39 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:14:29.453963946 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031358.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_10) | prefill | M=32768
Attempt: 2/3
GPU Mem Util: 0.45
Time: 2026-01-26 03:15:14
Duration: 36.8s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031440.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:14:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=904162)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=904162)[0;0m WARNING 01-26 03:15:09 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/tmp/torchinductor_root/t2/ct2qy3oaoj5e2t6od4xospdnak2efdzitg652mcxfeawkkpjdseq.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m ERROR 01-26 03:15:11 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 14.33 GiB memory in use. Of the allocated memory 11.39 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:14:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:14:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:14:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:14:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:14:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:14:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:14:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:14:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:14:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:15:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:15:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:15:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:15:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:15:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:15:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:15:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:15:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:03] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=904162)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=904162)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]
[0;36m(EngineCore_DP0 pid=904162)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01s/it]
[0;36m(EngineCore_DP0 pid=904162)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
[0;36m(EngineCore_DP0 pid=904162)[0;0m 
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=904162)[0;0m [2026-01-26 03:15:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
[0;36m(EngineCore_DP0 pid=904162)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=904162)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=904162)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=904162)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=904162)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=904162)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=904162)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=904162)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=904162)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=904162)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=904162)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=904162)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=904162)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=904162)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=904162)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=904162)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=904162)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=904162)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=904162)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=904162)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=904162)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=904162)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=904162)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=904162)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=904162)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/tmp/torchinductor_root/t2/ct2qy3oaoj5e2t6od4xospdnak2efdzitg652mcxfeawkkpjdseq.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=904162)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=904162)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=904162)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=904162)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=904162)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=904162)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=904162)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 14.33 GiB memory in use. Of the allocated memory 11.39 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:15:12.085337571 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031440.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_10) | prefill | M=32768
Attempt: 3/3
GPU Mem Util: 0.40
Time: 2026-01-26 03:15:57
Duration: 37.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 32768 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [32768]
  M_decode:         [32768]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031523.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=32768
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 32768
â”‚   M_prefill     = 32768 (= 32 x 1024)
â”‚   M_decode      = 32
â”‚   batched_tokens = 32768 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 4096
â”‚   --max-num-seqs           = 32
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 32768
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:15:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=905155)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=905155)[0;0m WARNING 01-26 03:15:52 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/tmp/torchinductor_root/t2/ct2qy3oaoj5e2t6od4xospdnak2efdzitg652mcxfeawkkpjdseq.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m ERROR 01-26 03:15:54 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 14.33 GiB memory in use. Of the allocated memory 11.39 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:15:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:15:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:15:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:15:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:15:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:15:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:15:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:15:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:15:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:15:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:15:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:15:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:15:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:15:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:15:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:15:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:15:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=905155)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=905155)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
[0;36m(EngineCore_DP0 pid=905155)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01s/it]
[0;36m(EngineCore_DP0 pid=905155)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
[0;36m(EngineCore_DP0 pid=905155)[0;0m 
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=905155)[0;0m [2026-01-26 03:15:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
[0;36m(EngineCore_DP0 pid=905155)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=905155)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=905155)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=905155)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=905155)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=905155)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=905155)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=905155)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=905155)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=905155)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=905155)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=905155)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=905155)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=905155)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=905155)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=905155)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=905155)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=905155)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "<eval_with_key>.58", line 332, in forward
[0;36m(EngineCore_DP0 pid=905155)[0;0m     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=905155)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return compiled_fn(full_args)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=905155)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=905155)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=905155)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=905155)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=905155)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/tmp/torchinductor_root/t2/ct2qy3oaoj5e2t6od4xospdnak2efdzitg652mcxfeawkkpjdseq.py", line 1078, in call
[0;36m(EngineCore_DP0 pid=905155)[0;0m     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-FP8')
[0;36m(EngineCore_DP0 pid=905155)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
[0;36m(EngineCore_DP0 pid=905155)[0;0m     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
[0;36m(EngineCore_DP0 pid=905155)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
[0;36m(EngineCore_DP0 pid=905155)[0;0m     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
[0;36m(EngineCore_DP0 pid=905155)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=905155)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 14.33 GiB memory in use. Of the allocated memory 11.39 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:15:55.022923577 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=32768 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
32768,1024,32,4096,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031523.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_10) | prefill | M=65536
Attempt: 1/3
GPU Mem Util: 0.45
Time: 2026-01-26 03:17:00
Duration: 52.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.45 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.45

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031611.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:16:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=906454)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=906454)[0;0m WARNING 01-26 03:16:54 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=906454)[0;0m ERROR 01-26 03:16:57 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.81 GiB is free. Including non-PyTorch memory, this process has 11.64 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 896.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:16:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:16:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:16:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:16:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:16:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:16:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:16:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:16:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:16:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:16:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:16:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:16:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:16:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:16:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:16:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:16:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:16:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=906454)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=906454)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
[0;36m(EngineCore_DP0 pid=906454)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01s/it]
[0;36m(EngineCore_DP0 pid=906454)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
[0;36m(EngineCore_DP0 pid=906454)[0;0m 
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=906454)[0;0m [2026-01-26 03:16:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
[0;36m(EngineCore_DP0 pid=906454)[0;0m [rank0]:W0126 03:16:57.395000 906454 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=906454)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=906454)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=906454)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=906454)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=906454)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=906454)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=906454)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=906454)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=906454)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=906454)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=906454)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=906454)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=906454)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=906454)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=906454)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=906454)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=906454)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=906454)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=906454)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=906454)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=906454)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=906454)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=906454)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=906454)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=906454)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=906454)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=906454)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=906454)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=906454)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=906454)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=906454)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=906454)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=906454)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=906454)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=906454)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=906454)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=906454)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.81 GiB is free. Including non-PyTorch memory, this process has 11.64 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 896.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:16:58.811890226 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031611.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_10) | prefill | M=65536
Attempt: 2/3
GPU Mem Util: 0.40
Time: 2026-01-26 03:17:58
Duration: 52.1s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.4 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.4

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031709.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:17:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=907651)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=907651)[0;0m WARNING 01-26 03:17:52 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=907651)[0;0m ERROR 01-26 03:17:55 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.81 GiB is free. Including non-PyTorch memory, this process has 11.64 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 896.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:17:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:17:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:17:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:17:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:17:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:17:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:17:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:17:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:17:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:17:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:17:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:17:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:17:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:17:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:17:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:17:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:17:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=907651)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=907651)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
[0;36m(EngineCore_DP0 pid=907651)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00s/it]
[0;36m(EngineCore_DP0 pid=907651)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
[0;36m(EngineCore_DP0 pid=907651)[0;0m 
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=907651)[0;0m [2026-01-26 03:17:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
[0;36m(EngineCore_DP0 pid=907651)[0;0m [rank0]:W0126 03:17:55.406000 907651 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=907651)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=907651)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=907651)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=907651)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=907651)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=907651)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=907651)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=907651)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=907651)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=907651)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=907651)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=907651)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=907651)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=907651)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=907651)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=907651)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=907651)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=907651)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=907651)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=907651)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=907651)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=907651)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=907651)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=907651)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=907651)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=907651)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=907651)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=907651)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=907651)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=907651)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=907651)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=907651)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=907651)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=907651)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=907651)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=907651)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=907651)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.81 GiB is free. Including non-PyTorch memory, this process has 11.64 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 896.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:17:56.824040345 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031709.log


======================================================================
Test: qwen2.5-7b-fp8 | cusparselt (2_10) | prefill | M=65536
Attempt: 3/3
GPU Mem Util: 0.35
Time: 2026-01-26 03:18:55
Duration: 51.8s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.35000000000000003 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-fp8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.35000000000000003

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031807.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-FP8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-FP8                                  â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:18:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=908855)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=908855)[0;0m WARNING 01-26 03:18:50 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=908855)[0;0m ERROR 01-26 03:18:53 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.81 GiB is free. Including non-PyTorch memory, this process has 11.64 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 896.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:18:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:18:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:18:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:18:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:18:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:18:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:18:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:18:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:18:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:18:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:18:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 03:18:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 03:18:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:18:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:18:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:18:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:18:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:43] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
[0;36m(EngineCore_DP0 pid=908855)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=908855)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]
[0;36m(EngineCore_DP0 pid=908855)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01s/it]
[0;36m(EngineCore_DP0 pid=908855)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
[0;36m(EngineCore_DP0 pid=908855)[0;0m 
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=908855)[0;0m [2026-01-26 03:18:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
[0;36m(EngineCore_DP0 pid=908855)[0;0m [rank0]:W0126 03:18:53.046000 908855 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[0;36m(EngineCore_DP0 pid=908855)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=908855)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=908855)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=908855)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=908855)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=908855)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=908855)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=908855)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=908855)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=908855)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=908855)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=908855)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=908855)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=908855)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=908855)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=908855)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=908855)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=908855)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=908855)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=908855)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=908855)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=908855)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=908855)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=908855)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=908855)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=908855)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=908855)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=908855)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=908855)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=908855)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=908855)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=908855)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=908855)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=908855)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=908855)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=908855)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=908855)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 3.81 GiB is free. Including non-PyTorch memory, this process has 11.64 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 896.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:18:53.459422033 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-FP8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-FP8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031807.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | decode | M=512
Attempt: 1/3
GPU Mem Util: 0.80
Time: 2026-01-26 03:19:53
Duration: 47.0s
Success: True
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage decode --backend cusparselt --M 512 --gpu-mem 0.8 --gpu-id 1 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['decode']
  M_prefill:        [512]
  M_decode:         [512]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031910.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | decode[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=512
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     decode                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 512
â”‚   M_prefill     = 8192 (= 512 x 16)
â”‚   M_decode      = 512
â”‚   batched_tokens = 512 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 16
â”‚   --output-len             = 256
â”‚   --num-prompts            = 512
â”‚   --max-num-seqs           = 512
â”‚   --max-model-len          = 272
â”‚   --max-num-batched-tokens = 512
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 1
â”‚   N_decode  = 256
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:19:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=909764)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=909764)[0;0m WARNING 01-26 03:19:25 [backends.py:609] Failed to read file <frozen os>
Throughput: 27.73 requests/s, 7542.40 total tokens/s, 7098.73 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:19:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:19:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:19:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:19:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:19:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:19:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:19:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:19:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:19:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:19:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:19:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:19:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:19:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:19:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:19:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:19:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:19:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:18] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=909764)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=909764)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]
[0;36m(EngineCore_DP0 pid=909764)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
[0;36m(EngineCore_DP0 pid=909764)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
[0;36m(EngineCore_DP0 pid=909764)[0;0m 
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
[0;36m(EngineCore_DP0 pid=909764)[0;0m [2026-01-26 03:19:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
[0;36m(EngineCore_DP0 pid=909764)[0;0m 2026-01-26 03:19:28,447 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=909764)[0;0m 2026-01-26 03:19:28,461 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=909764)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:02, 22.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:00<00:02, 22.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:00<00:01, 23.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 12/51 [00:00<00:01, 23.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:00<00:01, 24.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:00<00:01, 24.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:00<00:01, 25.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:00<00:00, 26.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:01<00:00, 27.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/51 [00:01<00:00, 27.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:01<00:00, 24.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:01<00:00, 19.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:01<00:00, 18.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:01<00:00, 20.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:02<00:00, 22.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:02<00:00, 23.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:02<00:00, 23.29it/s]
[0;36m(EngineCore_DP0 pid=909764)[0;0m 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|â–         | 1/51 [00:00<00:07,  6.86it/s]
Capturing CUDA graphs (decode, FULL):   8%|â–Š         | 4/51 [00:00<00:02, 16.37it/s]
Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 7/51 [00:00<00:02, 20.07it/s]
Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–‰        | 10/51 [00:00<00:01, 22.78it/s]
Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:00<00:01, 24.86it/s]
Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:00<00:01, 25.87it/s]
Capturing CUDA graphs (decode, FULL):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:00<00:01, 28.02it/s]
Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:00<00:00, 29.51it/s]
Capturing CUDA graphs (decode, FULL):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:01<00:00, 30.15it/s]
Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/51 [00:01<00:00, 30.60it/s]
Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:01<00:00, 30.93it/s]
Capturing CUDA graphs (decode, FULL):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:01<00:00, 31.50it/s]
Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:01<00:00, 31.96it/s]
Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:01<00:00, 32.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:01<00:00, 28.32it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:00<00:00, 8302.37it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:05<46:52,  5.50s/it, est. speed input: 2.91 toks/s, output: 46.51 toks/s]
Processed prompts:  17%|â–ˆâ–‹        | 89/512 [00:05<00:20, 21.08it/s, est. speed input: 243.06 toks/s, output: 3888.88 toks/s]
Processed prompts:  20%|â–ˆâ–ˆ        | 104/512 [00:07<00:21, 18.64it/s, est. speed input: 236.09 toks/s, output: 3777.37 toks/s]
Processed prompts:  22%|â–ˆâ–ˆâ–       | 113/512 [00:07<00:21, 18.57it/s, est. speed input: 239.65 toks/s, output: 3834.48 toks/s]
Processed prompts:  23%|â–ˆâ–ˆâ–       | 119/512 [00:07<00:21, 18.23it/s, est. speed input: 240.45 toks/s, output: 3847.23 toks/s]
Processed prompts:  24%|â–ˆâ–ˆâ–       | 124/512 [00:08<00:19, 19.52it/s, est. speed input: 246.55 toks/s, output: 3944.80 toks/s]
Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 129/512 [00:08<00:19, 19.47it/s, est. speed input: 248.44 toks/s, output: 3975.09 toks/s]
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 134/512 [00:08<00:17, 21.34it/s, est. speed input: 254.10 toks/s, output: 4065.64 toks/s]
Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 138/512 [00:08<00:18, 20.17it/s, est. speed input: 254.10 toks/s, output: 4065.61 toks/s]
Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 142/512 [00:08<00:16, 22.10it/s, est. speed input: 258.22 toks/s, output: 4131.44 toks/s]
Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 148/512 [00:08<00:14, 25.95it/s, est. speed input: 265.08 toks/s, output: 4241.23 toks/s]
Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 152/512 [00:09<00:14, 24.43it/s, est. speed input: 266.34 toks/s, output: 4261.42 toks/s]
Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 156/512 [00:09<00:13, 26.66it/s, est. speed input: 270.21 toks/s, output: 4323.28 toks/s]
Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 162/512 [00:09<00:11, 31.57it/s, est. speed input: 276.95 toks/s, output: 4431.27 toks/s]
Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–      | 168/512 [00:09<00:12, 27.94it/s, est. speed input: 279.28 toks/s, output: 4468.48 toks/s]
Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 174/512 [00:09<00:10, 32.21it/s, est. speed input: 285.54 toks/s, output: 4568.69 toks/s]
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 180/512 [00:09<00:09, 35.91it/s, est. speed input: 291.66 toks/s, output: 4666.49 toks/s]
Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 185/512 [00:09<00:08, 37.85it/s, est. speed input: 296.37 toks/s, output: 4741.94 toks/s]
Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 190/512 [00:10<00:10, 30.80it/s, est. speed input: 297.22 toks/s, output: 4755.59 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 196/512 [00:10<00:09, 34.66it/s, est. speed input: 302.83 toks/s, output: 4845.27 toks/s]
Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 201/512 [00:10<00:08, 37.73it/s, est. speed input: 307.52 toks/s, output: 4920.35 toks/s]
Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 207/512 [00:10<00:08, 37.92it/s, est. speed input: 312.03 toks/s, output: 4992.49 toks/s]
Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 213/512 [00:10<00:07, 41.70it/s, est. speed input: 317.70 toks/s, output: 5083.18 toks/s]
Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 218/512 [00:10<00:08, 33.78it/s, est. speed input: 318.54 toks/s, output: 5096.62 toks/s]
Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 224/512 [00:11<00:08, 35.90it/s, est. speed input: 323.04 toks/s, output: 5168.60 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 229/512 [00:11<00:07, 38.86it/s, est. speed input: 327.28 toks/s, output: 5236.42 toks/s]
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 236/512 [00:11<00:06, 44.34it/s, est. speed input: 333.78 toks/s, output: 5340.46 toks/s]
Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 242/512 [00:11<00:06, 43.73it/s, est. speed input: 338.04 toks/s, output: 5408.56 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 247/512 [00:11<00:09, 27.77it/s, est. speed input: 334.44 toks/s, output: 5351.08 toks/s]
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 251/512 [00:12<00:14, 18.58it/s, est. speed input: 327.38 toks/s, output: 5238.11 toks/s]
Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 254/512 [00:12<00:18, 13.98it/s, est. speed input: 320.13 toks/s, output: 5122.03 toks/s]
Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 257/512 [00:12<00:18, 13.70it/s, est. speed input: 318.03 toks/s, output: 5088.52 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 259/512 [00:13<00:18, 13.65it/s, est. speed input: 316.86 toks/s, output: 5069.78 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 262/512 [00:13<00:16, 15.26it/s, est. speed input: 317.27 toks/s, output: 5076.25 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 265/512 [00:13<00:14, 17.22it/s, est. speed input: 318.09 toks/s, output: 5089.45 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 269/512 [00:13<00:11, 20.34it/s, est. speed input: 319.77 toks/s, output: 5116.39 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 272/512 [00:13<00:10, 22.28it/s, est. speed input: 320.95 toks/s, output: 5135.23 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 277/512 [00:13<00:08, 26.85it/s, est. speed input: 323.82 toks/s, output: 5181.10 toks/s]
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 285/512 [00:13<00:06, 36.99it/s, est. speed input: 330.15 toks/s, output: 5282.37 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 290/512 [00:13<00:06, 34.04it/s, est. speed input: 331.75 toks/s, output: 5307.93 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 294/512 [00:14<00:06, 34.09it/s, est. speed input: 333.54 toks/s, output: 5336.58 toks/s]
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 298/512 [00:14<00:06, 35.10it/s, est. speed input: 335.58 toks/s, output: 5369.36 toks/s]
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 302/512 [00:14<00:06, 32.79it/s, est. speed input: 336.70 toks/s, output: 5387.14 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 310/512 [00:14<00:05, 39.07it/s, est. speed input: 341.92 toks/s, output: 5470.79 toks/s]
Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 315/512 [00:14<00:04, 41.39it/s, est. speed input: 345.00 toks/s, output: 5520.03 toks/s]
Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 320/512 [00:14<00:04, 43.19it/s, est. speed input: 348.02 toks/s, output: 5568.29 toks/s]
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 326/512 [00:14<00:03, 47.27it/s, est. speed input: 352.09 toks/s, output: 5633.42 toks/s]
Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 331/512 [00:14<00:04, 44.67it/s, est. speed input: 354.44 toks/s, output: 5670.98 toks/s]
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 336/512 [00:15<00:04, 42.94it/s, est. speed input: 356.74 toks/s, output: 5707.88 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 341/512 [00:15<00:03, 43.99it/s, est. speed input: 359.50 toks/s, output: 5752.02 toks/s]
Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 355/512 [00:15<00:02, 68.95it/s, est. speed input: 371.71 toks/s, output: 5947.40 toks/s]
Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 367/512 [00:15<00:01, 79.47it/s, est. speed input: 381.38 toks/s, output: 6102.12 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 376/512 [00:16<00:03, 34.34it/s, est. speed input: 375.80 toks/s, output: 6012.79 toks/s]
Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 383/512 [00:16<00:04, 27.68it/s, est. speed input: 373.44 toks/s, output: 5975.10 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 388/512 [00:16<00:04, 27.61it/s, est. speed input: 374.15 toks/s, output: 5986.35 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 393/512 [00:16<00:04, 28.20it/s, est. speed input: 375.26 toks/s, output: 6004.09 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 397/512 [00:16<00:03, 29.59it/s, est. speed input: 376.62 toks/s, output: 6025.94 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 401/512 [00:17<00:04, 27.09it/s, est. speed input: 376.17 toks/s, output: 6018.68 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 405/512 [00:17<00:03, 29.06it/s, est. speed input: 377.56 toks/s, output: 6040.94 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 409/512 [00:17<00:03, 26.68it/s, est. speed input: 377.22 toks/s, output: 6035.49 toks/s]
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 413/512 [00:17<00:03, 28.27it/s, est. speed input: 378.32 toks/s, output: 6053.11 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 418/512 [00:17<00:03, 30.87it/s, est. speed input: 380.05 toks/s, output: 6080.78 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 422/512 [00:17<00:02, 31.80it/s, est. speed input: 381.18 toks/s, output: 6098.94 toks/s]
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 429/512 [00:17<00:02, 40.67it/s, est. speed input: 385.31 toks/s, output: 6164.88 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 439/512 [00:17<00:01, 54.11it/s, est. speed input: 391.85 toks/s, output: 6269.62 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 449/512 [00:18<00:01, 62.72it/s, est. speed input: 398.11 toks/s, output: 6369.75 toks/s]
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 458/512 [00:18<00:00, 69.17it/s, est. speed input: 403.77 toks/s, output: 6460.35 toks/s]
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 477/512 [00:18<00:00, 101.56it/s, est. speed input: 418.21 toks/s, output: 6691.32 toks/s]
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 497/512 [00:18<00:00, 127.49it/s, est. speed input: 433.28 toks/s, output: 6932.41 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:18<00:00, 127.49it/s, est. speed input: 445.18 toks/s, output: 7122.85 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:18<00:00, 27.82it/s, est. speed input: 445.18 toks/s, output: 7122.85 toks/s] 
[rank0]:[W126 03:19:51.733666408 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[0;32m[SUCCESS][0m æµ‹è¯•å®Œæˆ! è€—æ—¶: 43.0s

[0;32mæµ‹è¯•ç»“æœ:[0m
  Requests/s:   27.73
  Tokens/s:     7542.40
  Total Reqs:   512
  Elapsed:      18.46s

  [Decode åˆ†æ]
  Total Decode Tokens:  131072
  Decode Tokens/s:      7098.73


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_decode.csv

é¢„è§ˆ:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
512,16,512,512,256,256,27.7294,7542.3963,18.4642

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 1 æˆåŠŸ, 0 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m1 æˆåŠŸ[0m, 0 å¤±è´¥
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_031910.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | decode | M=128
Attempt: 1/3
GPU Mem Util: 0.85
Time: 2026-01-26 03:20:30
Duration: 26.4s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage decode --backend cusparselt --M 128 --gpu-mem 0.85 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['decode']
  M_prefill:        [128]
  M_decode:         [128]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.85

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032007.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | decode[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=128
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     decode                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 128
â”‚   M_prefill     = 2048 (= 128 x 16)
â”‚   M_decode      = 128
â”‚   batched_tokens = 272 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 16
â”‚   --output-len             = 256
â”‚   --num-prompts            = 128
â”‚   --max-num-seqs           = 128
â”‚   --max-model-len          = 272
â”‚   --max-num-batched-tokens = 272
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 1
â”‚   N_decode  = 256
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:20:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=910986)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=910986)[0;0m WARNING 01-26 03:20:22 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     sampler_output = self.sampler(
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]                      ^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 90, in forward
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     logits = logits.to(torch.float32)
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 50.94 MiB is free. Including non-PyTorch memory, this process has 15.40 GiB memory in use. Of the allocated memory 12.51 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866] 
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866] The above exception was the direct cause of the following exception:
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866] 
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866]     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=910986)[0;0m ERROR 01-26 03:20:28 [core.py:866] RuntimeError: CUDA out of memory occurred when warming up sampler with 128 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:20:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:20:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:20:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:20:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:20:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:20:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:20:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:20:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:20:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:20:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:20:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:20:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:20:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:20:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:20:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:20:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:16] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=910986)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=910986)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.34it/s]
[0;36m(EngineCore_DP0 pid=910986)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
[0;36m(EngineCore_DP0 pid=910986)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]
[0;36m(EngineCore_DP0 pid=910986)[0;0m 
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=910986)[0;0m [2026-01-26 03:20:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=910986)[0;0m 2026-01-26 03:20:25,778 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=910986)[0;0m 2026-01-26 03:20:25,792 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=910986)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–Š         | 3/35 [00:00<00:01, 23.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 6/35 [00:00<00:01, 26.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 27.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 26.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15/35 [00:00<00:00, 27.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:00<00:00, 27.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:00<00:00, 27.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:00<00:00, 27.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:01<00:00, 28.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:01<00:00, 24.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:01<00:00, 22.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 24.95it/s]
[0;36m(EngineCore_DP0 pid=910986)[0;0m 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|â–Œ         | 1/19 [00:00<00:02,  6.93it/s]
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:00<00:00, 20.98it/s]
Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 25.94it/s]
Capturing CUDA graphs (decode, FULL):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [00:00<00:00, 28.54it/s]
Capturing CUDA graphs (decode, FULL):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:00<00:00, 30.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 27.38it/s]
[0;36m(EngineCore_DP0 pid=910986)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=910986)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=910986)[0;0m     sampler_output = self.sampler(
[0;36m(EngineCore_DP0 pid=910986)[0;0m                      ^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=910986)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=910986)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 90, in forward
[0;36m(EngineCore_DP0 pid=910986)[0;0m     logits = logits.to(torch.float32)
[0;36m(EngineCore_DP0 pid=910986)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 50.94 MiB is free. Including non-PyTorch memory, this process has 15.40 GiB memory in use. Of the allocated memory 12.51 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=910986)[0;0m 
[0;36m(EngineCore_DP0 pid=910986)[0;0m The above exception was the direct cause of the following exception:
[0;36m(EngineCore_DP0 pid=910986)[0;0m 
[0;36m(EngineCore_DP0 pid=910986)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=910986)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=910986)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=910986)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=910986)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=910986)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=910986)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=910986)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=910986)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[0;36m(EngineCore_DP0 pid=910986)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=910986)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=910986)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
[0;36m(EngineCore_DP0 pid=910986)[0;0m     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=910986)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=910986)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=910986)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=910986)[0;0m     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=910986)[0;0m RuntimeError: CUDA out of memory occurred when warming up sampler with 128 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[rank0]:[W126 03:20:28.511461783 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=128 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_decode.csv

é¢„è§ˆ:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
128,16,128,128,256,256,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032007.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | decode | M=128
Attempt: 2/3
GPU Mem Util: 0.80
Time: 2026-01-26 03:21:06
Duration: 30.1s
Success: True
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage decode --backend cusparselt --M 128 --gpu-mem 0.7999999999999999 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['decode']
  M_prefill:        [128]
  M_decode:         [128]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.7999999999999999

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032039.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | decode[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=128
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     decode                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 128
â”‚   M_prefill     = 2048 (= 128 x 16)
â”‚   M_decode      = 128
â”‚   batched_tokens = 272 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 16
â”‚   --output-len             = 256
â”‚   --num-prompts            = 128
â”‚   --max-num-seqs           = 128
â”‚   --max-model-len          = 272
â”‚   --max-num-batched-tokens = 272
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 1
â”‚   N_decode  = 256
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:20:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=911845)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=911845)[0;0m WARNING 01-26 03:20:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.18 requests/s, 8753.74 total tokens/s, 8238.81 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:20:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:20:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:20:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:20:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:20:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:20:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:20:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:20:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:20:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:20:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:20:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:20:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:20:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:20:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:20:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:20:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:20:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=911845)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=911845)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.37it/s]
[0;36m(EngineCore_DP0 pid=911845)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=911845)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[0;36m(EngineCore_DP0 pid=911845)[0;0m 
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=911845)[0;0m [2026-01-26 03:20:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=911845)[0;0m 2026-01-26 03:20:57,997 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=911845)[0;0m 2026-01-26 03:20:58,011 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=911845)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–Š         | 3/35 [00:00<00:01, 25.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 6/35 [00:00<00:01, 25.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 26.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 27.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15/35 [00:00<00:00, 27.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:00<00:00, 28.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:00<00:00, 27.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:00<00:00, 28.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 29/35 [00:01<00:00, 28.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:01<00:00, 29.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 28.15it/s]
[0;36m(EngineCore_DP0 pid=911845)[0;0m 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|â–Œ         | 1/19 [00:00<00:02,  7.52it/s]
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:00<00:00, 21.92it/s]
Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 26.68it/s]
Capturing CUDA graphs (decode, FULL):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [00:00<00:00, 29.23it/s]
Capturing CUDA graphs (decode, FULL):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:00<00:00, 28.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 26.07it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 6513.61it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<08:04,  3.81s/it, est. speed input: 4.19 toks/s, output: 67.11 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 98/128 [00:03<00:00, 35.15it/s, est. speed input: 399.63 toks/s, output: 6394.14 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:03<00:00, 35.15it/s, est. speed input: 517.67 toks/s, output: 8282.75 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:03<00:00, 32.35it/s, est. speed input: 517.67 toks/s, output: 8282.75 toks/s]
[rank0]:[W126 03:21:04.772065235 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[0;32m[SUCCESS][0m æµ‹è¯•å®Œæˆ! è€—æ—¶: 26.3s

[0;32mæµ‹è¯•ç»“æœ:[0m
  Requests/s:   32.18
  Tokens/s:     8753.74
  Total Reqs:   128
  Elapsed:      3.98s

  [Decode åˆ†æ]
  Total Decode Tokens:  32768
  Decode Tokens/s:      8238.81


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_decode.csv

é¢„è§ˆ:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
128,16,128,128,256,256,32.1829,8753.7397,3.9773

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 1 æˆåŠŸ, 0 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m1 æˆåŠŸ[0m, 0 å¤±è´¥
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032039.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | decode | M=256
Attempt: 1/3
GPU Mem Util: 0.85
Time: 2026-01-26 03:21:44
Duration: 26.7s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage decode --backend cusparselt --M 256 --gpu-mem 0.85 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['decode']
  M_prefill:        [256]
  M_decode:         [256]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.85

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032120.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | decode[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=256
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     decode                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 256
â”‚   M_prefill     = 4096 (= 256 x 16)
â”‚   M_decode      = 256
â”‚   batched_tokens = 272 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 16
â”‚   --output-len             = 256
â”‚   --num-prompts            = 256
â”‚   --max-num-seqs           = 256
â”‚   --max-model-len          = 272
â”‚   --max-num-batched-tokens = 272
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 1
â”‚   N_decode  = 256
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:21:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=912842)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=912842)[0;0m WARNING 01-26 03:21:35 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     sampler_output = self.sampler(
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]                      ^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     random_sampled, processed_logprobs = self.topk_topp_sampler(
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]                                          ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     logits = self.apply_top_k_top_p(logits, k, p)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 60.94 MiB is free. Including non-PyTorch memory, this process has 15.39 GiB memory in use. Of the allocated memory 12.42 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866] 
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866] The above exception was the direct cause of the following exception:
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866] 
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866]     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=912842)[0;0m ERROR 01-26 03:21:41 [core.py:866] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:21:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:21:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:21:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:21:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:21:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:21:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:21:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:21:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:21:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:21:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:21:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:21:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:21:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:21:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:21:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:21:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=912842)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=912842)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.37it/s]
[0;36m(EngineCore_DP0 pid=912842)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=912842)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[0;36m(EngineCore_DP0 pid=912842)[0;0m 
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=912842)[0;0m [2026-01-26 03:21:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=912842)[0;0m 2026-01-26 03:21:38,702 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=912842)[0;0m 2026-01-26 03:21:38,716 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=912842)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 3/36 [00:00<00:01, 26.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 6/36 [00:00<00:01, 17.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 8/36 [00:00<00:01, 17.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|â–ˆâ–ˆâ–Š       | 10/36 [00:00<00:01, 17.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 13/36 [00:00<00:01, 21.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/36 [00:00<00:00, 23.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/36 [00:00<00:00, 25.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 22/36 [00:00<00:00, 26.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/36 [00:01<00:00, 27.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 30/36 [00:01<00:00, 28.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 34/36 [00:01<00:00, 29.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:01<00:00, 24.91it/s]
[0;36m(EngineCore_DP0 pid=912842)[0;0m 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|â–         | 1/35 [00:00<00:04,  7.38it/s]
Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:01, 21.02it/s]
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:01, 25.79it/s]
Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:00<00:00, 28.09it/s]
Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:00<00:00, 29.66it/s]
Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:00<00:00, 30.75it/s]
Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:00<00:00, 31.48it/s]
Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 29/35 [00:01<00:00, 32.14it/s]
Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:01<00:00, 31.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 29.47it/s]
[0;36m(EngineCore_DP0 pid=912842)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=912842)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=912842)[0;0m     sampler_output = self.sampler(
[0;36m(EngineCore_DP0 pid=912842)[0;0m                      ^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=912842)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=912842)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
[0;36m(EngineCore_DP0 pid=912842)[0;0m     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
[0;36m(EngineCore_DP0 pid=912842)[0;0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
[0;36m(EngineCore_DP0 pid=912842)[0;0m     random_sampled, processed_logprobs = self.topk_topp_sampler(
[0;36m(EngineCore_DP0 pid=912842)[0;0m                                          ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=912842)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=912842)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
[0;36m(EngineCore_DP0 pid=912842)[0;0m     logits = self.apply_top_k_top_p(logits, k, p)
[0;36m(EngineCore_DP0 pid=912842)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
[0;36m(EngineCore_DP0 pid=912842)[0;0m     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[0;36m(EngineCore_DP0 pid=912842)[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 60.94 MiB is free. Including non-PyTorch memory, this process has 15.39 GiB memory in use. Of the allocated memory 12.42 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=912842)[0;0m 
[0;36m(EngineCore_DP0 pid=912842)[0;0m The above exception was the direct cause of the following exception:
[0;36m(EngineCore_DP0 pid=912842)[0;0m 
[0;36m(EngineCore_DP0 pid=912842)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=912842)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=912842)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=912842)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=912842)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=912842)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=912842)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=912842)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=912842)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[0;36m(EngineCore_DP0 pid=912842)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=912842)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=912842)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
[0;36m(EngineCore_DP0 pid=912842)[0;0m     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=912842)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=912842)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=912842)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=912842)[0;0m     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=912842)[0;0m RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[rank0]:[W126 03:21:42.984824881 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=256 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_decode.csv

é¢„è§ˆ:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
256,16,256,256,256,256,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032120.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | decode | M=256
Attempt: 2/3
GPU Mem Util: 0.80
Time: 2026-01-26 03:22:26
Duration: 36.6s
Success: True
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage decode --backend cusparselt --M 256 --gpu-mem 0.7999999999999999 --gpu-id 1 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['decode']
  M_prefill:        [256]
  M_decode:         [256]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.7999999999999999

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032153.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | decode[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=256
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     decode                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 256
â”‚   M_prefill     = 4096 (= 256 x 16)
â”‚   M_decode      = 256
â”‚   batched_tokens = 272 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 16
â”‚   --output-len             = 256
â”‚   --num-prompts            = 256
â”‚   --max-num-seqs           = 256
â”‚   --max-model-len          = 272
â”‚   --max-num-batched-tokens = 272
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 1
â”‚   N_decode  = 256
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:21:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=913700)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=913700)[0;0m WARNING 01-26 03:22:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 25.41 requests/s, 6910.98 total tokens/s, 6504.46 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:21:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:21:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:21:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:21:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:21:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:21:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:21:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:21:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:21:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:22:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:22:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:22:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:22:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:22:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:22:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:22:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:22:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=913700)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=913700)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.37it/s]
[0;36m(EngineCore_DP0 pid=913700)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
[0;36m(EngineCore_DP0 pid=913700)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]
[0;36m(EngineCore_DP0 pid=913700)[0;0m 
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
[0;36m(EngineCore_DP0 pid=913700)[0;0m [2026-01-26 03:22:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
[0;36m(EngineCore_DP0 pid=913700)[0;0m 2026-01-26 03:22:11,294 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=913700)[0;0m 2026-01-26 03:22:11,308 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=913700)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 3/36 [00:00<00:01, 27.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 6/36 [00:00<00:01, 27.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 9/36 [00:00<00:00, 28.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–      | 12/36 [00:00<00:00, 28.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15/36 [00:00<00:00, 28.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 18/36 [00:00<00:00, 18.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 21/36 [00:00<00:00, 18.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 24/36 [00:01<00:00, 21.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 28/36 [00:01<00:00, 23.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 32/36 [00:01<00:00, 25.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:01<00:00, 26.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:01<00:00, 24.48it/s]
[0;36m(EngineCore_DP0 pid=913700)[0;0m 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|â–         | 1/35 [00:00<00:04,  7.49it/s]
Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:01, 21.15it/s]
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 26.00it/s]
Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 26.66it/s]
Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:00, 28.86it/s]
Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:00<00:00, 30.41it/s]
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 31.28it/s]
Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:00<00:00, 31.79it/s]
Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:01<00:00, 32.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 29.37it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:00<00:00, 5531.30it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:05<24:03,  5.66s/it, est. speed input: 2.83 toks/s, output: 45.22 toks/s]
Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 75/256 [00:05<00:09, 18.36it/s, est. speed input: 208.11 toks/s, output: 3329.78 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 135/256 [00:05<00:03, 37.65it/s, est. speed input: 366.03 toks/s, output: 5856.46 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 178/256 [00:08<00:02, 28.30it/s, est. speed input: 351.19 toks/s, output: 5619.02 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 205/256 [00:08<00:01, 30.19it/s, est. speed input: 372.02 toks/s, output: 5952.36 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 224/256 [00:09<00:00, 32.00it/s, est. speed input: 386.92 toks/s, output: 6190.72 toks/s]
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 238/256 [00:09<00:00, 33.55it/s, est. speed input: 397.43 toks/s, output: 6358.81 toks/s]
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 249/256 [00:09<00:00, 34.30it/s, est. speed input: 403.85 toks/s, output: 6461.63 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:10<00:00, 34.30it/s, est. speed input: 408.48 toks/s, output: 6535.63 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:10<00:00, 25.53it/s, est. speed input: 408.48 toks/s, output: 6535.63 toks/s]
[rank0]:[W126 03:22:25.882958880 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[0;32m[SUCCESS][0m æµ‹è¯•å®Œæˆ! è€—æ—¶: 33.0s

[0;32mæµ‹è¯•ç»“æœ:[0m
  Requests/s:   25.41
  Tokens/s:     6910.98
  Total Reqs:   256
  Elapsed:      10.08s

  [Decode åˆ†æ]
  Total Decode Tokens:  65536
  Decode Tokens/s:      6504.46


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_decode.csv

é¢„è§ˆ:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
256,16,256,256,256,256,25.4080,6910.9842,10.0756

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 1 æˆåŠŸ, 0 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m1 æˆåŠŸ[0m, 0 å¤±è´¥
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032153.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | decode | M=128
Attempt: 1/3
GPU Mem Util: 0.85
Time: 2026-01-26 03:23:03
Duration: 26.0s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage decode --backend cusparselt --M 128 --gpu-mem 0.85 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['decode']
  M_prefill:        [128]
  M_decode:         [128]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.85

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032240.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | decode[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=128
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     decode                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 128
â”‚   M_prefill     = 2048 (= 128 x 16)
â”‚   M_decode      = 128
â”‚   batched_tokens = 272 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 16
â”‚   --output-len             = 256
â”‚   --num-prompts            = 128
â”‚   --max-num-seqs           = 128
â”‚   --max-model-len          = 272
â”‚   --max-num-batched-tokens = 272
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 1
â”‚   N_decode  = 256
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:22:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=914794)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=914794)[0;0m WARNING 01-26 03:22:56 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 459, in compile_or_warm_up_model
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     cuda_graph_memory_bytes = self.model_runner.capture_model()
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4540, in capture_model
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     self._capture_cudagraphs(
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4641, in _capture_cudagraphs
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     self._dummy_run(
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     def forward(
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "<eval_with_key>.58", line 346, in forward
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     submod_8 = self.submod_8(getitem_18, s72, l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_, getitem_19, l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_18 = l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = getitem_19 = l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/tmp/torchinductor_root/gf/cgfdacf5mdofsioguze4gd3476smitxucnxsvsy4pxfxg3ysc4fg.py", line 950, in call
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     buf12 = torch.ops.slidesparse.cusparselt_int8_mm.default(arg6_1, buf10, 37888, 5760, 'bf16')
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/slidesparse/core/gemm_wrapper.py", line 1120, in _cusparselt_int8_mm_impl
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     return ext.cusparselt_int8_mm(weight_compressed, qinput, N, K_slide, inner_dtype,
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]   File "/root/vllmbench/slidesparse/core/gemm_wrapper.py", line 800, in cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]     output = torch.empty((M_pad, N), dtype=out_dtype, device=qinput.device)
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m ERROR 01-26 03:23:01 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 18.94 MiB is free. Including non-PyTorch memory, this process has 15.43 GiB memory in use. Of the allocated memory 12.46 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:22:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:22:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:22:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:22:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:22:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:22:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:22:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:22:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:22:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:22:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:22:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:22:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:22:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:22:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:22:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:22:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:22:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=914794)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=914794)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.16it/s]
[0;36m(EngineCore_DP0 pid=914794)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.26s/it]
[0;36m(EngineCore_DP0 pid=914794)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.20s/it]
[0;36m(EngineCore_DP0 pid=914794)[0;0m 
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=914794)[0;0m [2026-01-26 03:22:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=914794)[0;0m 2026-01-26 03:22:59,489 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=914794)[0;0m 2026-01-26 03:22:59,503 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=914794)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–Š         | 3/35 [00:00<00:01, 23.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 6/35 [00:00<00:01, 26.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 26.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:01, 17.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15/35 [00:00<00:01, 17.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:00<00:00, 19.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:01<00:00, 21.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:01<00:00, 23.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:01<00:00, 22.05it/s]
[0;36m(EngineCore_DP0 pid=914794)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=914794)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=914794)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=914794)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=914794)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=914794)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=914794)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=914794)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[0;36m(EngineCore_DP0 pid=914794)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=914794)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 459, in compile_or_warm_up_model
[0;36m(EngineCore_DP0 pid=914794)[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()
[0;36m(EngineCore_DP0 pid=914794)[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4540, in capture_model
[0;36m(EngineCore_DP0 pid=914794)[0;0m     self._capture_cudagraphs(
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4641, in _capture_cudagraphs
[0;36m(EngineCore_DP0 pid=914794)[0;0m     self._dummy_run(
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=914794)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=914794)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=914794)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=914794)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=914794)[0;0m     def forward(
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return self.optimized_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "<eval_with_key>.58", line 346, in forward
[0;36m(EngineCore_DP0 pid=914794)[0;0m     submod_8 = self.submod_8(getitem_18, s72, l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_, getitem_19, l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_18 = l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = getitem_19 = l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[0;36m(EngineCore_DP0 pid=914794)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return range_entry.runnable(*args)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[0;36m(EngineCore_DP0 pid=914794)[0;0m     graph_output = inductor_compiled_graph(*args)
[0;36m(EngineCore_DP0 pid=914794)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return self._compiled_fn(*args)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[0;36m(EngineCore_DP0 pid=914794)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[0;36m(EngineCore_DP0 pid=914794)[0;0m     all_outs = call_func_at_runtime_with_args(
[0;36m(EngineCore_DP0 pid=914794)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[0;36m(EngineCore_DP0 pid=914794)[0;0m     out = normalize_as_list(f(args))
[0;36m(EngineCore_DP0 pid=914794)[0;0m                             ^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return compiled_fn(runtime_args)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return self.current_callable(inputs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
[0;36m(EngineCore_DP0 pid=914794)[0;0m     out = model(new_inputs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m           ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/tmp/torchinductor_root/gf/cgfdacf5mdofsioguze4gd3476smitxucnxsvsy4pxfxg3ysc4fg.py", line 950, in call
[0;36m(EngineCore_DP0 pid=914794)[0;0m     buf12 = torch.ops.slidesparse.cusparselt_int8_mm.default(arg6_1, buf10, 37888, 5760, 'bf16')
[0;36m(EngineCore_DP0 pid=914794)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/slidesparse/core/gemm_wrapper.py", line 1120, in _cusparselt_int8_mm_impl
[0;36m(EngineCore_DP0 pid=914794)[0;0m     return ext.cusparselt_int8_mm(weight_compressed, qinput, N, K_slide, inner_dtype,
[0;36m(EngineCore_DP0 pid=914794)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m   File "/root/vllmbench/slidesparse/core/gemm_wrapper.py", line 800, in cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=914794)[0;0m     output = torch.empty((M_pad, N), dtype=out_dtype, device=qinput.device)
[0;36m(EngineCore_DP0 pid=914794)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=914794)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 18.94 MiB is free. Including non-PyTorch memory, this process has 15.43 GiB memory in use. Of the allocated memory 12.46 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 03:23:01.375505093 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=128 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_decode.csv

é¢„è§ˆ:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
128,16,128,128,256,256,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032240.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | decode | M=128
Attempt: 2/3
GPU Mem Util: 0.80
Time: 2026-01-26 03:23:41
Duration: 31.5s
Success: True
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage decode --backend cusparselt --M 128 --gpu-mem 0.7999999999999999 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['decode']
  M_prefill:        [128]
  M_decode:         [128]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.7999999999999999

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032312.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | decode[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=128
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     decode                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 128
â”‚   M_prefill     = 2048 (= 128 x 16)
â”‚   M_decode      = 128
â”‚   batched_tokens = 272 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 16
â”‚   --output-len             = 256
â”‚   --num-prompts            = 128
â”‚   --max-num-seqs           = 128
â”‚   --max-model-len          = 272
â”‚   --max-num-batched-tokens = 272
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 1
â”‚   N_decode  = 256
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:23:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=915638)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=915638)[0;0m WARNING 01-26 03:23:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.25 requests/s, 8226.64 total tokens/s, 7742.72 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:23:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:23:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:23:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:23:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:23:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:23:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:23:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:23:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:23:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:23:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:23:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:23:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:23:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:23:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:23:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:23:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=915638)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=915638)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
[0;36m(EngineCore_DP0 pid=915638)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
[0;36m(EngineCore_DP0 pid=915638)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=915638)[0;0m 
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=915638)[0;0m [2026-01-26 03:23:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=915638)[0;0m 2026-01-26 03:23:31,885 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=915638)[0;0m 2026-01-26 03:23:31,899 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=915638)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–Š         | 3/35 [00:00<00:01, 25.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 6/35 [00:00<00:01, 26.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:01, 25.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 26.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15/35 [00:00<00:00, 27.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:00<00:00, 27.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:00<00:00, 27.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:00<00:00, 27.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:01<00:00, 23.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:01<00:00, 21.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:01<00:00, 22.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 24.54it/s]
[0;36m(EngineCore_DP0 pid=915638)[0;0m 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|â–Œ         | 1/19 [00:00<00:02,  7.58it/s]
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:00<00:00, 21.61it/s]
Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 26.25it/s]
Capturing CUDA graphs (decode, FULL):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [00:00<00:00, 28.62it/s]
Capturing CUDA graphs (decode, FULL):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:00<00:00, 30.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 27.53it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 4174.48it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<08:34,  4.05s/it, est. speed input: 3.95 toks/s, output: 63.24 toks/s]
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 87/128 [00:04<00:01, 29.52it/s, est. speed input: 335.30 toks/s, output: 5364.76 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:04<00:00, 29.52it/s, est. speed input: 487.75 toks/s, output: 7803.96 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:04<00:00, 30.48it/s, est. speed input: 487.75 toks/s, output: 7803.96 toks/s]
[rank0]:[W126 03:23:39.081488101 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[0;32m[SUCCESS][0m æµ‹è¯•å®Œæˆ! è€—æ—¶: 27.5s

[0;32mæµ‹è¯•ç»“æœ:[0m
  Requests/s:   30.25
  Tokens/s:     8226.64
  Total Reqs:   128
  Elapsed:      4.23s

  [Decode åˆ†æ]
  Total Decode Tokens:  32768
  Decode Tokens/s:      7742.72


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_decode.csv

é¢„è§ˆ:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
128,16,128,128,256,256,30.2450,8226.6421,4.2321

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 1 æˆåŠŸ, 0 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m1 æˆåŠŸ[0m, 0 å¤±è´¥
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032312.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | decode | M=512
Attempt: 1/3
GPU Mem Util: 0.80
Time: 2026-01-26 03:24:20
Duration: 28.7s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage decode --backend cusparselt --M 512 --gpu-mem 0.8 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['decode']
  M_prefill:        [512]
  M_decode:         [512]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032354.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | decode[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=512
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     decode                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 512
â”‚   M_prefill     = 8192 (= 512 x 16)
â”‚   M_decode      = 512
â”‚   batched_tokens = 512 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 16
â”‚   --output-len             = 256
â”‚   --num-prompts            = 512
â”‚   --max-num-seqs           = 512
â”‚   --max-model-len          = 272
â”‚   --max-num-batched-tokens = 512
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 1
â”‚   N_decode  = 256
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:23:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=916656)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=916656)[0;0m WARNING 01-26 03:24:10 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     sampler_output = self.sampler(
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]                      ^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     random_sampled, processed_logprobs = self.topk_topp_sampler(
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]                                          ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     logits = self.apply_top_k_top_p(logits, k, p)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 892.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 790.94 MiB is free. Including non-PyTorch memory, this process has 14.67 GiB memory in use. Of the allocated memory 11.57 GiB is allocated by PyTorch, with 142.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866] 
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866] The above exception was the direct cause of the following exception:
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866] 
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866]     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=916656)[0;0m ERROR 01-26 03:24:17 [core.py:866] RuntimeError: CUDA out of memory occurred when warming up sampler with 512 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:23:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:23:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:23:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:23:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:23:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:23:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:23:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:23:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:23:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:24:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:24:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:24:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:24:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:24:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:24:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:24:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:24:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=916656)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=916656)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
[0;36m(EngineCore_DP0 pid=916656)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
[0;36m(EngineCore_DP0 pid=916656)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=916656)[0;0m 
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=916656)[0;0m [2026-01-26 03:24:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=916656)[0;0m 2026-01-26 03:24:13,486 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=916656)[0;0m 2026-01-26 03:24:13,500 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=916656)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:02, 20.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:00<00:02, 20.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:00<00:01, 21.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 12/51 [00:00<00:01, 21.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:00<00:01, 22.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:00<00:01, 23.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:00<00:01, 24.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:01<00:01, 25.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/51 [00:01<00:00, 26.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:01<00:00, 26.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:01<00:00, 26.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:01<00:00, 18.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:01<00:00, 18.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:01<00:00, 21.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [00:01<00:00, 22.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:02<00:00, 25.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:02<00:00, 23.30it/s]
[0;36m(EngineCore_DP0 pid=916656)[0;0m 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|â–         | 1/51 [00:00<00:07,  6.84it/s]
Capturing CUDA graphs (decode, FULL):   8%|â–Š         | 4/51 [00:00<00:02, 15.76it/s]
Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 7/51 [00:00<00:02, 19.08it/s]
Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–‰        | 10/51 [00:00<00:01, 20.86it/s]
Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:00<00:01, 22.99it/s]
Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:00<00:01, 24.58it/s]
Capturing CUDA graphs (decode, FULL):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:00<00:01, 26.58it/s]
Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:01<00:00, 27.97it/s]
Capturing CUDA graphs (decode, FULL):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:01<00:00, 29.10it/s]
Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/51 [00:01<00:00, 29.86it/s]
Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:01<00:00, 30.78it/s]
Capturing CUDA graphs (decode, FULL):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:01<00:00, 31.55it/s]
Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:01<00:00, 30.59it/s]
Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:01<00:00, 31.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:01<00:00, 27.46it/s]
[0;36m(EngineCore_DP0 pid=916656)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=916656)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=916656)[0;0m     sampler_output = self.sampler(
[0;36m(EngineCore_DP0 pid=916656)[0;0m                      ^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=916656)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=916656)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
[0;36m(EngineCore_DP0 pid=916656)[0;0m     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
[0;36m(EngineCore_DP0 pid=916656)[0;0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
[0;36m(EngineCore_DP0 pid=916656)[0;0m     random_sampled, processed_logprobs = self.topk_topp_sampler(
[0;36m(EngineCore_DP0 pid=916656)[0;0m                                          ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=916656)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=916656)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
[0;36m(EngineCore_DP0 pid=916656)[0;0m     logits = self.apply_top_k_top_p(logits, k, p)
[0;36m(EngineCore_DP0 pid=916656)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
[0;36m(EngineCore_DP0 pid=916656)[0;0m     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[0;36m(EngineCore_DP0 pid=916656)[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 892.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 790.94 MiB is free. Including non-PyTorch memory, this process has 14.67 GiB memory in use. Of the allocated memory 11.57 GiB is allocated by PyTorch, with 142.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=916656)[0;0m 
[0;36m(EngineCore_DP0 pid=916656)[0;0m The above exception was the direct cause of the following exception:
[0;36m(EngineCore_DP0 pid=916656)[0;0m 
[0;36m(EngineCore_DP0 pid=916656)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=916656)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=916656)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=916656)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=916656)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=916656)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=916656)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=916656)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=916656)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[0;36m(EngineCore_DP0 pid=916656)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=916656)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=916656)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
[0;36m(EngineCore_DP0 pid=916656)[0;0m     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=916656)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=916656)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=916656)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
[0;36m(EngineCore_DP0 pid=916656)[0;0m     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=916656)[0;0m RuntimeError: CUDA out of memory occurred when warming up sampler with 512 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[rank0]:[W126 03:24:18.188071163 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=512 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_decode.csv

é¢„è§ˆ:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
512,16,512,512,256,256,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032354.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | decode | M=512
Attempt: 2/3
GPU Mem Util: 0.75
Time: 2026-01-26 03:24:50
Duration: 23.9s
Success: False
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage decode --backend cusparselt --M 512 --gpu-mem 0.75 --gpu-id 1 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA GeForce RTX 5080                   â”‚â”‚
â”‚ GPU (short):      RTX5080                                   â”‚
â”‚ Memory:           15.5 GB                                    â”‚
â”‚ CC:               cc120 (Blackwell)                            â”‚
â”‚ SM Code:          sm_120                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ“                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['decode']
  M_prefill:        [512]
  M_decode:         [512]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.75

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032429.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | decode[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=512
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     decode                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 512
â”‚   M_prefill     = 8192 (= 512 x 16)
â”‚   M_decode      = 512
â”‚   batched_tokens = 512 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 16
â”‚   --output-len             = 256
â”‚   --num-prompts            = 512
â”‚   --max-num-seqs           = 512
â”‚   --max-model-len          = 272
â”‚   --max-num-batched-tokens = 512
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 1
â”‚   N_decode  = 256
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:24:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=917550)[0;0m [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=917550)[0;0m WARNING 01-26 03:24:44 [backends.py:609] Failed to read file <frozen os>
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=917550)[0;0m ERROR 01-26 03:24:47 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 03:24:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:24:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:24:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:24:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:24:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:24:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:24:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:24:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:24:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:24:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:24:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:24:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:24:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:24:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:24:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:24:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:24:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
[0;36m(EngineCore_DP0 pid=917550)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=917550)[0;0m 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]
[0;36m(EngineCore_DP0 pid=917550)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
[0;36m(EngineCore_DP0 pid=917550)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
[0;36m(EngineCore_DP0 pid=917550)[0;0m 
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
[0;36m(EngineCore_DP0 pid=917550)[0;0m [2026-01-26 03:24:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
[0;36m(EngineCore_DP0 pid=917550)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=917550)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=917550)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=917550)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=917550)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=917550)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=917550)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=917550)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=917550)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=917550)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=917550)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=917550)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=917550)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=917550)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=917550)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=917550)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=917550)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=917550)[0;0m     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=917550)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=917550)[0;0m   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=917550)[0;0m     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=917550)[0;0m   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=917550)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=917550)[0;0m ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 03:24:48.934048668 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=512 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_decode.csv

é¢„è§ˆ:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
512,16,512,512,256,256,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_032429.log

