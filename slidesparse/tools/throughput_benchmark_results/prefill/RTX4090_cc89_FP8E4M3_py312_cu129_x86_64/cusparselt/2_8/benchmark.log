
========== M=512 ==========
Time: 2026-01-25 19:11:15
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:11:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:11:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=296230) WARNING 01-25 19:11:42 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=296230) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=296230) WARNING 01-25 19:11:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.18 requests/s, 8299.54 total tokens/s, 16.18 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 19:11:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:11:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:11:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:11:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:11:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:11:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:11:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:11:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:11:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:11:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:11:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:11:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:11:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:11:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:11:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:11:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:11:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:11:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:11:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:11:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:11:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:11:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:11:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:11:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:11:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:11:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:11:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:11:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W125 19:11:42.860646455 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:43] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=296230) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=296230) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.96s/it]
(EngineCore_DP0 pid=296230) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.96s/it]
(EngineCore_DP0 pid=296230) 
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=296230) [2026-01-25 19:11:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=296230) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  1.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]
(EngineCore_DP0 pid=296230) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.60it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  43%|████▎     | 55/128 [00:00<00:00, 548.75it/s]
Adding requests:  88%|████████▊ | 113/128 [00:00<00:00, 564.42it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 561.16it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:14,  8.99it/s, est. speed input: 4602.62 toks/s, output: 8.99 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:08, 13.90it/s, est. speed input: 6750.32 toks/s, output: 13.18 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:08, 15.30it/s, est. speed input: 7401.67 toks/s, output: 14.46 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:07, 16.19it/s, est. speed input: 7797.64 toks/s, output: 15.23 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 16.69it/s, est. speed input: 8036.13 toks/s, output: 15.69 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:06, 17.08it/s, est. speed input: 8216.51 toks/s, output: 16.05 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:06, 17.37it/s, est. speed input: 8354.89 toks/s, output: 16.32 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:06, 17.32it/s, est. speed input: 8413.78 toks/s, output: 16.43 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 17.48it/s, est. speed input: 8492.40 toks/s, output: 16.59 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 17.58it/s, est. speed input: 8554.53 toks/s, output: 16.71 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 17.57it/s, est. speed input: 8592.97 toks/s, output: 16.78 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 17.50it/s, est. speed input: 8617.25 toks/s, output: 16.83 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.55it/s, est. speed input: 8650.08 toks/s, output: 16.89 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.14it/s, est. speed input: 8625.43 toks/s, output: 16.85 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 16.89it/s, est. speed input: 8606.48 toks/s, output: 16.81 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.77it/s, est. speed input: 8596.78 toks/s, output: 16.79 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 16.58it/s, est. speed input: 8576.26 toks/s, output: 16.75 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.34it/s, est. speed input: 8546.94 toks/s, output: 16.69 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.36it/s, est. speed input: 8538.95 toks/s, output: 16.68 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.27it/s, est. speed input: 8522.19 toks/s, output: 16.64 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.26it/s, est. speed input: 8511.51 toks/s, output: 16.62 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.30it/s, est. speed input: 8506.13 toks/s, output: 16.61 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 16.31it/s, est. speed input: 8499.97 toks/s, output: 16.60 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 16.38it/s, est. speed input: 8498.34 toks/s, output: 16.60 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 16.33it/s, est. speed input: 8490.55 toks/s, output: 16.58 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 16.22it/s, est. speed input: 8477.62 toks/s, output: 16.56 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 16.27it/s, est. speed input: 8474.44 toks/s, output: 16.55 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.34it/s, est. speed input: 8473.77 toks/s, output: 16.55 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.26it/s, est. speed input: 8464.66 toks/s, output: 16.53 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.22it/s, est. speed input: 8457.30 toks/s, output: 16.52 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.17it/s, est. speed input: 8449.33 toks/s, output: 16.50 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.28it/s, est. speed input: 8450.02 toks/s, output: 16.50 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 16.36it/s, est. speed input: 8450.73 toks/s, output: 16.51 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.31it/s, est. speed input: 8445.96 toks/s, output: 16.50 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.27it/s, est. speed input: 8441.03 toks/s, output: 16.49 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.33it/s, est. speed input: 8440.77 toks/s, output: 16.49 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.30it/s, est. speed input: 8437.27 toks/s, output: 16.48 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.23it/s, est. speed input: 8431.26 toks/s, output: 16.47 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.31it/s, est. speed input: 8432.10 toks/s, output: 16.47 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:03, 16.30it/s, est. speed input: 8429.54 toks/s, output: 16.46 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.45it/s, est. speed input: 8433.53 toks/s, output: 16.47 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 16.49it/s, est. speed input: 8435.13 toks/s, output: 16.47 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.45it/s, est. speed input: 8433.79 toks/s, output: 16.47 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.34it/s, est. speed input: 8429.11 toks/s, output: 16.46 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.41it/s, est. speed input: 8430.35 toks/s, output: 16.47 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.52it/s, est. speed input: 8433.99 toks/s, output: 16.47 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.60it/s, est. speed input: 8437.26 toks/s, output: 16.48 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.66it/s, est. speed input: 8440.70 toks/s, output: 16.49 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.72it/s, est. speed input: 8444.45 toks/s, output: 16.49 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.72it/s, est. speed input: 8446.85 toks/s, output: 16.50 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.47it/s, est. speed input: 8440.81 toks/s, output: 16.49 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.53it/s, est. speed input: 8442.43 toks/s, output: 16.49 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.54it/s, est. speed input: 8443.21 toks/s, output: 16.49 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.83it/s, est. speed input: 8452.68 toks/s, output: 16.51 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.99it/s, est. speed input: 8460.54 toks/s, output: 16.52 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:00, 17.10it/s, est. speed input: 8467.81 toks/s, output: 16.54 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 17.28it/s, est. speed input: 8477.77 toks/s, output: 16.56 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 17.44it/s, est. speed input: 8488.36 toks/s, output: 16.58 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 17.47it/s, est. speed input: 8496.21 toks/s, output: 16.59 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 17.50it/s, est. speed input: 8504.13 toks/s, output: 16.61 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 17.55it/s, est. speed input: 8512.56 toks/s, output: 16.63 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 17.60it/s, est. speed input: 8521.06 toks/s, output: 16.64 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 17.68it/s, est. speed input: 8530.46 toks/s, output: 16.66 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 17.37it/s, est. speed input: 8530.69 toks/s, output: 16.66 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.37it/s, est. speed input: 8531.07 toks/s, output: 16.66 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.66it/s, est. speed input: 8531.07 toks/s, output: 16.66 toks/s]
[rank0]:[W125 19:12:13.681225675 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 19:12:14
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:12:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:12:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=297341) WARNING 01-25 19:12:33 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=297341) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=297341) WARNING 01-25 19:12:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.80 requests/s, 16194.85 total tokens/s, 15.80 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 19:12:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:12:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:12:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:12:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:12:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:12:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:12:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:12:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:12:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:12:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:12:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:12:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:12:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:12:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:12:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:12:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:12:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:12:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:12:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:12:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:12:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:12:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:12:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:12:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:12:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:12:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:12:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:12:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=297341) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=297341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.44it/s]
(EngineCore_DP0 pid=297341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.44it/s]
(EngineCore_DP0 pid=297341) 
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=297341) [2026-01-25 19:12:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=297341) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.78it/s]
(EngineCore_DP0 pid=297341) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 271.39it/s]
Adding requests:  46%|████▌     | 59/128 [00:00<00:00, 288.87it/s]
Adding requests:  71%|███████   | 91/128 [00:00<00:00, 299.59it/s]
Adding requests:  96%|█████████▌| 123/128 [00:00<00:00, 305.72it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 299.87it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 45.47it/s, est. speed input: 46571.37 toks/s, output: 45.47 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:04, 23.87it/s, est. speed input: 26502.36 toks/s, output: 25.88 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:05, 21.14it/s, est. speed input: 23824.61 toks/s, output: 23.27 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:05, 19.70it/s, est. speed input: 22409.59 toks/s, output: 21.88 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:05, 18.81it/s, est. speed input: 21498.70 toks/s, output: 20.99 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:05, 18.57it/s, est. speed input: 21152.70 toks/s, output: 20.66 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:05, 18.25it/s, est. speed input: 20819.13 toks/s, output: 20.33 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 18.20it/s, est. speed input: 20620.36 toks/s, output: 20.14 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 18.04it/s, est. speed input: 20412.75 toks/s, output: 19.93 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 17.84it/s, est. speed input: 20210.98 toks/s, output: 19.74 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 17.60it/s, est. speed input: 20012.25 toks/s, output: 19.54 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:05, 17.48it/s, est. speed input: 19852.64 toks/s, output: 19.39 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:05, 17.33it/s, est. speed input: 19696.19 toks/s, output: 19.23 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:05, 17.15it/s, est. speed input: 19542.08 toks/s, output: 19.08 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 17.20it/s, est. speed input: 19443.37 toks/s, output: 18.99 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 17.05it/s, est. speed input: 19318.27 toks/s, output: 18.87 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:05, 16.72it/s, est. speed input: 19161.00 toks/s, output: 18.71 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 16.43it/s, est. speed input: 19009.04 toks/s, output: 18.56 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 16.23it/s, est. speed input: 18870.43 toks/s, output: 18.43 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 16.12it/s, est. speed input: 18749.50 toks/s, output: 18.31 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:04, 16.14it/s, est. speed input: 18654.54 toks/s, output: 18.22 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:02<00:04, 15.91it/s, est. speed input: 18529.62 toks/s, output: 18.10 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 15.90it/s, est. speed input: 18436.64 toks/s, output: 18.00 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 15.90it/s, est. speed input: 18352.99 toks/s, output: 17.92 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:04, 15.93it/s, est. speed input: 18280.07 toks/s, output: 17.85 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:04, 15.90it/s, est. speed input: 18204.88 toks/s, output: 17.78 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:04, 15.82it/s, est. speed input: 18126.91 toks/s, output: 17.70 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 15.81it/s, est. speed input: 18061.20 toks/s, output: 17.64 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 15.82it/s, est. speed input: 18001.07 toks/s, output: 17.58 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:03, 15.81it/s, est. speed input: 17942.53 toks/s, output: 17.52 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 15.78it/s, est. speed input: 17885.49 toks/s, output: 17.47 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 15.81it/s, est. speed input: 17836.80 toks/s, output: 17.42 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 15.79it/s, est. speed input: 17787.73 toks/s, output: 17.37 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:03, 15.79it/s, est. speed input: 17742.43 toks/s, output: 17.33 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:03, 15.84it/s, est. speed input: 17704.17 toks/s, output: 17.29 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 15.95it/s, est. speed input: 17675.90 toks/s, output: 17.26 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:04<00:02, 16.03it/s, est. speed input: 17648.39 toks/s, output: 17.23 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:04<00:02, 16.05it/s, est. speed input: 17619.42 toks/s, output: 17.21 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 15.91it/s, est. speed input: 17577.76 toks/s, output: 17.17 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 15.95it/s, est. speed input: 17551.03 toks/s, output: 17.14 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 16.03it/s, est. speed input: 17528.95 toks/s, output: 17.12 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:02, 15.97it/s, est. speed input: 17499.05 toks/s, output: 17.09 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:02, 15.92it/s, est. speed input: 17469.60 toks/s, output: 17.06 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 15.92it/s, est. speed input: 17443.84 toks/s, output: 17.03 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:05<00:01, 15.93it/s, est. speed input: 17420.07 toks/s, output: 17.01 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:06<00:01, 15.87it/s, est. speed input: 17392.76 toks/s, output: 16.99 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 15.82it/s, est. speed input: 17365.25 toks/s, output: 16.96 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 15.63it/s, est. speed input: 17327.36 toks/s, output: 16.92 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 15.62it/s, est. speed input: 17300.11 toks/s, output: 16.89 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 15.56it/s, est. speed input: 17270.16 toks/s, output: 16.87 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:01, 15.39it/s, est. speed input: 17232.28 toks/s, output: 16.83 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 15.46it/s, est. speed input: 17209.24 toks/s, output: 16.81 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:06<00:00, 15.55it/s, est. speed input: 17189.60 toks/s, output: 16.79 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:07<00:00, 15.57it/s, est. speed input: 17167.86 toks/s, output: 16.77 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:07<00:00, 15.49it/s, est. speed input: 17140.50 toks/s, output: 16.74 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 15.54it/s, est. speed input: 17121.19 toks/s, output: 16.72 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 15.54it/s, est. speed input: 17099.90 toks/s, output: 16.70 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 15.58it/s, est. speed input: 17082.36 toks/s, output: 16.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 15.90it/s, est. speed input: 17082.78 toks/s, output: 16.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 15.90it/s, est. speed input: 17082.78 toks/s, output: 16.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.68it/s, est. speed input: 17082.78 toks/s, output: 16.68 toks/s]
[rank0]:[W125 19:13:01.511330917 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 19:13:03
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:13:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:13:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=298253) WARNING 01-25 19:13:21 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=298253) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=298253) WARNING 01-25 19:13:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.96 requests/s, 32760.79 total tokens/s, 31.96 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 19:13:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:13:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:13:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:13:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:13:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:13:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:13:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:13:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:13:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:13:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:13:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:13:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:13:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:13:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:13:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:13:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:13:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:13:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:13:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:13:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:13:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:13:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:13:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:13:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:13:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:13:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:13:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:13:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=298253) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=298253) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=298253) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=298253) 
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=298253) [2026-01-25 19:13:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=298253) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.31it/s]
(EngineCore_DP0 pid=298253) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  8.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.49it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  12%|█▏        | 30/256 [00:00<00:00, 297.14it/s]
Adding requests:  25%|██▍       | 63/256 [00:00<00:00, 313.04it/s]
Adding requests:  37%|███▋      | 95/256 [00:00<00:00, 312.82it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 318.33it/s]
Adding requests:  63%|██████▎   | 162/256 [00:00<00:00, 321.49it/s]
Adding requests:  76%|███████▌  | 195/256 [00:00<00:00, 319.69it/s]
Adding requests:  89%|████████▉ | 229/256 [00:00<00:00, 325.53it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 321.11it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:00<00:01, 140.72it/s, est. speed input: 144127.74 toks/s, output: 140.73 toks/s]
Processed prompts:  14%|█▍        | 37/256 [00:00<00:03, 57.15it/s, est. speed input: 65459.28 toks/s, output: 63.92 toks/s]   
Processed prompts:  18%|█▊        | 45/256 [00:00<00:04, 47.98it/s, est. speed input: 56424.48 toks/s, output: 55.10 toks/s]
Processed prompts:  20%|█▉        | 51/256 [00:00<00:04, 43.65it/s, est. speed input: 52404.69 toks/s, output: 51.18 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:01<00:05, 39.01it/s, est. speed input: 48792.44 toks/s, output: 47.65 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:01<00:04, 39.85it/s, est. speed input: 48351.25 toks/s, output: 47.22 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:05, 36.36it/s, est. speed input: 46115.59 toks/s, output: 45.03 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:05, 35.28it/s, est. speed input: 45062.97 toks/s, output: 44.01 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:05, 34.71it/s, est. speed input: 44265.55 toks/s, output: 43.23 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:05, 34.48it/s, est. speed input: 43642.80 toks/s, output: 42.62 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:05, 34.15it/s, est. speed input: 43053.01 toks/s, output: 42.04 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:02<00:05, 33.77it/s, est. speed input: 42497.15 toks/s, output: 41.50 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:04, 33.65it/s, est. speed input: 42041.84 toks/s, output: 41.06 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 33.63it/s, est. speed input: 41645.93 toks/s, output: 40.67 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 33.65it/s, est. speed input: 41298.61 toks/s, output: 40.33 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 33.33it/s, est. speed input: 40916.36 toks/s, output: 39.96 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 33.05it/s, est. speed input: 40558.93 toks/s, output: 39.61 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:04, 32.48it/s, est. speed input: 40165.75 toks/s, output: 39.22 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:04, 32.51it/s, est. speed input: 39880.23 toks/s, output: 38.95 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:03<00:04, 32.92it/s, est. speed input: 39682.28 toks/s, output: 38.75 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:03<00:04, 32.86it/s, est. speed input: 39443.80 toks/s, output: 38.52 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:03, 33.29it/s, est. speed input: 39291.95 toks/s, output: 38.37 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 33.38it/s, est. speed input: 39120.24 toks/s, output: 38.20 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:03<00:00, 162.33it/s, est. speed input: 54806.15 toks/s, output: 53.52 toks/s]
Processed prompts:  80%|████████  | 205/256 [00:03<00:00, 87.94it/s, est. speed input: 52829.84 toks/s, output: 51.59 toks/s] 
Processed prompts:  85%|████████▍ | 217/256 [00:04<00:00, 65.02it/s, est. speed input: 51242.18 toks/s, output: 50.04 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:04<00:00, 53.27it/s, est. speed input: 49881.93 toks/s, output: 48.71 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:04<00:00, 47.59it/s, est. speed input: 49064.38 toks/s, output: 47.91 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:05<00:00, 44.37it/s, est. speed input: 48533.57 toks/s, output: 47.40 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:05<00:00, 41.71it/s, est. speed input: 48044.02 toks/s, output: 46.92 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:05<00:00, 41.81it/s, est. speed input: 47938.47 toks/s, output: 46.81 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 37.72it/s, est. speed input: 47298.76 toks/s, output: 46.19 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 37.72it/s, est. speed input: 47298.76 toks/s, output: 46.19 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 46.19it/s, est. speed input: 47298.76 toks/s, output: 46.19 toks/s]
[rank0]:[W125 19:13:49.021433838 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 19:13:52
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:14:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:14:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=299213) WARNING 01-25 19:14:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=299213) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=299213) WARNING 01-25 19:14:18 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.57 requests/s, 64129.20 total tokens/s, 62.57 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 19:14:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:14:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:14:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:14:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:14:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:14:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:14:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:14:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:14:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:14:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:14:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:14:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:14:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:14:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:14:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:14:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:14:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:14:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=299213) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=299213) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=299213) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=299213) 
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=299213) [2026-01-25 19:14:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=299213) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  3.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  5.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  4.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  4.35it/s]
(EngineCore_DP0 pid=299213) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.69it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 26/512 [00:00<00:01, 252.67it/s]
Adding requests:  11%|█         | 56/512 [00:00<00:01, 273.60it/s]
Adding requests:  17%|█▋        | 85/512 [00:00<00:01, 276.76it/s]
Adding requests:  22%|██▏       | 115/512 [00:00<00:01, 282.56it/s]
Adding requests:  28%|██▊       | 144/512 [00:00<00:01, 284.73it/s]
Adding requests:  34%|███▍      | 173/512 [00:00<00:01, 285.15it/s]
Adding requests:  39%|███▉      | 202/512 [00:00<00:01, 284.75it/s]
Adding requests:  45%|████▌     | 232/512 [00:00<00:00, 288.07it/s]
Adding requests:  51%|█████     | 262/512 [00:00<00:00, 287.40it/s]
Adding requests:  57%|█████▋    | 291/512 [00:01<00:00, 286.36it/s]
Adding requests:  62%|██████▎   | 320/512 [00:01<00:00, 283.96it/s]
Adding requests:  68%|██████▊   | 349/512 [00:01<00:00, 283.93it/s]
Adding requests:  74%|███████▍  | 379/512 [00:01<00:00, 286.51it/s]
Adding requests:  80%|████████  | 410/512 [00:01<00:00, 291.32it/s]
Adding requests:  86%|████████▌ | 441/512 [00:01<00:00, 293.72it/s]
Adding requests:  92%|█████████▏| 471/512 [00:01<00:00, 295.09it/s]
Adding requests:  98%|█████████▊| 501/512 [00:01<00:00, 293.59it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 287.22it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:00<00:00, 793.00it/s, est. speed input: 812208.37 toks/s, output: 793.05 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:01<00:03, 106.64it/s, est. speed input: 126176.23 toks/s, output: 123.22 toks/s]
Processed prompts:  40%|███▉      | 203/512 [00:01<00:03, 91.68it/s, est. speed input: 109353.44 toks/s, output: 106.79 toks/s] 
Processed prompts:  44%|████▍     | 226/512 [00:02<00:03, 83.85it/s, est. speed input: 101875.36 toks/s, output: 99.49 toks/s] 
Processed prompts:  47%|████▋     | 243/512 [00:02<00:03, 81.16it/s, est. speed input: 98991.33 toks/s, output: 96.67 toks/s] 
Processed prompts:  50%|█████     | 256/512 [00:02<00:03, 78.37it/s, est. speed input: 96729.53 toks/s, output: 94.46 toks/s]
Processed prompts:  52%|█████▏    | 267/512 [00:02<00:03, 74.46it/s, est. speed input: 94392.14 toks/s, output: 92.18 toks/s]
Processed prompts:  54%|█████▍    | 277/512 [00:03<00:03, 75.06it/s, est. speed input: 93781.46 toks/s, output: 91.58 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:03<00:03, 67.98it/s, est. speed input: 91138.14 toks/s, output: 89.00 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:03<00:03, 67.82it/s, est. speed input: 90336.88 toks/s, output: 88.22 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:03<00:03, 68.56it/s, est. speed input: 89781.11 toks/s, output: 87.68 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:03<00:02, 68.79it/s, est. speed input: 89180.97 toks/s, output: 87.09 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:02, 69.42it/s, est. speed input: 88690.39 toks/s, output: 86.61 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:03<00:02, 69.73it/s, est. speed input: 88199.87 toks/s, output: 86.13 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:03<00:02, 70.10it/s, est. speed input: 87756.63 toks/s, output: 85.70 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:04<00:02, 69.01it/s, est. speed input: 87162.44 toks/s, output: 85.12 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:04<00:02, 69.54it/s, est. speed input: 86764.28 toks/s, output: 84.73 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:04<00:02, 69.56it/s, est. speed input: 86345.26 toks/s, output: 84.32 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:04<00:02, 70.14it/s, est. speed input: 86010.22 toks/s, output: 83.99 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:04<00:01, 70.69it/s, est. speed input: 85706.22 toks/s, output: 83.70 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:04<00:01, 71.29it/s, est. speed input: 85437.44 toks/s, output: 83.43 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:04<00:01, 70.94it/s, est. speed input: 85106.16 toks/s, output: 83.11 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:04<00:01, 70.45it/s, est. speed input: 84767.02 toks/s, output: 82.78 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:04<00:01, 70.42it/s, est. speed input: 84472.57 toks/s, output: 82.49 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:05<00:01, 70.58it/s, est. speed input: 84208.61 toks/s, output: 82.23 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:05<00:01, 70.99it/s, est. speed input: 83981.26 toks/s, output: 82.01 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:05<00:01, 71.18it/s, est. speed input: 83755.33 toks/s, output: 81.79 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:05<00:01, 71.43it/s, est. speed input: 83548.16 toks/s, output: 81.59 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:05<00:00, 71.60it/s, est. speed input: 83348.74 toks/s, output: 81.39 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:05<00:00, 70.98it/s, est. speed input: 83099.90 toks/s, output: 81.15 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:05<00:00, 70.26it/s, est. speed input: 82838.30 toks/s, output: 80.90 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:05<00:00, 70.60it/s, est. speed input: 82651.14 toks/s, output: 80.71 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:05<00:00, 70.61it/s, est. speed input: 82454.43 toks/s, output: 80.52 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:06<00:00, 71.12it/s, est. speed input: 82301.02 toks/s, output: 80.37 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:06<00:00, 70.96it/s, est. speed input: 82116.87 toks/s, output: 80.19 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:06<00:00, 71.00it/s, est. speed input: 81949.79 toks/s, output: 80.03 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:06<00:00, 68.55it/s, est. speed input: 81614.96 toks/s, output: 79.70 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 68.55it/s, est. speed input: 81932.06 toks/s, output: 80.01 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 80.01it/s, est. speed input: 81932.06 toks/s, output: 80.01 toks/s]
[rank0]:[W125 19:14:40.124573247 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 19:14:43
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:14:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:14:57 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=300173) WARNING 01-25 19:15:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=300173) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=300173) WARNING 01-25 19:15:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 87.45 requests/s, 89640.28 total tokens/s, 87.45 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-25 19:14:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:14:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:14:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:14:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:14:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:14:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:14:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:14:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:14:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:14:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:15:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:15:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:15:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:15:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:15:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:15:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:15:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:15:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:15:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:15:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:15:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:15:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:15:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:15:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:06] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:06] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:06] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:06] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:06] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=300173) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=300173) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]
(EngineCore_DP0 pid=300173) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]
(EngineCore_DP0 pid=300173) 
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=300173) [2026-01-25 19:15:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=300173) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  4.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  5.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  4.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.63it/s]
(EngineCore_DP0 pid=300173) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.52it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  5.33it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  6.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  7.20it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 26/1024 [00:00<00:03, 259.28it/s]
Adding requests:   6%|▌         | 58/1024 [00:00<00:03, 290.32it/s]
Adding requests:   9%|▉         | 92/1024 [00:00<00:03, 308.98it/s]
Adding requests:  12%|█▏        | 126/1024 [00:00<00:02, 319.12it/s]
Adding requests:  15%|█▌        | 158/1024 [00:00<00:02, 313.34it/s]
Adding requests:  19%|█▊        | 190/1024 [00:00<00:02, 313.01it/s]
Adding requests:  22%|██▏       | 222/1024 [00:00<00:02, 311.67it/s]
Adding requests:  25%|██▍       | 254/1024 [00:00<00:02, 313.59it/s]
Adding requests:  28%|██▊       | 286/1024 [00:00<00:02, 314.25it/s]
Adding requests:  31%|███       | 318/1024 [00:01<00:02, 306.74it/s]
Adding requests:  34%|███▍      | 349/1024 [00:01<00:02, 306.61it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:02, 306.46it/s]
Adding requests:  40%|████      | 413/1024 [00:01<00:01, 312.96it/s]
Adding requests:  44%|████▎     | 446/1024 [00:01<00:01, 315.76it/s]
Adding requests:  47%|████▋     | 478/1024 [00:01<00:01, 310.50it/s]
Adding requests:  50%|████▉     | 510/1024 [00:01<00:01, 301.71it/s]
Adding requests:  53%|█████▎    | 541/1024 [00:01<00:01, 298.03it/s]
Adding requests:  56%|█████▌    | 573/1024 [00:01<00:01, 303.27it/s]
Adding requests:  59%|█████▉    | 604/1024 [00:01<00:01, 302.96it/s]
Adding requests:  62%|██████▏   | 635/1024 [00:02<00:01, 303.37it/s]
Adding requests:  65%|██████▌   | 668/1024 [00:02<00:01, 310.77it/s]
Adding requests:  68%|██████▊   | 701/1024 [00:02<00:01, 315.84it/s]
Adding requests:  72%|███████▏  | 734/1024 [00:02<00:00, 318.39it/s]
Adding requests:  75%|███████▍  | 766/1024 [00:02<00:00, 318.57it/s]
Adding requests:  78%|███████▊  | 798/1024 [00:02<00:00, 317.47it/s]
Adding requests:  81%|████████  | 830/1024 [00:02<00:00, 313.28it/s]
Adding requests:  84%|████████▍ | 863/1024 [00:02<00:00, 317.78it/s]
Adding requests:  87%|████████▋ | 895/1024 [00:02<00:00, 306.40it/s]
Adding requests:  90%|█████████ | 926/1024 [00:02<00:00, 303.38it/s]
Adding requests:  93%|█████████▎| 957/1024 [00:03<00:00, 292.73it/s]
Adding requests:  96%|█████████▋| 987/1024 [00:03<00:00, 286.79it/s]
Adding requests:  99%|█████████▉| 1017/1024 [00:03<00:00, 289.72it/s]
Adding requests: 100%|██████████| 1024/1024 [00:03<00:00, 305.65it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:00<00:00, 2005.71it/s, est. speed input: 2054581.33 toks/s, output: 2006.10 toks/s]
Processed prompts:  46%|████▋     | 475/1024 [00:02<00:03, 166.08it/s, est. speed input: 202156.12 toks/s, output: 197.42 toks/s]   
Processed prompts:  55%|█████▍    | 563/1024 [00:03<00:03, 136.53it/s, est. speed input: 169115.75 toks/s, output: 165.15 toks/s]
Processed prompts:  60%|██████    | 615/1024 [00:03<00:03, 127.10it/s, est. speed input: 159187.95 toks/s, output: 155.46 toks/s]
Processed prompts:  64%|██████▎   | 651/1024 [00:04<00:03, 116.78it/s, est. speed input: 151043.41 toks/s, output: 147.50 toks/s]
Processed prompts:  66%|██████▌   | 677/1024 [00:04<00:03, 113.47it/s, est. speed input: 147929.57 toks/s, output: 144.46 toks/s]
Processed prompts:  68%|██████▊   | 697/1024 [00:04<00:02, 113.25it/s, est. speed input: 146698.05 toks/s, output: 143.26 toks/s]
Processed prompts:  70%|██████▉   | 715/1024 [00:05<00:02, 104.81it/s, est. speed input: 143082.60 toks/s, output: 139.73 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:05<00:02, 102.35it/s, est. speed input: 141453.79 toks/s, output: 138.14 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:05<00:02, 100.99it/s, est. speed input: 140093.86 toks/s, output: 136.81 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:05<00:02, 99.83it/s, est. speed input: 138832.72 toks/s, output: 135.58 toks/s] 
Processed prompts:  76%|███████▌  | 778/1024 [00:05<00:02, 98.72it/s, est. speed input: 137626.24 toks/s, output: 134.40 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:05<00:02, 97.86it/s, est. speed input: 136496.59 toks/s, output: 133.30 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:06<00:02, 97.26it/s, est. speed input: 135440.02 toks/s, output: 132.26 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:06<00:02, 96.71it/s, est. speed input: 134426.98 toks/s, output: 131.28 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:06<00:01, 96.10it/s, est. speed input: 133441.94 toks/s, output: 130.31 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:06<00:01, 95.84it/s, est. speed input: 132530.28 toks/s, output: 129.42 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:06<00:01, 95.64it/s, est. speed input: 131661.64 toks/s, output: 128.58 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:06<00:01, 95.28it/s, est. speed input: 130810.89 toks/s, output: 127.74 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:07<00:01, 95.19it/s, est. speed input: 130018.59 toks/s, output: 126.97 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:07<00:01, 95.05it/s, est. speed input: 129254.57 toks/s, output: 126.22 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:07<00:00, 96.33it/s, est. speed input: 128664.91 toks/s, output: 125.65 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:07<00:00, 93.70it/s, est. speed input: 127749.76 toks/s, output: 124.76 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:07<00:00, 92.00it/s, est. speed input: 126883.93 toks/s, output: 123.91 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:08<00:00, 92.12it/s, est. speed input: 126185.23 toks/s, output: 123.23 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:08<00:00, 90.95it/s, est. speed input: 125393.76 toks/s, output: 122.45 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:08<00:00, 91.28it/s, est. speed input: 124746.60 toks/s, output: 121.82 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:08<00:00, 91.28it/s, est. speed input: 125477.67 toks/s, output: 122.54 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:08<00:00, 122.53it/s, est. speed input: 125477.67 toks/s, output: 122.54 toks/s]
[rank0]:[W125 19:15:37.528615287 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-25 19:15:40
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:15:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:16:00 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=301291) WARNING 01-25 19:16:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=301291) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=301291) WARNING 01-25 19:16:16 [backends.py:609] Failed to read file <frozen os>
Throughput: 87.93 requests/s, 90129.27 total tokens/s, 87.93 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-25 19:15:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:15:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:15:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:15:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:15:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:15:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:15:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:15:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:15:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:15:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:15:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:15:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:15:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:15:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:16:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:16:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:16:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:16:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:16:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:16:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:16:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:16:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:16:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:16:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:16:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:16:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:16:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:16:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=301291) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=301291) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]
(EngineCore_DP0 pid=301291) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]
(EngineCore_DP0 pid=301291) 
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=301291) [2026-01-25 19:16:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=301291) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  5.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:01,  4.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  5.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  6.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  5.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  6.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  6.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.87it/s]
(EngineCore_DP0 pid=301291) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.34it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  9.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.72it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.45it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 27/2048 [00:00<00:07, 264.87it/s]
Adding requests:   3%|▎         | 57/2048 [00:00<00:06, 284.78it/s]
Adding requests:   4%|▍         | 86/2048 [00:00<00:06, 280.29it/s]
Adding requests:   6%|▌         | 116/2048 [00:00<00:06, 286.07it/s]
Adding requests:   7%|▋         | 145/2048 [00:00<00:06, 287.11it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:06, 283.36it/s]
Adding requests:  10%|▉         | 203/2048 [00:00<00:06, 280.14it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:06, 292.02it/s]
Adding requests:  13%|█▎        | 265/2048 [00:00<00:06, 292.98it/s]
Adding requests:  14%|█▍        | 296/2048 [00:01<00:05, 295.54it/s]
Adding requests:  16%|█▌        | 328/2048 [00:01<00:05, 301.41it/s]
Adding requests:  18%|█▊        | 362/2048 [00:01<00:05, 310.86it/s]
Adding requests:  19%|█▉        | 395/2048 [00:01<00:05, 314.73it/s]
Adding requests:  21%|██        | 428/2048 [00:01<00:05, 317.72it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:05, 315.75it/s]
Adding requests:  24%|██▍       | 493/2048 [00:01<00:04, 318.16it/s]
Adding requests:  26%|██▌       | 525/2048 [00:01<00:04, 304.90it/s]
Adding requests:  27%|██▋       | 558/2048 [00:01<00:04, 309.31it/s]
Adding requests:  29%|██▉       | 591/2048 [00:01<00:04, 315.22it/s]
Adding requests:  31%|███       | 625/2048 [00:02<00:04, 320.78it/s]
Adding requests:  32%|███▏      | 658/2048 [00:02<00:04, 319.20it/s]
Adding requests:  34%|███▍      | 692/2048 [00:02<00:04, 324.53it/s]
Adding requests:  35%|███▌      | 726/2048 [00:02<00:04, 326.65it/s]
Adding requests:  37%|███▋      | 759/2048 [00:02<00:04, 315.01it/s]
Adding requests:  39%|███▊      | 791/2048 [00:02<00:03, 314.77it/s]
Adding requests:  40%|████      | 823/2048 [00:02<00:04, 302.07it/s]
Adding requests:  42%|████▏     | 854/2048 [00:02<00:03, 300.70it/s]
Adding requests:  43%|████▎     | 886/2048 [00:02<00:03, 305.41it/s]
Adding requests:  45%|████▍     | 917/2048 [00:03<00:03, 306.14it/s]
Adding requests:  46%|████▋     | 948/2048 [00:03<00:03, 304.37it/s]
Adding requests:  48%|████▊     | 980/2048 [00:03<00:03, 307.57it/s]
Adding requests:  49%|████▉     | 1012/2048 [00:03<00:03, 310.79it/s]
Adding requests:  51%|█████     | 1044/2048 [00:03<00:03, 304.12it/s]
Adding requests:  52%|█████▏    | 1075/2048 [00:03<00:03, 295.38it/s]
Adding requests:  54%|█████▍    | 1105/2048 [00:03<00:03, 292.51it/s]
Adding requests:  55%|█████▌    | 1135/2048 [00:03<00:03, 285.27it/s]
Adding requests:  57%|█████▋    | 1164/2048 [00:03<00:03, 284.98it/s]
Adding requests:  58%|█████▊    | 1196/2048 [00:03<00:02, 292.96it/s]
Adding requests:  60%|█████▉    | 1226/2048 [00:04<00:02, 293.26it/s]
Adding requests:  61%|██████▏   | 1256/2048 [00:04<00:02, 291.10it/s]
Adding requests:  63%|██████▎   | 1286/2048 [00:04<00:02, 283.83it/s]
Adding requests:  64%|██████▍   | 1316/2048 [00:04<00:02, 287.16it/s]
Adding requests:  66%|██████▌   | 1347/2048 [00:04<00:02, 292.94it/s]
Adding requests:  67%|██████▋   | 1378/2048 [00:04<00:02, 297.23it/s]
Adding requests:  69%|██████▉   | 1409/2048 [00:04<00:02, 300.92it/s]
Adding requests:  70%|███████   | 1440/2048 [00:04<00:02, 294.19it/s]
Adding requests:  72%|███████▏  | 1472/2048 [00:04<00:01, 300.22it/s]
Adding requests:  73%|███████▎  | 1503/2048 [00:04<00:01, 300.96it/s]
Adding requests:  75%|███████▍  | 1534/2048 [00:05<00:01, 297.63it/s]
Adding requests:  76%|███████▋  | 1565/2048 [00:05<00:01, 299.11it/s]
Adding requests:  78%|███████▊  | 1595/2048 [00:05<00:01, 296.04it/s]
Adding requests:  79%|███████▉  | 1626/2048 [00:05<00:01, 299.04it/s]
Adding requests:  81%|████████  | 1656/2048 [00:05<00:01, 299.09it/s]
Adding requests:  82%|████████▏ | 1687/2048 [00:05<00:01, 300.52it/s]
Adding requests:  84%|████████▍ | 1718/2048 [00:05<00:01, 302.86it/s]
Adding requests:  85%|████████▌ | 1749/2048 [00:05<00:00, 301.54it/s]
Adding requests:  87%|████████▋ | 1780/2048 [00:05<00:00, 291.21it/s]
Adding requests:  88%|████████▊ | 1811/2048 [00:06<00:00, 295.74it/s]
Adding requests:  90%|████████▉ | 1841/2048 [00:06<00:00, 295.95it/s]
Adding requests:  91%|█████████▏| 1871/2048 [00:06<00:00, 283.12it/s]
Adding requests:  93%|█████████▎| 1902/2048 [00:06<00:00, 287.01it/s]
Adding requests:  94%|█████████▍| 1932/2048 [00:06<00:00, 288.36it/s]
Adding requests:  96%|█████████▌| 1962/2048 [00:06<00:00, 290.39it/s]
Adding requests:  97%|█████████▋| 1992/2048 [00:06<00:00, 291.78it/s]
Adding requests:  99%|█████████▉| 2023/2048 [00:06<00:00, 296.20it/s]
Adding requests: 100%|██████████| 2048/2048 [00:06<00:00, 299.36it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:00<00:00, 2336.39it/s, est. speed input: 2392780.95 toks/s, output: 2336.49 toks/s]
Processed prompts:  40%|███▉      | 812/2048 [00:02<00:05, 239.24it/s, est. speed input: 303073.82 toks/s, output: 295.97 toks/s]   
Processed prompts:  45%|████▍     | 914/2048 [00:03<00:06, 179.75it/s, est. speed input: 238192.02 toks/s, output: 232.61 toks/s]
Processed prompts:  48%|████▊     | 974/2048 [00:04<00:06, 168.42it/s, est. speed input: 225172.71 toks/s, output: 219.89 toks/s]
Processed prompts:  50%|████▉     | 1015/2048 [00:04<00:06, 147.68it/s, est. speed input: 209279.60 toks/s, output: 204.37 toks/s]
Processed prompts:  51%|█████     | 1044/2048 [00:05<00:07, 135.32it/s, est. speed input: 200454.13 toks/s, output: 195.76 toks/s]
Processed prompts:  52%|█████▏    | 1066/2048 [00:05<00:07, 133.53it/s, est. speed input: 197857.75 toks/s, output: 193.22 toks/s]
Processed prompts:  53%|█████▎    | 1085/2048 [00:05<00:07, 129.29it/s, est. speed input: 194888.24 toks/s, output: 190.32 toks/s]
Processed prompts:  54%|█████▍    | 1101/2048 [00:05<00:07, 122.26it/s, est. speed input: 191593.60 toks/s, output: 187.10 toks/s]
Processed prompts:  54%|█████▍    | 1115/2048 [00:06<00:08, 113.42it/s, est. speed input: 188162.03 toks/s, output: 183.75 toks/s]
Processed prompts:  55%|█████▌    | 1127/2048 [00:06<00:08, 103.04it/s, est. speed input: 184599.16 toks/s, output: 180.27 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:06<00:09, 92.84it/s, est. speed input: 181073.57 toks/s, output: 176.83 toks/s] 
Processed prompts:  56%|█████▋    | 1154/2048 [00:06<00:09, 92.50it/s, est. speed input: 178757.91 toks/s, output: 174.57 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:06<00:09, 91.11it/s, est. speed input: 176347.72 toks/s, output: 172.21 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:06<00:09, 90.11it/s, est. speed input: 174071.85 toks/s, output: 169.99 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:07<00:09, 89.36it/s, est. speed input: 171911.74 toks/s, output: 167.88 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:07<00:09, 88.74it/s, est. speed input: 169848.82 toks/s, output: 165.87 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:07<00:09, 88.23it/s, est. speed input: 167874.32 toks/s, output: 163.94 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:07<00:09, 87.89it/s, est. speed input: 165998.30 toks/s, output: 162.11 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:07<00:08, 87.69it/s, est. speed input: 164215.95 toks/s, output: 160.37 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:08<00:08, 87.51it/s, est. speed input: 162509.77 toks/s, output: 158.70 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:08<00:08, 87.37it/s, est. speed input: 160876.55 toks/s, output: 157.11 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:08<00:08, 87.18it/s, est. speed input: 159304.69 toks/s, output: 155.57 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:08<00:08, 87.07it/s, est. speed input: 157799.09 toks/s, output: 154.10 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:08<00:08, 87.00it/s, est. speed input: 156359.50 toks/s, output: 152.69 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:08<00:07, 86.92it/s, est. speed input: 154974.62 toks/s, output: 151.34 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:09<00:07, 86.92it/s, est. speed input: 153652.93 toks/s, output: 150.05 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:09<00:07, 87.64it/s, est. speed input: 152464.30 toks/s, output: 148.89 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:09<00:07, 89.46it/s, est. speed input: 151461.42 toks/s, output: 147.91 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:09<00:06, 90.76it/s, est. speed input: 150491.21 toks/s, output: 146.96 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:09<00:06, 91.75it/s, est. speed input: 149560.73 toks/s, output: 146.06 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:10<00:06, 92.54it/s, est. speed input: 148668.39 toks/s, output: 145.18 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:10<00:06, 93.07it/s, est. speed input: 147804.00 toks/s, output: 144.34 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:10<00:05, 93.44it/s, est. speed input: 146966.59 toks/s, output: 143.52 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:10<00:05, 93.51it/s, est. speed input: 146140.66 toks/s, output: 142.72 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:10<00:05, 93.62it/s, est. speed input: 145345.68 toks/s, output: 141.94 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:10<00:05, 93.73it/s, est. speed input: 144578.16 toks/s, output: 141.19 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:11<00:05, 93.90it/s, est. speed input: 143841.49 toks/s, output: 140.47 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:11<00:05, 94.02it/s, est. speed input: 143127.62 toks/s, output: 139.77 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:11<00:04, 94.07it/s, est. speed input: 142432.13 toks/s, output: 139.09 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:11<00:04, 94.03it/s, est. speed input: 141751.61 toks/s, output: 138.43 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:11<00:04, 94.15it/s, est. speed input: 141101.69 toks/s, output: 137.79 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:11<00:04, 93.96it/s, est. speed input: 140450.21 toks/s, output: 137.16 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:12<00:04, 93.99it/s, est. speed input: 139828.86 toks/s, output: 136.55 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:12<00:04, 93.99it/s, est. speed input: 139223.55 toks/s, output: 135.96 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:12<00:03, 94.00it/s, est. speed input: 138635.51 toks/s, output: 135.39 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:12<00:03, 92.77it/s, est. speed input: 137979.99 toks/s, output: 134.75 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:12<00:03, 90.87it/s, est. speed input: 137270.91 toks/s, output: 134.05 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:12<00:03, 89.68it/s, est. speed input: 136587.62 toks/s, output: 133.39 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:13<00:03, 88.84it/s, est. speed input: 135922.70 toks/s, output: 132.74 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:13<00:03, 88.22it/s, est. speed input: 135272.66 toks/s, output: 132.10 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:13<00:03, 87.80it/s, est. speed input: 134641.22 toks/s, output: 131.49 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:13<00:02, 87.48it/s, est. speed input: 134024.74 toks/s, output: 130.88 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:13<00:02, 87.32it/s, est. speed input: 133428.53 toks/s, output: 130.30 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:14<00:02, 87.21it/s, est. speed input: 132848.49 toks/s, output: 129.73 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:14<00:02, 87.09it/s, est. speed input: 132280.36 toks/s, output: 129.18 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:14<00:02, 86.98it/s, est. speed input: 131725.14 toks/s, output: 128.64 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:14<00:01, 88.18it/s, est. speed input: 131263.44 toks/s, output: 128.19 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:14<00:01, 87.69it/s, est. speed input: 130731.42 toks/s, output: 127.67 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:14<00:01, 87.35it/s, est. speed input: 130212.32 toks/s, output: 127.16 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:15<00:01, 87.15it/s, est. speed input: 129708.34 toks/s, output: 126.67 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:15<00:01, 87.01it/s, est. speed input: 129216.12 toks/s, output: 126.19 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:15<00:01, 88.15it/s, est. speed input: 128807.32 toks/s, output: 125.79 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:14<00:00, 88.15it/s, est. speed input: 141929.10 toks/s, output: 138.60 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:14<00:00, 138.60it/s, est. speed input: 141929.10 toks/s, output: 138.60 toks/s]
[rank0]:[W125 19:16:51.109227778 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-25 19:16:54
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:17:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:17:28 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=302764) WARNING 01-25 19:17:36 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=302764) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=302764) WARNING 01-25 19:17:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 89.28 requests/s, 91509.08 total tokens/s, 89.28 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-25 19:17:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:17:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:17:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:17:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:17:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:17:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:17:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:17:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:17:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:17:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:17:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:17:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:17:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:17:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:17:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:17:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:17:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:17:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:17:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:17:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:17:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:17:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:17:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:17:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:17:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:17:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:17:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:17:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=302764) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=302764) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=302764) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=302764) 
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=302764) [2026-01-25 19:17:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=302764) [rank0]:W0125 19:17:49.535000 302764 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=302764) [rank0]:W0125 19:17:49.619000 302764 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=302764) [rank0]:W0125 19:17:50.751000 302764 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=302764) [rank0]:W0125 19:17:50.866000 302764 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=302764) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  5.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  9.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.04it/s]
(EngineCore_DP0 pid=302764) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.56it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.59it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.71it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 277.84it/s]
Adding requests:   1%|▏         | 57/4096 [00:00<00:14, 284.69it/s]
Adding requests:   2%|▏         | 87/4096 [00:00<00:13, 290.26it/s]
Adding requests:   3%|▎         | 119/4096 [00:00<00:13, 300.73it/s]
Adding requests:   4%|▎         | 151/4096 [00:00<00:12, 304.64it/s]
Adding requests:   4%|▍         | 182/4096 [00:00<00:12, 302.20it/s]
Adding requests:   5%|▌         | 213/4096 [00:00<00:13, 296.88it/s]
Adding requests:   6%|▌         | 246/4096 [00:00<00:12, 304.39it/s]
Adding requests:   7%|▋         | 278/4096 [00:00<00:12, 306.79it/s]
Adding requests:   8%|▊         | 309/4096 [00:01<00:12, 304.74it/s]
Adding requests:   8%|▊         | 340/4096 [00:01<00:12, 305.62it/s]
Adding requests:   9%|▉         | 373/4096 [00:01<00:11, 310.89it/s]
Adding requests:  10%|▉         | 405/4096 [00:01<00:12, 307.46it/s]
Adding requests:  11%|█         | 436/4096 [00:01<00:12, 300.59it/s]
Adding requests:  11%|█▏        | 467/4096 [00:01<00:12, 299.15it/s]
Adding requests:  12%|█▏        | 497/4096 [00:01<00:12, 289.60it/s]
Adding requests:  13%|█▎        | 527/4096 [00:01<00:12, 282.58it/s]
Adding requests:  14%|█▎        | 558/4096 [00:01<00:12, 289.63it/s]
Adding requests:  14%|█▍        | 589/4096 [00:01<00:11, 293.60it/s]
Adding requests:  15%|█▌        | 619/4096 [00:02<00:11, 292.36it/s]
Adding requests:  16%|█▌        | 650/4096 [00:02<00:11, 295.14it/s]
Adding requests:  17%|█▋        | 680/4096 [00:02<00:11, 295.16it/s]
Adding requests:  17%|█▋        | 711/4096 [00:02<00:11, 298.25it/s]
Adding requests:  18%|█▊        | 741/4096 [00:02<00:11, 296.61it/s]
Adding requests:  19%|█▉        | 772/4096 [00:02<00:11, 300.04it/s]
Adding requests:  20%|█▉        | 803/4096 [00:02<00:11, 298.79it/s]
Adding requests:  20%|██        | 833/4096 [00:02<00:11, 294.37it/s]
Adding requests:  21%|██        | 865/4096 [00:02<00:10, 301.22it/s]
Adding requests:  22%|██▏       | 898/4096 [00:03<00:10, 308.71it/s]
Adding requests:  23%|██▎       | 929/4096 [00:03<00:10, 306.59it/s]
Adding requests:  23%|██▎       | 962/4096 [00:03<00:10, 310.80it/s]
Adding requests:  24%|██▍       | 995/4096 [00:03<00:09, 315.84it/s]
Adding requests:  25%|██▌       | 1029/4096 [00:03<00:09, 321.81it/s]
Adding requests:  26%|██▌       | 1062/4096 [00:03<00:09, 310.51it/s]
Adding requests:  27%|██▋       | 1094/4096 [00:03<00:09, 311.35it/s]
Adding requests:  27%|██▋       | 1126/4096 [00:03<00:09, 311.51it/s]
Adding requests:  28%|██▊       | 1159/4096 [00:03<00:09, 314.53it/s]
Adding requests:  29%|██▉       | 1192/4096 [00:03<00:09, 318.83it/s]
Adding requests:  30%|██▉       | 1225/4096 [00:04<00:08, 320.23it/s]
Adding requests:  31%|███       | 1258/4096 [00:04<00:09, 311.57it/s]
Adding requests:  31%|███▏      | 1290/4096 [00:04<00:08, 312.63it/s]
Adding requests:  32%|███▏      | 1325/4096 [00:04<00:08, 322.89it/s]
Adding requests:  33%|███▎      | 1358/4096 [00:04<00:08, 322.98it/s]
Adding requests:  34%|███▍      | 1391/4096 [00:04<00:08, 316.21it/s]
Adding requests:  35%|███▍      | 1423/4096 [00:04<00:08, 309.06it/s]
Adding requests:  35%|███▌      | 1454/4096 [00:04<00:08, 303.12it/s]
Adding requests:  36%|███▋      | 1485/4096 [00:04<00:08, 293.26it/s]
Adding requests:  37%|███▋      | 1515/4096 [00:04<00:08, 293.85it/s]
Adding requests:  38%|███▊      | 1545/4096 [00:05<00:08, 292.47it/s]
Adding requests:  38%|███▊      | 1575/4096 [00:05<00:08, 289.83it/s]
Adding requests:  39%|███▉      | 1605/4096 [00:05<00:08, 288.84it/s]
Adding requests:  40%|███▉      | 1634/4096 [00:05<00:08, 288.10it/s]
Adding requests:  41%|████      | 1663/4096 [00:05<00:08, 286.06it/s]
Adding requests:  41%|████▏     | 1692/4096 [00:05<00:08, 283.96it/s]
Adding requests:  42%|████▏     | 1721/4096 [00:05<00:08, 282.30it/s]
Adding requests:  43%|████▎     | 1752/4096 [00:05<00:08, 288.22it/s]
Adding requests:  43%|████▎     | 1781/4096 [00:05<00:08, 286.96it/s]
Adding requests:  44%|████▍     | 1811/4096 [00:06<00:07, 290.64it/s]
Adding requests:  45%|████▍     | 1842/4096 [00:06<00:07, 293.41it/s]
Adding requests:  46%|████▌     | 1873/4096 [00:06<00:07, 296.03it/s]
Adding requests:  46%|████▋     | 1903/4096 [00:06<00:07, 295.88it/s]
Adding requests:  47%|████▋     | 1933/4096 [00:06<00:07, 291.35it/s]
Adding requests:  48%|████▊     | 1964/4096 [00:06<00:07, 295.58it/s]
Adding requests:  49%|████▊     | 1994/4096 [00:06<00:07, 295.62it/s]
Adding requests:  49%|████▉     | 2024/4096 [00:06<00:07, 295.93it/s]
Adding requests:  50%|█████     | 2055/4096 [00:06<00:06, 299.95it/s]
Adding requests:  51%|█████     | 2086/4096 [00:06<00:06, 296.40it/s]
Adding requests:  52%|█████▏    | 2116/4096 [00:07<00:06, 294.86it/s]
Adding requests:  52%|█████▏    | 2146/4096 [00:07<00:06, 291.02it/s]
Adding requests:  53%|█████▎    | 2176/4096 [00:07<00:06, 293.49it/s]
Adding requests:  54%|█████▍    | 2206/4096 [00:07<00:06, 295.39it/s]
Adding requests:  55%|█████▍    | 2236/4096 [00:07<00:06, 286.46it/s]
Adding requests:  55%|█████▌    | 2267/4096 [00:07<00:06, 291.14it/s]
Adding requests:  56%|█████▌    | 2299/4096 [00:07<00:06, 299.39it/s]
Adding requests:  57%|█████▋    | 2330/4096 [00:07<00:06, 292.69it/s]
Adding requests:  58%|█████▊    | 2360/4096 [00:07<00:05, 291.91it/s]
Adding requests:  58%|█████▊    | 2391/4096 [00:07<00:05, 295.60it/s]
Adding requests:  59%|█████▉    | 2422/4096 [00:08<00:05, 297.50it/s]
Adding requests:  60%|█████▉    | 2452/4096 [00:08<00:05, 293.07it/s]
Adding requests:  61%|██████    | 2484/4096 [00:08<00:05, 298.25it/s]
Adding requests:  61%|██████▏   | 2514/4096 [00:08<00:05, 298.06it/s]
Adding requests:  62%|██████▏   | 2545/4096 [00:08<00:05, 300.01it/s]
Adding requests:  63%|██████▎   | 2576/4096 [00:08<00:05, 298.26it/s]
Adding requests:  64%|██████▎   | 2606/4096 [00:08<00:05, 297.86it/s]
Adding requests:  64%|██████▍   | 2637/4096 [00:08<00:04, 299.45it/s]
Adding requests:  65%|██████▌   | 2667/4096 [00:08<00:04, 294.23it/s]
Adding requests:  66%|██████▌   | 2697/4096 [00:09<00:04, 286.03it/s]
Adding requests:  67%|██████▋   | 2729/4096 [00:09<00:04, 294.98it/s]
Adding requests:  67%|██████▋   | 2760/4096 [00:09<00:04, 299.11it/s]
Adding requests:  68%|██████▊   | 2790/4096 [00:09<00:04, 296.24it/s]
Adding requests:  69%|██████▉   | 2822/4096 [00:09<00:04, 302.95it/s]
Adding requests:  70%|██████▉   | 2855/4096 [00:09<00:03, 310.56it/s]
Adding requests:  70%|███████   | 2887/4096 [00:09<00:03, 307.39it/s]
Adding requests:  71%|███████   | 2918/4096 [00:09<00:03, 306.62it/s]
Adding requests:  72%|███████▏  | 2949/4096 [00:09<00:03, 300.41it/s]
Adding requests:  73%|███████▎  | 2980/4096 [00:09<00:03, 302.01it/s]
Adding requests:  74%|███████▎  | 3011/4096 [00:10<00:03, 297.07it/s]
Adding requests:  74%|███████▍  | 3043/4096 [00:10<00:03, 303.53it/s]
Adding requests:  75%|███████▌  | 3074/4096 [00:10<00:03, 304.20it/s]
Adding requests:  76%|███████▌  | 3105/4096 [00:10<00:03, 301.17it/s]
Adding requests:  77%|███████▋  | 3136/4096 [00:10<00:03, 302.03it/s]
Adding requests:  77%|███████▋  | 3168/4096 [00:10<00:03, 306.73it/s]
Adding requests:  78%|███████▊  | 3200/4096 [00:10<00:02, 309.07it/s]
Adding requests:  79%|███████▉  | 3233/4096 [00:10<00:02, 315.04it/s]
Adding requests:  80%|███████▉  | 3268/4096 [00:10<00:02, 323.74it/s]
Adding requests:  81%|████████  | 3302/4096 [00:10<00:02, 325.86it/s]
Adding requests:  81%|████████▏ | 3335/4096 [00:11<00:02, 311.71it/s]
Adding requests:  82%|████████▏ | 3369/4096 [00:11<00:02, 318.15it/s]
Adding requests:  83%|████████▎ | 3401/4096 [00:11<00:02, 315.59it/s]
Adding requests:  84%|████████▍ | 3434/4096 [00:11<00:02, 318.37it/s]
Adding requests:  85%|████████▍ | 3466/4096 [00:11<00:01, 315.31it/s]
Adding requests:  85%|████████▌ | 3498/4096 [00:11<00:01, 316.16it/s]
Adding requests:  86%|████████▌ | 3531/4096 [00:11<00:01, 318.39it/s]
Adding requests:  87%|████████▋ | 3563/4096 [00:11<00:01, 305.48it/s]
Adding requests:  88%|████████▊ | 3595/4096 [00:11<00:01, 307.93it/s]
Adding requests:  89%|████████▊ | 3627/4096 [00:12<00:01, 308.50it/s]
Adding requests:  89%|████████▉ | 3658/4096 [00:12<00:01, 300.83it/s]
Adding requests:  90%|█████████ | 3693/4096 [00:12<00:01, 314.82it/s]
Adding requests:  91%|█████████ | 3729/4096 [00:12<00:01, 325.39it/s]
Adding requests:  92%|█████████▏| 3762/4096 [00:12<00:01, 325.29it/s]
Adding requests:  93%|█████████▎| 3797/4096 [00:12<00:00, 331.32it/s]
Adding requests:  94%|█████████▎| 3831/4096 [00:12<00:00, 324.29it/s]
Adding requests:  94%|█████████▍| 3864/4096 [00:12<00:00, 323.59it/s]
Adding requests:  95%|█████████▌| 3897/4096 [00:12<00:00, 319.88it/s]
Adding requests:  96%|█████████▌| 3930/4096 [00:12<00:00, 319.82it/s]
Adding requests:  97%|█████████▋| 3963/4096 [00:13<00:00, 319.94it/s]
Adding requests:  98%|█████████▊| 3996/4096 [00:13<00:00, 300.60it/s]
Adding requests:  98%|█████████▊| 4027/4096 [00:13<00:00, 301.56it/s]
Adding requests:  99%|█████████▉| 4059/4096 [00:13<00:00, 304.79it/s]
Adding requests: 100%|█████████▉| 4090/4096 [00:13<00:00, 304.23it/s]
Adding requests: 100%|██████████| 4096/4096 [00:13<00:00, 302.94it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:00<00:00, 4285.41it/s, est. speed input: 4388748.11 toks/s, output: 4285.56 toks/s]
Processed prompts:  39%|███▉      | 1615/4096 [00:04<00:09, 256.16it/s, est. speed input: 330839.95 toks/s, output: 323.09 toks/s]   
Processed prompts:  44%|████▍     | 1797/4096 [00:07<00:11, 194.21it/s, est. speed input: 261627.59 toks/s, output: 255.50 toks/s]
Processed prompts:  46%|████▋     | 1900/4096 [00:08<00:12, 171.89it/s, est. speed input: 239299.00 toks/s, output: 233.69 toks/s]
Processed prompts:  48%|████▊     | 1966/4096 [00:08<00:13, 157.75it/s, est. speed input: 227272.63 toks/s, output: 221.95 toks/s]
Processed prompts:  49%|████▉     | 2012/4096 [00:09<00:13, 154.07it/s, est. speed input: 223302.09 toks/s, output: 218.07 toks/s]
Processed prompts:  50%|████▉     | 2047/4096 [00:09<00:14, 145.70it/s, est. speed input: 218367.75 toks/s, output: 213.25 toks/s]
Processed prompts:  51%|█████     | 2074/4096 [00:09<00:15, 133.66it/s, est. speed input: 213015.49 toks/s, output: 208.02 toks/s]
Processed prompts:  51%|█████     | 2095/4096 [00:10<00:16, 119.11it/s, est. speed input: 207507.69 toks/s, output: 202.64 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:10<00:18, 104.70it/s, est. speed input: 202184.81 toks/s, output: 197.45 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:11<00:19, 100.50it/s, est. speed input: 198410.44 toks/s, output: 193.76 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:11<00:19, 97.07it/s, est. speed input: 194881.02 toks/s, output: 190.31 toks/s] 
Processed prompts:  54%|█████▍    | 2210/4096 [00:11<00:19, 95.58it/s, est. speed input: 191861.34 toks/s, output: 187.36 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:12<00:19, 93.58it/s, est. speed input: 188834.96 toks/s, output: 184.41 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:12<00:19, 94.47it/s, est. speed input: 186469.76 toks/s, output: 182.10 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:12<00:19, 94.21it/s, est. speed input: 184050.43 toks/s, output: 179.74 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:13<00:18, 94.80it/s, est. speed input: 181894.91 toks/s, output: 177.63 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:13<00:18, 94.56it/s, est. speed input: 179735.03 toks/s, output: 175.52 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:13<00:07, 216.21it/s, est. speed input: 188353.59 toks/s, output: 183.94 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:14<00:08, 181.90it/s, est. speed input: 186154.48 toks/s, output: 181.79 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:14<00:09, 156.72it/s, est. speed input: 184055.84 toks/s, output: 179.74 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:14<00:10, 138.44it/s, est. speed input: 182048.29 toks/s, output: 177.78 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:15<00:11, 123.58it/s, est. speed input: 179934.11 toks/s, output: 175.72 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:15<00:12, 112.15it/s, est. speed input: 177782.79 toks/s, output: 173.62 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:16<00:12, 104.36it/s, est. speed input: 175736.59 toks/s, output: 171.62 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:16<00:13, 98.99it/s, est. speed input: 173781.98 toks/s, output: 169.71 toks/s] 
Processed prompts:  69%|██████▉   | 2818/4096 [00:16<00:13, 95.27it/s, est. speed input: 171912.84 toks/s, output: 167.88 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:17<00:13, 92.66it/s, est. speed input: 170121.10 toks/s, output: 166.13 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:17<00:13, 90.85it/s, est. speed input: 168406.09 toks/s, output: 164.46 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:17<00:13, 89.55it/s, est. speed input: 166757.75 toks/s, output: 162.85 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:18<00:12, 88.79it/s, est. speed input: 165193.47 toks/s, output: 161.32 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:18<00:12, 87.99it/s, est. speed input: 163658.65 toks/s, output: 159.82 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:19<00:12, 87.55it/s, est. speed input: 162197.34 toks/s, output: 158.40 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:19<00:12, 87.26it/s, est. speed input: 160795.17 toks/s, output: 157.03 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:19<00:11, 87.05it/s, est. speed input: 159443.73 toks/s, output: 155.71 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:20<00:11, 86.87it/s, est. speed input: 158138.44 toks/s, output: 154.43 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:20<00:10, 87.47it/s, est. speed input: 156958.72 toks/s, output: 153.28 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:20<00:10, 89.28it/s, est. speed input: 155960.50 toks/s, output: 152.30 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:21<00:09, 90.57it/s, est. speed input: 154993.11 toks/s, output: 151.36 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:21<00:09, 91.51it/s, est. speed input: 154057.08 toks/s, output: 150.45 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:21<00:09, 92.17it/s, est. speed input: 153149.46 toks/s, output: 149.56 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:22<00:08, 92.62it/s, est. speed input: 152268.57 toks/s, output: 148.70 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:22<00:08, 92.99it/s, est. speed input: 151418.19 toks/s, output: 147.87 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:22<00:07, 93.24it/s, est. speed input: 150592.88 toks/s, output: 147.06 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:23<00:07, 92.76it/s, est. speed input: 149739.44 toks/s, output: 146.23 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:23<00:07, 90.81it/s, est. speed input: 148781.39 toks/s, output: 145.29 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:23<00:07, 89.53it/s, est. speed input: 147856.07 toks/s, output: 144.39 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:24<00:06, 88.63it/s, est. speed input: 146956.81 toks/s, output: 143.51 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:24<00:06, 88.00it/s, est. speed input: 146083.89 toks/s, output: 142.66 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:25<00:06, 87.59it/s, est. speed input: 145238.41 toks/s, output: 141.83 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:25<00:05, 87.30it/s, est. speed input: 144416.80 toks/s, output: 141.03 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:25<00:05, 87.09it/s, est. speed input: 143618.83 toks/s, output: 140.25 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:26<00:05, 86.92it/s, est. speed input: 142841.68 toks/s, output: 139.49 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:26<00:04, 86.84it/s, est. speed input: 142088.19 toks/s, output: 138.76 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:26<00:04, 87.42it/s, est. speed input: 141403.24 toks/s, output: 138.09 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:27<00:04, 87.18it/s, est. speed input: 140689.04 toks/s, output: 137.39 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:27<00:03, 87.01it/s, est. speed input: 139993.91 toks/s, output: 136.71 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:28<00:03, 86.88it/s, est. speed input: 139316.17 toks/s, output: 136.05 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:28<00:02, 86.80it/s, est. speed input: 138657.02 toks/s, output: 135.41 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:28<00:02, 87.66it/s, est. speed input: 138076.46 toks/s, output: 134.84 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:29<00:02, 89.43it/s, est. speed input: 137584.51 toks/s, output: 134.36 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [00:29<00:01, 90.72it/s, est. speed input: 137104.20 toks/s, output: 133.89 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [00:29<00:01, 91.65it/s, est. speed input: 136634.73 toks/s, output: 133.43 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [00:30<00:01, 92.32it/s, est. speed input: 136176.81 toks/s, output: 132.99 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [00:30<00:00, 93.49it/s, est. speed input: 135767.00 toks/s, output: 132.58 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [00:30<00:00, 94.67it/s, est. speed input: 135384.38 toks/s, output: 132.21 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:30<00:00, 94.67it/s, est. speed input: 136380.58 toks/s, output: 133.18 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:30<00:00, 133.18it/s, est. speed input: 136380.58 toks/s, output: 133.18 toks/s]
[rank0]:[W125 19:18:42.449935085 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-25 19:18:45
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:19:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:19:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=304878) WARNING 01-25 19:19:51 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=304878) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=304878) WARNING 01-25 19:19:57 [backends.py:609] Failed to read file <frozen os>
Throughput: 26.77 requests/s, 27444.15 total tokens/s, 26.77 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-25 19:19:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:19:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:19:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:19:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:19:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:19:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:19:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:19:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:19:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:19:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:19:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:19:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:19:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:19:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:19:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:19:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:19:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:19:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:19:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:19:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:19:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:19:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:19:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:19:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:19:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:19:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:19:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:19:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:52] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:52] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:52] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:52] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:52] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=304878) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=304878) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=304878) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=304878) 
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=304878) [2026-01-25 19:19:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=304878) [rank0]:W0125 19:20:03.098000 304878 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=304878) [rank0]:W0125 19:20:03.182000 304878 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=304878) [rank0]:W0125 19:20:04.324000 304878 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=304878) [rank0]:W0125 19:20:04.436000 304878 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=304878) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:13,  1.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:01<00:08,  1.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:01<00:05,  3.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:03,  4.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:02,  6.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  7.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:00,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  9.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  9.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  9.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  9.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  9.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  9.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.68it/s]
(EngineCore_DP0 pid=304878) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.00it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  6.92it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:01,  4.51it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:01,  5.65it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:01,  5.91it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:01<00:00,  5.92it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:01<00:00,  6.34it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:01<00:00,  7.16it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  7.83it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  8.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  6.98it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 26/8192 [00:00<00:31, 258.65it/s]
Adding requests:   1%|          | 54/8192 [00:00<00:30, 266.67it/s]
Adding requests:   1%|          | 83/8192 [00:00<00:29, 277.14it/s]
Adding requests:   1%|▏         | 111/8192 [00:00<00:29, 277.84it/s]
Adding requests:   2%|▏         | 139/8192 [00:00<00:28, 277.95it/s]
Adding requests:   2%|▏         | 168/8192 [00:00<00:28, 278.61it/s]
Adding requests:   2%|▏         | 197/8192 [00:00<00:28, 280.36it/s]
Adding requests:   3%|▎         | 226/8192 [00:00<00:28, 282.47it/s]
Adding requests:   3%|▎         | 255/8192 [00:00<00:28, 283.00it/s]
Adding requests:   3%|▎         | 284/8192 [00:01<00:27, 282.96it/s]
Adding requests:   4%|▍         | 315/8192 [00:01<00:27, 289.04it/s]
Adding requests:   4%|▍         | 347/8192 [00:01<00:26, 296.90it/s]
Adding requests:   5%|▍         | 380/8192 [00:01<00:25, 304.64it/s]
Adding requests:   5%|▌         | 412/8192 [00:01<00:25, 308.14it/s]
Adding requests:   5%|▌         | 445/8192 [00:01<00:24, 313.36it/s]
Adding requests:   6%|▌         | 478/8192 [00:01<00:24, 317.78it/s]
Adding requests:   6%|▌         | 510/8192 [00:01<00:24, 315.05it/s]
Adding requests:   7%|▋         | 542/8192 [00:01<00:24, 312.81it/s]
Adding requests:   7%|▋         | 576/8192 [00:01<00:23, 318.13it/s]
Adding requests:   7%|▋         | 608/8192 [00:02<00:24, 312.33it/s]
Adding requests:   8%|▊         | 641/8192 [00:02<00:23, 317.10it/s]
Adding requests:   8%|▊         | 673/8192 [00:02<00:24, 312.91it/s]
Adding requests:   9%|▊         | 705/8192 [00:02<00:24, 309.51it/s]
Adding requests:   9%|▉         | 738/8192 [00:02<00:23, 315.11it/s]
Adding requests:   9%|▉         | 770/8192 [00:02<00:23, 312.20it/s]
Adding requests:  10%|▉         | 802/8192 [00:02<00:23, 310.32it/s]
Adding requests:  10%|█         | 834/8192 [00:02<00:23, 308.12it/s]
Adding requests:  11%|█         | 865/8192 [00:02<00:24, 304.45it/s]
Adding requests:  11%|█         | 896/8192 [00:02<00:23, 305.52it/s]
Adding requests:  11%|█▏        | 927/8192 [00:03<00:23, 305.65it/s]
Adding requests:  12%|█▏        | 958/8192 [00:03<00:23, 303.56it/s]
Adding requests:  12%|█▏        | 989/8192 [00:03<00:23, 304.30it/s]
Adding requests:  12%|█▏        | 1020/8192 [00:03<00:23, 305.66it/s]
Adding requests:  13%|█▎        | 1051/8192 [00:03<00:23, 304.55it/s]
Adding requests:  13%|█▎        | 1082/8192 [00:03<00:23, 302.65it/s]
Adding requests:  14%|█▎        | 1113/8192 [00:03<00:23, 300.63it/s]
Adding requests:  14%|█▍        | 1144/8192 [00:03<00:24, 293.22it/s]
Adding requests:  14%|█▍        | 1174/8192 [00:03<00:24, 291.45it/s]
Adding requests:  15%|█▍        | 1204/8192 [00:04<00:23, 292.46it/s]
Adding requests:  15%|█▌        | 1234/8192 [00:04<00:24, 286.50it/s]
Adding requests:  15%|█▌        | 1263/8192 [00:04<00:24, 281.20it/s]
Adding requests:  16%|█▌        | 1292/8192 [00:04<00:24, 277.25it/s]
Adding requests:  16%|█▌        | 1322/8192 [00:04<00:24, 283.59it/s]
Adding requests:  17%|█▋        | 1353/8192 [00:04<00:23, 288.95it/s]
Adding requests:  17%|█▋        | 1382/8192 [00:04<00:23, 286.76it/s]
Adding requests:  17%|█▋        | 1414/8192 [00:04<00:23, 293.51it/s]
Adding requests:  18%|█▊        | 1445/8192 [00:04<00:22, 295.52it/s]
Adding requests:  18%|█▊        | 1477/8192 [00:04<00:22, 300.53it/s]
Adding requests:  18%|█▊        | 1508/8192 [00:05<00:22, 299.56it/s]
Adding requests:  19%|█▉        | 1538/8192 [00:05<00:22, 298.99it/s]
Adding requests:  19%|█▉        | 1570/8192 [00:05<00:21, 303.11it/s]
Adding requests:  20%|█▉        | 1601/8192 [00:05<00:21, 304.79it/s]
Adding requests:  20%|█▉        | 1632/8192 [00:05<00:22, 296.91it/s]
Adding requests:  20%|██        | 1662/8192 [00:05<00:22, 296.35it/s]
Adding requests:  21%|██        | 1692/8192 [00:05<00:22, 292.29it/s]
Adding requests:  21%|██        | 1722/8192 [00:05<00:22, 293.44it/s]
Adding requests:  21%|██▏       | 1752/8192 [00:05<00:21, 294.97it/s]
Adding requests:  22%|██▏       | 1782/8192 [00:05<00:22, 288.87it/s]
Adding requests:  22%|██▏       | 1811/8192 [00:06<00:22, 287.07it/s]
Adding requests:  22%|██▏       | 1840/8192 [00:06<00:22, 287.91it/s]
Adding requests:  23%|██▎       | 1869/8192 [00:06<00:23, 270.95it/s]
Adding requests:  23%|██▎       | 1897/8192 [00:06<00:23, 270.96it/s]
Adding requests:  24%|██▎       | 1926/8192 [00:06<00:22, 275.73it/s]
Adding requests:  24%|██▍       | 1955/8192 [00:06<00:22, 278.82it/s]
Adding requests:  24%|██▍       | 1983/8192 [00:06<00:22, 277.41it/s]
Adding requests:  25%|██▍       | 2011/8192 [00:06<00:22, 277.26it/s]
Adding requests:  25%|██▍       | 2039/8192 [00:06<00:22, 272.23it/s]
Adding requests:  25%|██▌       | 2068/8192 [00:07<00:22, 275.69it/s]
Adding requests:  26%|██▌       | 2096/8192 [00:07<00:22, 275.55it/s]
Adding requests:  26%|██▌       | 2124/8192 [00:07<00:22, 275.40it/s]
Adding requests:  26%|██▋       | 2152/8192 [00:07<00:21, 275.72it/s]
Adding requests:  27%|██▋       | 2180/8192 [00:07<00:21, 275.95it/s]
Adding requests:  27%|██▋       | 2208/8192 [00:07<00:21, 273.73it/s]
Adding requests:  27%|██▋       | 2238/8192 [00:07<00:21, 280.97it/s]
Adding requests:  28%|██▊       | 2269/8192 [00:07<00:20, 287.22it/s]
Adding requests:  28%|██▊       | 2299/8192 [00:07<00:20, 290.66it/s]
Adding requests:  28%|██▊       | 2329/8192 [00:07<00:20, 288.27it/s]
Adding requests:  29%|██▉       | 2360/8192 [00:08<00:19, 292.25it/s]
Adding requests:  29%|██▉       | 2391/8192 [00:08<00:19, 295.12it/s]
Adding requests:  30%|██▉       | 2421/8192 [00:08<00:19, 293.99it/s]
Adding requests:  30%|██▉       | 2453/8192 [00:08<00:19, 299.73it/s]
Adding requests:  30%|███       | 2483/8192 [00:08<00:19, 296.63it/s]
Adding requests:  31%|███       | 2515/8192 [00:08<00:18, 301.61it/s]
Adding requests:  31%|███       | 2548/8192 [00:08<00:18, 307.88it/s]
Adding requests:  32%|███▏      | 2582/8192 [00:08<00:17, 316.59it/s]
Adding requests:  32%|███▏      | 2615/8192 [00:08<00:17, 318.46it/s]
Adding requests:  32%|███▏      | 2648/8192 [00:08<00:17, 319.53it/s]
Adding requests:  33%|███▎      | 2682/8192 [00:09<00:17, 323.67it/s]
Adding requests:  33%|███▎      | 2715/8192 [00:09<00:17, 320.19it/s]
Adding requests:  34%|███▎      | 2748/8192 [00:09<00:17, 314.81it/s]
Adding requests:  34%|███▍      | 2780/8192 [00:09<00:17, 311.77it/s]
Adding requests:  34%|███▍      | 2812/8192 [00:09<00:17, 313.66it/s]
Adding requests:  35%|███▍      | 2844/8192 [00:09<00:17, 313.41it/s]
Adding requests:  35%|███▌      | 2876/8192 [00:09<00:17, 312.60it/s]
Adding requests:  36%|███▌      | 2909/8192 [00:09<00:16, 316.74it/s]
Adding requests:  36%|███▌      | 2941/8192 [00:09<00:16, 311.34it/s]
Adding requests:  36%|███▋      | 2974/8192 [00:10<00:16, 315.08it/s]
Adding requests:  37%|███▋      | 3006/8192 [00:10<00:16, 313.25it/s]
Adding requests:  37%|███▋      | 3038/8192 [00:10<00:16, 313.18it/s]
Adding requests:  37%|███▋      | 3070/8192 [00:10<00:16, 313.25it/s]
Adding requests:  38%|███▊      | 3102/8192 [00:10<00:16, 302.10it/s]
Adding requests:  38%|███▊      | 3133/8192 [00:10<00:17, 297.10it/s]
Adding requests:  39%|███▊      | 3166/8192 [00:10<00:16, 304.19it/s]
Adding requests:  39%|███▉      | 3198/8192 [00:10<00:16, 306.29it/s]
Adding requests:  39%|███▉      | 3229/8192 [00:10<00:16, 297.73it/s]
Adding requests:  40%|███▉      | 3262/8192 [00:10<00:16, 305.85it/s]
Adding requests:  40%|████      | 3295/8192 [00:11<00:15, 312.52it/s]
Adding requests:  41%|████      | 3327/8192 [00:11<00:15, 314.29it/s]
Adding requests:  41%|████      | 3362/8192 [00:11<00:14, 322.28it/s]
Adding requests:  41%|████▏     | 3396/8192 [00:11<00:14, 326.25it/s]
Adding requests:  42%|████▏     | 3430/8192 [00:11<00:14, 327.20it/s]
Adding requests:  42%|████▏     | 3463/8192 [00:11<00:15, 313.59it/s]
Adding requests:  43%|████▎     | 3495/8192 [00:11<00:14, 313.29it/s]
Adding requests:  43%|████▎     | 3527/8192 [00:11<00:14, 312.02it/s]
Adding requests:  43%|████▎     | 3559/8192 [00:11<00:14, 310.73it/s]
Adding requests:  44%|████▍     | 3591/8192 [00:12<00:15, 305.12it/s]
Adding requests:  44%|████▍     | 3622/8192 [00:12<00:15, 304.02it/s]
Adding requests:  45%|████▍     | 3653/8192 [00:12<00:14, 303.93it/s]
Adding requests:  45%|████▍     | 3684/8192 [00:12<00:15, 295.37it/s]
Adding requests:  45%|████▌     | 3714/8192 [00:12<00:15, 294.44it/s]
Adding requests:  46%|████▌     | 3744/8192 [00:12<00:15, 282.77it/s]
Adding requests:  46%|████▌     | 3773/8192 [00:12<00:15, 278.26it/s]
Adding requests:  46%|████▋     | 3801/8192 [00:12<00:15, 276.86it/s]
Adding requests:  47%|████▋     | 3830/8192 [00:12<00:15, 279.90it/s]
Adding requests:  47%|████▋     | 3861/8192 [00:12<00:15, 286.96it/s]
Adding requests:  47%|████▋     | 3890/8192 [00:13<00:15, 279.26it/s]
Adding requests:  48%|████▊     | 3920/8192 [00:13<00:15, 282.46it/s]
Adding requests:  54%|█████▍    | 4439/8192 [00:13<00:02, 1689.01it/s]
Adding requests:  56%|█████▋    | 4611/8192 [00:13<00:05, 690.66it/s] 
Adding requests:  58%|█████▊    | 4740/8192 [00:14<00:06, 515.02it/s]
Adding requests:  59%|█████▉    | 4839/8192 [00:14<00:07, 437.00it/s]
Adding requests:  60%|██████    | 4917/8192 [00:14<00:08, 384.70it/s]
Adding requests:  61%|██████    | 4979/8192 [00:15<00:09, 356.34it/s]
Adding requests:  61%|██████▏   | 5031/8192 [00:15<00:09, 343.03it/s]
Adding requests:  62%|██████▏   | 5076/8192 [00:15<00:09, 331.92it/s]
Adding requests:  62%|██████▏   | 5116/8192 [00:15<00:09, 323.57it/s]
Adding requests:  63%|██████▎   | 5153/8192 [00:15<00:09, 314.90it/s]
Adding requests:  63%|██████▎   | 5188/8192 [00:15<00:09, 305.17it/s]
Adding requests:  64%|██████▎   | 5221/8192 [00:16<00:09, 301.44it/s]
Adding requests:  64%|██████▍   | 5253/8192 [00:16<00:10, 291.31it/s]
Adding requests:  64%|██████▍   | 5283/8192 [00:16<00:10, 287.58it/s]
Adding requests:  65%|██████▍   | 5314/8192 [00:16<00:09, 291.11it/s]
Adding requests:  65%|██████▌   | 5346/8192 [00:16<00:09, 297.64it/s]
Adding requests:  66%|██████▌   | 5377/8192 [00:16<00:09, 299.51it/s]
Adding requests:  66%|██████▌   | 5410/8192 [00:16<00:09, 306.54it/s]
Adding requests:  66%|██████▋   | 5442/8192 [00:16<00:08, 308.15it/s]
Adding requests:  67%|██████▋   | 5473/8192 [00:16<00:08, 308.66it/s]
Adding requests:  67%|██████▋   | 5504/8192 [00:17<00:09, 297.75it/s]
Adding requests:  68%|██████▊   | 5534/8192 [00:17<00:09, 295.29it/s]
Adding requests:  68%|██████▊   | 5564/8192 [00:17<00:09, 281.13it/s]
Adding requests:  68%|██████▊   | 5593/8192 [00:17<00:09, 275.70it/s]
Adding requests:  69%|██████▊   | 5622/8192 [00:17<00:09, 277.51it/s]
Adding requests:  69%|██████▉   | 5652/8192 [00:17<00:09, 281.75it/s]
Adding requests:  69%|██████▉   | 5683/8192 [00:17<00:08, 287.77it/s]
Adding requests:  70%|██████▉   | 5712/8192 [00:17<00:08, 287.31it/s]
Adding requests:  70%|███████   | 5742/8192 [00:17<00:08, 288.85it/s]
Adding requests:  70%|███████   | 5773/8192 [00:17<00:08, 293.99it/s]
Adding requests:  71%|███████   | 5803/8192 [00:18<00:08, 290.62it/s]
Adding requests:  71%|███████   | 5833/8192 [00:18<00:08, 288.64it/s]
Adding requests:  72%|███████▏  | 5864/8192 [00:18<00:07, 294.41it/s]
Adding requests:  72%|███████▏  | 5894/8192 [00:18<00:07, 292.95it/s]
Adding requests:  72%|███████▏  | 5925/8192 [00:18<00:07, 297.32it/s]
Adding requests:  73%|███████▎  | 5955/8192 [00:18<00:07, 284.98it/s]
Adding requests:  73%|███████▎  | 5989/8192 [00:18<00:07, 299.95it/s]
Adding requests:  73%|███████▎  | 6021/8192 [00:18<00:07, 304.09it/s]
Adding requests:  74%|███████▍  | 6052/8192 [00:18<00:07, 305.22it/s]
Adding requests:  74%|███████▍  | 6084/8192 [00:19<00:06, 309.31it/s]
Adding requests:  75%|███████▍  | 6115/8192 [00:19<00:06, 309.22it/s]
Adding requests:  75%|███████▌  | 6148/8192 [00:19<00:06, 312.89it/s]
Adding requests:  75%|███████▌  | 6180/8192 [00:19<00:06, 314.12it/s]
Adding requests:  76%|███████▌  | 6212/8192 [00:19<00:06, 310.41it/s]
Adding requests:  76%|███████▌  | 6244/8192 [00:19<00:06, 304.40it/s]
Adding requests:  77%|███████▋  | 6277/8192 [00:19<00:06, 308.96it/s]
Adding requests:  77%|███████▋  | 6308/8192 [00:19<00:06, 306.48it/s]
Adding requests:  77%|███████▋  | 6339/8192 [00:19<00:06, 306.98it/s]
Adding requests:  78%|███████▊  | 6370/8192 [00:19<00:06, 302.05it/s]
Adding requests:  78%|███████▊  | 6401/8192 [00:20<00:06, 297.16it/s]
Adding requests:  79%|███████▊  | 6431/8192 [00:20<00:05, 294.82it/s]
Adding requests:  79%|███████▉  | 6461/8192 [00:20<00:05, 295.16it/s]
Adding requests:  79%|███████▉  | 6491/8192 [00:20<00:05, 293.10it/s]
Adding requests:  80%|███████▉  | 6522/8192 [00:20<00:05, 295.86it/s]
Adding requests:  80%|███████▉  | 6552/8192 [00:20<00:05, 293.58it/s]
Adding requests:  80%|████████  | 6582/8192 [00:20<00:05, 293.77it/s]
Adding requests:  81%|████████  | 6613/8192 [00:20<00:05, 296.15it/s]
Adding requests:  81%|████████  | 6643/8192 [00:20<00:05, 292.67it/s]
Adding requests:  81%|████████▏ | 6673/8192 [00:20<00:05, 282.34it/s]
Adding requests:  82%|████████▏ | 6703/8192 [00:21<00:05, 284.25it/s]
Adding requests:  82%|████████▏ | 6732/8192 [00:21<00:05, 280.39it/s]
Adding requests:  83%|████████▎ | 6761/8192 [00:21<00:05, 278.98it/s]
Adding requests:  83%|████████▎ | 6792/8192 [00:21<00:04, 284.81it/s]
Adding requests:  83%|████████▎ | 6821/8192 [00:21<00:04, 280.13it/s]
Adding requests:  84%|████████▎ | 6851/8192 [00:21<00:04, 283.78it/s]
Adding requests:  84%|████████▍ | 6883/8192 [00:21<00:04, 293.58it/s]
Adding requests:  84%|████████▍ | 6914/8192 [00:21<00:04, 296.64it/s]
Adding requests:  85%|████████▍ | 6947/8192 [00:21<00:04, 303.81it/s]
Adding requests:  85%|████████▌ | 6978/8192 [00:22<00:03, 303.55it/s]
Adding requests:  86%|████████▌ | 7009/8192 [00:22<00:03, 302.97it/s]
Adding requests:  86%|████████▌ | 7040/8192 [00:22<00:03, 302.80it/s]
Adding requests:  86%|████████▋ | 7071/8192 [00:22<00:03, 299.90it/s]
Adding requests:  87%|████████▋ | 7102/8192 [00:22<00:03, 297.70it/s]
Adding requests:  87%|████████▋ | 7134/8192 [00:22<00:03, 302.75it/s]
Adding requests:  87%|████████▋ | 7165/8192 [00:22<00:03, 303.76it/s]
Adding requests:  88%|████████▊ | 7196/8192 [00:22<00:03, 301.73it/s]
Adding requests:  88%|████████▊ | 7227/8192 [00:22<00:03, 296.16it/s]
Adding requests:  89%|████████▊ | 7259/8192 [00:22<00:03, 302.30it/s]
Adding requests:  89%|████████▉ | 7291/8192 [00:23<00:02, 306.66it/s]
Adding requests:  89%|████████▉ | 7322/8192 [00:23<00:03, 285.35it/s]
Adding requests:  90%|████████▉ | 7351/8192 [00:23<00:02, 283.10it/s]
Adding requests:  90%|█████████ | 7380/8192 [00:23<00:02, 282.89it/s]
Adding requests:  90%|█████████ | 7409/8192 [00:23<00:02, 277.61it/s]
Adding requests:  91%|█████████ | 7439/8192 [00:23<00:02, 282.40it/s]
Adding requests:  91%|█████████ | 7469/8192 [00:23<00:02, 286.42it/s]
Adding requests:  92%|█████████▏| 7499/8192 [00:23<00:02, 287.63it/s]
Adding requests:  92%|█████████▏| 7528/8192 [00:23<00:02, 287.76it/s]
Adding requests:  92%|█████████▏| 7557/8192 [00:24<00:02, 287.78it/s]
Adding requests:  93%|█████████▎| 7587/8192 [00:24<00:02, 288.46it/s]
Adding requests:  93%|█████████▎| 7616/8192 [00:24<00:01, 288.69it/s]
Adding requests:  93%|█████████▎| 7645/8192 [00:24<00:01, 287.24it/s]
Adding requests:  94%|█████████▎| 7674/8192 [00:24<00:01, 287.25it/s]
Adding requests:  94%|█████████▍| 7703/8192 [00:24<00:01, 286.16it/s]
Adding requests:  94%|█████████▍| 7732/8192 [00:24<00:01, 279.02it/s]
Adding requests:  95%|█████████▍| 7762/8192 [00:24<00:01, 283.21it/s]
Adding requests:  95%|█████████▌| 7793/8192 [00:24<00:01, 290.76it/s]
Adding requests:  96%|█████████▌| 7824/8192 [00:24<00:01, 295.24it/s]
Adding requests:  96%|█████████▌| 7855/8192 [00:25<00:01, 298.84it/s]
Adding requests:  96%|█████████▋| 7885/8192 [00:25<00:01, 299.11it/s]
Adding requests:  97%|█████████▋| 7915/8192 [00:25<00:00, 297.41it/s]
Adding requests:  97%|█████████▋| 7945/8192 [00:25<00:00, 294.81it/s]
Adding requests:  97%|█████████▋| 7976/8192 [00:25<00:00, 296.41it/s]
Adding requests:  98%|█████████▊| 8006/8192 [00:25<00:00, 290.11it/s]
Adding requests:  98%|█████████▊| 8036/8192 [00:25<00:00, 266.21it/s]
Adding requests:  98%|█████████▊| 8064/8192 [00:25<00:00, 250.79it/s]
Adding requests:  99%|█████████▉| 8090/8192 [00:25<00:00, 245.09it/s]
Adding requests:  99%|█████████▉| 8120/8192 [00:26<00:00, 257.52it/s]
Adding requests:  99%|█████████▉| 8151/8192 [00:26<00:00, 270.46it/s]
Adding requests: 100%|█████████▉| 8183/8192 [00:26<00:00, 282.38it/s]
Adding requests: 100%|██████████| 8192/8192 [00:26<00:00, 311.94it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 744/8192 [00:02<00:20, 369.39it/s, est. speed input: 378263.17 toks/s, output: 369.39 toks/s]
Processed prompts:  10%|▉         | 808/8192 [00:04<00:49, 150.28it/s, est. speed input: 184040.81 toks/s, output: 179.73 toks/s]
Processed prompts:  11%|█         | 872/8192 [00:06<01:19, 91.64it/s, est. speed input: 128718.94 toks/s, output: 125.70 toks/s] 
Processed prompts:  11%|█▏        | 936/8192 [00:09<01:44, 69.21it/s, est. speed input: 106104.12 toks/s, output: 103.62 toks/s]
Processed prompts:  12%|█▏        | 1000/8192 [00:11<02:16, 52.87it/s, est. speed input: 89202.01 toks/s, output: 87.11 toks/s] 
Processed prompts:  13%|█▎        | 1064/8192 [00:13<02:44, 43.24it/s, est. speed input: 78025.30 toks/s, output: 76.20 toks/s]
Processed prompts:  14%|█▍        | 1128/8192 [00:16<03:02, 38.80it/s, est. speed input: 71416.97 toks/s, output: 69.74 toks/s]
Processed prompts:  15%|█▍        | 1192/8192 [00:16<02:35, 45.13it/s, est. speed input: 72248.89 toks/s, output: 70.56 toks/s]
Processed prompts:  15%|█▌        | 1256/8192 [00:19<03:03, 37.70it/s, est. speed input: 66372.34 toks/s, output: 64.82 toks/s]
Processed prompts:  16%|█▌        | 1320/8192 [00:21<03:24, 33.58it/s, est. speed input: 61891.11 toks/s, output: 60.44 toks/s]
Processed prompts:  17%|█▋        | 1384/8192 [00:24<03:34, 31.75it/s, est. speed input: 58714.73 toks/s, output: 57.34 toks/s]
Processed prompts:  18%|█▊        | 1448/8192 [00:26<03:43, 30.21it/s, est. speed input: 55927.91 toks/s, output: 54.62 toks/s]
Processed prompts:  18%|█▊        | 1512/8192 [00:29<03:52, 28.75it/s, est. speed input: 53384.98 toks/s, output: 52.13 toks/s]
Processed prompts:  19%|█▉        | 1576/8192 [00:31<03:58, 27.78it/s, est. speed input: 51242.45 toks/s, output: 50.04 toks/s]
Processed prompts:  20%|██        | 1640/8192 [00:33<03:58, 27.52it/s, est. speed input: 49581.95 toks/s, output: 48.42 toks/s]
Processed prompts:  21%|██        | 1704/8192 [00:36<04:00, 26.96it/s, est. speed input: 47990.75 toks/s, output: 46.87 toks/s]
Processed prompts:  22%|██▏       | 1768/8192 [00:38<04:01, 26.58it/s, est. speed input: 46602.81 toks/s, output: 45.51 toks/s]
Processed prompts:  22%|██▏       | 1832/8192 [00:41<03:53, 27.27it/s, est. speed input: 45697.44 toks/s, output: 44.63 toks/s]
Processed prompts:  23%|██▎       | 1896/8192 [00:43<03:53, 26.96it/s, est. speed input: 44645.32 toks/s, output: 43.60 toks/s]
Processed prompts:  24%|██▍       | 1960/8192 [00:45<03:51, 26.95it/s, est. speed input: 43760.90 toks/s, output: 42.74 toks/s]
Processed prompts:  25%|██▍       | 2024/8192 [00:48<03:47, 27.10it/s, est. speed input: 43002.94 toks/s, output: 42.00 toks/s]
Processed prompts:  25%|██▌       | 2088/8192 [00:48<02:55, 34.85it/s, est. speed input: 43809.26 toks/s, output: 42.78 toks/s]
Processed prompts:  26%|██▋       | 2152/8192 [00:51<03:09, 31.82it/s, est. speed input: 43019.27 toks/s, output: 42.01 toks/s]
Processed prompts:  27%|██▋       | 2216/8192 [00:53<03:15, 30.63it/s, est. speed input: 42417.24 toks/s, output: 41.42 toks/s]
Processed prompts:  28%|██▊       | 2280/8192 [00:55<03:16, 30.12it/s, est. speed input: 41912.98 toks/s, output: 40.93 toks/s]
Processed prompts:  29%|██▊       | 2344/8192 [00:58<03:19, 29.27it/s, est. speed input: 41359.56 toks/s, output: 40.39 toks/s]
Processed prompts:  29%|██▉       | 2408/8192 [01:00<03:25, 28.13it/s, est. speed input: 40745.24 toks/s, output: 39.79 toks/s]
Processed prompts:  30%|███       | 2472/8192 [01:02<03:26, 27.74it/s, est. speed input: 40244.41 toks/s, output: 39.30 toks/s]
Processed prompts:  31%|███       | 2536/8192 [01:05<03:23, 27.78it/s, est. speed input: 39832.09 toks/s, output: 38.90 toks/s]
Processed prompts:  32%|███▏      | 2600/8192 [01:07<03:24, 27.29it/s, est. speed input: 39362.86 toks/s, output: 38.44 toks/s]
Processed prompts:  33%|███▎      | 2664/8192 [01:10<03:26, 26.82it/s, est. speed input: 38905.09 toks/s, output: 37.99 toks/s]
Processed prompts:  33%|███▎      | 2728/8192 [01:12<03:22, 26.93it/s, est. speed input: 38546.09 toks/s, output: 37.64 toks/s]
Processed prompts:  34%|███▍      | 2792/8192 [01:14<03:20, 26.89it/s, est. speed input: 38191.80 toks/s, output: 37.30 toks/s]
Processed prompts:  35%|███▍      | 2856/8192 [01:17<03:20, 26.58it/s, est. speed input: 37817.90 toks/s, output: 36.93 toks/s]
Processed prompts:  36%|███▌      | 2920/8192 [01:18<02:38, 33.27it/s, est. speed input: 38272.78 toks/s, output: 37.38 toks/s]
Processed prompts:  36%|███▋      | 2984/8192 [01:20<02:45, 31.45it/s, est. speed input: 37995.33 toks/s, output: 37.10 toks/s]
Processed prompts:  37%|███▋      | 3048/8192 [01:22<02:51, 29.91it/s, est. speed input: 37692.54 toks/s, output: 36.81 toks/s]
Processed prompts:  38%|███▊      | 3112/8192 [01:25<02:55, 28.93it/s, est. speed input: 37408.19 toks/s, output: 36.53 toks/s]
Processed prompts:  39%|███▉      | 3176/8192 [01:27<02:59, 28.00it/s, est. speed input: 37107.51 toks/s, output: 36.24 toks/s]
Processed prompts:  40%|███▉      | 3240/8192 [01:29<02:57, 27.92it/s, est. speed input: 36884.09 toks/s, output: 36.02 toks/s]
Processed prompts:  40%|████      | 3304/8192 [01:32<02:59, 27.27it/s, est. speed input: 36605.41 toks/s, output: 35.75 toks/s]
Processed prompts:  41%|████      | 3368/8192 [01:34<03:00, 26.78it/s, est. speed input: 36335.91 toks/s, output: 35.48 toks/s]
Processed prompts:  42%|████▏     | 3432/8192 [01:37<02:56, 27.03it/s, est. speed input: 36144.00 toks/s, output: 35.30 toks/s]
Processed prompts:  43%|████▎     | 3496/8192 [01:39<02:55, 26.77it/s, est. speed input: 35915.16 toks/s, output: 35.07 toks/s]
Processed prompts:  43%|████▎     | 3560/8192 [01:42<02:55, 26.44it/s, est. speed input: 35681.76 toks/s, output: 34.85 toks/s]
Processed prompts:  44%|████▍     | 3624/8192 [01:44<02:51, 26.63it/s, est. speed input: 35501.32 toks/s, output: 34.67 toks/s]
Processed prompts:  45%|████▌     | 3688/8192 [01:46<02:46, 27.03it/s, est. speed input: 35355.41 toks/s, output: 34.53 toks/s]
Processed prompts:  46%|████▌     | 3752/8192 [01:49<02:46, 26.63it/s, est. speed input: 35150.68 toks/s, output: 34.33 toks/s]
Processed prompts:  47%|████▋     | 3816/8192 [01:50<02:13, 32.68it/s, est. speed input: 35451.96 toks/s, output: 34.62 toks/s]
Processed prompts:  47%|████▋     | 3880/8192 [01:52<02:20, 30.68it/s, est. speed input: 35283.80 toks/s, output: 34.46 toks/s]
Processed prompts:  48%|████▊     | 3944/8192 [01:54<02:24, 29.42it/s, est. speed input: 35122.02 toks/s, output: 34.30 toks/s]
Processed prompts:  49%|████▉     | 4008/8192 [01:57<02:26, 28.63it/s, est. speed input: 34969.21 toks/s, output: 34.15 toks/s]
Processed prompts:  50%|████▉     | 4072/8192 [01:59<02:28, 27.81it/s, est. speed input: 34799.48 toks/s, output: 33.98 toks/s]
Processed prompts:  50%|█████     | 4136/8192 [02:02<02:26, 27.74it/s, est. speed input: 34674.76 toks/s, output: 33.86 toks/s]
Processed prompts:  51%|█████▏    | 4200/8192 [02:04<02:25, 27.47it/s, est. speed input: 34538.15 toks/s, output: 33.73 toks/s]
Processed prompts:  52%|█████▏    | 4264/8192 [02:06<02:24, 27.28it/s, est. speed input: 34405.32 toks/s, output: 33.60 toks/s]
Processed prompts:  53%|█████▎    | 4328/8192 [02:09<02:19, 27.68it/s, est. speed input: 34318.20 toks/s, output: 33.51 toks/s]
Processed prompts:  54%|█████▎    | 4392/8192 [02:11<02:19, 27.33it/s, est. speed input: 34187.12 toks/s, output: 33.39 toks/s]
Processed prompts:  54%|█████▍    | 4456/8192 [02:14<02:19, 26.82it/s, est. speed input: 34040.91 toks/s, output: 33.24 toks/s]
Processed prompts:  55%|█████▌    | 4520/8192 [02:16<02:16, 26.87it/s, est. speed input: 33929.99 toks/s, output: 33.13 toks/s]
Processed prompts:  56%|█████▌    | 4584/8192 [02:18<02:13, 26.93it/s, est. speed input: 33823.84 toks/s, output: 33.03 toks/s]
Processed prompts:  57%|█████▋    | 4648/8192 [02:19<01:45, 33.59it/s, est. speed input: 34098.08 toks/s, output: 33.30 toks/s]
Processed prompts:  58%|█████▊    | 4712/8192 [02:22<01:53, 30.72it/s, est. speed input: 33959.59 toks/s, output: 33.16 toks/s]
Processed prompts:  58%|█████▊    | 4776/8192 [02:24<01:56, 29.39it/s, est. speed input: 33850.01 toks/s, output: 33.06 toks/s]
Processed prompts:  59%|█████▉    | 4840/8192 [02:26<01:55, 29.07it/s, est. speed input: 33775.69 toks/s, output: 32.98 toks/s]
Processed prompts:  60%|█████▉    | 4904/8192 [02:29<01:57, 27.98it/s, est. speed input: 33651.91 toks/s, output: 32.86 toks/s]
Processed prompts:  61%|██████    | 4968/8192 [02:31<01:56, 27.63it/s, est. speed input: 33554.78 toks/s, output: 32.77 toks/s]
Processed prompts:  61%|██████▏   | 5032/8192 [02:33<01:54, 27.66it/s, est. speed input: 33477.43 toks/s, output: 32.69 toks/s]
Processed prompts:  62%|██████▏   | 5096/8192 [02:36<01:54, 27.07it/s, est. speed input: 33365.37 toks/s, output: 32.58 toks/s]
Processed prompts:  63%|██████▎   | 5160/8192 [02:38<01:53, 26.65it/s, est. speed input: 33255.53 toks/s, output: 32.48 toks/s]
Processed prompts:  64%|██████▍   | 5224/8192 [02:41<01:50, 26.90it/s, est. speed input: 33181.80 toks/s, output: 32.40 toks/s]
Processed prompts:  65%|██████▍   | 5288/8192 [02:43<01:48, 26.74it/s, est. speed input: 33090.50 toks/s, output: 32.31 toks/s]
Processed prompts:  65%|██████▌   | 5352/8192 [02:46<01:47, 26.41it/s, est. speed input: 32988.07 toks/s, output: 32.21 toks/s]
Processed prompts:  66%|██████▌   | 5416/8192 [02:48<01:43, 26.70it/s, est. speed input: 32919.78 toks/s, output: 32.15 toks/s]
Processed prompts:  67%|██████▋   | 5480/8192 [02:50<01:41, 26.83it/s, est. speed input: 32849.03 toks/s, output: 32.08 toks/s]
Processed prompts:  68%|██████▊   | 5544/8192 [02:51<01:18, 33.79it/s, est. speed input: 33087.96 toks/s, output: 32.31 toks/s]
Processed prompts:  68%|██████▊   | 5608/8192 [02:54<01:23, 30.89it/s, est. speed input: 32991.77 toks/s, output: 32.22 toks/s]
Processed prompts:  69%|██████▉   | 5672/8192 [02:56<01:25, 29.48it/s, est. speed input: 32914.05 toks/s, output: 32.14 toks/s]
Processed prompts:  70%|███████   | 5736/8192 [02:58<01:25, 28.73it/s, est. speed input: 32846.02 toks/s, output: 32.08 toks/s]
Processed prompts:  71%|███████   | 5800/8192 [03:01<01:26, 27.78it/s, est. speed input: 32758.13 toks/s, output: 31.99 toks/s]
Processed prompts:  72%|███████▏  | 5864/8192 [03:03<01:25, 27.15it/s, est. speed input: 32672.43 toks/s, output: 31.91 toks/s]
Processed prompts:  72%|███████▏  | 5928/8192 [03:06<01:22, 27.34it/s, est. speed input: 32620.42 toks/s, output: 31.86 toks/s]
Processed prompts:  73%|███████▎  | 5992/8192 [03:08<01:21, 26.84it/s, est. speed input: 32537.89 toks/s, output: 31.78 toks/s]
Processed prompts:  74%|███████▍  | 6056/8192 [03:11<01:20, 26.51it/s, est. speed input: 32457.85 toks/s, output: 31.70 toks/s]
Processed prompts:  75%|███████▍  | 6120/8192 [03:13<01:16, 26.92it/s, est. speed input: 32411.98 toks/s, output: 31.65 toks/s]
Processed prompts:  75%|███████▌  | 6184/8192 [03:15<01:15, 26.73it/s, est. speed input: 32343.70 toks/s, output: 31.59 toks/s]
Processed prompts:  76%|███████▋  | 6248/8192 [03:18<01:13, 26.42it/s, est. speed input: 32268.41 toks/s, output: 31.51 toks/s]
Processed prompts:  77%|███████▋  | 6312/8192 [03:20<01:10, 26.59it/s, est. speed input: 32213.72 toks/s, output: 31.46 toks/s]
Processed prompts:  78%|███████▊  | 6376/8192 [03:21<00:53, 33.77it/s, est. speed input: 32427.13 toks/s, output: 31.67 toks/s]
Processed prompts:  79%|███████▊  | 6440/8192 [03:23<00:56, 30.86it/s, est. speed input: 32352.33 toks/s, output: 31.59 toks/s]
Processed prompts:  79%|███████▉  | 6504/8192 [03:26<00:57, 29.15it/s, est. speed input: 32281.07 toks/s, output: 31.52 toks/s]
Processed prompts:  80%|████████  | 6568/8192 [03:28<00:56, 29.00it/s, est. speed input: 32249.56 toks/s, output: 31.49 toks/s]
Processed prompts:  81%|████████  | 6632/8192 [03:30<00:54, 28.85it/s, est. speed input: 32217.25 toks/s, output: 31.46 toks/s]
Processed prompts:  82%|████████▏ | 6696/8192 [03:33<00:53, 27.81it/s, est. speed input: 32147.37 toks/s, output: 31.39 toks/s]
Processed prompts:  83%|████████▎ | 6760/8192 [03:35<00:52, 27.15it/s, est. speed input: 32080.63 toks/s, output: 31.33 toks/s]
Processed prompts:  83%|████████▎ | 6824/8192 [03:38<00:50, 27.16it/s, est. speed input: 32034.96 toks/s, output: 31.28 toks/s]
Processed prompts:  84%|████████▍ | 6888/8192 [03:40<00:48, 26.74it/s, est. speed input: 31971.81 toks/s, output: 31.22 toks/s]
Processed prompts:  85%|████████▍ | 6952/8192 [03:43<00:46, 26.42it/s, est. speed input: 31908.57 toks/s, output: 31.16 toks/s]
Processed prompts:  86%|████████▌ | 7016/8192 [03:45<00:44, 26.71it/s, est. speed input: 31868.78 toks/s, output: 31.12 toks/s]
Processed prompts:  86%|████████▋ | 7080/8192 [03:47<00:41, 26.65it/s, est. speed input: 31818.70 toks/s, output: 31.07 toks/s]
Processed prompts:  87%|████████▋ | 7144/8192 [03:50<00:39, 26.73it/s, est. speed input: 31774.80 toks/s, output: 31.03 toks/s]
Processed prompts:  88%|████████▊ | 7208/8192 [03:52<00:36, 26.78it/s, est. speed input: 31731.38 toks/s, output: 30.99 toks/s]
Processed prompts:  89%|████████▉ | 7272/8192 [03:53<00:26, 35.05it/s, est. speed input: 31943.15 toks/s, output: 31.19 toks/s]
Processed prompts:  90%|████████▉ | 7336/8192 [03:55<00:26, 32.07it/s, est. speed input: 31896.97 toks/s, output: 31.15 toks/s]
Processed prompts:  90%|█████████ | 7400/8192 [03:57<00:26, 29.86it/s, est. speed input: 31839.11 toks/s, output: 31.09 toks/s]
Processed prompts:  91%|█████████ | 7464/8192 [04:00<00:25, 28.71it/s, est. speed input: 31789.85 toks/s, output: 31.04 toks/s]
Processed prompts:  92%|█████████▏| 7528/8192 [04:02<00:23, 28.33it/s, est. speed input: 31754.72 toks/s, output: 31.01 toks/s]
Processed prompts:  93%|█████████▎| 7592/8192 [04:05<00:21, 27.48it/s, est. speed input: 31699.52 toks/s, output: 30.96 toks/s]
Processed prompts:  93%|█████████▎| 7656/8192 [04:07<00:19, 26.94it/s, est. speed input: 31646.04 toks/s, output: 30.90 toks/s]
Processed prompts:  94%|█████████▍| 7720/8192 [04:10<00:17, 27.19it/s, est. speed input: 31616.76 toks/s, output: 30.88 toks/s]
Processed prompts:  95%|█████████▌| 7784/8192 [04:12<00:15, 26.85it/s, est. speed input: 31569.26 toks/s, output: 30.83 toks/s]
Processed prompts:  96%|█████████▌| 7848/8192 [04:14<00:12, 26.47it/s, est. speed input: 31517.02 toks/s, output: 30.78 toks/s]
Processed prompts:  97%|█████████▋| 7912/8192 [04:17<00:10, 26.66it/s, est. speed input: 31482.44 toks/s, output: 30.74 toks/s]
Processed prompts:  97%|█████████▋| 7976/8192 [04:19<00:08, 26.72it/s, est. speed input: 31445.98 toks/s, output: 30.71 toks/s]
Processed prompts:  98%|█████████▊| 8040/8192 [04:22<00:05, 26.39it/s, est. speed input: 31396.69 toks/s, output: 30.66 toks/s]
Processed prompts:  99%|█████████▉| 8104/8192 [04:22<00:02, 33.67it/s, est. speed input: 31565.13 toks/s, output: 30.83 toks/s]
Processed prompts: 100%|█████████▉| 8168/8192 [04:23<00:00, 45.47it/s, est. speed input: 31783.32 toks/s, output: 31.04 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [04:23<00:00, 45.47it/s, est. speed input: 31876.66 toks/s, output: 31.13 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [04:23<00:00, 31.13it/s, est. speed input: 31876.66 toks/s, output: 31.13 toks/s]
[rank0]:[W125 19:25:04.793488469 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-25 22:31:49
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:31:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:31:59 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=485042) WARNING 01-25 22:32:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=485042) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=485042) WARNING 01-25 22:32:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 14.88 requests/s, 7634.71 total tokens/s, 14.88 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 22:31:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:31:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:31:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:31:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:31:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:31:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:31:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:31:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:31:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:31:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:31:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:31:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:31:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:31:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:32:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:32:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:32:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:32:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:32:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:32:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:32:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:32:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:32:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:32:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:32:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:32:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:32:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:32:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:08] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:08] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:08] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:08] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:08] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=485042) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=485042) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.88s/it]
(EngineCore_DP0 pid=485042) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.88s/it]
(EngineCore_DP0 pid=485042) 
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=485042) [2026-01-25 22:32:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=485042) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.96it/s]
(EngineCore_DP0 pid=485042) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  38%|███▊      | 49/128 [00:00<00:00, 480.48it/s]
Adding requests:  78%|███████▊  | 100/128 [00:00<00:00, 496.01it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 499.33it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:16,  7.77it/s, est. speed input: 3980.34 toks/s, output: 7.77 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:12, 10.34it/s, est. speed input: 5126.12 toks/s, output: 10.01 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:10, 12.14it/s, est. speed input: 5852.48 toks/s, output: 11.43 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:09, 12.92it/s, est. speed input: 6195.35 toks/s, output: 12.10 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 13.46it/s, est. speed input: 6427.25 toks/s, output: 12.55 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:08, 13.83it/s, est. speed input: 6594.70 toks/s, output: 12.88 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:08, 14.07it/s, est. speed input: 6713.37 toks/s, output: 13.11 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 14.19it/s, est. speed input: 6797.28 toks/s, output: 13.28 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 14.34it/s, est. speed input: 6875.49 toks/s, output: 13.43 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:07, 14.56it/s, est. speed input: 6954.50 toks/s, output: 13.58 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:07, 14.91it/s, est. speed input: 7048.17 toks/s, output: 13.77 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 15.33it/s, est. speed input: 7147.48 toks/s, output: 13.96 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 15.64it/s, est. speed input: 7233.00 toks/s, output: 14.13 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 15.83it/s, est. speed input: 7304.45 toks/s, output: 14.27 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:06, 16.05it/s, est. speed input: 7376.24 toks/s, output: 14.41 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 16.15it/s, est. speed input: 7434.00 toks/s, output: 14.52 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 16.30it/s, est. speed input: 7492.01 toks/s, output: 14.63 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.32it/s, est. speed input: 7537.54 toks/s, output: 14.72 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.32it/s, est. speed input: 7578.02 toks/s, output: 14.80 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.22it/s, est. speed input: 7606.61 toks/s, output: 14.86 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.12it/s, est. speed input: 7631.07 toks/s, output: 14.90 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.14it/s, est. speed input: 7659.60 toks/s, output: 14.96 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:03<00:05, 16.07it/s, est. speed input: 7679.77 toks/s, output: 15.00 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:05, 16.01it/s, est. speed input: 7697.93 toks/s, output: 15.03 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:03<00:00, 85.41it/s, est. speed input: 12051.46 toks/s, output: 23.54 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:01, 37.56it/s, est. speed input: 11447.04 toks/s, output: 22.36 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:04<00:01, 27.97it/s, est. speed input: 11071.19 toks/s, output: 21.62 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:04<00:01, 23.81it/s, est. speed input: 10809.50 toks/s, output: 21.11 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:05<00:01, 21.14it/s, est. speed input: 10588.05 toks/s, output: 20.68 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:05<00:00, 19.55it/s, est. speed input: 10434.77 toks/s, output: 20.38 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:05<00:00, 18.51it/s, est. speed input: 10326.35 toks/s, output: 20.17 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:05<00:00, 17.65it/s, est. speed input: 10229.23 toks/s, output: 19.98 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:05<00:00, 16.89it/s, est. speed input: 10134.84 toks/s, output: 19.79 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:06<00:00, 16.48it/s, est. speed input: 10076.92 toks/s, output: 19.68 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:06<00:00, 16.11it/s, est. speed input: 10022.46 toks/s, output: 19.58 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:06<00:00, 15.84it/s, est. speed input: 9972.68 toks/s, output: 19.48 toks/s] 
Processed prompts:  98%|█████████▊| 126/128 [00:06<00:00, 15.66it/s, est. speed input: 9927.07 toks/s, output: 19.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 15.49it/s, est. speed input: 9881.77 toks/s, output: 19.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 15.49it/s, est. speed input: 9881.77 toks/s, output: 19.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 19.30it/s, est. speed input: 9881.77 toks/s, output: 19.30 toks/s]
[rank0]:[W125 22:32:49.831943853 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 22:32:52
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:33:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:33:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=486188) WARNING 01-25 22:33:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=486188) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=486188) WARNING 01-25 22:33:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 14.29 requests/s, 14643.57 total tokens/s, 14.29 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 22:33:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:33:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:33:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:33:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:33:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:33:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:33:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:33:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:33:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:33:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:33:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:33:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:33:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:33:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:33:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:33:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:33:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:33:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:33:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:33:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:33:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:33:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:33:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:33:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:33:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:33:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:33:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:33:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:11] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=486188) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=486188) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.08s/it]
(EngineCore_DP0 pid=486188) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.08s/it]
(EngineCore_DP0 pid=486188) 
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=486188) [2026-01-25 22:33:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=486188) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]
(EngineCore_DP0 pid=486188) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.82it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 269.23it/s]
Adding requests:  45%|████▍     | 57/128 [00:00<00:00, 286.05it/s]
Adding requests:  69%|██████▉   | 88/128 [00:00<00:00, 295.74it/s]
Adding requests:  93%|█████████▎| 119/128 [00:00<00:00, 300.83it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 295.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:05, 22.03it/s, est. speed input: 22561.95 toks/s, output: 22.03 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:06, 18.14it/s, est. speed input: 19084.81 toks/s, output: 18.64 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:06, 17.49it/s, est. speed input: 18464.55 toks/s, output: 18.03 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:06, 16.92it/s, est. speed input: 17983.22 toks/s, output: 17.56 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:06, 16.62it/s, est. speed input: 17705.71 toks/s, output: 17.29 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 16.47it/s, est. speed input: 17527.98 toks/s, output: 17.12 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 16.35it/s, est. speed input: 17388.89 toks/s, output: 16.98 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:06, 16.21it/s, est. speed input: 17261.49 toks/s, output: 16.86 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 16.00it/s, est. speed input: 17117.23 toks/s, output: 16.72 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 15.95it/s, est. speed input: 17030.21 toks/s, output: 16.63 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 15.94it/s, est. speed input: 16965.33 toks/s, output: 16.57 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:06, 15.88it/s, est. speed input: 16898.98 toks/s, output: 16.50 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:06, 15.86it/s, est. speed input: 16845.12 toks/s, output: 16.45 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:06, 15.88it/s, est. speed input: 16807.97 toks/s, output: 16.41 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:06, 15.87it/s, est. speed input: 16771.48 toks/s, output: 16.38 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:02<00:05, 15.89it/s, est. speed input: 16743.29 toks/s, output: 16.35 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:05, 15.81it/s, est. speed input: 16700.35 toks/s, output: 16.31 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 15.61it/s, est. speed input: 16634.93 toks/s, output: 16.24 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 15.33it/s, est. speed input: 16548.81 toks/s, output: 16.16 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 15.11it/s, est. speed input: 16465.62 toks/s, output: 16.08 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:05, 14.90it/s, est. speed input: 16380.22 toks/s, output: 16.00 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:05, 14.86it/s, est. speed input: 16321.83 toks/s, output: 15.94 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:03<00:05, 14.75it/s, est. speed input: 16254.09 toks/s, output: 15.87 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:03<00:05, 14.67it/s, est. speed input: 16192.88 toks/s, output: 15.81 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:05, 14.62it/s, est. speed input: 16136.38 toks/s, output: 15.76 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:05, 14.60it/s, est. speed input: 16086.55 toks/s, output: 15.71 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 14.63it/s, est. speed input: 16047.48 toks/s, output: 15.67 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 14.62it/s, est. speed input: 16006.94 toks/s, output: 15.63 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:04, 14.56it/s, est. speed input: 15962.43 toks/s, output: 15.59 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:04, 14.57it/s, est. speed input: 15927.34 toks/s, output: 15.55 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:04<00:04, 14.55it/s, est. speed input: 15891.80 toks/s, output: 15.52 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:04<00:04, 14.56it/s, est. speed input: 15860.08 toks/s, output: 15.49 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:04, 14.51it/s, est. speed input: 15825.08 toks/s, output: 15.45 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 14.57it/s, est. speed input: 15802.17 toks/s, output: 15.43 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 14.57it/s, est. speed input: 15775.94 toks/s, output: 15.41 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 14.55it/s, est. speed input: 15749.98 toks/s, output: 15.38 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 14.54it/s, est. speed input: 15725.30 toks/s, output: 15.36 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:05<00:03, 14.55it/s, est. speed input: 15703.39 toks/s, output: 15.34 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:05<00:03, 14.54it/s, est. speed input: 15681.82 toks/s, output: 15.31 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:05<00:03, 14.49it/s, est. speed input: 15656.37 toks/s, output: 15.29 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:05<00:03, 14.42it/s, est. speed input: 15629.70 toks/s, output: 15.26 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 14.39it/s, est. speed input: 15606.13 toks/s, output: 15.24 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 14.46it/s, est. speed input: 15591.23 toks/s, output: 15.23 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 14.49it/s, est. speed input: 15575.10 toks/s, output: 15.21 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:06<00:02, 14.52it/s, est. speed input: 15560.77 toks/s, output: 15.20 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:06<00:02, 14.54it/s, est. speed input: 15546.98 toks/s, output: 15.18 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:06<00:02, 14.49it/s, est. speed input: 15529.00 toks/s, output: 15.16 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:06<00:02, 14.48it/s, est. speed input: 15513.56 toks/s, output: 15.15 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:06<00:00, 62.89it/s, est. speed input: 19031.15 toks/s, output: 18.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 62.89it/s, est. speed input: 18822.73 toks/s, output: 18.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 18.38it/s, est. speed input: 18822.73 toks/s, output: 18.38 toks/s]
[rank0]:[W125 22:33:47.730430084 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 22:33:50
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:34:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:34:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=487230) WARNING 01-25 22:34:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=487230) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=487230) WARNING 01-25 22:34:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 29.61 requests/s, 30350.90 total tokens/s, 29.61 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 22:34:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:34:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:34:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:34:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:34:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:34:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:34:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:34:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:34:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:34:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:34:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:34:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:34:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:34:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:34:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:34:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:34:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:34:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:34:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:34:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:34:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:34:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:34:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:34:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:34:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:34:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:34:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:34:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:11] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=487230) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=487230) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.02it/s]
(EngineCore_DP0 pid=487230) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.02it/s]
(EngineCore_DP0 pid=487230) 
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=487230) [2026-01-25 22:34:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=487230) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.05it/s]
(EngineCore_DP0 pid=487230) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.53it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.34it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  10%|▉         | 25/256 [00:00<00:00, 242.63it/s]
Adding requests:  21%|██        | 54/256 [00:00<00:00, 270.03it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 280.94it/s]
Adding requests:  44%|████▍     | 113/256 [00:00<00:00, 283.35it/s]
Adding requests:  55%|█████▌    | 142/256 [00:00<00:00, 284.05it/s]
Adding requests:  67%|██████▋   | 171/256 [00:00<00:00, 284.20it/s]
Adding requests:  78%|███████▊  | 200/256 [00:00<00:00, 275.93it/s]
Adding requests:  90%|████████▉ | 230/256 [00:00<00:00, 281.96it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 278.92it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:00<00:01, 185.96it/s, est. speed input: 190475.32 toks/s, output: 185.98 toks/s]
Processed prompts:  16%|█▌        | 41/256 [00:00<00:04, 50.86it/s, est. speed input: 58982.16 toks/s, output: 57.60 toks/s]   
Processed prompts:  20%|█▉        | 51/256 [00:01<00:04, 42.50it/s, est. speed input: 50203.53 toks/s, output: 49.03 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:01<00:05, 37.29it/s, est. speed input: 45463.10 toks/s, output: 44.40 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:05, 35.24it/s, est. speed input: 43394.86 toks/s, output: 42.38 toks/s]
Processed prompts:  27%|██▋       | 69/256 [00:01<00:05, 35.77it/s, est. speed input: 43028.57 toks/s, output: 42.02 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:05, 32.67it/s, est. speed input: 41177.92 toks/s, output: 40.21 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:05, 32.08it/s, est. speed input: 40469.57 toks/s, output: 39.52 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:02<00:05, 31.65it/s, est. speed input: 39877.44 toks/s, output: 38.94 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:02<00:05, 31.23it/s, est. speed input: 39332.46 toks/s, output: 38.41 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:05, 30.97it/s, est. speed input: 38866.93 toks/s, output: 37.96 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:05, 30.76it/s, est. speed input: 38446.96 toks/s, output: 37.55 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:05, 30.69it/s, est. speed input: 38087.93 toks/s, output: 37.19 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:05, 30.50it/s, est. speed input: 37735.25 toks/s, output: 36.85 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 30.42it/s, est. speed input: 37424.92 toks/s, output: 36.55 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:03<00:04, 31.01it/s, est. speed input: 37258.10 toks/s, output: 36.38 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:03<00:04, 31.29it/s, est. speed input: 37079.85 toks/s, output: 36.21 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:03<00:04, 31.44it/s, est. speed input: 36905.21 toks/s, output: 36.04 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:03<00:04, 31.69it/s, est. speed input: 36765.58 toks/s, output: 35.90 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:04, 31.85it/s, est. speed input: 36633.80 toks/s, output: 35.77 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 31.85it/s, est. speed input: 36494.52 toks/s, output: 35.64 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 31.89it/s, est. speed input: 36370.81 toks/s, output: 35.52 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 31.94it/s, est. speed input: 36257.26 toks/s, output: 35.41 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:04<00:03, 31.84it/s, est. speed input: 36134.73 toks/s, output: 35.29 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:04<00:03, 31.80it/s, est. speed input: 36023.62 toks/s, output: 35.18 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:04<00:03, 31.82it/s, est. speed input: 35924.09 toks/s, output: 35.08 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:04<00:03, 31.85it/s, est. speed input: 35831.88 toks/s, output: 34.99 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:04<00:03, 31.93it/s, est. speed input: 35750.78 toks/s, output: 34.91 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:04<00:02, 31.84it/s, est. speed input: 35659.69 toks/s, output: 34.82 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 31.88it/s, est. speed input: 35583.07 toks/s, output: 34.75 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 31.93it/s, est. speed input: 35512.39 toks/s, output: 34.68 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:05<00:02, 31.93it/s, est. speed input: 35442.42 toks/s, output: 34.61 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:05<00:02, 31.79it/s, est. speed input: 35362.87 toks/s, output: 34.53 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:05<00:02, 31.79it/s, est. speed input: 35295.94 toks/s, output: 34.47 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:05<00:02, 31.98it/s, est. speed input: 35248.42 toks/s, output: 34.42 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:05<00:02, 32.13it/s, est. speed input: 35204.19 toks/s, output: 34.38 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:05<00:01, 32.18it/s, est. speed input: 35157.14 toks/s, output: 34.33 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:05<00:01, 32.31it/s, est. speed input: 35120.32 toks/s, output: 34.30 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:05<00:01, 32.37it/s, est. speed input: 35081.98 toks/s, output: 34.26 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:06<00:01, 31.47it/s, est. speed input: 34974.06 toks/s, output: 34.15 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:06<00:01, 30.95it/s, est. speed input: 34877.15 toks/s, output: 34.06 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:06<00:01, 30.44it/s, est. speed input: 34771.48 toks/s, output: 33.96 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:06<00:01, 30.21it/s, est. speed input: 34679.83 toks/s, output: 33.87 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:06<00:01, 30.17it/s, est. speed input: 34601.55 toks/s, output: 33.79 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:06<00:00, 30.05it/s, est. speed input: 34518.95 toks/s, output: 33.71 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:06<00:00, 29.84it/s, est. speed input: 34430.32 toks/s, output: 33.62 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:06<00:00, 29.74it/s, est. speed input: 34348.70 toks/s, output: 33.54 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:07<00:00, 29.71it/s, est. speed input: 34272.65 toks/s, output: 33.47 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:07<00:00, 29.55it/s, est. speed input: 34189.68 toks/s, output: 33.39 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:07<00:00, 29.40it/s, est. speed input: 34106.62 toks/s, output: 33.31 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:07<00:00, 29.43it/s, est. speed input: 34036.62 toks/s, output: 33.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 29.43it/s, est. speed input: 43245.28 toks/s, output: 42.23 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 42.23it/s, est. speed input: 43245.28 toks/s, output: 42.23 toks/s]
[rank0]:[W125 22:34:47.902661470 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 22:34:50
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:35:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:35:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=488309) WARNING 01-25 22:35:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=488309) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=488309) WARNING 01-25 22:35:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.18 requests/s, 39133.04 total tokens/s, 38.18 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 22:35:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:35:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:35:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:35:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:35:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:35:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:35:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:35:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:35:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:35:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:35:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:35:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:35:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:35:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:35:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:35:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:35:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:35:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:35:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:35:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:35:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:35:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:35:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:35:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:35:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:35:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:35:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:35:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:12] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=488309) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=488309) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.11s/it]
(EngineCore_DP0 pid=488309) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.11s/it]
(EngineCore_DP0 pid=488309) 
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=488309) [2026-01-25 22:35:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=488309) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  3.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  2.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  3.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:01<00:00,  2.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:01<00:00,  2.69it/s]
(EngineCore_DP0 pid=488309) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  5.42it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  7.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  7.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  7.39it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 276.37it/s]
Adding requests:  12%|█▏        | 61/512 [00:00<00:01, 302.90it/s]
Adding requests:  18%|█▊        | 92/512 [00:00<00:01, 294.19it/s]
Adding requests:  24%|██▍       | 123/512 [00:00<00:01, 298.97it/s]
Adding requests:  30%|██▉       | 153/512 [00:00<00:01, 287.84it/s]
Adding requests:  36%|███▌      | 183/512 [00:00<00:01, 290.07it/s]
Adding requests:  42%|████▏     | 213/512 [00:00<00:01, 291.63it/s]
Adding requests:  48%|████▊     | 245/512 [00:00<00:00, 298.55it/s]
Adding requests:  54%|█████▎    | 275/512 [00:00<00:00, 297.51it/s]
Adding requests:  60%|█████▉    | 305/512 [00:01<00:00, 291.16it/s]
Adding requests:  65%|██████▌   | 335/512 [00:01<00:00, 291.19it/s]
Adding requests:  71%|███████▏  | 365/512 [00:01<00:00, 293.03it/s]
Adding requests:  77%|███████▋  | 395/512 [00:01<00:00, 285.70it/s]
Adding requests:  83%|████████▎ | 424/512 [00:01<00:00, 284.41it/s]
Adding requests:  88%|████████▊ | 453/512 [00:01<00:00, 280.42it/s]
Adding requests:  94%|█████████▍| 482/512 [00:01<00:00, 282.68it/s]
Adding requests: 100%|█████████▉| 511/512 [00:01<00:00, 283.01it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 288.96it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:00<00:00, 476.12it/s, est. speed input: 487667.61 toks/s, output: 476.16 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:01<00:06, 66.87it/s, est. speed input: 80123.46 toks/s, output: 78.24 toks/s]  
Processed prompts:  26%|██▌       | 132/512 [00:01<00:06, 57.77it/s, est. speed input: 69768.13 toks/s, output: 68.13 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:02<00:07, 50.51it/s, est. speed input: 63233.52 toks/s, output: 61.75 toks/s]
Processed prompts:  30%|███       | 156/512 [00:02<00:07, 49.93it/s, est. speed input: 61996.57 toks/s, output: 60.54 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:02<00:07, 47.48it/s, est. speed input: 60196.84 toks/s, output: 58.79 toks/s]
Processed prompts:  33%|███▎      | 171/512 [00:03<00:07, 44.30it/s, est. speed input: 58326.47 toks/s, output: 56.96 toks/s]
Processed prompts:  35%|███▍      | 177/512 [00:03<00:07, 45.96it/s, est. speed input: 58312.23 toks/s, output: 56.95 toks/s]
Processed prompts:  36%|███▌      | 183/512 [00:03<00:08, 41.02it/s, est. speed input: 56431.73 toks/s, output: 55.11 toks/s]
Processed prompts:  37%|███▋      | 188/512 [00:03<00:07, 42.04it/s, est. speed input: 56180.86 toks/s, output: 54.86 toks/s]
Processed prompts:  38%|███▊      | 193/512 [00:03<00:07, 43.04it/s, est. speed input: 55948.37 toks/s, output: 54.64 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:03<00:08, 36.11it/s, est. speed input: 54136.24 toks/s, output: 52.87 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:03<00:08, 36.41it/s, est. speed input: 53704.74 toks/s, output: 52.45 toks/s]
Processed prompts:  40%|████      | 206/512 [00:03<00:08, 36.63it/s, est. speed input: 53288.04 toks/s, output: 52.04 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:07, 38.14it/s, est. speed input: 52737.69 toks/s, output: 51.50 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:07, 39.04it/s, est. speed input: 52240.66 toks/s, output: 51.02 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:07, 39.58it/s, est. speed input: 51782.96 toks/s, output: 50.57 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:04<00:06, 39.94it/s, est. speed input: 51363.50 toks/s, output: 50.16 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:04<00:06, 40.12it/s, est. speed input: 50968.36 toks/s, output: 49.77 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 40.27it/s, est. speed input: 50609.27 toks/s, output: 49.42 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:06, 40.40it/s, est. speed input: 50279.60 toks/s, output: 49.10 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:05, 40.44it/s, est. speed input: 49966.08 toks/s, output: 48.79 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:05<00:05, 40.57it/s, est. speed input: 49688.94 toks/s, output: 48.52 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:05<00:05, 40.60it/s, est. speed input: 49421.81 toks/s, output: 48.26 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 40.62it/s, est. speed input: 49172.27 toks/s, output: 48.02 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:06<00:00, 150.31it/s, est. speed input: 60186.79 toks/s, output: 58.78 toks/s]
Processed prompts:  76%|███████▌  | 387/512 [00:06<00:01, 99.61it/s, est. speed input: 59237.09 toks/s, output: 57.85 toks/s] 
Processed prompts:  78%|███████▊  | 400/512 [00:06<00:01, 80.85it/s, est. speed input: 58629.16 toks/s, output: 57.25 toks/s]
Processed prompts:  80%|████████  | 411/512 [00:07<00:01, 66.45it/s, est. speed input: 57767.55 toks/s, output: 56.41 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [00:07<00:01, 60.45it/s, est. speed input: 57359.55 toks/s, output: 56.02 toks/s]
Processed prompts:  83%|████████▎ | 427/512 [00:07<00:01, 53.27it/s, est. speed input: 56712.29 toks/s, output: 55.38 toks/s]
Processed prompts:  85%|████████▍ | 433/512 [00:07<00:01, 53.73it/s, est. speed input: 56723.30 toks/s, output: 55.39 toks/s]
Processed prompts:  86%|████████▌ | 439/512 [00:08<00:01, 45.74it/s, est. speed input: 55980.57 toks/s, output: 54.67 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [00:08<00:01, 45.96it/s, est. speed input: 55876.87 toks/s, output: 54.57 toks/s]
Processed prompts:  88%|████████▊ | 449/512 [00:08<00:01, 46.18it/s, est. speed input: 55777.06 toks/s, output: 54.47 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:08<00:01, 37.61it/s, est. speed input: 54963.28 toks/s, output: 53.67 toks/s]
Processed prompts:  90%|████████▉ | 459/512 [00:08<00:01, 39.53it/s, est. speed input: 54874.44 toks/s, output: 53.59 toks/s]
Processed prompts:  91%|█████████ | 464/512 [00:08<00:01, 41.23it/s, est. speed input: 54791.05 toks/s, output: 53.51 toks/s]
Processed prompts:  92%|█████████▏| 469/512 [00:08<00:01, 42.55it/s, est. speed input: 54703.57 toks/s, output: 53.42 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:08<00:01, 34.58it/s, est. speed input: 53969.65 toks/s, output: 52.70 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:09<00:00, 35.22it/s, est. speed input: 53786.41 toks/s, output: 52.52 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:09<00:00, 35.78it/s, est. speed input: 53606.47 toks/s, output: 52.35 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:09<00:00, 36.20it/s, est. speed input: 53431.31 toks/s, output: 52.18 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:09<00:00, 36.51it/s, est. speed input: 53258.52 toks/s, output: 52.01 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:09<00:00, 36.71it/s, est. speed input: 53087.62 toks/s, output: 51.84 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:09<00:00, 36.90it/s, est. speed input: 52923.19 toks/s, output: 51.68 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:09<00:00, 37.05it/s, est. speed input: 52762.91 toks/s, output: 51.53 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:09<00:00, 37.19it/s, est. speed input: 52607.57 toks/s, output: 51.37 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 37.19it/s, est. speed input: 52766.43 toks/s, output: 51.53 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 51.53it/s, est. speed input: 52766.43 toks/s, output: 51.53 toks/s]
[rank0]:[W125 22:35:54.059842527 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 22:35:57
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:36:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:36:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=489513) WARNING 01-25 22:36:21 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=489513) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=489513) WARNING 01-25 22:36:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.39 requests/s, 38326.49 total tokens/s, 37.39 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-25 22:36:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:36:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:36:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:36:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:36:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:36:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:36:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:36:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:36:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:36:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:36:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:36:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:36:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:36:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:36:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:36:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:36:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:36:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:36:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:36:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:36:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:36:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:36:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:36:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:36:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:36:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:36:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:36:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=489513) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=489513) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.00it/s]
(EngineCore_DP0 pid=489513) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.00it/s]
(EngineCore_DP0 pid=489513) 
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=489513) [2026-01-25 22:36:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=489513) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  3.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  5.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  6.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  6.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  5.21it/s]
(EngineCore_DP0 pid=489513) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  6.80it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  5.72it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  3.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  4.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  4.30it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 27/1024 [00:00<00:03, 263.89it/s]
Adding requests:   5%|▌         | 56/1024 [00:00<00:03, 274.64it/s]
Adding requests:   8%|▊         | 84/1024 [00:00<00:03, 276.48it/s]
Adding requests:  11%|█         | 113/1024 [00:00<00:03, 280.55it/s]
Adding requests:  14%|█▍        | 142/1024 [00:00<00:03, 280.16it/s]
Adding requests:  17%|█▋        | 171/1024 [00:00<00:03, 277.71it/s]
Adding requests:  20%|█▉        | 200/1024 [00:00<00:02, 278.81it/s]
Adding requests:  22%|██▏       | 230/1024 [00:00<00:02, 284.79it/s]
Adding requests:  25%|██▌       | 261/1024 [00:00<00:02, 292.52it/s]
Adding requests:  28%|██▊       | 291/1024 [00:01<00:02, 290.70it/s]
Adding requests:  31%|███▏      | 322/1024 [00:01<00:02, 294.52it/s]
Adding requests:  34%|███▍      | 353/1024 [00:01<00:02, 297.26it/s]
Adding requests:  38%|███▊      | 384/1024 [00:01<00:02, 299.10it/s]
Adding requests:  40%|████      | 414/1024 [00:01<00:02, 298.70it/s]
Adding requests:  43%|████▎     | 445/1024 [00:01<00:01, 299.71it/s]
Adding requests:  46%|████▋     | 476/1024 [00:01<00:01, 300.98it/s]
Adding requests:  50%|████▉     | 507/1024 [00:01<00:01, 292.56it/s]
Adding requests:  52%|█████▏    | 537/1024 [00:01<00:01, 283.07it/s]
Adding requests:  55%|█████▌    | 567/1024 [00:01<00:01, 286.34it/s]
Adding requests:  58%|█████▊    | 597/1024 [00:02<00:01, 288.98it/s]
Adding requests:  61%|██████▏   | 628/1024 [00:02<00:01, 293.57it/s]
Adding requests:  64%|██████▍   | 659/1024 [00:02<00:01, 296.67it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:02<00:01, 304.91it/s]
Adding requests:  71%|███████   | 724/1024 [00:02<00:00, 307.17it/s]
Adding requests:  74%|███████▍  | 756/1024 [00:02<00:00, 308.12it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:02<00:00, 311.05it/s]
Adding requests:  80%|████████  | 820/1024 [00:02<00:00, 302.45it/s]
Adding requests:  83%|████████▎ | 851/1024 [00:02<00:00, 304.57it/s]
Adding requests:  86%|████████▌ | 883/1024 [00:02<00:00, 307.50it/s]
Adding requests:  89%|████████▉ | 916/1024 [00:03<00:00, 311.89it/s]
Adding requests:  93%|█████████▎| 948/1024 [00:03<00:00, 308.66it/s]
Adding requests:  96%|█████████▌| 979/1024 [00:03<00:00, 301.34it/s]
Adding requests:  99%|█████████▊| 1010/1024 [00:03<00:00, 298.69it/s]
Adding requests: 100%|██████████| 1024/1024 [00:03<00:00, 295.34it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:00<00:00, 1174.62it/s, est. speed input: 1203196.39 toks/s, output: 1174.73 toks/s]
Processed prompts:  23%|██▎       | 240/1024 [00:03<00:12, 65.13it/s, est. speed input: 77913.68 toks/s, output: 76.09 toks/s]      
Processed prompts:  28%|██▊       | 291/1024 [00:04<00:14, 52.07it/s, est. speed input: 63643.89 toks/s, output: 62.15 toks/s]
Processed prompts:  31%|███▏      | 321/1024 [00:05<00:13, 50.81it/s, est. speed input: 61589.35 toks/s, output: 60.15 toks/s]
Processed prompts:  33%|███▎      | 341/1024 [00:05<00:14, 46.82it/s, est. speed input: 58603.32 toks/s, output: 57.23 toks/s]
Processed prompts:  35%|███▍      | 355/1024 [00:06<00:14, 44.81it/s, est. speed input: 57142.98 toks/s, output: 55.80 toks/s]
Processed prompts:  36%|███▌      | 366/1024 [00:06<00:14, 45.84it/s, est. speed input: 57103.49 toks/s, output: 55.77 toks/s]
Processed prompts:  37%|███▋      | 375/1024 [00:06<00:14, 45.68it/s, est. speed input: 56762.34 toks/s, output: 55.43 toks/s]
Processed prompts:  37%|███▋      | 383/1024 [00:06<00:14, 44.72it/s, est. speed input: 56293.47 toks/s, output: 54.97 toks/s]
Processed prompts:  38%|███▊      | 390/1024 [00:07<00:14, 42.88it/s, est. speed input: 55710.54 toks/s, output: 54.40 toks/s]
Processed prompts:  39%|███▊      | 396/1024 [00:07<00:15, 40.14it/s, est. speed input: 55018.69 toks/s, output: 53.73 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:07<00:16, 37.76it/s, est. speed input: 54365.26 toks/s, output: 53.09 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:16, 38.21it/s, est. speed input: 54005.56 toks/s, output: 52.74 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:15, 38.40it/s, est. speed input: 53641.84 toks/s, output: 52.38 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:08<00:15, 37.92it/s, est. speed input: 53215.11 toks/s, output: 51.97 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:08<00:15, 37.57it/s, est. speed input: 52810.54 toks/s, output: 51.57 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:08<00:15, 37.30it/s, est. speed input: 52424.12 toks/s, output: 51.20 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:15, 37.08it/s, est. speed input: 52053.05 toks/s, output: 50.83 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:09<00:15, 36.93it/s, est. speed input: 51701.91 toks/s, output: 50.49 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:09<00:15, 36.84it/s, est. speed input: 51367.56 toks/s, output: 50.16 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:09<00:14, 36.76it/s, est. speed input: 51047.35 toks/s, output: 49.85 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:09<00:14, 36.69it/s, est. speed input: 50740.12 toks/s, output: 49.55 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:14, 36.65it/s, est. speed input: 50448.10 toks/s, output: 49.27 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:10<00:14, 36.63it/s, est. speed input: 50168.24 toks/s, output: 48.99 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:10<00:14, 36.58it/s, est. speed input: 49897.36 toks/s, output: 48.73 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:10<00:13, 36.57it/s, est. speed input: 49639.43 toks/s, output: 48.48 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:10<00:13, 36.57it/s, est. speed input: 49393.02 toks/s, output: 48.24 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:11<00:13, 36.54it/s, est. speed input: 49153.26 toks/s, output: 48.00 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:11<00:13, 36.54it/s, est. speed input: 48925.70 toks/s, output: 47.78 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:11<00:13, 36.53it/s, est. speed input: 48705.69 toks/s, output: 47.56 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:11<00:12, 36.54it/s, est. speed input: 48493.97 toks/s, output: 47.36 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:11<00:12, 36.56it/s, est. speed input: 48291.99 toks/s, output: 47.16 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:12<00:12, 36.55it/s, est. speed input: 48096.12 toks/s, output: 46.97 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:12<00:12, 36.53it/s, est. speed input: 47905.14 toks/s, output: 46.78 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:12<00:11, 36.52it/s, est. speed input: 47721.83 toks/s, output: 46.60 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:12<00:11, 36.52it/s, est. speed input: 47544.39 toks/s, output: 46.43 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:13<00:11, 36.50it/s, est. speed input: 47371.84 toks/s, output: 46.26 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:13<00:11, 36.60it/s, est. speed input: 47213.72 toks/s, output: 46.11 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:13<00:10, 37.43it/s, est. speed input: 47112.20 toks/s, output: 46.01 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:13<00:10, 38.03it/s, est. speed input: 47013.01 toks/s, output: 45.91 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:13<00:10, 38.45it/s, est. speed input: 46916.67 toks/s, output: 45.82 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:14<00:09, 38.76it/s, est. speed input: 46823.40 toks/s, output: 45.73 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:14<00:09, 38.97it/s, est. speed input: 46732.23 toks/s, output: 45.64 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:14<00:09, 39.13it/s, est. speed input: 46644.09 toks/s, output: 45.55 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:14<00:09, 39.24it/s, est. speed input: 46558.29 toks/s, output: 45.47 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:14<00:08, 39.31it/s, est. speed input: 46474.81 toks/s, output: 45.39 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:15<00:08, 39.35it/s, est. speed input: 46392.71 toks/s, output: 45.31 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:15<00:08, 39.39it/s, est. speed input: 46313.73 toks/s, output: 45.23 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:15<00:08, 39.45it/s, est. speed input: 46238.28 toks/s, output: 45.15 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:15<00:08, 39.47it/s, est. speed input: 46163.53 toks/s, output: 45.08 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:15<00:07, 39.46it/s, est. speed input: 46089.65 toks/s, output: 45.01 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:16<00:07, 39.01it/s, est. speed input: 45995.79 toks/s, output: 44.92 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:16<00:07, 38.21it/s, est. speed input: 45879.03 toks/s, output: 44.80 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:16<00:07, 37.66it/s, est. speed input: 45764.93 toks/s, output: 44.69 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:16<00:07, 37.26it/s, est. speed input: 45653.13 toks/s, output: 44.58 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:16<00:07, 37.00it/s, est. speed input: 45544.57 toks/s, output: 44.48 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:17<00:07, 36.81it/s, est. speed input: 45437.97 toks/s, output: 44.37 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:17<00:06, 36.69it/s, est. speed input: 45335.41 toks/s, output: 44.27 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:17<00:06, 36.61it/s, est. speed input: 45235.00 toks/s, output: 44.17 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:17<00:06, 36.56it/s, est. speed input: 45137.33 toks/s, output: 44.08 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:18<00:06, 36.54it/s, est. speed input: 45043.34 toks/s, output: 43.99 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:18<00:06, 36.51it/s, est. speed input: 44950.41 toks/s, output: 43.90 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:18<00:05, 36.47it/s, est. speed input: 44858.74 toks/s, output: 43.81 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:18<00:05, 36.45it/s, est. speed input: 44769.84 toks/s, output: 43.72 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:18<00:05, 36.44it/s, est. speed input: 44683.07 toks/s, output: 43.64 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:19<00:05, 36.44it/s, est. speed input: 44598.41 toks/s, output: 43.55 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:19<00:04, 36.41it/s, est. speed input: 44514.40 toks/s, output: 43.47 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:19<00:04, 36.41it/s, est. speed input: 44433.60 toks/s, output: 43.39 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:19<00:04, 36.41it/s, est. speed input: 44354.20 toks/s, output: 43.31 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:20<00:04, 36.40it/s, est. speed input: 44275.96 toks/s, output: 43.24 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:20<00:04, 36.38it/s, est. speed input: 44199.22 toks/s, output: 43.16 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:20<00:03, 36.35it/s, est. speed input: 44123.22 toks/s, output: 43.09 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:20<00:03, 36.51it/s, est. speed input: 44056.52 toks/s, output: 43.02 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:20<00:03, 37.34it/s, est. speed input: 44020.74 toks/s, output: 42.99 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:21<00:03, 37.93it/s, est. speed input: 43985.42 toks/s, output: 42.95 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:21<00:02, 38.36it/s, est. speed input: 43950.56 toks/s, output: 42.92 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:21<00:02, 38.67it/s, est. speed input: 43916.55 toks/s, output: 42.89 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:21<00:02, 38.90it/s, est. speed input: 43883.81 toks/s, output: 42.86 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:21<00:02, 39.05it/s, est. speed input: 43850.87 toks/s, output: 42.82 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:22<00:01, 39.14it/s, est. speed input: 43818.44 toks/s, output: 42.79 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:22<00:01, 39.21it/s, est. speed input: 43786.44 toks/s, output: 42.76 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:22<00:01, 39.26it/s, est. speed input: 43755.08 toks/s, output: 42.73 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:22<00:01, 39.30it/s, est. speed input: 43724.55 toks/s, output: 42.70 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:22<00:01, 39.29it/s, est. speed input: 43693.36 toks/s, output: 42.67 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:23<00:00, 39.30it/s, est. speed input: 43663.37 toks/s, output: 42.64 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:23<00:00, 39.30it/s, est. speed input: 43633.49 toks/s, output: 42.61 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:23<00:00, 39.32it/s, est. speed input: 43604.76 toks/s, output: 42.58 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:23<00:00, 39.34it/s, est. speed input: 43576.78 toks/s, output: 42.56 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:23<00:00, 40.59it/s, est. speed input: 43586.71 toks/s, output: 42.57 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:23<00:00, 40.59it/s, est. speed input: 43843.11 toks/s, output: 42.82 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:23<00:00, 42.82it/s, est. speed input: 43843.11 toks/s, output: 42.82 toks/s]
[rank0]:[W125 22:37:20.902948982 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-25 22:37:23
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:37:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:37:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=491018) WARNING 01-25 22:37:53 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=491018) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=491018) WARNING 01-25 22:38:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.95 requests/s, 37873.27 total tokens/s, 36.95 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-25 22:37:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:37:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:37:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:37:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:37:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:37:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:37:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:37:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:37:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:37:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:37:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:37:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:37:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:37:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:37:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:37:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:37:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:37:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:37:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:37:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:37:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:37:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:37:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:37:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:37:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:37:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:37:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:37:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=491018) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=491018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.08s/it]
(EngineCore_DP0 pid=491018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.08s/it]
(EngineCore_DP0 pid=491018) 
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=491018) [2026-01-25 22:37:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=491018) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.92it/s]
(EngineCore_DP0 pid=491018) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.01it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  6.89it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  3.38it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  4.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:01<00:00,  4.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:01<00:00,  4.77it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 27/2048 [00:00<00:07, 265.01it/s]
Adding requests:   3%|▎         | 59/2048 [00:00<00:06, 294.98it/s]
Adding requests:   4%|▍         | 89/2048 [00:00<00:06, 289.73it/s]
Adding requests:   6%|▌         | 118/2048 [00:00<00:07, 252.10it/s]
Adding requests:   7%|▋         | 144/2048 [00:00<00:08, 231.78it/s]
Adding requests:   8%|▊         | 168/2048 [00:00<00:08, 217.06it/s]
Adding requests:   9%|▉         | 191/2048 [00:00<00:08, 210.65it/s]
Adding requests:  34%|███▎      | 690/2048 [00:00<00:00, 1532.00it/s]
Adding requests:  42%|████▏     | 862/2048 [00:01<00:01, 655.23it/s] 
Adding requests:  48%|████▊     | 989/2048 [00:01<00:02, 489.66it/s]
Adding requests:  53%|█████▎    | 1085/2048 [00:02<00:02, 421.88it/s]
Adding requests:  57%|█████▋    | 1161/2048 [00:02<00:02, 386.45it/s]
Adding requests:  60%|█████▉    | 1222/2048 [00:02<00:02, 362.73it/s]
Adding requests:  62%|██████▏   | 1274/2048 [00:03<00:02, 342.83it/s]
Adding requests:  64%|██████▍   | 1318/2048 [00:03<00:02, 330.47it/s]
Adding requests:  66%|██████▋   | 1358/2048 [00:03<00:02, 314.60it/s]
Adding requests:  68%|██████▊   | 1394/2048 [00:03<00:02, 306.91it/s]
Adding requests:  70%|██████▉   | 1428/2048 [00:03<00:02, 303.55it/s]
Adding requests:  71%|███████▏  | 1460/2048 [00:03<00:01, 300.24it/s]
Adding requests:  73%|███████▎  | 1491/2048 [00:03<00:01, 301.63it/s]
Adding requests:  74%|███████▍  | 1522/2048 [00:03<00:01, 303.00it/s]
Adding requests:  76%|███████▌  | 1553/2048 [00:03<00:01, 303.29it/s]
Adding requests:  77%|███████▋  | 1584/2048 [00:04<00:01, 303.08it/s]
Adding requests:  79%|███████▉  | 1615/2048 [00:04<00:01, 304.46it/s]
Adding requests:  80%|████████  | 1646/2048 [00:04<00:01, 303.66it/s]
Adding requests:  82%|████████▏ | 1677/2048 [00:04<00:01, 303.65it/s]
Adding requests:  83%|████████▎ | 1708/2048 [00:04<00:01, 296.77it/s]
Adding requests:  85%|████████▍ | 1738/2048 [00:04<00:01, 297.64it/s]
Adding requests:  86%|████████▋ | 1771/2048 [00:04<00:00, 304.37it/s]
Adding requests:  88%|████████▊ | 1803/2048 [00:04<00:00, 307.61it/s]
Adding requests:  90%|████████▉ | 1834/2048 [00:04<00:00, 300.19it/s]
Adding requests:  91%|█████████ | 1865/2048 [00:05<00:00, 300.95it/s]
Adding requests:  93%|█████████▎| 1898/2048 [00:05<00:00, 307.16it/s]
Adding requests:  94%|█████████▍| 1929/2048 [00:05<00:00, 306.93it/s]
Adding requests:  96%|█████████▌| 1960/2048 [00:05<00:00, 303.69it/s]
Adding requests:  97%|█████████▋| 1991/2048 [00:05<00:00, 304.69it/s]
Adding requests:  99%|█████████▊| 2022/2048 [00:05<00:00, 304.99it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 365.62it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:00<00:03, 536.17it/s, est. speed input: 549075.61 toks/s, output: 536.18 toks/s]
Processed prompts:  15%|█▌        | 312/2048 [00:01<00:11, 151.00it/s, est. speed input: 188152.55 toks/s, output: 183.74 toks/s]
Processed prompts:  16%|█▋        | 337/2048 [00:02<00:13, 126.69it/s, est. speed input: 164014.42 toks/s, output: 160.17 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:02<00:21, 79.51it/s, est. speed input: 123369.56 toks/s, output: 120.48 toks/s] 
Processed prompts:  18%|█▊        | 370/2048 [00:03<00:24, 68.70it/s, est. speed input: 112157.80 toks/s, output: 109.53 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:03<00:27, 60.17it/s, est. speed input: 103517.93 toks/s, output: 101.09 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:04<00:30, 53.64it/s, est. speed input: 96656.56 toks/s, output: 94.39 toks/s]  
Processed prompts:  20%|██        | 418/2048 [00:04<00:33, 48.78it/s, est. speed input: 91086.54 toks/s, output: 88.95 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:05<00:35, 45.20it/s, est. speed input: 86464.86 toks/s, output: 84.44 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:05<00:37, 42.61it/s, est. speed input: 82566.54 toks/s, output: 80.63 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:06<00:38, 40.75it/s, est. speed input: 79242.20 toks/s, output: 77.38 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:06<00:39, 39.42it/s, est. speed input: 76364.78 toks/s, output: 74.57 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:06<00:40, 38.48it/s, est. speed input: 73856.28 toks/s, output: 72.12 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:07<00:40, 37.64it/s, est. speed input: 71582.17 toks/s, output: 69.90 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:07<00:40, 37.23it/s, est. speed input: 69629.82 toks/s, output: 68.00 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:08<00:40, 36.94it/s, est. speed input: 67888.87 toks/s, output: 66.30 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:08<00:40, 36.72it/s, est. speed input: 66321.11 toks/s, output: 64.77 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:09<00:39, 36.81it/s, est. speed input: 64971.43 toks/s, output: 63.45 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:09<00:38, 37.50it/s, est. speed input: 63909.01 toks/s, output: 62.41 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:09<00:37, 37.99it/s, est. speed input: 62932.98 toks/s, output: 61.46 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:10<00:37, 38.35it/s, est. speed input: 62032.74 toks/s, output: 60.58 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:10<00:36, 38.44it/s, est. speed input: 61169.27 toks/s, output: 59.73 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:11<00:36, 37.74it/s, est. speed input: 60216.94 toks/s, output: 58.81 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:11<00:36, 37.25it/s, est. speed input: 59333.22 toks/s, output: 57.94 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:12<00:36, 36.90it/s, est. speed input: 58513.81 toks/s, output: 57.14 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:12<00:36, 36.67it/s, est. speed input: 57752.80 toks/s, output: 56.40 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:12<00:36, 36.49it/s, est. speed input: 57040.12 toks/s, output: 55.70 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:13<00:36, 36.39it/s, est. speed input: 56379.13 toks/s, output: 55.06 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:13<00:35, 36.31it/s, est. speed input: 55759.18 toks/s, output: 54.45 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:14<00:35, 36.25it/s, est. speed input: 55176.88 toks/s, output: 53.88 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:14<00:34, 36.20it/s, est. speed input: 54628.38 toks/s, output: 53.35 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:15<00:34, 36.17it/s, est. speed input: 54112.21 toks/s, output: 52.84 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:15<00:34, 36.14it/s, est. speed input: 53624.43 toks/s, output: 52.37 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:16<00:33, 36.37it/s, est. speed input: 53198.06 toks/s, output: 51.95 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:16<00:32, 37.13it/s, est. speed input: 52868.77 toks/s, output: 51.63 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:16<00:31, 37.69it/s, est. speed input: 52556.73 toks/s, output: 51.32 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:17<00:30, 38.10it/s, est. speed input: 52259.85 toks/s, output: 51.03 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:17<00:29, 38.38it/s, est. speed input: 51976.00 toks/s, output: 50.76 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:18<00:29, 38.59it/s, est. speed input: 51705.65 toks/s, output: 50.49 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:18<00:28, 38.73it/s, est. speed input: 51446.13 toks/s, output: 50.24 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:18<00:28, 38.32it/s, est. speed input: 51149.12 toks/s, output: 49.95 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:19<00:28, 37.60it/s, est. speed input: 50822.20 toks/s, output: 49.63 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:19<00:28, 37.12it/s, est. speed input: 50510.64 toks/s, output: 49.33 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:20<00:28, 36.78it/s, est. speed input: 50211.66 toks/s, output: 49.03 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:20<00:28, 36.56it/s, est. speed input: 49926.70 toks/s, output: 48.76 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:21<00:28, 36.41it/s, est. speed input: 49654.39 toks/s, output: 48.49 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:21<00:27, 36.29it/s, est. speed input: 49391.93 toks/s, output: 48.23 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:22<00:27, 36.22it/s, est. speed input: 49140.64 toks/s, output: 47.99 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:22<00:26, 36.17it/s, est. speed input: 48899.18 toks/s, output: 47.75 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:22<00:26, 36.12it/s, est. speed input: 48666.50 toks/s, output: 47.53 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:23<00:26, 36.09it/s, est. speed input: 48442.58 toks/s, output: 47.31 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:23<00:25, 36.60it/s, est. speed input: 48270.40 toks/s, output: 47.14 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:24<00:24, 37.31it/s, est. speed input: 48130.74 toks/s, output: 47.00 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:24<00:23, 38.35it/s, est. speed input: 48033.63 toks/s, output: 46.91 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:25<00:22, 38.56it/s, est. speed input: 47901.84 toks/s, output: 46.78 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:25<00:22, 38.70it/s, est. speed input: 47774.34 toks/s, output: 46.65 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:25<00:21, 38.82it/s, est. speed input: 47651.37 toks/s, output: 46.53 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:26<00:08, 89.06it/s, est. speed input: 50132.61 toks/s, output: 48.96 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:26<00:10, 73.71it/s, est. speed input: 49977.31 toks/s, output: 48.81 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:27<00:11, 62.09it/s, est. speed input: 49784.60 toks/s, output: 48.62 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:27<00:13, 53.86it/s, est. speed input: 49578.16 toks/s, output: 48.42 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:27<00:14, 48.32it/s, est. speed input: 49377.72 toks/s, output: 48.22 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:28<00:15, 44.58it/s, est. speed input: 49184.98 toks/s, output: 48.03 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:28<00:15, 42.00it/s, est. speed input: 48997.81 toks/s, output: 47.85 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:29<00:16, 40.22it/s, est. speed input: 48816.01 toks/s, output: 47.67 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:29<00:16, 38.97it/s, est. speed input: 48639.05 toks/s, output: 47.50 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:30<00:16, 38.10it/s, est. speed input: 48467.27 toks/s, output: 47.33 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:30<00:16, 37.50it/s, est. speed input: 48300.52 toks/s, output: 47.17 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:31<00:15, 37.08it/s, est. speed input: 48139.04 toks/s, output: 47.01 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:31<00:15, 36.78it/s, est. speed input: 47981.49 toks/s, output: 46.86 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:31<00:15, 36.60it/s, est. speed input: 47829.76 toks/s, output: 46.71 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:32<00:14, 37.02it/s, est. speed input: 47714.11 toks/s, output: 46.60 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:32<00:13, 37.63it/s, est. speed input: 47618.72 toks/s, output: 46.50 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:33<00:13, 38.09it/s, est. speed input: 47527.03 toks/s, output: 46.41 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:33<00:12, 38.37it/s, est. speed input: 47434.72 toks/s, output: 46.32 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:33<00:12, 38.60it/s, est. speed input: 47346.33 toks/s, output: 46.24 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:34<00:11, 38.76it/s, est. speed input: 47259.73 toks/s, output: 46.15 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:34<00:11, 38.81it/s, est. speed input: 47172.48 toks/s, output: 46.07 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:35<00:11, 37.96it/s, est. speed input: 47044.07 toks/s, output: 45.94 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:35<00:11, 37.38it/s, est. speed input: 46918.82 toks/s, output: 45.82 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:36<00:10, 37.00it/s, est. speed input: 46797.13 toks/s, output: 45.70 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:36<00:10, 36.74it/s, est. speed input: 46678.65 toks/s, output: 45.58 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:36<00:10, 36.54it/s, est. speed input: 46562.28 toks/s, output: 45.47 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:37<00:09, 36.42it/s, est. speed input: 46449.00 toks/s, output: 45.36 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:37<00:09, 36.33it/s, est. speed input: 46338.52 toks/s, output: 45.25 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:38<00:08, 36.28it/s, est. speed input: 46230.81 toks/s, output: 45.15 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:38<00:08, 36.23it/s, est. speed input: 46125.44 toks/s, output: 45.04 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:39<00:07, 36.20it/s, est. speed input: 46022.02 toks/s, output: 44.94 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:39<00:07, 36.17it/s, est. speed input: 45921.06 toks/s, output: 44.84 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:40<00:07, 36.16it/s, est. speed input: 45822.30 toks/s, output: 44.75 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:40<00:06, 36.89it/s, est. speed input: 45759.58 toks/s, output: 44.69 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:40<00:05, 37.54it/s, est. speed input: 45702.89 toks/s, output: 44.63 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:41<00:05, 38.01it/s, est. speed input: 45647.25 toks/s, output: 44.58 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:41<00:04, 38.33it/s, est. speed input: 45592.15 toks/s, output: 44.52 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:42<00:04, 39.14it/s, est. speed input: 45560.69 toks/s, output: 44.49 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:42<00:04, 39.14it/s, est. speed input: 45508.05 toks/s, output: 44.44 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:42<00:03, 38.74it/s, est. speed input: 45441.36 toks/s, output: 44.38 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:43<00:03, 37.92it/s, est. speed input: 45355.28 toks/s, output: 44.29 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:43<00:02, 37.36it/s, est. speed input: 45270.77 toks/s, output: 44.21 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:44<00:02, 36.99it/s, est. speed input: 45188.16 toks/s, output: 44.13 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:44<00:02, 36.72it/s, est. speed input: 45106.78 toks/s, output: 44.05 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:45<00:01, 36.54it/s, est. speed input: 45027.21 toks/s, output: 43.97 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:45<00:01, 36.42it/s, est. speed input: 44949.48 toks/s, output: 43.90 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:46<00:00, 36.33it/s, est. speed input: 44872.82 toks/s, output: 43.82 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:46<00:00, 36.89it/s, est. speed input: 44821.52 toks/s, output: 43.77 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:46<00:00, 36.89it/s, est. speed input: 45129.60 toks/s, output: 44.07 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:46<00:00, 44.07it/s, est. speed input: 45129.60 toks/s, output: 44.07 toks/s]
[rank0]:[W125 22:39:16.877382030 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-25 22:39:19
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:39:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:39:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=493031) WARNING 01-25 22:40:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=493031) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=493031) WARNING 01-25 22:40:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.13 requests/s, 38054.52 total tokens/s, 37.13 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-25 22:39:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:39:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:39:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:39:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:39:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:39:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:39:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:39:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:39:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:39:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:39:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:39:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:39:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:39:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:40:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:40:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:40:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:40:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:40:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:40:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:40:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:40:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:40:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:40:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:40:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:40:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:40:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:40:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=493031) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=493031) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.10s/it]
(EngineCore_DP0 pid=493031) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.10s/it]
(EngineCore_DP0 pid=493031) 
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=493031) [2026-01-25 22:40:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=493031) [rank0]:W0125 22:40:21.635000 493031 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=493031) [rank0]:W0125 22:40:21.762000 493031 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=493031) [rank0]:W0125 22:40:23.268000 493031 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=493031) [rank0]:W0125 22:40:23.444000 493031 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=493031) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  7.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  7.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  5.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  3.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  3.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  5.53it/s]
(EngineCore_DP0 pid=493031) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:01,  3.18it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:01,  3.60it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  4.98it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  6.09it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  6.94it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  7.62it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  8.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  6.33it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 27/4096 [00:00<00:15, 265.57it/s]
Adding requests:   1%|▏         | 56/4096 [00:00<00:14, 275.87it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:14, 276.75it/s]
Adding requests:   3%|▎         | 112/4096 [00:00<00:14, 276.69it/s]
Adding requests:   3%|▎         | 141/4096 [00:00<00:14, 276.92it/s]
Adding requests:   4%|▍         | 169/4096 [00:00<00:14, 271.74it/s]
Adding requests:   5%|▍         | 197/4096 [00:00<00:14, 273.71it/s]
Adding requests:   6%|▌         | 228/4096 [00:00<00:13, 283.72it/s]
Adding requests:   6%|▋         | 259/4096 [00:00<00:13, 290.76it/s]
Adding requests:   7%|▋         | 289/4096 [00:01<00:13, 286.51it/s]
Adding requests:   8%|▊         | 318/4096 [00:01<00:13, 285.90it/s]
Adding requests:   8%|▊         | 348/4096 [00:01<00:12, 288.83it/s]
Adding requests:   9%|▉         | 379/4096 [00:01<00:12, 293.77it/s]
Adding requests:  10%|▉         | 409/4096 [00:01<00:12, 295.54it/s]
Adding requests:  11%|█         | 439/4096 [00:01<00:12, 294.08it/s]
Adding requests:  11%|█▏        | 469/4096 [00:01<00:12, 289.39it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:12, 286.64it/s]
Adding requests:  13%|█▎        | 527/4096 [00:01<00:12, 278.92it/s]
Adding requests:  14%|█▎        | 559/4096 [00:01<00:12, 289.52it/s]
Adding requests:  14%|█▍        | 591/4096 [00:02<00:11, 295.77it/s]
Adding requests:  15%|█▌        | 621/4096 [00:02<00:12, 289.42it/s]
Adding requests:  16%|█▌        | 651/4096 [00:02<00:11, 289.61it/s]
Adding requests:  17%|█▋        | 681/4096 [00:02<00:11, 288.53it/s]
Adding requests:  17%|█▋        | 710/4096 [00:02<00:11, 284.00it/s]
Adding requests:  18%|█▊        | 740/4096 [00:02<00:11, 285.92it/s]
Adding requests:  19%|█▉        | 769/4096 [00:02<00:11, 283.04it/s]
Adding requests:  19%|█▉        | 798/4096 [00:02<00:11, 278.24it/s]
Adding requests:  20%|██        | 826/4096 [00:02<00:12, 272.43it/s]
Adding requests:  21%|██        | 854/4096 [00:03<00:11, 274.05it/s]
Adding requests:  22%|██▏       | 882/4096 [00:03<00:11, 275.73it/s]
Adding requests:  22%|██▏       | 911/4096 [00:03<00:11, 279.17it/s]
Adding requests:  23%|██▎       | 939/4096 [00:03<00:11, 278.79it/s]
Adding requests:  24%|██▎       | 968/4096 [00:03<00:11, 279.46it/s]
Adding requests:  24%|██▍       | 996/4096 [00:03<00:11, 278.83it/s]
Adding requests:  25%|██▌       | 1026/4096 [00:03<00:10, 282.52it/s]
Adding requests:  26%|██▌       | 1055/4096 [00:03<00:10, 280.66it/s]
Adding requests:  26%|██▋       | 1084/4096 [00:03<00:10, 274.64it/s]
Adding requests:  27%|██▋       | 1112/4096 [00:03<00:10, 271.42it/s]
Adding requests:  28%|██▊       | 1141/4096 [00:04<00:10, 276.17it/s]
Adding requests:  29%|██▊       | 1171/4096 [00:04<00:10, 281.63it/s]
Adding requests:  29%|██▉       | 1200/4096 [00:04<00:10, 276.03it/s]
Adding requests:  30%|███       | 1231/4096 [00:04<00:10, 283.74it/s]
Adding requests:  31%|███       | 1260/4096 [00:04<00:09, 284.29it/s]
Adding requests:  31%|███▏      | 1289/4096 [00:04<00:09, 285.41it/s]
Adding requests:  32%|███▏      | 1319/4096 [00:04<00:09, 288.93it/s]
Adding requests:  33%|███▎      | 1351/4096 [00:04<00:09, 295.17it/s]
Adding requests:  34%|███▍      | 1383/4096 [00:04<00:09, 300.10it/s]
Adding requests:  35%|███▍      | 1415/4096 [00:04<00:08, 305.23it/s]
Adding requests:  35%|███▌      | 1446/4096 [00:05<00:08, 295.95it/s]
Adding requests:  36%|███▌      | 1476/4096 [00:05<00:09, 290.28it/s]
Adding requests:  37%|███▋      | 1506/4096 [00:05<00:08, 289.82it/s]
Adding requests:  38%|███▊      | 1536/4096 [00:05<00:08, 289.57it/s]
Adding requests:  38%|███▊      | 1565/4096 [00:05<00:08, 289.23it/s]
Adding requests:  39%|███▉      | 1594/4096 [00:05<00:08, 286.23it/s]
Adding requests:  40%|███▉      | 1623/4096 [00:05<00:08, 286.09it/s]
Adding requests:  40%|████      | 1653/4096 [00:05<00:08, 288.84it/s]
Adding requests:  41%|████      | 1683/4096 [00:05<00:08, 289.56it/s]
Adding requests:  42%|████▏     | 1715/4096 [00:06<00:08, 296.41it/s]
Adding requests:  43%|████▎     | 1747/4096 [00:06<00:07, 301.15it/s]
Adding requests:  43%|████▎     | 1778/4096 [00:06<00:07, 300.79it/s]
Adding requests:  44%|████▍     | 1809/4096 [00:06<00:07, 303.37it/s]
Adding requests:  45%|████▍     | 1841/4096 [00:06<00:07, 306.62it/s]
Adding requests:  46%|████▌     | 1873/4096 [00:06<00:07, 308.51it/s]
Adding requests:  46%|████▋     | 1904/4096 [00:06<00:07, 308.39it/s]
Adding requests:  47%|████▋     | 1935/4096 [00:06<00:07, 308.07it/s]
Adding requests:  48%|████▊     | 1966/4096 [00:06<00:07, 303.17it/s]
Adding requests:  49%|████▉     | 1998/4096 [00:06<00:06, 306.37it/s]
Adding requests:  50%|████▉     | 2030/4096 [00:07<00:06, 308.28it/s]
Adding requests:  50%|█████     | 2063/4096 [00:07<00:06, 314.08it/s]
Adding requests:  51%|█████     | 2096/4096 [00:07<00:06, 316.87it/s]
Adding requests:  52%|█████▏    | 2128/4096 [00:07<00:06, 309.81it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:07<00:06, 311.74it/s]
Adding requests:  54%|█████▎    | 2192/4096 [00:07<00:06, 312.60it/s]
Adding requests:  54%|█████▍    | 2225/4096 [00:07<00:05, 316.23it/s]
Adding requests:  55%|█████▌    | 2259/4096 [00:07<00:05, 322.85it/s]
Adding requests:  56%|█████▌    | 2293/4096 [00:07<00:05, 324.60it/s]
Adding requests:  57%|█████▋    | 2326/4096 [00:07<00:05, 315.53it/s]
Adding requests:  58%|█████▊    | 2359/4096 [00:08<00:05, 319.52it/s]
Adding requests:  58%|█████▊    | 2392/4096 [00:08<00:05, 321.61it/s]
Adding requests:  59%|█████▉    | 2425/4096 [00:08<00:05, 306.17it/s]
Adding requests:  60%|█████▉    | 2456/4096 [00:08<00:05, 297.59it/s]
Adding requests:  61%|██████    | 2486/4096 [00:08<00:05, 290.43it/s]
Adding requests:  61%|██████▏   | 2516/4096 [00:08<00:05, 291.05it/s]
Adding requests:  62%|██████▏   | 2546/4096 [00:08<00:05, 290.52it/s]
Adding requests:  63%|██████▎   | 2576/4096 [00:08<00:05, 291.35it/s]
Adding requests:  64%|██████▎   | 2606/4096 [00:08<00:05, 291.11it/s]
Adding requests:  64%|██████▍   | 2636/4096 [00:09<00:05, 290.05it/s]
Adding requests:  65%|██████▌   | 2667/4096 [00:09<00:04, 292.99it/s]
Adding requests:  66%|██████▌   | 2697/4096 [00:09<00:04, 287.43it/s]
Adding requests:  67%|██████▋   | 2726/4096 [00:09<00:04, 280.44it/s]
Adding requests:  67%|██████▋   | 2755/4096 [00:09<00:04, 276.45it/s]
Adding requests:  68%|██████▊   | 2784/4096 [00:09<00:04, 279.92it/s]
Adding requests:  69%|██████▊   | 2813/4096 [00:09<00:04, 279.65it/s]
Adding requests:  69%|██████▉   | 2842/4096 [00:09<00:04, 281.68it/s]
Adding requests:  70%|███████   | 2871/4096 [00:09<00:04, 282.29it/s]
Adding requests:  71%|███████   | 2901/4096 [00:09<00:04, 285.49it/s]
Adding requests:  72%|███████▏  | 2930/4096 [00:10<00:04, 278.75it/s]
Adding requests:  72%|███████▏  | 2958/4096 [00:10<00:04, 272.91it/s]
Adding requests:  73%|███████▎  | 2988/4096 [00:10<00:03, 277.99it/s]
Adding requests:  74%|███████▎  | 3016/4096 [00:10<00:03, 276.81it/s]
Adding requests:  74%|███████▍  | 3046/4096 [00:10<00:03, 281.60it/s]
Adding requests:  75%|███████▌  | 3077/4096 [00:10<00:03, 287.29it/s]
Adding requests:  76%|███████▌  | 3108/4096 [00:10<00:03, 293.39it/s]
Adding requests:  77%|███████▋  | 3139/4096 [00:10<00:03, 298.22it/s]
Adding requests:  77%|███████▋  | 3170/4096 [00:10<00:03, 300.79it/s]
Adding requests:  78%|███████▊  | 3201/4096 [00:10<00:02, 299.54it/s]
Adding requests:  79%|███████▉  | 3232/4096 [00:11<00:02, 300.78it/s]
Adding requests:  80%|███████▉  | 3263/4096 [00:11<00:02, 298.16it/s]
Adding requests:  80%|████████  | 3293/4096 [00:11<00:02, 296.44it/s]
Adding requests:  81%|████████  | 3323/4096 [00:11<00:02, 292.97it/s]
Adding requests:  82%|████████▏ | 3353/4096 [00:11<00:02, 295.02it/s]
Adding requests:  83%|████████▎ | 3385/4096 [00:11<00:02, 299.03it/s]
Adding requests:  83%|████████▎ | 3415/4096 [00:11<00:02, 294.65it/s]
Adding requests:  84%|████████▍ | 3445/4096 [00:11<00:02, 290.44it/s]
Adding requests:  85%|████████▍ | 3475/4096 [00:11<00:02, 283.07it/s]
Adding requests:  86%|████████▌ | 3504/4096 [00:12<00:02, 284.36it/s]
Adding requests:  86%|████████▋ | 3533/4096 [00:12<00:02, 281.28it/s]
Adding requests:  87%|████████▋ | 3562/4096 [00:12<00:01, 281.40it/s]
Adding requests:  88%|████████▊ | 3591/4096 [00:12<00:01, 283.47it/s]
Adding requests:  88%|████████▊ | 3620/4096 [00:12<00:01, 281.62it/s]
Adding requests:  89%|████████▉ | 3649/4096 [00:12<00:01, 281.47it/s]
Adding requests:  90%|████████▉ | 3678/4096 [00:12<00:01, 283.21it/s]
Adding requests:  91%|█████████ | 3708/4096 [00:12<00:01, 285.72it/s]
Adding requests:  91%|█████████ | 3737/4096 [00:12<00:01, 282.86it/s]
Adding requests:  92%|█████████▏| 3766/4096 [00:12<00:01, 267.96it/s]
Adding requests:  93%|█████████▎| 3796/4096 [00:13<00:01, 275.97it/s]
Adding requests:  93%|█████████▎| 3825/4096 [00:13<00:00, 277.35it/s]
Adding requests:  94%|█████████▍| 3855/4096 [00:13<00:00, 282.54it/s]
Adding requests:  95%|█████████▍| 3884/4096 [00:13<00:00, 284.22it/s]
Adding requests:  96%|█████████▌| 3914/4096 [00:13<00:00, 287.84it/s]
Adding requests:  96%|█████████▋| 3947/4096 [00:13<00:00, 299.57it/s]
Adding requests:  97%|█████████▋| 3978/4096 [00:13<00:00, 299.39it/s]
Adding requests:  98%|█████████▊| 4012/4096 [00:13<00:00, 309.23it/s]
Adding requests:  99%|█████████▉| 4045/4096 [00:13<00:00, 312.44it/s]
Adding requests: 100%|█████████▉| 4078/4096 [00:14<00:00, 316.64it/s]
Adding requests: 100%|██████████| 4096/4096 [00:14<00:00, 291.28it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:00<00:02, 1413.94it/s, est. speed input: 1448019.59 toks/s, output: 1413.98 toks/s]
Processed prompts:  15%|█▌        | 624/4096 [00:03<00:26, 132.72it/s, est. speed input: 172028.85 toks/s, output: 168.00 toks/s]   
Processed prompts:  17%|█▋        | 685/4096 [00:05<00:36, 92.51it/s, est. speed input: 128172.95 toks/s, output: 125.17 toks/s] 
Processed prompts:  18%|█▊        | 720/4096 [00:06<00:41, 80.57it/s, est. speed input: 116069.83 toks/s, output: 113.35 toks/s]
Processed prompts:  18%|█▊        | 743/4096 [00:07<00:49, 67.73it/s, est. speed input: 105577.30 toks/s, output: 103.10 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:08<00:56, 59.20it/s, est. speed input: 98282.96 toks/s, output: 95.98 toks/s]  
Processed prompts:  20%|█▉        | 802/4096 [00:08<01:01, 53.99it/s, est. speed input: 92910.66 toks/s, output: 90.73 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:09<01:05, 49.98it/s, est. speed input: 88448.42 toks/s, output: 86.38 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:10<01:09, 46.48it/s, est. speed input: 84431.08 toks/s, output: 82.45 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:11<01:13, 43.39it/s, est. speed input: 80749.23 toks/s, output: 78.86 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:11<00:44, 69.86it/s, est. speed input: 85509.13 toks/s, output: 83.50 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:12<00:53, 57.56it/s, est. speed input: 82050.03 toks/s, output: 80.13 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:13<01:01, 50.22it/s, est. speed input: 79048.01 toks/s, output: 77.20 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:14<01:06, 45.63it/s, est. speed input: 76434.93 toks/s, output: 74.64 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:15<01:10, 42.58it/s, est. speed input: 74111.27 toks/s, output: 72.37 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:15<01:12, 41.02it/s, est. speed input: 72193.98 toks/s, output: 70.50 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:16<01:12, 40.76it/s, est. speed input: 70708.70 toks/s, output: 69.05 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:17<01:12, 40.30it/s, est. speed input: 69286.32 toks/s, output: 67.66 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:18<01:12, 39.76it/s, est. speed input: 67935.83 toks/s, output: 66.34 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:19<01:13, 38.64it/s, est. speed input: 66517.61 toks/s, output: 64.96 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:20<01:14, 37.90it/s, est. speed input: 65227.98 toks/s, output: 63.70 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:21<01:14, 37.41it/s, est. speed input: 64050.78 toks/s, output: 62.55 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:21<01:14, 37.03it/s, est. speed input: 62959.76 toks/s, output: 61.48 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:22<01:13, 36.80it/s, est. speed input: 61959.46 toks/s, output: 60.51 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:23<01:13, 36.64it/s, est. speed input: 61033.94 toks/s, output: 59.60 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:24<01:11, 37.09it/s, est. speed input: 60284.10 toks/s, output: 58.87 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:25<01:09, 37.72it/s, est. speed input: 59638.54 toks/s, output: 58.24 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:26<01:07, 38.18it/s, est. speed input: 59033.07 toks/s, output: 57.65 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:27<01:07, 37.63it/s, est. speed input: 58325.21 toks/s, output: 56.96 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:27<01:07, 37.17it/s, est. speed input: 57647.86 toks/s, output: 56.30 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:28<01:07, 36.89it/s, est. speed input: 57017.89 toks/s, output: 55.68 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:29<01:07, 36.70it/s, est. speed input: 56426.00 toks/s, output: 55.10 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:30<01:06, 36.56it/s, est. speed input: 55867.83 toks/s, output: 54.56 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:31<01:05, 36.83it/s, est. speed input: 55392.25 toks/s, output: 54.09 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:32<01:03, 37.53it/s, est. speed input: 55008.26 toks/s, output: 53.72 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:33<01:01, 38.04it/s, est. speed input: 54642.89 toks/s, output: 53.36 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:33<00:59, 38.40it/s, est. speed input: 54295.45 toks/s, output: 53.02 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:34<00:59, 38.16it/s, est. speed input: 53908.57 toks/s, output: 52.65 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:35<00:59, 37.87it/s, est. speed input: 53525.72 toks/s, output: 52.27 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:36<00:59, 37.36it/s, est. speed input: 53127.87 toks/s, output: 51.88 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:37<00:58, 37.02it/s, est. speed input: 52748.70 toks/s, output: 51.51 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:38<00:58, 36.78it/s, est. speed input: 52387.17 toks/s, output: 51.16 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:39<00:57, 36.62it/s, est. speed input: 52041.99 toks/s, output: 50.82 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:39<00:55, 37.21it/s, est. speed input: 51783.68 toks/s, output: 50.57 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:40<00:54, 37.80it/s, est. speed input: 51551.61 toks/s, output: 50.34 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:41<00:52, 38.22it/s, est. speed input: 51328.61 toks/s, output: 50.13 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:42<00:51, 38.41it/s, est. speed input: 51104.56 toks/s, output: 49.91 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:42<00:28, 67.54it/s, est. speed input: 52516.03 toks/s, output: 51.29 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:43<00:33, 55.70it/s, est. speed input: 52206.03 toks/s, output: 50.98 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:44<00:37, 48.90it/s, est. speed input: 51908.28 toks/s, output: 50.69 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:45<00:40, 44.71it/s, est. speed input: 51621.85 toks/s, output: 50.41 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:45<00:42, 42.01it/s, est. speed input: 51346.58 toks/s, output: 50.14 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:46<00:43, 40.23it/s, est. speed input: 51081.75 toks/s, output: 49.88 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:47<00:43, 39.82it/s, est. speed input: 50887.07 toks/s, output: 49.69 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:48<00:42, 39.66it/s, est. speed input: 50707.47 toks/s, output: 49.52 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:49<00:42, 39.54it/s, est. speed input: 50533.84 toks/s, output: 49.35 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:50<00:41, 39.45it/s, est. speed input: 50365.82 toks/s, output: 49.19 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:50<00:41, 38.90it/s, est. speed input: 50169.17 toks/s, output: 48.99 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:51<00:41, 38.07it/s, est. speed input: 49946.63 toks/s, output: 48.78 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:52<00:40, 37.50it/s, est. speed input: 49731.39 toks/s, output: 48.57 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:53<00:40, 37.11it/s, est. speed input: 49523.33 toks/s, output: 48.36 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:54<00:39, 36.84it/s, est. speed input: 49322.18 toks/s, output: 48.17 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:55<00:39, 36.65it/s, est. speed input: 49127.06 toks/s, output: 47.98 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:56<00:38, 36.53it/s, est. speed input: 48938.49 toks/s, output: 47.79 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:57<00:37, 37.05it/s, est. speed input: 48797.07 toks/s, output: 47.65 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:57<00:35, 37.68it/s, est. speed input: 48675.88 toks/s, output: 47.54 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:58<00:34, 37.65it/s, est. speed input: 48528.11 toks/s, output: 47.39 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:59<00:34, 37.21it/s, est. speed input: 48358.90 toks/s, output: 47.23 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:00<00:33, 36.91it/s, est. speed input: 48194.64 toks/s, output: 47.07 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:01<00:33, 36.70it/s, est. speed input: 48034.78 toks/s, output: 46.91 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:02<00:32, 36.56it/s, est. speed input: 47879.77 toks/s, output: 46.76 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:03<00:31, 36.46it/s, est. speed input: 47729.14 toks/s, output: 46.61 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:04<00:30, 36.90it/s, est. speed input: 47612.71 toks/s, output: 46.50 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:04<00:28, 37.57it/s, est. speed input: 47519.18 toks/s, output: 46.41 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:05<00:27, 38.06it/s, est. speed input: 47428.26 toks/s, output: 46.32 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:06<00:26, 38.24it/s, est. speed input: 47330.87 toks/s, output: 46.22 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:07<00:26, 37.61it/s, est. speed input: 47196.78 toks/s, output: 46.09 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:08<00:25, 37.19it/s, est. speed input: 47066.00 toks/s, output: 45.96 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:09<00:25, 36.89it/s, est. speed input: 46938.53 toks/s, output: 45.84 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:10<00:24, 36.69it/s, est. speed input: 46814.49 toks/s, output: 45.72 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:10<00:23, 36.55it/s, est. speed input: 46693.45 toks/s, output: 45.60 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:11<00:22, 36.88it/s, est. speed input: 46597.30 toks/s, output: 45.51 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:12<00:21, 37.56it/s, est. speed input: 46525.37 toks/s, output: 45.43 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:13<00:20, 38.05it/s, est. speed input: 46455.06 toks/s, output: 45.37 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:14<00:10, 62.80it/s, est. speed input: 47307.40 toks/s, output: 46.20 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:15<00:11, 55.04it/s, est. speed input: 47208.43 toks/s, output: 46.10 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:15<00:12, 49.53it/s, est. speed input: 47105.00 toks/s, output: 46.00 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:16<00:12, 45.77it/s, est. speed input: 47004.48 toks/s, output: 45.90 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:17<00:12, 43.15it/s, est. speed input: 46905.11 toks/s, output: 45.81 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:18<00:12, 41.34it/s, est. speed input: 46808.15 toks/s, output: 45.71 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:19<00:11, 40.08it/s, est. speed input: 46713.46 toks/s, output: 45.62 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:20<00:11, 39.21it/s, est. speed input: 46620.71 toks/s, output: 45.53 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:21<00:10, 38.93it/s, est. speed input: 46543.71 toks/s, output: 45.45 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:21<00:09, 39.01it/s, est. speed input: 46480.16 toks/s, output: 45.39 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:22<00:09, 38.81it/s, est. speed input: 46407.25 toks/s, output: 45.32 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:23<00:08, 38.00it/s, est. speed input: 46308.59 toks/s, output: 45.22 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:24<00:07, 37.45it/s, est. speed input: 46211.76 toks/s, output: 45.13 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:25<00:06, 37.07it/s, est. speed input: 46117.24 toks/s, output: 45.04 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:26<00:06, 36.81it/s, est. speed input: 46024.47 toks/s, output: 44.95 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:27<00:05, 36.62it/s, est. speed input: 45933.62 toks/s, output: 44.86 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:27<00:04, 36.50it/s, est. speed input: 45844.70 toks/s, output: 44.77 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:28<00:03, 36.75it/s, est. speed input: 45771.33 toks/s, output: 44.70 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:29<00:02, 37.46it/s, est. speed input: 45720.38 toks/s, output: 44.65 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:30<00:01, 38.26it/s, est. speed input: 45680.80 toks/s, output: 44.61 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:31<00:00, 38.01it/s, est. speed input: 45612.01 toks/s, output: 44.54 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:31<00:00, 38.01it/s, est. speed input: 45948.26 toks/s, output: 44.87 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:31<00:00, 44.87it/s, est. speed input: 45948.26 toks/s, output: 44.87 toks/s]
[rank0]:[W125 22:42:18.896452394 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-25 22:42:21
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:43:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:43:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=496188) WARNING 01-25 22:43:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=496188) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=496188) WARNING 01-25 22:43:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.21 requests/s, 17638.47 total tokens/s, 17.21 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-25 22:43:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:43:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:43:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:43:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:43:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:43:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:43:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:43:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:43:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:43:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:43:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:43:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:43:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:43:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:43:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:43:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:43:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:43:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:43:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:43:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:43:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:43:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:43:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:43:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:43:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:43:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:43:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:43:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:32] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:32] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:32] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:32] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:32] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=496188) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=496188) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, -1.83it/s]
(EngineCore_DP0 pid=496188) 
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=496188) [2026-01-25 22:43:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=496188) [rank0]:W0125 22:43:51.459000 496188 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=496188) [rank0]:W0125 22:43:51.588000 496188 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=496188) [rank0]:W0125 22:43:53.423000 496188 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=496188) [rank0]:W0125 22:43:53.600000 496188 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=496188) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  4.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  5.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:02,  6.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:02,  5.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:03,  3.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:02,  3.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:02<00:03,  3.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:02<00:02,  3.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:02<00:01,  4.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:01,  5.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  6.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  6.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  7.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:03<00:00,  7.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:03<00:00,  7.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:03<00:00,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  7.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  5.41it/s]
(EngineCore_DP0 pid=496188) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.63it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  7.55it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:01,  7.29it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:01,  6.29it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:01<00:01,  3.92it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:01<00:01,  4.14it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:01<00:00,  4.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 17.89it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 25/8192 [00:00<00:33, 244.47it/s]
Adding requests:   1%|          | 53/8192 [00:00<00:30, 264.92it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:29, 273.74it/s]
Adding requests:   1%|▏         | 110/8192 [00:00<00:29, 275.35it/s]
Adding requests:   2%|▏         | 138/8192 [00:00<00:29, 270.41it/s]
Adding requests:   2%|▏         | 166/8192 [00:00<00:29, 272.61it/s]
Adding requests:   2%|▏         | 194/8192 [00:00<00:29, 268.08it/s]
Adding requests:   3%|▎         | 222/8192 [00:00<00:29, 269.29it/s]
Adding requests:   3%|▎         | 251/8192 [00:00<00:28, 274.73it/s]
Adding requests:   3%|▎         | 279/8192 [00:01<00:29, 269.42it/s]
Adding requests:   4%|▎         | 306/8192 [00:01<00:29, 264.91it/s]
Adding requests:   4%|▍         | 333/8192 [00:01<00:30, 255.84it/s]
Adding requests:   4%|▍         | 360/8192 [00:01<00:30, 258.53it/s]
Adding requests:   5%|▍         | 390/8192 [00:01<00:28, 269.64it/s]
Adding requests:   5%|▌         | 420/8192 [00:01<00:28, 276.99it/s]
Adding requests:   5%|▌         | 448/8192 [00:01<00:27, 277.78it/s]
Adding requests:   6%|▌         | 477/8192 [00:01<00:27, 281.01it/s]
Adding requests:   6%|▌         | 506/8192 [00:01<00:27, 282.51it/s]
Adding requests:   7%|▋         | 535/8192 [00:01<00:27, 274.25it/s]
Adding requests:   7%|▋         | 565/8192 [00:02<00:27, 279.69it/s]
Adding requests:   7%|▋         | 594/8192 [00:02<00:27, 279.65it/s]
Adding requests:   8%|▊         | 623/8192 [00:02<00:27, 279.07it/s]
Adding requests:   8%|▊         | 653/8192 [00:02<00:26, 283.09it/s]
Adding requests:   8%|▊         | 683/8192 [00:02<00:26, 287.98it/s]
Adding requests:   9%|▊         | 713/8192 [00:02<00:25, 290.02it/s]
Adding requests:   9%|▉         | 743/8192 [00:02<00:25, 287.31it/s]
Adding requests:   9%|▉         | 773/8192 [00:02<00:25, 288.10it/s]
Adding requests:  10%|▉         | 803/8192 [00:02<00:25, 288.38it/s]
Adding requests:  10%|█         | 832/8192 [00:03<00:25, 286.29it/s]
Adding requests:  11%|█         | 864/8192 [00:03<00:24, 294.65it/s]
Adding requests:  11%|█         | 894/8192 [00:03<00:24, 294.88it/s]
Adding requests:  11%|█▏        | 924/8192 [00:03<00:24, 294.87it/s]
Adding requests:  12%|█▏        | 954/8192 [00:03<00:24, 291.27it/s]
Adding requests:  12%|█▏        | 984/8192 [00:03<00:24, 291.23it/s]
Adding requests:  12%|█▏        | 1015/8192 [00:03<00:24, 296.47it/s]
Adding requests:  13%|█▎        | 1045/8192 [00:03<00:24, 293.87it/s]
Adding requests:  13%|█▎        | 1075/8192 [00:03<00:24, 294.18it/s]
Adding requests:  13%|█▎        | 1105/8192 [00:03<00:24, 289.01it/s]
Adding requests:  14%|█▍        | 1134/8192 [00:04<00:24, 288.25it/s]
Adding requests:  14%|█▍        | 1166/8192 [00:04<00:23, 297.16it/s]
Adding requests:  15%|█▍        | 1196/8192 [00:04<00:23, 295.57it/s]
Adding requests:  15%|█▍        | 1227/8192 [00:04<00:23, 299.74it/s]
Adding requests:  15%|█▌        | 1257/8192 [00:04<00:23, 292.95it/s]
Adding requests:  16%|█▌        | 1288/8192 [00:04<00:23, 296.74it/s]
Adding requests:  16%|█▌        | 1320/8192 [00:04<00:22, 302.68it/s]
Adding requests:  16%|█▋        | 1351/8192 [00:04<00:22, 301.29it/s]
Adding requests:  17%|█▋        | 1382/8192 [00:04<00:23, 295.89it/s]
Adding requests:  17%|█▋        | 1412/8192 [00:04<00:23, 282.98it/s]
Adding requests:  18%|█▊        | 1442/8192 [00:05<00:23, 286.93it/s]
Adding requests:  18%|█▊        | 1473/8192 [00:05<00:22, 292.45it/s]
Adding requests:  18%|█▊        | 1503/8192 [00:05<00:22, 291.86it/s]
Adding requests:  19%|█▊        | 1533/8192 [00:05<00:22, 292.79it/s]
Adding requests:  19%|█▉        | 1563/8192 [00:05<00:22, 290.99it/s]
Adding requests:  19%|█▉        | 1593/8192 [00:05<00:22, 287.38it/s]
Adding requests:  20%|█▉        | 1622/8192 [00:05<00:22, 287.66it/s]
Adding requests:  20%|██        | 1651/8192 [00:05<00:22, 284.89it/s]
Adding requests:  21%|██        | 1680/8192 [00:05<00:23, 279.87it/s]
Adding requests:  21%|██        | 1709/8192 [00:06<00:23, 278.57it/s]
Adding requests:  21%|██        | 1738/8192 [00:06<00:22, 281.24it/s]
Adding requests:  22%|██▏       | 1767/8192 [00:06<00:23, 277.42it/s]
Adding requests:  22%|██▏       | 1795/8192 [00:06<00:23, 277.94it/s]
Adding requests:  22%|██▏       | 1825/8192 [00:06<00:22, 283.42it/s]
Adding requests:  23%|██▎       | 1854/8192 [00:06<00:22, 283.98it/s]
Adding requests:  23%|██▎       | 1883/8192 [00:06<00:23, 270.57it/s]
Adding requests:  23%|██▎       | 1911/8192 [00:06<00:23, 271.66it/s]
Adding requests:  24%|██▎       | 1939/8192 [00:06<00:22, 273.07it/s]
Adding requests:  24%|██▍       | 1967/8192 [00:06<00:23, 267.34it/s]
Adding requests:  24%|██▍       | 1996/8192 [00:07<00:22, 272.97it/s]
Adding requests:  25%|██▍       | 2024/8192 [00:07<00:22, 272.27it/s]
Adding requests:  25%|██▌       | 2053/8192 [00:07<00:22, 275.45it/s]
Adding requests:  25%|██▌       | 2082/8192 [00:07<00:21, 277.94it/s]
Adding requests:  26%|██▌       | 2110/8192 [00:07<00:22, 272.99it/s]
Adding requests:  26%|██▌       | 2138/8192 [00:07<00:22, 270.57it/s]
Adding requests:  26%|██▋       | 2166/8192 [00:07<00:22, 267.30it/s]
Adding requests:  27%|██▋       | 2194/8192 [00:07<00:22, 269.63it/s]
Adding requests:  27%|██▋       | 2223/8192 [00:07<00:21, 275.02it/s]
Adding requests:  27%|██▋       | 2252/8192 [00:07<00:21, 278.00it/s]
Adding requests:  28%|██▊       | 2280/8192 [00:08<00:21, 278.39it/s]
Adding requests:  28%|██▊       | 2310/8192 [00:08<00:20, 284.65it/s]
Adding requests:  29%|██▊       | 2339/8192 [00:08<00:20, 281.22it/s]
Adding requests:  29%|██▉       | 2369/8192 [00:08<00:20, 285.21it/s]
Adding requests:  29%|██▉       | 2399/8192 [00:08<00:20, 288.44it/s]
Adding requests:  30%|██▉       | 2429/8192 [00:08<00:19, 289.50it/s]
Adding requests:  30%|███       | 2458/8192 [00:08<00:19, 287.54it/s]
Adding requests:  30%|███       | 2487/8192 [00:08<00:20, 280.53it/s]
Adding requests:  31%|███       | 2516/8192 [00:08<00:20, 274.05it/s]
Adding requests:  31%|███       | 2544/8192 [00:09<00:20, 271.12it/s]
Adding requests:  31%|███▏      | 2573/8192 [00:09<00:20, 275.44it/s]
Adding requests:  32%|███▏      | 2602/8192 [00:09<00:20, 276.89it/s]
Adding requests:  32%|███▏      | 2630/8192 [00:09<00:20, 277.01it/s]
Adding requests:  32%|███▏      | 2659/8192 [00:09<00:19, 277.45it/s]
Adding requests:  33%|███▎      | 2687/8192 [00:09<00:20, 273.25it/s]
Adding requests:  33%|███▎      | 2715/8192 [00:09<00:20, 271.32it/s]
Adding requests:  33%|███▎      | 2744/8192 [00:09<00:19, 274.63it/s]
Adding requests:  34%|███▍      | 2772/8192 [00:09<00:19, 274.74it/s]
Adding requests:  34%|███▍      | 2800/8192 [00:09<00:19, 272.47it/s]
Adding requests:  35%|███▍      | 2828/8192 [00:10<00:19, 273.85it/s]
Adding requests:  35%|███▍      | 2856/8192 [00:10<00:19, 275.43it/s]
Adding requests:  35%|███▌      | 2885/8192 [00:10<00:19, 279.26it/s]
Adding requests:  36%|███▌      | 2913/8192 [00:10<00:18, 279.11it/s]
Adding requests:  36%|███▌      | 2941/8192 [00:10<00:19, 273.67it/s]
Adding requests:  36%|███▋      | 2970/8192 [00:10<00:18, 276.19it/s]
Adding requests:  37%|███▋      | 3000/8192 [00:10<00:18, 282.01it/s]
Adding requests:  37%|███▋      | 3030/8192 [00:10<00:18, 285.28it/s]
Adding requests:  37%|███▋      | 3060/8192 [00:10<00:17, 289.61it/s]
Adding requests:  38%|███▊      | 3091/8192 [00:10<00:17, 295.27it/s]
Adding requests:  38%|███▊      | 3122/8192 [00:11<00:16, 298.25it/s]
Adding requests:  38%|███▊      | 3152/8192 [00:11<00:17, 296.43it/s]
Adding requests:  39%|███▉      | 3185/8192 [00:11<00:16, 304.05it/s]
Adding requests:  39%|███▉      | 3218/8192 [00:11<00:16, 309.51it/s]
Adding requests:  40%|███▉      | 3249/8192 [00:11<00:16, 300.42it/s]
Adding requests:  40%|████      | 3280/8192 [00:11<00:16, 299.83it/s]
Adding requests:  40%|████      | 3311/8192 [00:11<00:16, 297.23it/s]
Adding requests:  41%|████      | 3343/8192 [00:11<00:16, 302.30it/s]
Adding requests:  41%|████      | 3374/8192 [00:11<00:16, 297.94it/s]
Adding requests:  42%|████▏     | 3405/8192 [00:12<00:15, 301.28it/s]
Adding requests:  42%|████▏     | 3437/8192 [00:12<00:15, 303.85it/s]
Adding requests:  42%|████▏     | 3468/8192 [00:12<00:15, 297.66it/s]
Adding requests:  43%|████▎     | 3498/8192 [00:12<00:15, 296.20it/s]
Adding requests:  43%|████▎     | 3528/8192 [00:12<00:16, 291.25it/s]
Adding requests:  43%|████▎     | 3560/8192 [00:12<00:15, 297.68it/s]
Adding requests:  44%|████▍     | 3590/8192 [00:12<00:15, 295.71it/s]
Adding requests:  44%|████▍     | 3620/8192 [00:12<00:15, 292.68it/s]
Adding requests:  45%|████▍     | 3650/8192 [00:12<00:15, 293.93it/s]
Adding requests:  45%|████▍     | 3680/8192 [00:12<00:15, 285.73it/s]
Adding requests:  45%|████▌     | 3709/8192 [00:13<00:15, 282.04it/s]
Adding requests:  46%|████▌     | 3738/8192 [00:13<00:16, 274.05it/s]
Adding requests:  46%|████▌     | 3767/8192 [00:13<00:15, 277.13it/s]
Adding requests:  46%|████▋     | 3796/8192 [00:13<00:15, 278.47it/s]
Adding requests:  47%|████▋     | 3824/8192 [00:13<00:15, 278.00it/s]
Adding requests:  47%|████▋     | 3853/8192 [00:13<00:15, 280.90it/s]
Adding requests:  47%|████▋     | 3882/8192 [00:13<00:15, 274.34it/s]
Adding requests:  48%|████▊     | 3910/8192 [00:13<00:15, 270.90it/s]
Adding requests:  48%|████▊     | 3938/8192 [00:13<00:15, 267.22it/s]
Adding requests:  48%|████▊     | 3966/8192 [00:14<00:15, 270.02it/s]
Adding requests:  49%|████▉     | 3995/8192 [00:14<00:15, 273.71it/s]
Adding requests:  49%|████▉     | 4026/8192 [00:14<00:14, 281.91it/s]
Adding requests:  49%|████▉     | 4055/8192 [00:14<00:14, 282.44it/s]
Adding requests:  50%|████▉     | 4085/8192 [00:14<00:14, 285.56it/s]
Adding requests:  50%|█████     | 4115/8192 [00:14<00:14, 289.58it/s]
Adding requests:  51%|█████     | 4144/8192 [00:14<00:14, 287.74it/s]
Adding requests:  51%|█████     | 4175/8192 [00:14<00:13, 292.45it/s]
Adding requests:  51%|█████▏    | 4205/8192 [00:14<00:13, 291.34it/s]
Adding requests:  52%|█████▏    | 4235/8192 [00:14<00:13, 286.27it/s]
Adding requests:  52%|█████▏    | 4264/8192 [00:15<00:13, 285.07it/s]
Adding requests:  52%|█████▏    | 4293/8192 [00:15<00:14, 271.17it/s]
Adding requests:  53%|█████▎    | 4321/8192 [00:15<00:14, 273.61it/s]
Adding requests:  53%|█████▎    | 4349/8192 [00:15<00:13, 274.75it/s]
Adding requests:  53%|█████▎    | 4378/8192 [00:15<00:13, 276.36it/s]
Adding requests:  54%|█████▍    | 4406/8192 [00:15<00:13, 275.73it/s]
Adding requests:  54%|█████▍    | 4434/8192 [00:15<00:13, 274.63it/s]
Adding requests:  54%|█████▍    | 4462/8192 [00:15<00:13, 271.79it/s]
Adding requests:  55%|█████▍    | 4490/8192 [00:15<00:13, 271.47it/s]
Adding requests:  55%|█████▌    | 4518/8192 [00:15<00:13, 268.37it/s]
Adding requests:  55%|█████▌    | 4546/8192 [00:16<00:13, 270.68it/s]
Adding requests:  56%|█████▌    | 4574/8192 [00:16<00:13, 267.77it/s]
Adding requests:  56%|█████▌    | 4601/8192 [00:16<00:13, 256.91it/s]
Adding requests:  57%|█████▋    | 4630/8192 [00:16<00:13, 264.27it/s]
Adding requests:  57%|█████▋    | 4658/8192 [00:16<00:13, 265.60it/s]
Adding requests:  57%|█████▋    | 4685/8192 [00:16<00:13, 266.42it/s]
Adding requests:  58%|█████▊    | 4713/8192 [00:16<00:12, 268.67it/s]
Adding requests:  58%|█████▊    | 4741/8192 [00:16<00:12, 271.74it/s]
Adding requests:  58%|█████▊    | 4769/8192 [00:16<00:12, 269.58it/s]
Adding requests:  59%|█████▊    | 4796/8192 [00:17<00:12, 267.09it/s]
Adding requests:  59%|█████▉    | 4823/8192 [00:17<00:12, 263.83it/s]
Adding requests:  59%|█████▉    | 4850/8192 [00:17<00:12, 264.00it/s]
Adding requests:  60%|█████▉    | 4877/8192 [00:17<00:12, 263.72it/s]
Adding requests:  60%|█████▉    | 4906/8192 [00:17<00:12, 269.12it/s]
Adding requests:  60%|██████    | 4935/8192 [00:17<00:11, 274.34it/s]
Adding requests:  61%|██████    | 4966/8192 [00:17<00:11, 283.61it/s]
Adding requests:  61%|██████    | 4998/8192 [00:17<00:10, 291.97it/s]
Adding requests:  61%|██████▏   | 5028/8192 [00:17<00:10, 291.92it/s]
Adding requests:  62%|██████▏   | 5058/8192 [00:17<00:10, 290.80it/s]
Adding requests:  62%|██████▏   | 5091/8192 [00:18<00:10, 300.73it/s]
Adding requests:  63%|██████▎   | 5122/8192 [00:18<00:10, 300.73it/s]
Adding requests:  63%|██████▎   | 5154/8192 [00:18<00:09, 304.51it/s]
Adding requests:  63%|██████▎   | 5185/8192 [00:18<00:10, 295.08it/s]
Adding requests:  64%|██████▎   | 5215/8192 [00:18<00:10, 294.68it/s]
Adding requests:  64%|██████▍   | 5245/8192 [00:18<00:10, 293.17it/s]
Adding requests:  64%|██████▍   | 5275/8192 [00:18<00:10, 290.24it/s]
Adding requests:  65%|██████▍   | 5306/8192 [00:18<00:09, 293.66it/s]
Adding requests:  65%|██████▌   | 5336/8192 [00:18<00:09, 293.22it/s]
Adding requests:  66%|██████▌   | 5367/8192 [00:19<00:09, 296.25it/s]
Adding requests:  66%|██████▌   | 5397/8192 [00:19<00:09, 292.45it/s]
Adding requests:  66%|██████▋   | 5428/8192 [00:19<00:09, 295.67it/s]
Adding requests:  67%|██████▋   | 5458/8192 [00:19<00:09, 290.99it/s]
Adding requests:  67%|██████▋   | 5488/8192 [00:19<00:09, 289.75it/s]
Adding requests:  67%|██████▋   | 5518/8192 [00:19<00:09, 292.18it/s]
Adding requests:  68%|██████▊   | 5548/8192 [00:19<00:09, 293.27it/s]
Adding requests:  68%|██████▊   | 5578/8192 [00:19<00:08, 291.29it/s]
Adding requests:  68%|██████▊   | 5608/8192 [00:19<00:09, 286.63it/s]
Adding requests:  69%|██████▉   | 5637/8192 [00:19<00:09, 282.20it/s]
Adding requests:  69%|██████▉   | 5666/8192 [00:20<00:08, 282.68it/s]
Adding requests:  70%|██████▉   | 5696/8192 [00:20<00:08, 285.13it/s]
Adding requests:  70%|██████▉   | 5727/8192 [00:20<00:08, 290.66it/s]
Adding requests:  70%|███████   | 5757/8192 [00:20<00:08, 290.93it/s]
Adding requests:  71%|███████   | 5787/8192 [00:20<00:08, 275.01it/s]
Adding requests:  71%|███████   | 5815/8192 [00:20<00:09, 248.08it/s]
Adding requests:  71%|███████▏  | 5841/8192 [00:20<00:10, 234.97it/s]
Adding requests:  72%|███████▏  | 5865/8192 [00:20<00:09, 233.09it/s]
Adding requests:  72%|███████▏  | 5889/8192 [00:20<00:10, 225.88it/s]
Adding requests:  72%|███████▏  | 5912/8192 [00:21<00:10, 223.62it/s]
Adding requests:  72%|███████▏  | 5935/8192 [00:21<00:10, 217.30it/s]
Adding requests:  73%|███████▎  | 5957/8192 [00:21<00:11, 199.17it/s]
Adding requests:  73%|███████▎  | 5978/8192 [00:21<00:11, 195.46it/s]
Adding requests:  73%|███████▎  | 6000/8192 [00:21<00:10, 199.97it/s]
Adding requests:  74%|███████▎  | 6026/8192 [00:21<00:10, 216.28it/s]
Adding requests:  74%|███████▍  | 6055/8192 [00:21<00:09, 235.54it/s]
Adding requests:  74%|███████▍  | 6082/8192 [00:21<00:08, 244.44it/s]
Adding requests:  75%|███████▍  | 6110/8192 [00:21<00:08, 252.29it/s]
Adding requests:  75%|███████▍  | 6137/8192 [00:22<00:08, 256.84it/s]
Adding requests:  75%|███████▌  | 6165/8192 [00:22<00:07, 261.98it/s]
Adding requests:  76%|███████▌  | 6193/8192 [00:22<00:07, 265.76it/s]
Adding requests:  76%|███████▌  | 6223/8192 [00:22<00:07, 274.92it/s]
Adding requests:  76%|███████▋  | 6251/8192 [00:22<00:07, 274.55it/s]
Adding requests:  77%|███████▋  | 6279/8192 [00:22<00:06, 276.03it/s]
Adding requests:  77%|███████▋  | 6308/8192 [00:22<00:06, 278.18it/s]
Adding requests:  77%|███████▋  | 6337/8192 [00:22<00:06, 279.23it/s]
Adding requests:  78%|███████▊  | 6365/8192 [00:22<00:06, 279.04it/s]
Adding requests:  78%|███████▊  | 6393/8192 [00:22<00:06, 278.73it/s]
Adding requests:  78%|███████▊  | 6421/8192 [00:23<00:06, 276.92it/s]
Adding requests:  79%|███████▊  | 6449/8192 [00:23<00:06, 277.34it/s]
Adding requests:  79%|███████▉  | 6478/8192 [00:23<00:06, 280.69it/s]
Adding requests:  79%|███████▉  | 6507/8192 [00:23<00:06, 278.90it/s]
Adding requests:  80%|███████▉  | 6535/8192 [00:23<00:06, 269.33it/s]
Adding requests:  80%|████████  | 6563/8192 [00:23<00:05, 272.30it/s]
Adding requests:  80%|████████  | 6591/8192 [00:23<00:05, 272.59it/s]
Adding requests:  81%|████████  | 6621/8192 [00:23<00:05, 279.04it/s]
Adding requests:  81%|████████  | 6650/8192 [00:23<00:05, 279.98it/s]
Adding requests:  82%|████████▏ | 6679/8192 [00:23<00:05, 273.84it/s]
Adding requests:  82%|████████▏ | 6708/8192 [00:24<00:05, 276.36it/s]
Adding requests:  82%|████████▏ | 6738/8192 [00:24<00:05, 281.96it/s]
Adding requests:  83%|████████▎ | 6768/8192 [00:24<00:04, 286.93it/s]
Adding requests:  83%|████████▎ | 6800/8192 [00:24<00:04, 294.52it/s]
Adding requests:  83%|████████▎ | 6830/8192 [00:24<00:04, 295.70it/s]
Adding requests:  84%|████████▎ | 6860/8192 [00:24<00:04, 290.48it/s]
Adding requests:  84%|████████▍ | 6890/8192 [00:24<00:04, 286.22it/s]
Adding requests:  84%|████████▍ | 6919/8192 [00:24<00:04, 282.65it/s]
Adding requests:  85%|████████▍ | 6949/8192 [00:24<00:04, 285.03it/s]
Adding requests:  85%|████████▌ | 6978/8192 [00:25<00:04, 278.37it/s]
Adding requests:  86%|████████▌ | 7006/8192 [00:25<00:04, 275.87it/s]
Adding requests:  86%|████████▌ | 7034/8192 [00:25<00:04, 276.75it/s]
Adding requests:  86%|████████▌ | 7063/8192 [00:25<00:04, 279.82it/s]
Adding requests:  87%|████████▋ | 7092/8192 [00:25<00:04, 272.03it/s]
Adding requests:  87%|████████▋ | 7120/8192 [00:25<00:04, 267.87it/s]
Adding requests:  87%|████████▋ | 7148/8192 [00:25<00:03, 269.37it/s]
Adding requests:  88%|████████▊ | 7176/8192 [00:25<00:03, 269.39it/s]
Adding requests:  88%|████████▊ | 7207/8192 [00:25<00:03, 280.93it/s]
Adding requests:  88%|████████▊ | 7236/8192 [00:25<00:03, 283.37it/s]
Adding requests:  89%|████████▊ | 7267/8192 [00:26<00:03, 288.48it/s]
Adding requests:  89%|████████▉ | 7296/8192 [00:26<00:03, 284.88it/s]
Adding requests:  89%|████████▉ | 7326/8192 [00:26<00:03, 273.22it/s]
Adding requests:  90%|████████▉ | 7356/8192 [00:26<00:02, 279.19it/s]
Adding requests:  90%|█████████ | 7386/8192 [00:26<00:02, 284.17it/s]
Adding requests:  91%|█████████ | 7417/8192 [00:26<00:02, 289.37it/s]
Adding requests:  91%|█████████ | 7447/8192 [00:26<00:02, 288.26it/s]
Adding requests:  91%|█████████▏| 7479/8192 [00:26<00:02, 296.14it/s]
Adding requests:  92%|█████████▏| 7512/8192 [00:26<00:02, 305.37it/s]
Adding requests:  92%|█████████▏| 7544/8192 [00:27<00:02, 309.26it/s]
Adding requests:  92%|█████████▏| 7575/8192 [00:27<00:02, 307.57it/s]
Adding requests:  93%|█████████▎| 7606/8192 [00:27<00:01, 305.77it/s]
Adding requests:  93%|█████████▎| 7638/8192 [00:27<00:01, 308.07it/s]
Adding requests:  94%|█████████▎| 7671/8192 [00:27<00:01, 312.38it/s]
Adding requests:  94%|█████████▍| 7704/8192 [00:27<00:01, 315.04it/s]
Adding requests:  94%|█████████▍| 7736/8192 [00:27<00:01, 309.34it/s]
Adding requests:  95%|█████████▍| 7767/8192 [00:27<00:01, 302.93it/s]
Adding requests:  95%|█████████▌| 7798/8192 [00:27<00:01, 295.47it/s]
Adding requests:  96%|█████████▌| 7830/8192 [00:27<00:01, 301.20it/s]
Adding requests:  96%|█████████▌| 7861/8192 [00:28<00:01, 299.99it/s]
Adding requests:  96%|█████████▋| 7892/8192 [00:28<00:01, 297.94it/s]
Adding requests:  97%|█████████▋| 7922/8192 [00:28<00:00, 294.50it/s]
Adding requests:  97%|█████████▋| 7952/8192 [00:28<00:00, 295.98it/s]
Adding requests:  97%|█████████▋| 7983/8192 [00:28<00:00, 299.63it/s]
Adding requests:  98%|█████████▊| 8013/8192 [00:28<00:00, 297.70it/s]
Adding requests:  98%|█████████▊| 8043/8192 [00:28<00:00, 298.13it/s]
Adding requests:  99%|█████████▊| 8074/8192 [00:28<00:00, 300.77it/s]
Adding requests:  99%|█████████▉| 8105/8192 [00:28<00:00, 297.57it/s]
Adding requests:  99%|█████████▉| 8135/8192 [00:28<00:00, 287.00it/s]
Adding requests: 100%|█████████▉| 8164/8192 [00:29<00:00, 281.21it/s]
Adding requests: 100%|██████████| 8192/8192 [00:29<00:00, 280.53it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 514/8192 [00:03<00:56, 135.67it/s, est. speed input: 138932.29 toks/s, output: 135.68 toks/s]
Processed prompts:   7%|▋         | 578/8192 [00:07<01:51, 68.42it/s, est. speed input: 80744.28 toks/s, output: 78.85 toks/s]   
Processed prompts:   8%|▊         | 642/8192 [00:11<02:50, 44.40it/s, est. speed input: 59131.69 toks/s, output: 57.75 toks/s]
Processed prompts:   9%|▊         | 706/8192 [00:14<03:43, 33.53it/s, est. speed input: 48620.92 toks/s, output: 47.48 toks/s]
Processed prompts:   9%|▉         | 770/8192 [00:18<04:29, 27.59it/s, est. speed input: 42364.56 toks/s, output: 41.37 toks/s]
Processed prompts:  10%|█         | 834/8192 [00:22<05:05, 24.07it/s, est. speed input: 38247.36 toks/s, output: 37.35 toks/s]
Processed prompts:  11%|█         | 898/8192 [00:26<05:33, 21.88it/s, est. speed input: 35333.37 toks/s, output: 34.51 toks/s]
Processed prompts:  12%|█▏        | 962/8192 [00:29<05:54, 20.37it/s, est. speed input: 33106.35 toks/s, output: 32.33 toks/s]
Processed prompts:  13%|█▎        | 1026/8192 [00:31<05:11, 22.98it/s, est. speed input: 33237.57 toks/s, output: 32.46 toks/s]
Processed prompts:  13%|█▎        | 1090/8192 [00:35<05:38, 20.95it/s, est. speed input: 31578.75 toks/s, output: 30.84 toks/s]
Processed prompts:  14%|█▍        | 1154/8192 [00:38<05:54, 19.84it/s, est. speed input: 30304.60 toks/s, output: 29.59 toks/s]
Processed prompts:  15%|█▍        | 1218/8192 [00:42<06:07, 18.95it/s, est. speed input: 29183.77 toks/s, output: 28.50 toks/s]
Processed prompts:  16%|█▌        | 1282/8192 [00:46<06:19, 18.23it/s, est. speed input: 28190.09 toks/s, output: 27.53 toks/s]
Processed prompts:  16%|█▋        | 1346/8192 [00:50<06:20, 17.97it/s, est. speed input: 27428.20 toks/s, output: 26.79 toks/s]
Processed prompts:  17%|█▋        | 1410/8192 [00:54<06:24, 17.63it/s, est. speed input: 26714.91 toks/s, output: 26.09 toks/s]
Processed prompts:  18%|█▊        | 1474/8192 [00:57<06:20, 17.65it/s, est. speed input: 26177.35 toks/s, output: 25.56 toks/s]
Processed prompts:  19%|█▉        | 1538/8192 [01:01<06:23, 17.37it/s, est. speed input: 25614.86 toks/s, output: 25.01 toks/s]
Processed prompts:  20%|█▉        | 1602/8192 [01:03<05:24, 20.30it/s, est. speed input: 25878.52 toks/s, output: 25.27 toks/s]
Processed prompts:  20%|██        | 1666/8192 [01:07<05:39, 19.22it/s, est. speed input: 25412.21 toks/s, output: 24.82 toks/s]
Processed prompts:  21%|██        | 1730/8192 [01:10<05:48, 18.56it/s, est. speed input: 25000.05 toks/s, output: 24.41 toks/s]
Processed prompts:  22%|██▏       | 1794/8192 [01:14<05:52, 18.17it/s, est. speed input: 24641.25 toks/s, output: 24.06 toks/s]
Processed prompts:  23%|██▎       | 1858/8192 [01:18<05:53, 17.93it/s, est. speed input: 24319.65 toks/s, output: 23.75 toks/s]
Processed prompts:  23%|██▎       | 1922/8192 [01:21<05:51, 17.84it/s, est. speed input: 24042.62 toks/s, output: 23.48 toks/s]
Processed prompts:  24%|██▍       | 1986/8192 [01:25<05:55, 17.48it/s, est. speed input: 23730.96 toks/s, output: 23.17 toks/s]
Processed prompts:  25%|██▌       | 2050/8192 [01:29<05:49, 17.57it/s, est. speed input: 23509.48 toks/s, output: 22.96 toks/s]
Processed prompts:  26%|██▌       | 2114/8192 [01:33<05:50, 17.32it/s, est. speed input: 23249.03 toks/s, output: 22.70 toks/s]
Processed prompts:  27%|██▋       | 2178/8192 [01:35<05:03, 19.81it/s, est. speed input: 23413.24 toks/s, output: 22.86 toks/s]
Processed prompts:  27%|██▋       | 2242/8192 [01:38<05:13, 18.95it/s, est. speed input: 23195.30 toks/s, output: 22.65 toks/s]
Processed prompts:  28%|██▊       | 2306/8192 [01:42<05:21, 18.30it/s, est. speed input: 22981.08 toks/s, output: 22.44 toks/s]
Processed prompts:  29%|██▉       | 2370/8192 [01:46<05:22, 18.03it/s, est. speed input: 22803.57 toks/s, output: 22.27 toks/s]
Processed prompts:  30%|██▉       | 2434/8192 [01:50<05:26, 17.64it/s, est. speed input: 22610.34 toks/s, output: 22.08 toks/s]
Processed prompts:  30%|███       | 2498/8192 [01:53<05:19, 17.80it/s, est. speed input: 22486.54 toks/s, output: 21.96 toks/s]
Processed prompts:  31%|███▏      | 2562/8192 [01:57<05:22, 17.47it/s, est. speed input: 22312.71 toks/s, output: 21.79 toks/s]
Processed prompts:  32%|███▏      | 2626/8192 [02:01<05:16, 17.60it/s, est. speed input: 22196.32 toks/s, output: 21.68 toks/s]
Processed prompts:  33%|███▎      | 2690/8192 [02:03<04:34, 20.04it/s, est. speed input: 22338.44 toks/s, output: 21.81 toks/s]
Processed prompts:  34%|███▎      | 2754/8192 [02:07<04:45, 19.07it/s, est. speed input: 22197.84 toks/s, output: 21.68 toks/s]
Processed prompts:  34%|███▍      | 2818/8192 [02:10<04:50, 18.53it/s, est. speed input: 22073.73 toks/s, output: 21.56 toks/s]
Processed prompts:  35%|███▌      | 2882/8192 [02:14<04:55, 17.97it/s, est. speed input: 21935.41 toks/s, output: 21.42 toks/s]
Processed prompts:  36%|███▌      | 2946/8192 [02:18<04:53, 17.86it/s, est. speed input: 21832.62 toks/s, output: 21.32 toks/s]
Processed prompts:  37%|███▋      | 3010/8192 [02:22<04:56, 17.50it/s, est. speed input: 21705.78 toks/s, output: 21.20 toks/s]
Processed prompts:  38%|███▊      | 3074/8192 [02:25<04:50, 17.61it/s, est. speed input: 21621.71 toks/s, output: 21.11 toks/s]
Processed prompts:  38%|███▊      | 3138/8192 [02:29<04:51, 17.34it/s, est. speed input: 21506.96 toks/s, output: 21.00 toks/s]
Processed prompts:  39%|███▉      | 3202/8192 [02:32<04:44, 17.54it/s, est. speed input: 21436.61 toks/s, output: 20.93 toks/s]
Processed prompts:  40%|███▉      | 3266/8192 [02:35<04:06, 19.95it/s, est. speed input: 21557.43 toks/s, output: 21.05 toks/s]
Processed prompts:  41%|████      | 3330/8192 [02:38<04:16, 18.94it/s, est. speed input: 21457.28 toks/s, output: 20.95 toks/s]
Processed prompts:  41%|████▏     | 3394/8192 [02:42<04:19, 18.50it/s, est. speed input: 21379.45 toks/s, output: 20.88 toks/s]
Processed prompts:  42%|████▏     | 3458/8192 [02:46<04:23, 17.94it/s, est. speed input: 21282.15 toks/s, output: 20.78 toks/s]
Processed prompts:  43%|████▎     | 3522/8192 [02:50<04:21, 17.84it/s, est. speed input: 21213.06 toks/s, output: 20.72 toks/s]
Processed prompts:  44%|████▍     | 3586/8192 [02:53<04:23, 17.49it/s, est. speed input: 21122.89 toks/s, output: 20.63 toks/s]
Processed prompts:  45%|████▍     | 3650/8192 [02:57<04:17, 17.61it/s, est. speed input: 21066.81 toks/s, output: 20.57 toks/s]
Processed prompts:  45%|████▌     | 3714/8192 [03:01<04:18, 17.34it/s, est. speed input: 20983.89 toks/s, output: 20.49 toks/s]
Processed prompts:  46%|████▌     | 3778/8192 [03:04<04:12, 17.45it/s, est. speed input: 20928.23 toks/s, output: 20.44 toks/s]
Processed prompts:  47%|████▋     | 3842/8192 [03:06<03:38, 19.94it/s, est. speed input: 21038.80 toks/s, output: 20.55 toks/s]
Processed prompts:  48%|████▊     | 3906/8192 [03:10<03:45, 18.98it/s, est. speed input: 20968.98 toks/s, output: 20.48 toks/s]
Processed prompts:  48%|████▊     | 3970/8192 [03:14<03:46, 18.60it/s, est. speed input: 20917.64 toks/s, output: 20.43 toks/s]
Processed prompts:  49%|████▉     | 4034/8192 [03:18<03:48, 18.16it/s, est. speed input: 20855.70 toks/s, output: 20.37 toks/s]
Processed prompts:  50%|█████     | 4098/8192 [03:21<03:48, 17.94it/s, est. speed input: 20801.44 toks/s, output: 20.31 toks/s]
Processed prompts:  51%|█████     | 4162/8192 [03:25<03:49, 17.56it/s, est. speed input: 20733.28 toks/s, output: 20.25 toks/s]
Processed prompts:  52%|█████▏    | 4226/8192 [03:29<03:43, 17.75it/s, est. speed input: 20698.19 toks/s, output: 20.21 toks/s]
Processed prompts:  52%|█████▏    | 4290/8192 [03:32<03:43, 17.50it/s, est. speed input: 20638.53 toks/s, output: 20.15 toks/s]
Processed prompts:  53%|█████▎    | 4354/8192 [03:36<03:39, 17.52it/s, est. speed input: 20594.29 toks/s, output: 20.11 toks/s]
Processed prompts:  54%|█████▍    | 4418/8192 [03:38<03:04, 20.48it/s, est. speed input: 20715.54 toks/s, output: 20.23 toks/s]
Processed prompts:  55%|█████▍    | 4482/8192 [03:42<03:13, 19.19it/s, est. speed input: 20653.96 toks/s, output: 20.17 toks/s]
Processed prompts:  55%|█████▌    | 4546/8192 [03:45<03:14, 18.75it/s, est. speed input: 20615.20 toks/s, output: 20.13 toks/s]
Processed prompts:  56%|█████▋    | 4610/8192 [03:49<03:17, 18.10it/s, est. speed input: 20557.47 toks/s, output: 20.08 toks/s]
Processed prompts:  57%|█████▋    | 4674/8192 [03:53<03:15, 17.95it/s, est. speed input: 20518.12 toks/s, output: 20.04 toks/s]
Processed prompts:  58%|█████▊    | 4738/8192 [03:57<03:16, 17.62it/s, est. speed input: 20466.49 toks/s, output: 19.99 toks/s]
Processed prompts:  59%|█████▊    | 4802/8192 [04:00<03:13, 17.56it/s, est. speed input: 20426.47 toks/s, output: 19.95 toks/s]
Processed prompts:  59%|█████▉    | 4866/8192 [04:04<03:11, 17.41it/s, est. speed input: 20381.43 toks/s, output: 19.90 toks/s]
Processed prompts:  60%|██████    | 4930/8192 [04:06<02:43, 19.97it/s, est. speed input: 20473.26 toks/s, output: 19.99 toks/s]
Processed prompts:  61%|██████    | 4994/8192 [04:10<02:43, 19.52it/s, est. speed input: 20452.73 toks/s, output: 19.97 toks/s]
Processed prompts:  62%|██████▏   | 5058/8192 [04:13<02:48, 18.57it/s, est. speed input: 20401.91 toks/s, output: 19.92 toks/s]
Processed prompts:  63%|██████▎   | 5122/8192 [04:17<02:48, 18.26it/s, est. speed input: 20367.94 toks/s, output: 19.89 toks/s]
Processed prompts:  63%|██████▎   | 5186/8192 [04:21<02:48, 17.81it/s, est. speed input: 20322.43 toks/s, output: 19.85 toks/s]
Processed prompts:  64%|██████▍   | 5250/8192 [04:24<02:46, 17.68it/s, est. speed input: 20287.63 toks/s, output: 19.81 toks/s]
Processed prompts:  65%|██████▍   | 5314/8192 [04:28<02:44, 17.49it/s, est. speed input: 20248.01 toks/s, output: 19.77 toks/s]
Processed prompts:  66%|██████▌   | 5378/8192 [04:32<02:41, 17.42it/s, est. speed input: 20213.19 toks/s, output: 19.74 toks/s]
Processed prompts:  66%|██████▋   | 5442/8192 [04:36<02:38, 17.37it/s, est. speed input: 20179.11 toks/s, output: 19.71 toks/s]
Processed prompts:  67%|██████▋   | 5506/8192 [04:38<02:16, 19.67it/s, est. speed input: 20251.34 toks/s, output: 19.78 toks/s]
Processed prompts:  68%|██████▊   | 5570/8192 [04:41<02:17, 19.08it/s, est. speed input: 20225.96 toks/s, output: 19.75 toks/s]
Processed prompts:  69%|██████▉   | 5634/8192 [04:45<02:19, 18.29it/s, est. speed input: 20183.98 toks/s, output: 19.71 toks/s]
Processed prompts:  70%|██████▉   | 5698/8192 [04:49<02:18, 18.04it/s, est. speed input: 20155.15 toks/s, output: 19.68 toks/s]
Processed prompts:  70%|███████   | 5762/8192 [04:53<02:17, 17.68it/s, est. speed input: 20118.28 toks/s, output: 19.65 toks/s]
Processed prompts:  71%|███████   | 5826/8192 [04:56<02:14, 17.58it/s, est. speed input: 20088.85 toks/s, output: 19.62 toks/s]
Processed prompts:  72%|███████▏  | 5890/8192 [05:00<02:11, 17.45it/s, est. speed input: 20057.66 toks/s, output: 19.59 toks/s]
Processed prompts:  73%|███████▎  | 5954/8192 [05:04<02:08, 17.38it/s, est. speed input: 20027.90 toks/s, output: 19.56 toks/s]
Processed prompts:  73%|███████▎  | 6018/8192 [05:08<02:05, 17.36it/s, est. speed input: 20000.18 toks/s, output: 19.53 toks/s]
Processed prompts:  74%|███████▍  | 6082/8192 [05:10<01:46, 19.83it/s, est. speed input: 20072.67 toks/s, output: 19.60 toks/s]
Processed prompts:  75%|███████▌  | 6146/8192 [05:13<01:46, 19.14it/s, est. speed input: 20050.38 toks/s, output: 19.58 toks/s]
Processed prompts:  76%|███████▌  | 6210/8192 [05:17<01:47, 18.38it/s, est. speed input: 20016.58 toks/s, output: 19.55 toks/s]
Processed prompts:  77%|███████▋  | 6274/8192 [05:21<01:46, 18.04it/s, est. speed input: 19989.77 toks/s, output: 19.52 toks/s]
Processed prompts:  77%|███████▋  | 6338/8192 [05:25<01:44, 17.72it/s, est. speed input: 19960.35 toks/s, output: 19.49 toks/s]
Processed prompts:  78%|███████▊  | 6402/8192 [05:28<01:41, 17.58it/s, est. speed input: 19934.67 toks/s, output: 19.47 toks/s]
Processed prompts:  79%|███████▉  | 6466/8192 [05:32<01:38, 17.50it/s, est. speed input: 19910.01 toks/s, output: 19.44 toks/s]
Processed prompts:  80%|███████▉  | 6530/8192 [05:36<01:35, 17.38it/s, est. speed input: 19883.42 toks/s, output: 19.42 toks/s]
Processed prompts:  80%|████████  | 6594/8192 [05:39<01:31, 17.56it/s, est. speed input: 19868.17 toks/s, output: 19.40 toks/s]
Processed prompts:  81%|████████▏ | 6658/8192 [05:41<01:15, 20.21it/s, est. speed input: 19940.50 toks/s, output: 19.47 toks/s]
Processed prompts:  82%|████████▏ | 6722/8192 [05:45<01:16, 19.34it/s, est. speed input: 19919.96 toks/s, output: 19.45 toks/s]
Processed prompts:  83%|████████▎ | 6786/8192 [05:49<01:15, 18.59it/s, est. speed input: 19893.68 toks/s, output: 19.43 toks/s]
Processed prompts:  84%|████████▎ | 6850/8192 [05:53<01:14, 18.04it/s, est. speed input: 19865.61 toks/s, output: 19.40 toks/s]
Processed prompts:  84%|████████▍ | 6914/8192 [05:56<01:11, 17.81it/s, est. speed input: 19843.20 toks/s, output: 19.38 toks/s]
Processed prompts:  85%|████████▌ | 6978/8192 [06:00<01:09, 17.57it/s, est. speed input: 19818.16 toks/s, output: 19.35 toks/s]
Processed prompts:  86%|████████▌ | 7042/8192 [06:04<01:05, 17.56it/s, est. speed input: 19799.52 toks/s, output: 19.34 toks/s]
Processed prompts:  87%|████████▋ | 7106/8192 [06:07<01:02, 17.42it/s, est. speed input: 19776.20 toks/s, output: 19.31 toks/s]
Processed prompts:  88%|████████▊ | 7170/8192 [06:09<00:50, 20.22it/s, est. speed input: 19847.69 toks/s, output: 19.38 toks/s]
Processed prompts:  88%|████████▊ | 7234/8192 [06:13<00:50, 19.04it/s, est. speed input: 19820.08 toks/s, output: 19.36 toks/s]
Processed prompts:  89%|████████▉ | 7298/8192 [06:17<00:48, 18.54it/s, est. speed input: 19801.58 toks/s, output: 19.34 toks/s]
Processed prompts:  90%|████████▉ | 7362/8192 [06:21<00:45, 18.13it/s, est. speed input: 19780.47 toks/s, output: 19.32 toks/s]
Processed prompts:  91%|█████████ | 7426/8192 [06:24<00:43, 17.68it/s, est. speed input: 19753.97 toks/s, output: 19.29 toks/s]
Processed prompts:  91%|█████████▏| 7490/8192 [06:28<00:39, 17.59it/s, est. speed input: 19735.39 toks/s, output: 19.27 toks/s]
Processed prompts:  92%|█████████▏| 7554/8192 [06:32<00:36, 17.38it/s, est. speed input: 19712.17 toks/s, output: 19.25 toks/s]
Processed prompts:  93%|█████████▎| 7618/8192 [06:36<00:32, 17.50it/s, est. speed input: 19698.34 toks/s, output: 19.24 toks/s]
Processed prompts:  94%|█████████▍| 7682/8192 [06:39<00:29, 17.26it/s, est. speed input: 19673.93 toks/s, output: 19.21 toks/s]
Processed prompts:  95%|█████████▍| 7746/8192 [06:41<00:21, 20.37it/s, est. speed input: 19747.94 toks/s, output: 19.29 toks/s]
Processed prompts:  95%|█████████▌| 7810/8192 [06:45<00:19, 19.31it/s, est. speed input: 19728.54 toks/s, output: 19.27 toks/s]
Processed prompts:  96%|█████████▌| 7874/8192 [06:49<00:17, 18.58it/s, est. speed input: 19707.81 toks/s, output: 19.25 toks/s]
Processed prompts:  97%|█████████▋| 7938/8192 [06:52<00:13, 18.22it/s, est. speed input: 19691.38 toks/s, output: 19.23 toks/s]
Processed prompts:  98%|█████████▊| 8002/8192 [06:56<00:10, 17.76it/s, est. speed input: 19668.16 toks/s, output: 19.21 toks/s]
Processed prompts:  98%|█████████▊| 8066/8192 [07:00<00:07, 17.88it/s, est. speed input: 19659.33 toks/s, output: 19.20 toks/s]
Processed prompts:  99%|█████████▉| 8130/8192 [07:03<00:03, 17.69it/s, est. speed input: 19641.98 toks/s, output: 19.18 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [07:03<00:00, 17.69it/s, est. speed input: 19791.72 toks/s, output: 19.33 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [07:03<00:00, 19.33it/s, est. speed input: 19791.72 toks/s, output: 19.33 toks/s]
[rank0]:[W125 22:51:39.870910724 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 04:20:27
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:20:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:20:35 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=167672) WARNING 01-26 04:20:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=167672) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=167672) WARNING 01-26 04:21:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.11 requests/s, 8263.47 total tokens/s, 16.11 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 04:20:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:20:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:20:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:20:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:20:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:20:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:20:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:20:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:20:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:20:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:20:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:20:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:20:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:20:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:20:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:20:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:20:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:43] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=167672) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=167672) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.91s/it]
(EngineCore_DP0 pid=167672) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.64s/it]
(EngineCore_DP0 pid=167672) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.53s/it]
(EngineCore_DP0 pid=167672) 
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=167672) [2026-01-26 04:20:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=167672) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.47it/s]
(EngineCore_DP0 pid=167672) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  35%|███▌      | 45/128 [00:00<00:00, 445.91it/s]
Adding requests:  72%|███████▏  | 92/128 [00:00<00:00, 455.61it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 460.67it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:17,  7.46it/s, est. speed input: 3821.85 toks/s, output: 7.46 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:15,  8.29it/s, est. speed input: 4174.20 toks/s, output: 8.15 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:11, 10.83it/s, est. speed input: 5168.50 toks/s, output: 10.09 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:09, 13.03it/s, est. speed input: 5965.85 toks/s, output: 11.65 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:08, 14.27it/s, est. speed input: 6448.07 toks/s, output: 12.59 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:07, 15.10it/s, est. speed input: 6786.31 toks/s, output: 13.25 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:07, 15.63it/s, est. speed input: 7031.50 toks/s, output: 13.73 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:07, 15.99it/s, est. speed input: 7219.55 toks/s, output: 14.10 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:01<00:06, 16.29it/s, est. speed input: 7375.08 toks/s, output: 14.40 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:06, 16.47it/s, est. speed input: 7497.24 toks/s, output: 14.64 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 16.54it/s, est. speed input: 7590.79 toks/s, output: 14.83 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 16.59it/s, est. speed input: 7669.32 toks/s, output: 14.98 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 16.72it/s, est. speed input: 7746.32 toks/s, output: 15.13 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:06, 16.50it/s, est. speed input: 7779.30 toks/s, output: 15.19 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:06, 16.66it/s, est. speed input: 7840.48 toks/s, output: 15.31 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 16.71it/s, est. speed input: 7887.31 toks/s, output: 15.40 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:02<00:05, 16.80it/s, est. speed input: 7934.55 toks/s, output: 15.50 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:02<00:05, 16.93it/s, est. speed input: 7981.64 toks/s, output: 15.59 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:05, 17.04it/s, est. speed input: 8026.17 toks/s, output: 15.68 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 17.00it/s, est. speed input: 8057.18 toks/s, output: 15.74 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 17.02it/s, est. speed input: 8088.83 toks/s, output: 15.80 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 17.02it/s, est. speed input: 8116.16 toks/s, output: 15.85 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:04, 16.91it/s, est. speed input: 8134.44 toks/s, output: 15.89 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 16.95it/s, est. speed input: 8158.18 toks/s, output: 15.93 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:03<00:04, 17.02it/s, est. speed input: 8183.26 toks/s, output: 15.98 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:03<00:04, 17.05it/s, est. speed input: 8204.83 toks/s, output: 16.02 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:04, 17.13it/s, est. speed input: 8228.78 toks/s, output: 16.07 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 17.18it/s, est. speed input: 8250.19 toks/s, output: 16.11 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 17.22it/s, est. speed input: 8271.00 toks/s, output: 16.15 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 17.23it/s, est. speed input: 8289.14 toks/s, output: 16.19 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:03, 17.09it/s, est. speed input: 8298.64 toks/s, output: 16.21 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 16.92it/s, est. speed input: 8304.15 toks/s, output: 16.22 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 16.88it/s, est. speed input: 8312.98 toks/s, output: 16.24 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:04<00:03, 16.82it/s, est. speed input: 8319.75 toks/s, output: 16.25 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:03, 16.85it/s, est. speed input: 8329.31 toks/s, output: 16.27 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 16.89it/s, est. speed input: 8339.37 toks/s, output: 16.29 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 16.88it/s, est. speed input: 8347.06 toks/s, output: 16.30 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 16.80it/s, est. speed input: 8351.34 toks/s, output: 16.31 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 16.83it/s, est. speed input: 8358.88 toks/s, output: 16.33 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:02, 16.91it/s, est. speed input: 8368.73 toks/s, output: 16.35 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 16.87it/s, est. speed input: 8374.22 toks/s, output: 16.36 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:05<00:02, 16.92it/s, est. speed input: 8382.26 toks/s, output: 16.37 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:05<00:02, 17.01it/s, est. speed input: 8392.14 toks/s, output: 16.39 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 17.09it/s, est. speed input: 8402.29 toks/s, output: 16.41 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 17.13it/s, est. speed input: 8411.29 toks/s, output: 16.43 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 17.15it/s, est. speed input: 8419.80 toks/s, output: 16.44 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 17.16it/s, est. speed input: 8427.59 toks/s, output: 16.46 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:01, 17.15it/s, est. speed input: 8434.50 toks/s, output: 16.47 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 17.22it/s, est. speed input: 8443.88 toks/s, output: 16.49 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 17.19it/s, est. speed input: 8450.08 toks/s, output: 16.50 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:06<00:01, 17.23it/s, est. speed input: 8458.06 toks/s, output: 16.52 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:06<00:01, 17.27it/s, est. speed input: 8466.13 toks/s, output: 16.54 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 17.39it/s, est. speed input: 8476.70 toks/s, output: 16.56 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 17.48it/s, est. speed input: 8487.02 toks/s, output: 16.58 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 17.48it/s, est. speed input: 8495.22 toks/s, output: 16.59 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 17.38it/s, est. speed input: 8500.27 toks/s, output: 16.60 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:00, 17.40it/s, est. speed input: 8507.57 toks/s, output: 16.62 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 17.45it/s, est. speed input: 8515.62 toks/s, output: 16.63 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:06<00:00, 17.36it/s, est. speed input: 8519.99 toks/s, output: 16.64 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:07<00:00, 17.32it/s, est. speed input: 8524.93 toks/s, output: 16.65 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:07<00:00, 17.31it/s, est. speed input: 8530.26 toks/s, output: 16.66 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 17.30it/s, est. speed input: 8535.20 toks/s, output: 16.67 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 17.32it/s, est. speed input: 8540.82 toks/s, output: 16.68 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 17.31it/s, est. speed input: 8545.56 toks/s, output: 16.69 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.22it/s, est. speed input: 8548.10 toks/s, output: 16.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.22it/s, est. speed input: 8548.10 toks/s, output: 16.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.69it/s, est. speed input: 8548.10 toks/s, output: 16.70 toks/s]
[rank0]:[W126 04:21:28.222630852 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 04:21:30
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:21:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:21:39 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=168856) WARNING 01-26 04:21:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=168856) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=168856) WARNING 01-26 04:22:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.15 requests/s, 16556.07 total tokens/s, 16.15 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 04:21:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:21:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:21:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:21:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:21:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:21:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:21:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:21:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:21:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:21:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:21:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:21:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:21:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:21:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:21:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:21:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:21:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:47] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=168856) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=168856) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.85s/it]
(EngineCore_DP0 pid=168856) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.44s/it]
(EngineCore_DP0 pid=168856) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.35s/it]
(EngineCore_DP0 pid=168856) 
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=168856) [2026-01-26 04:21:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=168856) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.35it/s]
(EngineCore_DP0 pid=168856) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  20%|█▉        | 25/128 [00:00<00:00, 238.61it/s]
Adding requests:  39%|███▉      | 50/128 [00:00<00:00, 240.35it/s]
Adding requests:  61%|██████    | 78/128 [00:00<00:00, 255.51it/s]
Adding requests:  82%|████████▏ | 105/128 [00:00<00:00, 259.78it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 254.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 38.05it/s, est. speed input: 38976.51 toks/s, output: 38.06 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:05, 21.99it/s, est. speed input: 24037.77 toks/s, output: 23.47 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:05, 19.74it/s, est. speed input: 21767.40 toks/s, output: 21.26 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 18.84it/s, est. speed input: 20782.09 toks/s, output: 20.29 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 18.44it/s, est. speed input: 20349.31 toks/s, output: 19.87 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:06, 18.12it/s, est. speed input: 20021.26 toks/s, output: 19.55 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 17.90it/s, est. speed input: 19767.54 toks/s, output: 19.30 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 17.63it/s, est. speed input: 19523.59 toks/s, output: 19.06 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:05, 17.50it/s, est. speed input: 19348.13 toks/s, output: 18.89 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 17.39it/s, est. speed input: 19194.48 toks/s, output: 18.74 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 17.21it/s, est. speed input: 19037.32 toks/s, output: 18.59 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 17.11it/s, est. speed input: 18909.42 toks/s, output: 18.47 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 17.04it/s, est. speed input: 18800.01 toks/s, output: 18.36 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:05, 17.03it/s, est. speed input: 18712.10 toks/s, output: 18.27 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:05, 17.02it/s, est. speed input: 18635.67 toks/s, output: 18.20 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 17.00it/s, est. speed input: 18562.79 toks/s, output: 18.13 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 17.02it/s, est. speed input: 18505.85 toks/s, output: 18.07 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 17.12it/s, est. speed input: 18469.07 toks/s, output: 18.04 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:04, 17.15it/s, est. speed input: 18428.51 toks/s, output: 18.00 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 17.05it/s, est. speed input: 18372.72 toks/s, output: 17.94 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 17.08it/s, est. speed input: 18338.45 toks/s, output: 17.91 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 17.11it/s, est. speed input: 18308.09 toks/s, output: 17.88 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:04, 17.16it/s, est. speed input: 18281.78 toks/s, output: 17.85 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 17.23it/s, est. speed input: 18265.13 toks/s, output: 17.84 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 17.18it/s, est. speed input: 18236.39 toks/s, output: 17.81 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 17.12it/s, est. speed input: 18204.42 toks/s, output: 17.78 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:03, 17.15it/s, est. speed input: 18184.23 toks/s, output: 17.76 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 17.15it/s, est. speed input: 18163.74 toks/s, output: 17.74 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 17.09it/s, est. speed input: 18137.18 toks/s, output: 17.71 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 17.05it/s, est. speed input: 18113.54 toks/s, output: 17.69 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 16.98it/s, est. speed input: 18085.79 toks/s, output: 17.66 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:03, 17.04it/s, est. speed input: 18071.62 toks/s, output: 17.65 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 17.10it/s, est. speed input: 18058.97 toks/s, output: 17.64 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 17.14it/s, est. speed input: 18047.81 toks/s, output: 17.62 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 17.08it/s, est. speed input: 18028.29 toks/s, output: 17.61 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:02, 16.98it/s, est. speed input: 18004.97 toks/s, output: 17.58 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 16.84it/s, est. speed input: 17976.37 toks/s, output: 17.55 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 16.78it/s, est. speed input: 17952.22 toks/s, output: 17.53 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:04<00:02, 16.78it/s, est. speed input: 17933.25 toks/s, output: 17.51 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:04<00:02, 16.79it/s, est. speed input: 17915.45 toks/s, output: 17.50 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 16.77it/s, est. speed input: 17896.98 toks/s, output: 17.48 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 16.79it/s, est. speed input: 17881.94 toks/s, output: 17.46 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 16.76it/s, est. speed input: 17863.89 toks/s, output: 17.45 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:02, 16.86it/s, est. speed input: 17855.70 toks/s, output: 17.44 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 16.90it/s, est. speed input: 17845.99 toks/s, output: 17.43 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 17.01it/s, est. speed input: 17842.62 toks/s, output: 17.42 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:05<00:01, 17.11it/s, est. speed input: 17841.79 toks/s, output: 17.42 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:05<00:01, 17.07it/s, est. speed input: 17832.16 toks/s, output: 17.41 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:05<00:01, 16.98it/s, est. speed input: 17818.74 toks/s, output: 17.40 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 16.89it/s, est. speed input: 17804.73 toks/s, output: 17.39 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 16.76it/s, est. speed input: 17786.53 toks/s, output: 17.37 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 16.70it/s, est. speed input: 17770.36 toks/s, output: 17.35 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:00, 16.59it/s, est. speed input: 17750.82 toks/s, output: 17.33 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 16.46it/s, est. speed input: 17728.19 toks/s, output: 17.31 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:06<00:00, 16.42it/s, est. speed input: 17709.67 toks/s, output: 17.29 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:06<00:00, 16.54it/s, est. speed input: 17701.26 toks/s, output: 17.29 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:06<00:00, 16.60it/s, est. speed input: 17691.91 toks/s, output: 17.28 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 16.69it/s, est. speed input: 17685.35 toks/s, output: 17.27 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 16.77it/s, est. speed input: 17680.11 toks/s, output: 17.27 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 16.83it/s, est. speed input: 17675.22 toks/s, output: 17.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.77it/s, est. speed input: 17664.84 toks/s, output: 17.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.77it/s, est. speed input: 17664.84 toks/s, output: 17.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.25it/s, est. speed input: 17664.84 toks/s, output: 17.25 toks/s]
[rank0]:[W126 04:22:30.137095441 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 04:22:33
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:22:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:22:42 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=170008) WARNING 01-26 04:22:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=170008) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=170008) WARNING 01-26 04:23:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.87 requests/s, 18320.29 total tokens/s, 17.87 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 04:22:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:22:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:22:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:22:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:22:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:22:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:22:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:22:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:22:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:22:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:22:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:22:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:22:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:22:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:22:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:22:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:22:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=170008) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=170008) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.06it/s]
(EngineCore_DP0 pid=170008) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]
(EngineCore_DP0 pid=170008) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.18s/it]
(EngineCore_DP0 pid=170008) 
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=170008) [2026-01-26 04:22:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=170008) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.20it/s]
(EngineCore_DP0 pid=170008) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.18it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   9%|▉         | 23/256 [00:00<00:01, 228.27it/s]
Adding requests:  19%|█▉        | 48/256 [00:00<00:00, 234.07it/s]
Adding requests:  29%|██▉       | 74/256 [00:00<00:00, 245.29it/s]
Adding requests:  39%|███▊      | 99/256 [00:00<00:00, 242.02it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 238.51it/s]
Adding requests:  58%|█████▊    | 149/256 [00:00<00:00, 239.26it/s]
Adding requests:  69%|██████▉   | 176/256 [00:00<00:00, 246.57it/s]
Adding requests:  79%|███████▊  | 201/256 [00:00<00:00, 247.30it/s]
Adding requests:  89%|████████▉ | 228/256 [00:00<00:00, 252.20it/s]
Adding requests:  99%|█████████▉| 254/256 [00:01<00:00, 249.24it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 244.89it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:03, 76.90it/s, est. speed input: 78759.32 toks/s, output: 76.91 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:00<00:07, 32.92it/s, est. speed input: 38068.29 toks/s, output: 37.17 toks/s]
Processed prompts:  11%|█▏        | 29/256 [00:00<00:07, 29.43it/s, est. speed input: 34368.95 toks/s, output: 33.56 toks/s]
Processed prompts:  13%|█▎        | 33/256 [00:01<00:08, 25.71it/s, est. speed input: 31199.20 toks/s, output: 30.47 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:01<00:10, 21.84it/s, est. speed input: 28312.50 toks/s, output: 27.65 toks/s]
Processed prompts:  15%|█▌        | 39/256 [00:01<00:09, 22.89it/s, est. speed input: 28298.30 toks/s, output: 27.63 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:01<00:10, 19.68it/s, est. speed input: 26379.42 toks/s, output: 25.76 toks/s]
Processed prompts:  18%|█▊        | 45/256 [00:01<00:09, 21.22it/s, est. speed input: 26485.63 toks/s, output: 25.86 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:01<00:11, 18.49it/s, est. speed input: 25093.29 toks/s, output: 24.50 toks/s]
Processed prompts:  20%|█▉        | 51/256 [00:02<00:10, 20.32it/s, est. speed input: 25248.42 toks/s, output: 24.66 toks/s]
Processed prompts:  21%|██        | 54/256 [00:02<00:11, 17.87it/s, est. speed input: 24174.84 toks/s, output: 23.61 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:02<00:11, 17.96it/s, est. speed input: 23925.51 toks/s, output: 23.36 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:02<00:10, 18.04it/s, est. speed input: 23700.93 toks/s, output: 23.14 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:02<00:10, 18.11it/s, est. speed input: 23493.67 toks/s, output: 22.94 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:02<00:10, 18.15it/s, est. speed input: 23301.94 toks/s, output: 22.76 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:02<00:10, 18.20it/s, est. speed input: 23126.54 toks/s, output: 22.58 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:02<00:10, 18.23it/s, est. speed input: 22964.00 toks/s, output: 22.43 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:03<00:10, 18.24it/s, est. speed input: 22812.11 toks/s, output: 22.28 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:03<00:10, 18.25it/s, est. speed input: 22669.47 toks/s, output: 22.14 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:03<00:10, 18.27it/s, est. speed input: 22538.46 toks/s, output: 22.01 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:03<00:09, 18.29it/s, est. speed input: 22417.35 toks/s, output: 21.89 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:03<00:09, 18.30it/s, est. speed input: 22303.30 toks/s, output: 21.78 toks/s]
Processed prompts:  30%|███       | 78/256 [00:03<00:09, 18.30it/s, est. speed input: 22195.11 toks/s, output: 21.67 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:03<00:09, 18.28it/s, est. speed input: 22090.40 toks/s, output: 21.57 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:03<00:09, 18.28it/s, est. speed input: 21994.31 toks/s, output: 21.48 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:03<00:09, 18.29it/s, est. speed input: 21904.08 toks/s, output: 21.39 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:04<00:09, 18.29it/s, est. speed input: 21817.64 toks/s, output: 21.31 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:04<00:09, 18.26it/s, est. speed input: 21733.67 toks/s, output: 21.22 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:04<00:09, 18.26it/s, est. speed input: 21654.87 toks/s, output: 21.15 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:04<00:08, 18.28it/s, est. speed input: 21582.53 toks/s, output: 21.08 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:04<00:08, 18.28it/s, est. speed input: 21512.79 toks/s, output: 21.01 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:04<00:08, 18.29it/s, est. speed input: 21446.64 toks/s, output: 20.94 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:04<00:08, 18.29it/s, est. speed input: 21383.58 toks/s, output: 20.88 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:04<00:08, 18.28it/s, est. speed input: 21322.01 toks/s, output: 20.82 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:04<00:08, 18.28it/s, est. speed input: 21263.93 toks/s, output: 20.77 toks/s]
Processed prompts:  41%|████      | 104/256 [00:05<00:08, 18.27it/s, est. speed input: 21207.51 toks/s, output: 20.71 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:05<00:08, 18.27it/s, est. speed input: 21154.22 toks/s, output: 20.66 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:05<00:08, 18.26it/s, est. speed input: 21102.72 toks/s, output: 20.61 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:05<00:07, 18.26it/s, est. speed input: 21053.59 toks/s, output: 20.56 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:05<00:07, 18.26it/s, est. speed input: 21006.04 toks/s, output: 20.51 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:05<00:07, 18.25it/s, est. speed input: 20959.61 toks/s, output: 20.47 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:05<00:07, 18.25it/s, est. speed input: 20916.16 toks/s, output: 20.43 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:05<00:07, 18.25it/s, est. speed input: 20873.91 toks/s, output: 20.38 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:05<00:07, 18.25it/s, est. speed input: 20833.32 toks/s, output: 20.34 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:06<00:07, 18.26it/s, est. speed input: 20794.86 toks/s, output: 20.31 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:06<00:07, 18.27it/s, est. speed input: 20757.79 toks/s, output: 20.27 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:06<00:07, 18.28it/s, est. speed input: 20722.31 toks/s, output: 20.24 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:06<00:06, 18.29it/s, est. speed input: 20688.59 toks/s, output: 20.20 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:06<00:06, 18.28it/s, est. speed input: 20654.75 toks/s, output: 20.17 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:06<00:06, 18.28it/s, est. speed input: 20622.27 toks/s, output: 20.14 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:06<00:06, 18.29it/s, est. speed input: 20591.86 toks/s, output: 20.11 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:06<00:06, 18.30it/s, est. speed input: 20562.15 toks/s, output: 20.08 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:06<00:06, 18.29it/s, est. speed input: 20532.93 toks/s, output: 20.05 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:06<00:06, 18.28it/s, est. speed input: 20503.91 toks/s, output: 20.02 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:07<00:06, 18.28it/s, est. speed input: 20476.60 toks/s, output: 20.00 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:07<00:06, 18.30it/s, est. speed input: 20450.72 toks/s, output: 19.97 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:07<00:06, 18.31it/s, est. speed input: 20425.66 toks/s, output: 19.95 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:07<00:05, 18.30it/s, est. speed input: 20400.71 toks/s, output: 19.92 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:07<00:05, 18.30it/s, est. speed input: 20376.39 toks/s, output: 19.90 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:07<00:05, 18.30it/s, est. speed input: 20353.16 toks/s, output: 19.88 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:07<00:05, 18.30it/s, est. speed input: 20330.21 toks/s, output: 19.85 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:07<00:05, 18.27it/s, est. speed input: 20306.76 toks/s, output: 19.83 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:07<00:05, 18.27it/s, est. speed input: 20284.65 toks/s, output: 19.81 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:08<00:05, 18.26it/s, est. speed input: 20262.87 toks/s, output: 19.79 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:08<00:05, 18.26it/s, est. speed input: 20242.13 toks/s, output: 19.77 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:08<00:05, 18.26it/s, est. speed input: 20221.66 toks/s, output: 19.75 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:08<00:04, 18.25it/s, est. speed input: 20201.52 toks/s, output: 19.73 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:08<00:04, 18.23it/s, est. speed input: 20181.11 toks/s, output: 19.71 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:08<00:04, 18.24it/s, est. speed input: 20162.48 toks/s, output: 19.69 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:08<00:04, 18.27it/s, est. speed input: 20144.99 toks/s, output: 19.67 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:08<00:04, 18.28it/s, est. speed input: 20127.87 toks/s, output: 19.66 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:08<00:04, 18.28it/s, est. speed input: 20110.59 toks/s, output: 19.64 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:09<00:04, 18.27it/s, est. speed input: 20093.18 toks/s, output: 19.62 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:09<00:04, 18.26it/s, est. speed input: 20076.24 toks/s, output: 19.61 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:09<00:04, 18.26it/s, est. speed input: 20060.19 toks/s, output: 19.59 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:09<00:03, 18.25it/s, est. speed input: 20044.02 toks/s, output: 19.57 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:09<00:03, 18.23it/s, est. speed input: 20027.53 toks/s, output: 19.56 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:09<00:03, 18.22it/s, est. speed input: 20011.61 toks/s, output: 19.54 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:09<00:03, 18.21it/s, est. speed input: 19995.69 toks/s, output: 19.53 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:09<00:03, 18.19it/s, est. speed input: 19980.01 toks/s, output: 19.51 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:09<00:03, 18.17it/s, est. speed input: 19964.35 toks/s, output: 19.50 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:10<00:03, 18.17it/s, est. speed input: 19949.35 toks/s, output: 19.48 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:10<00:03, 18.17it/s, est. speed input: 19934.80 toks/s, output: 19.47 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:10<00:03, 18.18it/s, est. speed input: 19920.85 toks/s, output: 19.45 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:10<00:02, 20.02it/s, est. speed input: 19977.03 toks/s, output: 19.51 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:10<00:02, 19.53it/s, est. speed input: 19962.62 toks/s, output: 19.49 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:10<00:02, 19.15it/s, est. speed input: 19948.39 toks/s, output: 19.48 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:10<00:02, 18.87it/s, est. speed input: 19934.69 toks/s, output: 19.47 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:10<00:02, 18.66it/s, est. speed input: 19920.89 toks/s, output: 19.45 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:11<00:02, 18.50it/s, est. speed input: 19906.97 toks/s, output: 19.44 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:11<00:02, 18.38it/s, est. speed input: 19893.39 toks/s, output: 19.43 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:11<00:02, 18.30it/s, est. speed input: 19880.28 toks/s, output: 19.41 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:11<00:01, 18.24it/s, est. speed input: 19866.91 toks/s, output: 19.40 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:11<00:01, 18.19it/s, est. speed input: 19853.88 toks/s, output: 19.39 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:11<00:01, 18.17it/s, est. speed input: 19841.48 toks/s, output: 19.38 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:11<00:01, 18.16it/s, est. speed input: 19829.32 toks/s, output: 19.36 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:11<00:01, 18.14it/s, est. speed input: 19817.36 toks/s, output: 19.35 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:11<00:01, 18.15it/s, est. speed input: 19806.02 toks/s, output: 19.34 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:12<00:01, 18.14it/s, est. speed input: 19794.66 toks/s, output: 19.33 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:12<00:01, 18.14it/s, est. speed input: 19783.53 toks/s, output: 19.32 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:12<00:01, 18.14it/s, est. speed input: 19772.42 toks/s, output: 19.31 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:12<00:00, 18.15it/s, est. speed input: 19762.21 toks/s, output: 19.30 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:12<00:00, 18.15it/s, est. speed input: 19751.91 toks/s, output: 19.29 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:12<00:00, 18.13it/s, est. speed input: 19740.94 toks/s, output: 19.28 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:12<00:00, 18.13it/s, est. speed input: 19730.82 toks/s, output: 19.27 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:12<00:00, 18.14it/s, est. speed input: 19721.09 toks/s, output: 19.26 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:12<00:00, 18.14it/s, est. speed input: 19711.27 toks/s, output: 19.25 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:12<00:00, 18.13it/s, est. speed input: 19701.26 toks/s, output: 19.24 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:13<00:00, 18.14it/s, est. speed input: 19691.98 toks/s, output: 19.23 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:13<00:00, 18.13it/s, est. speed input: 19682.52 toks/s, output: 19.22 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.13it/s, est. speed input: 19746.35 toks/s, output: 19.28 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 19.28it/s, est. speed input: 19746.35 toks/s, output: 19.28 toks/s]
[rank0]:[W126 04:23:35.545445451 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 04:23:37
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:23:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:23:51 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=171203) WARNING 01-26 04:23:59 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=171203) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=171203) WARNING 01-26 04:24:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 18.76 requests/s, 19229.03 total tokens/s, 18.76 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 04:23:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:23:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:23:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:23:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:23:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:23:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:23:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:23:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:23:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:23:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:23:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:23:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:23:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:23:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:23:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:23:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:23:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=171203) [2026-01-26 04:23:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=171203) [2026-01-26 04:23:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=171203) [2026-01-26 04:23:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=171203) [2026-01-26 04:23:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=171203) [2026-01-26 04:23:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=171203) [2026-01-26 04:23:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=171203) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=171203) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.00it/s]
(EngineCore_DP0 pid=171203) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]
(EngineCore_DP0 pid=171203) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.18s/it]
(EngineCore_DP0 pid=171203) 
(EngineCore_DP0 pid=171203) [2026-01-26 04:24:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=171203) [2026-01-26 04:24:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=171203) [2026-01-26 04:24:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=171203) [2026-01-26 04:24:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=171203) [2026-01-26 04:24:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=171203) [2026-01-26 04:24:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=171203) [2026-01-26 04:24:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=171203) [2026-01-26 04:24:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=171203) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  4.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  5.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  5.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  5.21it/s]
(EngineCore_DP0 pid=171203) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  6.84it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  7.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.13it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 22/512 [00:00<00:02, 214.66it/s]
Adding requests:   9%|▉         | 48/512 [00:00<00:01, 236.28it/s]
Adding requests:  14%|█▍        | 73/512 [00:00<00:01, 238.77it/s]
Adding requests:  19%|█▉        | 97/512 [00:00<00:01, 228.60it/s]
Adding requests:  24%|██▎       | 121/512 [00:00<00:01, 231.82it/s]
Adding requests:  28%|██▊       | 145/512 [00:00<00:01, 232.89it/s]
Adding requests:  33%|███▎      | 169/512 [00:00<00:01, 233.07it/s]
Adding requests:  38%|███▊      | 195/512 [00:00<00:01, 240.71it/s]
Adding requests:  43%|████▎     | 221/512 [00:00<00:01, 245.07it/s]
Adding requests:  48%|████▊     | 246/512 [00:01<00:01, 243.19it/s]
Adding requests:  53%|█████▎    | 271/512 [00:01<00:00, 241.26it/s]
Adding requests:  58%|█████▊    | 297/512 [00:01<00:00, 246.57it/s]
Adding requests:  63%|██████▎   | 323/512 [00:01<00:00, 248.79it/s]
Adding requests:  68%|██████▊   | 349/512 [00:01<00:00, 251.87it/s]
Adding requests:  73%|███████▎  | 375/512 [00:01<00:00, 254.17it/s]
Adding requests:  79%|███████▊  | 403/512 [00:01<00:00, 260.81it/s]
Adding requests:  84%|████████▍ | 430/512 [00:01<00:00, 257.85it/s]
Adding requests:  89%|████████▉ | 456/512 [00:01<00:00, 256.41it/s]
Adding requests:  95%|█████████▍| 484/512 [00:01<00:00, 262.69it/s]
Adding requests: 100%|██████████| 512/512 [00:02<00:00, 265.44it/s]
Adding requests: 100%|██████████| 512/512 [00:02<00:00, 248.64it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:00<00:03, 128.56it/s, est. speed input: 131662.12 toks/s, output: 128.57 toks/s]
Processed prompts:  10%|▉         | 51/512 [00:00<00:09, 47.27it/s, est. speed input: 56377.89 toks/s, output: 55.06 toks/s]   
Processed prompts:  11%|█▏        | 58/512 [00:01<00:13, 34.48it/s, est. speed input: 44092.26 toks/s, output: 43.06 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:01<00:14, 32.02it/s, est. speed input: 41428.66 toks/s, output: 40.46 toks/s]
Processed prompts:  13%|█▎        | 67/512 [00:01<00:15, 28.81it/s, est. speed input: 38816.97 toks/s, output: 37.91 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:01<00:17, 25.01it/s, est. speed input: 36240.44 toks/s, output: 35.39 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:02<00:18, 23.37it/s, est. speed input: 34619.96 toks/s, output: 33.81 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:02<00:19, 22.15it/s, est. speed input: 33290.18 toks/s, output: 32.51 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:02<00:20, 21.26it/s, est. speed input: 32176.91 toks/s, output: 31.42 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:02<00:20, 20.60it/s, est. speed input: 31226.93 toks/s, output: 30.49 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:03<00:20, 20.13it/s, est. speed input: 30409.37 toks/s, output: 29.70 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:03<00:21, 19.80it/s, est. speed input: 29698.23 toks/s, output: 29.00 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:03<00:21, 19.56it/s, est. speed input: 29072.21 toks/s, output: 28.39 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:03<00:21, 19.40it/s, est. speed input: 28520.49 toks/s, output: 27.85 toks/s]
Processed prompts:  21%|██        | 106/512 [00:03<00:21, 19.27it/s, est. speed input: 28025.03 toks/s, output: 27.37 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:04<00:20, 19.18it/s, est. speed input: 27581.67 toks/s, output: 26.94 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:04<00:20, 19.12it/s, est. speed input: 27182.67 toks/s, output: 26.55 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:04<00:20, 19.08it/s, est. speed input: 26819.23 toks/s, output: 26.19 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:04<00:20, 19.05it/s, est. speed input: 26488.93 toks/s, output: 25.87 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:04<00:20, 19.03it/s, est. speed input: 26187.22 toks/s, output: 25.57 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:05<00:20, 19.01it/s, est. speed input: 25910.07 toks/s, output: 25.30 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:05<00:19, 19.01it/s, est. speed input: 25655.69 toks/s, output: 25.05 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:05<00:19, 19.00it/s, est. speed input: 25420.31 toks/s, output: 24.82 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:05<00:19, 18.98it/s, est. speed input: 25199.11 toks/s, output: 24.61 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:05<00:19, 18.98it/s, est. speed input: 24996.54 toks/s, output: 24.41 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:06<00:19, 18.98it/s, est. speed input: 24806.45 toks/s, output: 24.22 toks/s]
Processed prompts:  30%|███       | 154/512 [00:06<00:18, 18.97it/s, est. speed input: 24628.77 toks/s, output: 24.05 toks/s]
Processed prompts:  31%|███       | 158/512 [00:06<00:18, 18.97it/s, est. speed input: 24463.25 toks/s, output: 23.89 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:06<00:18, 18.96it/s, est. speed input: 24306.16 toks/s, output: 23.74 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:07<00:18, 18.97it/s, est. speed input: 24159.98 toks/s, output: 23.59 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:07<00:05, 52.63it/s, est. speed input: 27664.65 toks/s, output: 27.02 toks/s]
Processed prompts:  40%|███▉      | 203/512 [00:07<00:06, 46.82it/s, est. speed input: 27701.42 toks/s, output: 27.05 toks/s]
Processed prompts:  41%|████      | 208/512 [00:07<00:07, 40.04it/s, est. speed input: 27608.02 toks/s, output: 26.96 toks/s]
Processed prompts:  41%|████▏     | 212/512 [00:07<00:08, 33.78it/s, est. speed input: 27388.37 toks/s, output: 26.75 toks/s]
Processed prompts:  42%|████▏     | 216/512 [00:08<00:10, 29.36it/s, est. speed input: 27179.36 toks/s, output: 26.54 toks/s]
Processed prompts:  43%|████▎     | 219/512 [00:08<00:11, 24.83it/s, est. speed input: 26858.75 toks/s, output: 26.23 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:08<00:13, 21.66it/s, est. speed input: 26555.01 toks/s, output: 25.93 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:08<00:13, 20.83it/s, est. speed input: 26381.05 toks/s, output: 25.76 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:08<00:13, 20.26it/s, est. speed input: 26216.43 toks/s, output: 25.60 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:09<00:14, 19.85it/s, est. speed input: 26057.81 toks/s, output: 25.45 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:09<00:14, 19.54it/s, est. speed input: 25904.28 toks/s, output: 25.30 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:09<00:13, 19.33it/s, est. speed input: 25758.94 toks/s, output: 25.16 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:09<00:13, 19.20it/s, est. speed input: 25620.75 toks/s, output: 25.02 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:10<00:13, 19.10it/s, est. speed input: 25488.11 toks/s, output: 24.89 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:10<00:13, 19.02it/s, est. speed input: 25359.73 toks/s, output: 24.77 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:10<00:13, 18.98it/s, est. speed input: 25237.92 toks/s, output: 24.65 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:10<00:13, 18.93it/s, est. speed input: 25119.17 toks/s, output: 24.53 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:10<00:13, 18.91it/s, est. speed input: 25005.63 toks/s, output: 24.42 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:11<00:12, 18.89it/s, est. speed input: 24896.76 toks/s, output: 24.31 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:11<00:12, 18.89it/s, est. speed input: 24792.68 toks/s, output: 24.21 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:11<00:12, 18.88it/s, est. speed input: 24691.64 toks/s, output: 24.11 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:11<00:12, 18.86it/s, est. speed input: 24593.85 toks/s, output: 24.02 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:11<00:11, 18.87it/s, est. speed input: 24500.62 toks/s, output: 23.93 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:12<00:11, 18.86it/s, est. speed input: 24409.45 toks/s, output: 23.84 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:12<00:11, 18.85it/s, est. speed input: 24321.58 toks/s, output: 23.75 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:12<00:11, 18.84it/s, est. speed input: 24236.62 toks/s, output: 23.67 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:12<00:11, 18.84it/s, est. speed input: 24154.68 toks/s, output: 23.59 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:13<00:10, 18.83it/s, est. speed input: 24074.63 toks/s, output: 23.51 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:13<00:10, 18.83it/s, est. speed input: 23997.32 toks/s, output: 23.43 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:13<00:10, 18.81it/s, est. speed input: 23921.67 toks/s, output: 23.36 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:13<00:10, 18.82it/s, est. speed input: 23849.94 toks/s, output: 23.29 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:13<00:10, 18.82it/s, est. speed input: 23779.42 toks/s, output: 23.22 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:14<00:09, 18.82it/s, est. speed input: 23711.26 toks/s, output: 23.16 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:14<00:09, 18.82it/s, est. speed input: 23645.48 toks/s, output: 23.09 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:14<00:09, 18.81it/s, est. speed input: 23580.86 toks/s, output: 23.03 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:14<00:09, 18.80it/s, est. speed input: 23517.82 toks/s, output: 22.97 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:14<00:09, 18.80it/s, est. speed input: 23457.22 toks/s, output: 22.91 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:15<00:08, 18.80it/s, est. speed input: 23397.71 toks/s, output: 22.85 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:15<00:08, 18.79it/s, est. speed input: 23340.02 toks/s, output: 22.79 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:15<00:08, 18.80it/s, est. speed input: 23284.23 toks/s, output: 22.74 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:15<00:08, 18.78it/s, est. speed input: 23229.01 toks/s, output: 22.68 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:15<00:07, 18.79it/s, est. speed input: 23176.19 toks/s, output: 22.63 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:16<00:07, 18.79it/s, est. speed input: 23124.44 toks/s, output: 22.58 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:16<00:07, 18.77it/s, est. speed input: 23073.20 toks/s, output: 22.53 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:16<00:07, 18.77it/s, est. speed input: 23023.61 toks/s, output: 22.48 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:16<00:07, 18.76it/s, est. speed input: 22975.22 toks/s, output: 22.44 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:17<00:06, 18.78it/s, est. speed input: 22928.86 toks/s, output: 22.39 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:17<00:06, 18.77it/s, est. speed input: 22882.95 toks/s, output: 22.35 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:17<00:06, 18.78it/s, est. speed input: 22838.60 toks/s, output: 22.30 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:17<00:06, 18.78it/s, est. speed input: 22795.27 toks/s, output: 22.26 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:17<00:06, 18.78it/s, est. speed input: 22752.89 toks/s, output: 22.22 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:18<00:05, 18.78it/s, est. speed input: 22711.41 toks/s, output: 22.18 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:18<00:05, 18.78it/s, est. speed input: 22671.13 toks/s, output: 22.14 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:18<00:05, 18.78it/s, est. speed input: 22631.35 toks/s, output: 22.10 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:18<00:05, 18.78it/s, est. speed input: 22592.76 toks/s, output: 22.06 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:18<00:05, 18.76it/s, est. speed input: 22554.54 toks/s, output: 22.03 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:19<00:04, 18.77it/s, est. speed input: 22517.49 toks/s, output: 21.99 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:19<00:04, 18.76it/s, est. speed input: 22481.00 toks/s, output: 21.95 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:19<00:04, 18.76it/s, est. speed input: 22445.53 toks/s, output: 21.92 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:19<00:04, 18.76it/s, est. speed input: 22410.67 toks/s, output: 21.89 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:20<00:03, 18.76it/s, est. speed input: 22376.45 toks/s, output: 21.85 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:20<00:03, 18.76it/s, est. speed input: 22343.41 toks/s, output: 21.82 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:20<00:03, 18.76it/s, est. speed input: 22310.81 toks/s, output: 21.79 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:20<00:03, 18.77it/s, est. speed input: 22279.07 toks/s, output: 21.76 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:20<00:03, 18.77it/s, est. speed input: 22247.71 toks/s, output: 21.73 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:21<00:02, 18.76it/s, est. speed input: 22217.06 toks/s, output: 21.70 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:21<00:02, 18.77it/s, est. speed input: 22187.12 toks/s, output: 21.67 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:21<00:02, 18.77it/s, est. speed input: 22157.68 toks/s, output: 21.64 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:21<00:02, 18.76it/s, est. speed input: 22128.59 toks/s, output: 21.61 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:21<00:02, 18.78it/s, est. speed input: 22100.89 toks/s, output: 21.58 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:22<00:01, 18.76it/s, est. speed input: 22072.82 toks/s, output: 21.56 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:22<00:01, 18.77it/s, est. speed input: 22045.78 toks/s, output: 21.53 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:22<00:01, 18.76it/s, est. speed input: 22018.74 toks/s, output: 21.50 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:22<00:01, 18.77it/s, est. speed input: 21992.81 toks/s, output: 21.48 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:23<00:00, 18.76it/s, est. speed input: 21966.94 toks/s, output: 21.45 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:23<00:00, 18.77it/s, est. speed input: 21941.78 toks/s, output: 21.43 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:23<00:00, 18.77it/s, est. speed input: 21917.02 toks/s, output: 21.40 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:23<00:00, 18.78it/s, est. speed input: 21893.18 toks/s, output: 21.38 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:23<00:00, 20.08it/s, est. speed input: 21911.59 toks/s, output: 21.40 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:23<00:00, 20.08it/s, est. speed input: 21997.33 toks/s, output: 21.48 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:23<00:00, 21.48it/s, est. speed input: 21997.33 toks/s, output: 21.48 toks/s]
[rank0]:[W126 04:24:56.833029584 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 04:24:58
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:25:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:25:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=172627) WARNING 01-26 04:25:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=172627) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=172627) WARNING 01-26 04:25:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 18.92 requests/s, 19390.86 total tokens/s, 18.92 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 04:25:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:25:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:25:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:25:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:25:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:25:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:25:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:25:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:25:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:25:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:25:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:25:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:25:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:25:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:25:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:25:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:25:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=172627) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=172627) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=172627) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]
(EngineCore_DP0 pid=172627) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.18s/it]
(EngineCore_DP0 pid=172627) 
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=172627) [2026-01-26 04:25:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=172627) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  3.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  4.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  5.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.48it/s]
(EngineCore_DP0 pid=172627) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  6.95it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.03it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.43it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 20/1024 [00:00<00:05, 198.61it/s]
Adding requests:   4%|▍         | 45/1024 [00:00<00:04, 226.14it/s]
Adding requests:   7%|▋         | 70/1024 [00:00<00:04, 234.56it/s]
Adding requests:   9%|▉         | 94/1024 [00:00<00:04, 231.67it/s]
Adding requests:  12%|█▏        | 118/1024 [00:00<00:03, 233.96it/s]
Adding requests:  14%|█▍        | 143/1024 [00:00<00:03, 237.17it/s]
Adding requests:  16%|█▋        | 168/1024 [00:00<00:03, 240.51it/s]
Adding requests:  19%|█▉        | 194/1024 [00:00<00:03, 243.43it/s]
Adding requests:  21%|██▏       | 219/1024 [00:00<00:03, 233.57it/s]
Adding requests:  24%|██▍       | 244/1024 [00:01<00:03, 235.93it/s]
Adding requests:  26%|██▋       | 269/1024 [00:01<00:03, 238.50it/s]
Adding requests:  29%|██▉       | 295/1024 [00:01<00:02, 243.65it/s]
Adding requests:  31%|███▏      | 322/1024 [00:01<00:02, 250.51it/s]
Adding requests:  34%|███▍      | 348/1024 [00:01<00:02, 250.68it/s]
Adding requests:  37%|███▋      | 374/1024 [00:01<00:02, 253.07it/s]
Adding requests:  39%|███▉      | 400/1024 [00:01<00:02, 251.39it/s]
Adding requests:  42%|████▏     | 427/1024 [00:01<00:02, 256.41it/s]
Adding requests:  44%|████▍     | 453/1024 [00:01<00:02, 249.29it/s]
Adding requests:  47%|████▋     | 480/1024 [00:01<00:02, 255.18it/s]
Adding requests:  50%|████▉     | 508/1024 [00:02<00:01, 261.38it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:02<00:01, 262.84it/s]
Adding requests:  55%|█████▍    | 562/1024 [00:02<00:01, 260.91it/s]
Adding requests:  58%|█████▊    | 589/1024 [00:02<00:01, 253.66it/s]
Adding requests:  60%|██████    | 615/1024 [00:02<00:01, 253.20it/s]
Adding requests:  63%|██████▎   | 641/1024 [00:02<00:01, 253.59it/s]
Adding requests:  65%|██████▌   | 667/1024 [00:02<00:01, 250.88it/s]
Adding requests:  68%|██████▊   | 695/1024 [00:02<00:01, 256.38it/s]
Adding requests:  70%|███████   | 721/1024 [00:02<00:01, 245.13it/s]
Adding requests:  73%|███████▎  | 746/1024 [00:03<00:01, 245.41it/s]
Adding requests:  76%|███████▌  | 774/1024 [00:03<00:00, 253.63it/s]
Adding requests:  78%|███████▊  | 800/1024 [00:03<00:00, 255.17it/s]
Adding requests:  81%|████████  | 826/1024 [00:03<00:00, 256.45it/s]
Adding requests:  83%|████████▎ | 853/1024 [00:03<00:00, 258.73it/s]
Adding requests:  86%|████████▌ | 879/1024 [00:03<00:00, 246.22it/s]
Adding requests:  88%|████████▊ | 905/1024 [00:03<00:00, 247.60it/s]
Adding requests:  91%|█████████ | 930/1024 [00:03<00:00, 239.20it/s]
Adding requests:  93%|█████████▎| 955/1024 [00:03<00:00, 240.69it/s]
Adding requests:  96%|█████████▌| 980/1024 [00:03<00:00, 236.24it/s]
Adding requests:  98%|█████████▊| 1004/1024 [00:04<00:00, 231.86it/s]
Adding requests: 100%|██████████| 1024/1024 [00:04<00:00, 245.89it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:00<00:05, 160.92it/s, est. speed input: 164798.41 toks/s, output: 160.93 toks/s]
Processed prompts:   9%|▉         | 91/1024 [00:01<00:15, 59.67it/s, est. speed input: 72186.10 toks/s, output: 70.49 toks/s]   
Processed prompts:  10%|▉         | 99/1024 [00:01<00:20, 45.92it/s, est. speed input: 59393.70 toks/s, output: 58.00 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:02<00:25, 36.40it/s, est. speed input: 51123.34 toks/s, output: 49.92 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:02<00:29, 30.92it/s, est. speed input: 45965.97 toks/s, output: 44.89 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:02<00:33, 27.26it/s, est. speed input: 42259.77 toks/s, output: 41.27 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:03<00:36, 24.77it/s, est. speed input: 39467.33 toks/s, output: 38.54 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:03<00:38, 23.05it/s, est. speed input: 37284.80 toks/s, output: 36.41 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:04<00:40, 21.87it/s, est. speed input: 35535.33 toks/s, output: 34.70 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:04<00:41, 21.05it/s, est. speed input: 34099.15 toks/s, output: 33.30 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:05<00:42, 20.48it/s, est. speed input: 32900.19 toks/s, output: 32.13 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:05<00:42, 20.08it/s, est. speed input: 31882.99 toks/s, output: 31.14 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:05<00:42, 19.80it/s, est. speed input: 31010.65 toks/s, output: 30.28 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:06<00:42, 19.60it/s, est. speed input: 30252.55 toks/s, output: 29.54 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:06<00:42, 19.46it/s, est. speed input: 29587.49 toks/s, output: 28.89 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:07<00:41, 19.70it/s, est. speed input: 29100.20 toks/s, output: 28.42 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:07<00:41, 19.52it/s, est. speed input: 28569.40 toks/s, output: 27.90 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:07<00:41, 19.39it/s, est. speed input: 28094.41 toks/s, output: 27.44 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:08<00:41, 19.30it/s, est. speed input: 27666.83 toks/s, output: 27.02 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:08<00:41, 19.24it/s, est. speed input: 27279.61 toks/s, output: 26.64 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:09<00:40, 19.19it/s, est. speed input: 26926.97 toks/s, output: 26.30 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:09<00:40, 19.15it/s, est. speed input: 26604.34 toks/s, output: 25.98 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:10<00:40, 19.13it/s, est. speed input: 26309.23 toks/s, output: 25.69 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:10<00:39, 19.11it/s, est. speed input: 26036.70 toks/s, output: 25.43 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:10<00:39, 19.10it/s, est. speed input: 25785.31 toks/s, output: 25.18 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:11<00:38, 19.08it/s, est. speed input: 25552.34 toks/s, output: 24.95 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:11<00:38, 19.08it/s, est. speed input: 25336.08 toks/s, output: 24.74 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:12<00:38, 19.06it/s, est. speed input: 25133.94 toks/s, output: 24.54 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:12<00:37, 19.06it/s, est. speed input: 24945.63 toks/s, output: 24.36 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:12<00:37, 19.05it/s, est. speed input: 24768.84 toks/s, output: 24.19 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:13<00:36, 19.04it/s, est. speed input: 24603.13 toks/s, output: 24.03 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:13<00:36, 19.04it/s, est. speed input: 24447.57 toks/s, output: 23.87 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:14<00:36, 19.03it/s, est. speed input: 24300.86 toks/s, output: 23.73 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:14<00:16, 40.13it/s, est. speed input: 26085.05 toks/s, output: 25.47 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:14<00:19, 33.32it/s, est. speed input: 25898.72 toks/s, output: 25.29 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:15<00:22, 28.82it/s, est. speed input: 25723.68 toks/s, output: 25.12 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:15<00:24, 25.78it/s, est. speed input: 25557.62 toks/s, output: 24.96 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:16<00:26, 23.70it/s, est. speed input: 25400.00 toks/s, output: 24.80 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:16<00:27, 22.27it/s, est. speed input: 25250.03 toks/s, output: 24.66 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:17<00:28, 21.28it/s, est. speed input: 25107.26 toks/s, output: 24.52 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:17<00:29, 20.60it/s, est. speed input: 24971.60 toks/s, output: 24.39 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:17<00:29, 20.11it/s, est. speed input: 24841.72 toks/s, output: 24.26 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:18<00:29, 19.78it/s, est. speed input: 24717.82 toks/s, output: 24.14 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:18<00:29, 19.55it/s, est. speed input: 24599.89 toks/s, output: 24.02 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:19<00:29, 19.39it/s, est. speed input: 24487.32 toks/s, output: 23.91 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:19<00:28, 19.27it/s, est. speed input: 24379.21 toks/s, output: 23.81 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:19<00:28, 19.20it/s, est. speed input: 24275.90 toks/s, output: 23.71 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:20<00:28, 19.14it/s, est. speed input: 24176.57 toks/s, output: 23.61 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:20<00:27, 19.09it/s, est. speed input: 24080.91 toks/s, output: 23.52 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:21<00:27, 19.07it/s, est. speed input: 23989.60 toks/s, output: 23.43 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:21<00:27, 19.05it/s, est. speed input: 23901.57 toks/s, output: 23.34 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:22<00:26, 19.04it/s, est. speed input: 23817.26 toks/s, output: 23.26 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:22<00:26, 19.03it/s, est. speed input: 23735.75 toks/s, output: 23.18 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:22<00:25, 19.01it/s, est. speed input: 23656.78 toks/s, output: 23.10 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:23<00:25, 19.00it/s, est. speed input: 23580.50 toks/s, output: 23.03 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:23<00:25, 18.99it/s, est. speed input: 23506.93 toks/s, output: 22.96 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:24<00:24, 18.99it/s, est. speed input: 23435.83 toks/s, output: 22.89 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:24<00:24, 18.98it/s, est. speed input: 23367.27 toks/s, output: 22.82 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:25<00:23, 18.99it/s, est. speed input: 23301.52 toks/s, output: 22.76 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:25<00:23, 18.99it/s, est. speed input: 23237.88 toks/s, output: 22.69 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:25<00:23, 18.99it/s, est. speed input: 23176.40 toks/s, output: 22.63 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:26<00:22, 19.00it/s, est. speed input: 23116.85 toks/s, output: 22.58 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:26<00:22, 19.00it/s, est. speed input: 23059.22 toks/s, output: 22.52 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:27<00:21, 19.00it/s, est. speed input: 23003.52 toks/s, output: 22.46 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:27<00:21, 19.00it/s, est. speed input: 22949.46 toks/s, output: 22.41 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:27<00:20, 19.00it/s, est. speed input: 22897.01 toks/s, output: 22.36 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:28<00:20, 19.00it/s, est. speed input: 22846.15 toks/s, output: 22.31 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:28<00:20, 19.00it/s, est. speed input: 22796.68 toks/s, output: 22.26 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:29<00:19, 19.00it/s, est. speed input: 22748.51 toks/s, output: 22.22 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:29<00:19, 19.00it/s, est. speed input: 22701.86 toks/s, output: 22.17 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:30<00:18, 18.99it/s, est. speed input: 22656.09 toks/s, output: 22.13 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:30<00:18, 19.00it/s, est. speed input: 22612.08 toks/s, output: 22.08 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:30<00:18, 19.00it/s, est. speed input: 22569.13 toks/s, output: 22.04 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:31<00:17, 18.99it/s, est. speed input: 22526.93 toks/s, output: 22.00 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:31<00:17, 18.99it/s, est. speed input: 22485.95 toks/s, output: 21.96 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:32<00:16, 18.99it/s, est. speed input: 22446.09 toks/s, output: 21.92 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:32<00:16, 18.99it/s, est. speed input: 22407.32 toks/s, output: 21.88 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:33<00:15, 18.98it/s, est. speed input: 22369.39 toks/s, output: 21.85 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:33<00:15, 18.98it/s, est. speed input: 22332.29 toks/s, output: 21.81 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:33<00:15, 18.98it/s, est. speed input: 22296.31 toks/s, output: 21.77 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:34<00:14, 18.98it/s, est. speed input: 22261.24 toks/s, output: 21.74 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:34<00:14, 18.99it/s, est. speed input: 22227.14 toks/s, output: 21.71 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:35<00:13, 18.99it/s, est. speed input: 22193.73 toks/s, output: 21.67 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:35<00:13, 18.98it/s, est. speed input: 22161.10 toks/s, output: 21.64 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:36<00:12, 18.99it/s, est. speed input: 22129.31 toks/s, output: 21.61 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:36<00:12, 19.49it/s, est. speed input: 22120.07 toks/s, output: 21.60 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:36<00:11, 19.34it/s, est. speed input: 22089.54 toks/s, output: 21.57 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:37<00:11, 19.23it/s, est. speed input: 22059.54 toks/s, output: 21.54 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:37<00:11, 19.15it/s, est. speed input: 22030.07 toks/s, output: 21.51 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:38<00:10, 19.10it/s, est. speed input: 22001.53 toks/s, output: 21.49 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:38<00:10, 19.06it/s, est. speed input: 21973.27 toks/s, output: 21.46 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:38<00:09, 19.04it/s, est. speed input: 21945.97 toks/s, output: 21.43 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:39<00:09, 19.03it/s, est. speed input: 21919.32 toks/s, output: 21.41 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:39<00:09, 19.02it/s, est. speed input: 21893.15 toks/s, output: 21.38 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:40<00:08, 19.01it/s, est. speed input: 21867.68 toks/s, output: 21.36 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:40<00:08, 19.01it/s, est. speed input: 21842.45 toks/s, output: 21.33 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:41<00:07, 19.00it/s, est. speed input: 21817.98 toks/s, output: 21.31 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:41<00:07, 19.00it/s, est. speed input: 21793.97 toks/s, output: 21.28 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:41<00:07, 19.00it/s, est. speed input: 21770.39 toks/s, output: 21.26 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:42<00:06, 19.00it/s, est. speed input: 21747.27 toks/s, output: 21.24 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:42<00:06, 18.99it/s, est. speed input: 21724.48 toks/s, output: 21.22 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:43<00:05, 19.00it/s, est. speed input: 21702.40 toks/s, output: 21.19 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:43<00:05, 18.99it/s, est. speed input: 21680.47 toks/s, output: 21.17 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:43<00:04, 18.99it/s, est. speed input: 21658.99 toks/s, output: 21.15 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:44<00:04, 18.99it/s, est. speed input: 21637.91 toks/s, output: 21.13 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:44<00:04, 18.99it/s, est. speed input: 21617.24 toks/s, output: 21.11 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:45<00:03, 18.98it/s, est. speed input: 21596.88 toks/s, output: 21.09 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:45<00:03, 18.98it/s, est. speed input: 21576.98 toks/s, output: 21.07 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:45<00:00, 40.08it/s, est. speed input: 22158.38 toks/s, output: 21.64 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:46<00:00, 33.27it/s, est. speed input: 22133.59 toks/s, output: 21.61 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:46<00:00, 28.76it/s, est. speed input: 22109.24 toks/s, output: 21.59 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:47<00:00, 26.59it/s, est. speed input: 22106.10 toks/s, output: 21.59 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:47<00:00, 26.59it/s, est. speed input: 22236.26 toks/s, output: 21.72 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:47<00:00, 21.72it/s, est. speed input: 22236.26 toks/s, output: 21.72 toks/s]
[rank0]:[W126 04:26:45.634516468 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 04:26:48
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:27:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:27:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=174613) WARNING 01-26 04:27:18 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=174613) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=174613) WARNING 01-26 04:27:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 12.02 requests/s, 12322.10 total tokens/s, 12.02 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 04:27:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:27:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:27:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:27:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:27:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:27:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:27:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:27:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:27:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:27:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:27:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:27:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:27:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:27:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:27:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:27:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:27:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=174613) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=174613) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=174613) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.21s/it]
(EngineCore_DP0 pid=174613) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.17s/it]
(EngineCore_DP0 pid=174613) 
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=174613) [2026-01-26 04:27:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=174613) [rank0]:W0126 04:27:39.864000 174613 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=174613) [rank0]:W0126 04:27:39.977000 174613 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=174613) [rank0]:W0126 04:27:41.565000 174613 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=174613) [rank0]:W0126 04:27:41.747000 174613 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=174613) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.37it/s]
(EngineCore_DP0 pid=174613) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.17it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.77it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 24/2048 [00:00<00:08, 235.82it/s]
Adding requests:   2%|▏         | 50/2048 [00:00<00:08, 246.77it/s]
Adding requests:   4%|▍         | 77/2048 [00:00<00:07, 256.63it/s]
Adding requests:   5%|▌         | 103/2048 [00:00<00:07, 255.28it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:07, 250.38it/s]
Adding requests:   8%|▊         | 155/2048 [00:00<00:07, 249.88it/s]
Adding requests:   9%|▉         | 181/2048 [00:00<00:07, 251.50it/s]
Adding requests:  10%|█         | 208/2048 [00:00<00:07, 255.10it/s]
Adding requests:  11%|█▏        | 234/2048 [00:00<00:07, 256.25it/s]
Adding requests:  13%|█▎        | 260/2048 [00:01<00:07, 253.30it/s]
Adding requests:  14%|█▍        | 287/2048 [00:01<00:06, 256.10it/s]
Adding requests:  15%|█▌        | 314/2048 [00:01<00:06, 259.33it/s]
Adding requests:  17%|█▋        | 341/2048 [00:01<00:06, 260.94it/s]
Adding requests:  18%|█▊        | 368/2048 [00:01<00:06, 255.62it/s]
Adding requests:  19%|█▉        | 395/2048 [00:01<00:06, 257.18it/s]
Adding requests:  21%|██        | 422/2048 [00:01<00:06, 260.06it/s]
Adding requests:  22%|██▏       | 449/2048 [00:01<00:06, 253.62it/s]
Adding requests:  23%|██▎       | 475/2048 [00:01<00:06, 254.06it/s]
Adding requests:  25%|██▍       | 503/2048 [00:01<00:05, 259.98it/s]
Adding requests:  26%|██▌       | 532/2048 [00:02<00:05, 265.60it/s]
Adding requests:  27%|██▋       | 559/2048 [00:02<00:05, 260.22it/s]
Adding requests:  29%|██▊       | 586/2048 [00:02<00:05, 249.25it/s]
Adding requests:  30%|██▉       | 612/2048 [00:02<00:05, 250.01it/s]
Adding requests:  31%|███       | 639/2048 [00:02<00:05, 253.96it/s]
Adding requests:  32%|███▏      | 665/2048 [00:02<00:05, 251.07it/s]
Adding requests:  34%|███▎      | 691/2048 [00:02<00:05, 251.86it/s]
Adding requests:  35%|███▌      | 717/2048 [00:02<00:05, 251.96it/s]
Adding requests:  36%|███▋      | 743/2048 [00:02<00:05, 249.32it/s]
Adding requests:  38%|███▊      | 768/2048 [00:03<00:05, 248.59it/s]
Adding requests:  39%|███▉      | 794/2048 [00:03<00:05, 250.32it/s]
Adding requests:  40%|████      | 820/2048 [00:03<00:04, 246.68it/s]
Adding requests:  41%|████▏     | 847/2048 [00:03<00:04, 251.41it/s]
Adding requests:  43%|████▎     | 874/2048 [00:03<00:04, 253.66it/s]
Adding requests:  44%|████▍     | 900/2048 [00:03<00:04, 252.45it/s]
Adding requests:  45%|████▌     | 926/2048 [00:03<00:04, 249.47it/s]
Adding requests:  46%|████▋     | 951/2048 [00:03<00:04, 243.27it/s]
Adding requests:  48%|████▊     | 978/2048 [00:03<00:04, 249.98it/s]
Adding requests:  49%|████▉     | 1004/2048 [00:03<00:04, 248.56it/s]
Adding requests:  50%|█████     | 1030/2048 [00:04<00:04, 250.19it/s]
Adding requests:  52%|█████▏    | 1056/2048 [00:04<00:04, 245.93it/s]
Adding requests:  53%|█████▎    | 1082/2048 [00:04<00:03, 249.73it/s]
Adding requests:  54%|█████▍    | 1108/2048 [00:04<00:03, 245.98it/s]
Adding requests:  55%|█████▌    | 1135/2048 [00:04<00:03, 252.75it/s]
Adding requests:  57%|█████▋    | 1161/2048 [00:04<00:03, 245.12it/s]
Adding requests:  58%|█████▊    | 1187/2048 [00:04<00:03, 248.56it/s]
Adding requests:  59%|█████▉    | 1213/2048 [00:04<00:03, 249.39it/s]
Adding requests:  61%|██████    | 1240/2048 [00:04<00:03, 253.14it/s]
Adding requests:  62%|██████▏   | 1266/2048 [00:05<00:03, 248.95it/s]
Adding requests:  63%|██████▎   | 1291/2048 [00:05<00:03, 245.76it/s]
Adding requests:  64%|██████▍   | 1317/2048 [00:05<00:02, 249.28it/s]
Adding requests:  66%|██████▌   | 1342/2048 [00:05<00:02, 247.69it/s]
Adding requests:  67%|██████▋   | 1368/2048 [00:05<00:02, 248.78it/s]
Adding requests:  68%|██████▊   | 1393/2048 [00:05<00:02, 248.28it/s]
Adding requests:  69%|██████▉   | 1418/2048 [00:05<00:02, 245.50it/s]
Adding requests:  71%|███████   | 1445/2048 [00:05<00:02, 251.77it/s]
Adding requests:  72%|███████▏  | 1471/2048 [00:05<00:02, 250.99it/s]
Adding requests:  73%|███████▎  | 1498/2048 [00:05<00:02, 256.41it/s]
Adding requests:  74%|███████▍  | 1525/2048 [00:06<00:02, 259.13it/s]
Adding requests:  76%|███████▌  | 1551/2048 [00:06<00:01, 257.91it/s]
Adding requests:  77%|███████▋  | 1577/2048 [00:06<00:01, 250.65it/s]
Adding requests:  78%|███████▊  | 1603/2048 [00:06<00:01, 252.04it/s]
Adding requests:  80%|███████▉  | 1629/2048 [00:06<00:01, 250.79it/s]
Adding requests:  81%|████████  | 1655/2048 [00:06<00:01, 246.37it/s]
Adding requests:  82%|████████▏ | 1680/2048 [00:06<00:01, 246.20it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:06<00:01, 249.86it/s]
Adding requests:  85%|████████▍ | 1732/2048 [00:06<00:01, 248.82it/s]
Adding requests:  86%|████████▌ | 1759/2048 [00:06<00:01, 253.22it/s]
Adding requests:  87%|████████▋ | 1787/2048 [00:07<00:01, 257.90it/s]
Adding requests:  89%|████████▊ | 1813/2048 [00:07<00:00, 255.99it/s]
Adding requests:  90%|████████▉ | 1839/2048 [00:07<00:00, 254.87it/s]
Adding requests:  91%|█████████ | 1865/2048 [00:07<00:00, 256.24it/s]
Adding requests:  92%|█████████▏| 1892/2048 [00:07<00:00, 259.09it/s]
Adding requests:  94%|█████████▎| 1919/2048 [00:07<00:00, 258.98it/s]
Adding requests:  95%|█████████▍| 1945/2048 [00:07<00:00, 256.45it/s]
Adding requests:  96%|█████████▌| 1971/2048 [00:07<00:00, 253.70it/s]
Adding requests:  98%|█████████▊| 1997/2048 [00:07<00:00, 251.42it/s]
Adding requests:  99%|█████████▉| 2023/2048 [00:08<00:00, 246.51it/s]
Adding requests: 100%|██████████| 2048/2048 [00:08<00:00, 246.77it/s]
Adding requests: 100%|██████████| 2048/2048 [00:08<00:00, 251.91it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 82/2048 [00:00<00:05, 334.10it/s, est. speed input: 342160.64 toks/s, output: 334.11 toks/s]
Processed prompts:   6%|▌         | 116/2048 [00:02<00:59, 32.42it/s, est. speed input: 41057.28 toks/s, output: 40.09 toks/s]  
Processed prompts:   6%|▋         | 131/2048 [00:04<01:20, 23.95it/s, est. speed input: 31804.15 toks/s, output: 31.06 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:05<01:38, 19.34it/s, est. speed input: 26972.35 toks/s, output: 26.34 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:06<01:51, 16.85it/s, est. speed input: 24152.35 toks/s, output: 23.59 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:08<02:02, 15.28it/s, est. speed input: 22243.66 toks/s, output: 21.72 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:09<02:10, 14.25it/s, est. speed input: 20867.21 toks/s, output: 20.38 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:10<02:15, 13.57it/s, est. speed input: 19826.38 toks/s, output: 19.36 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:12<02:19, 13.10it/s, est. speed input: 19011.80 toks/s, output: 18.57 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:13<02:21, 12.78it/s, est. speed input: 18357.01 toks/s, output: 17.93 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:14<02:22, 12.56it/s, est. speed input: 17819.41 toks/s, output: 17.40 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:16<02:22, 12.41it/s, est. speed input: 17369.75 toks/s, output: 16.96 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:17<02:22, 12.30it/s, est. speed input: 16987.95 toks/s, output: 16.59 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:18<02:22, 12.21it/s, est. speed input: 16653.31 toks/s, output: 16.26 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:20<01:46, 16.04it/s, est. speed input: 17242.54 toks/s, output: 16.84 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:21<01:54, 14.82it/s, est. speed input: 16936.83 toks/s, output: 16.54 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:22<02:00, 13.98it/s, est. speed input: 16667.17 toks/s, output: 16.28 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:24<02:04, 13.39it/s, est. speed input: 16426.95 toks/s, output: 16.04 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:25<02:06, 12.98it/s, est. speed input: 16211.80 toks/s, output: 15.83 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:26<02:08, 12.69it/s, est. speed input: 16017.77 toks/s, output: 15.64 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:28<02:09, 12.49it/s, est. speed input: 15841.91 toks/s, output: 15.47 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:29<02:09, 12.35it/s, est. speed input: 15682.00 toks/s, output: 15.31 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:30<02:09, 12.25it/s, est. speed input: 15535.85 toks/s, output: 15.17 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:32<02:08, 12.18it/s, est. speed input: 15401.76 toks/s, output: 15.04 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:33<02:07, 12.13it/s, est. speed input: 15278.26 toks/s, output: 14.92 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:34<02:06, 12.10it/s, est. speed input: 15164.22 toks/s, output: 14.81 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:36<02:05, 12.07it/s, est. speed input: 15058.17 toks/s, output: 14.71 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:37<02:04, 12.05it/s, est. speed input: 14959.94 toks/s, output: 14.61 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:38<02:03, 12.04it/s, est. speed input: 14868.29 toks/s, output: 14.52 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:40<02:02, 12.03it/s, est. speed input: 14782.60 toks/s, output: 14.44 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:41<02:00, 12.02it/s, est. speed input: 14702.38 toks/s, output: 14.36 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:42<01:59, 12.02it/s, est. speed input: 14627.29 toks/s, output: 14.28 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:44<01:58, 12.01it/s, est. speed input: 14556.56 toks/s, output: 14.22 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:45<01:57, 12.01it/s, est. speed input: 14490.04 toks/s, output: 14.15 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:46<01:55, 12.01it/s, est. speed input: 14427.32 toks/s, output: 14.09 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:48<01:54, 12.01it/s, est. speed input: 14368.03 toks/s, output: 14.03 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:49<01:53, 12.01it/s, est. speed input: 14311.89 toks/s, output: 13.98 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:50<01:51, 12.01it/s, est. speed input: 14258.84 toks/s, output: 13.92 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:51<01:22, 15.85it/s, est. speed input: 14542.08 toks/s, output: 14.20 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:53<01:28, 14.68it/s, est. speed input: 14485.63 toks/s, output: 14.15 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:54<01:32, 13.87it/s, est. speed input: 14432.04 toks/s, output: 14.09 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:55<01:30, 13.96it/s, est. speed input: 14434.41 toks/s, output: 14.10 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:57<01:33, 13.35it/s, est. speed input: 14384.12 toks/s, output: 14.05 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:58<01:35, 12.93it/s, est. speed input: 14336.33 toks/s, output: 14.00 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:59<01:35, 12.65it/s, est. speed input: 14290.58 toks/s, output: 13.96 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [01:01<01:36, 12.45it/s, est. speed input: 14246.75 toks/s, output: 13.91 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [01:02<01:35, 12.31it/s, est. speed input: 14204.90 toks/s, output: 13.87 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [01:03<01:35, 12.22it/s, est. speed input: 14164.66 toks/s, output: 13.83 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [01:05<01:34, 12.15it/s, est. speed input: 14126.10 toks/s, output: 13.80 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [01:06<01:33, 12.10it/s, est. speed input: 14089.08 toks/s, output: 13.76 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [01:07<01:32, 12.07it/s, est. speed input: 14053.53 toks/s, output: 13.72 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:09<01:31, 12.05it/s, est. speed input: 14019.33 toks/s, output: 13.69 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:10<01:30, 12.03it/s, est. speed input: 13986.41 toks/s, output: 13.66 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:11<01:29, 12.02it/s, est. speed input: 13954.72 toks/s, output: 13.63 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:13<01:27, 12.01it/s, est. speed input: 13924.17 toks/s, output: 13.60 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:14<01:26, 12.01it/s, est. speed input: 13894.77 toks/s, output: 13.57 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:15<01:25, 12.00it/s, est. speed input: 13866.24 toks/s, output: 13.54 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:17<01:23, 12.00it/s, est. speed input: 13838.75 toks/s, output: 13.51 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:18<01:22, 12.00it/s, est. speed input: 13812.23 toks/s, output: 13.49 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:19<01:21, 11.99it/s, est. speed input: 13786.58 toks/s, output: 13.46 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:21<01:19, 11.99it/s, est. speed input: 13761.86 toks/s, output: 13.44 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:22<00:58, 15.82it/s, est. speed input: 13947.35 toks/s, output: 13.62 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:23<01:02, 14.66it/s, est. speed input: 13920.71 toks/s, output: 13.59 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:25<01:04, 13.85it/s, est. speed input: 13894.95 toks/s, output: 13.57 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:26<01:06, 13.29it/s, est. speed input: 13870.07 toks/s, output: 13.54 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:27<01:06, 12.90it/s, est. speed input: 13845.77 toks/s, output: 13.52 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:29<01:06, 12.63it/s, est. speed input: 13822.36 toks/s, output: 13.50 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:30<01:06, 12.43it/s, est. speed input: 13799.40 toks/s, output: 13.48 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:31<01:06, 12.30it/s, est. speed input: 13777.17 toks/s, output: 13.45 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:33<01:05, 12.21it/s, est. speed input: 13755.66 toks/s, output: 13.43 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:34<01:04, 12.14it/s, est. speed input: 13734.76 toks/s, output: 13.41 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:35<01:03, 12.09it/s, est. speed input: 13714.42 toks/s, output: 13.39 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:37<01:02, 12.06it/s, est. speed input: 13694.63 toks/s, output: 13.37 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:38<01:00, 12.04it/s, est. speed input: 13675.35 toks/s, output: 13.35 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:39<00:59, 12.02it/s, est. speed input: 13656.60 toks/s, output: 13.34 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:41<00:58, 12.01it/s, est. speed input: 13638.35 toks/s, output: 13.32 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:42<00:57, 12.00it/s, est. speed input: 13620.49 toks/s, output: 13.30 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:43<00:55, 12.00it/s, est. speed input: 13603.17 toks/s, output: 13.28 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:45<00:54, 11.99it/s, est. speed input: 13586.27 toks/s, output: 13.27 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:46<00:53, 11.99it/s, est. speed input: 13569.81 toks/s, output: 13.25 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:47<00:51, 11.99it/s, est. speed input: 13553.81 toks/s, output: 13.24 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:49<00:50, 11.99it/s, est. speed input: 13538.14 toks/s, output: 13.22 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:50<00:49, 11.99it/s, est. speed input: 13522.86 toks/s, output: 13.21 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:51<00:47, 11.99it/s, est. speed input: 13507.96 toks/s, output: 13.19 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:53<00:34, 15.83it/s, est. speed input: 13646.73 toks/s, output: 13.33 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:54<00:35, 14.67it/s, est. speed input: 13630.84 toks/s, output: 13.31 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:55<00:36, 13.86it/s, est. speed input: 13615.37 toks/s, output: 13.30 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:57<00:37, 13.30it/s, est. speed input: 13600.14 toks/s, output: 13.28 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:58<00:37, 12.91it/s, est. speed input: 13585.29 toks/s, output: 13.27 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:59<00:36, 12.63it/s, est. speed input: 13570.83 toks/s, output: 13.25 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [02:01<00:35, 12.44it/s, est. speed input: 13556.56 toks/s, output: 13.24 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [02:02<00:33, 12.91it/s, est. speed input: 13565.56 toks/s, output: 13.25 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [02:03<00:32, 12.62it/s, est. speed input: 13551.46 toks/s, output: 13.23 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [02:04<00:32, 12.42it/s, est. speed input: 13537.71 toks/s, output: 13.22 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [02:06<00:31, 12.29it/s, est. speed input: 13524.24 toks/s, output: 13.21 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [02:07<00:30, 12.19it/s, est. speed input: 13511.06 toks/s, output: 13.19 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [02:08<00:28, 12.13it/s, est. speed input: 13498.14 toks/s, output: 13.18 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [02:10<00:27, 12.08it/s, est. speed input: 13485.36 toks/s, output: 13.17 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [02:11<00:26, 12.05it/s, est. speed input: 13472.87 toks/s, output: 13.16 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [02:12<00:25, 12.02it/s, est. speed input: 13460.59 toks/s, output: 13.15 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [02:14<00:23, 12.00it/s, est. speed input: 13448.55 toks/s, output: 13.13 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [02:15<00:22, 11.99it/s, est. speed input: 13436.71 toks/s, output: 13.12 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [02:16<00:21, 11.99it/s, est. speed input: 13425.22 toks/s, output: 13.11 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [02:18<00:19, 11.99it/s, est. speed input: 13414.06 toks/s, output: 13.10 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [02:19<00:18, 11.98it/s, est. speed input: 13403.10 toks/s, output: 13.09 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [02:20<00:17, 11.98it/s, est. speed input: 13392.39 toks/s, output: 13.08 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [02:22<00:15, 11.98it/s, est. speed input: 13381.86 toks/s, output: 13.07 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [02:23<00:14, 11.98it/s, est. speed input: 13371.53 toks/s, output: 13.06 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [02:24<00:08, 15.87it/s, est. speed input: 13482.01 toks/s, output: 13.17 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [02:26<00:08, 14.71it/s, est. speed input: 13471.49 toks/s, output: 13.16 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [02:27<00:07, 13.90it/s, est. speed input: 13461.13 toks/s, output: 13.15 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [02:28<00:07, 13.34it/s, est. speed input: 13450.99 toks/s, output: 13.14 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [02:30<00:06, 12.95it/s, est. speed input: 13441.00 toks/s, output: 13.13 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [02:31<00:04, 12.67it/s, est. speed input: 13431.21 toks/s, output: 13.12 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [02:32<00:03, 12.47it/s, est. speed input: 13421.34 toks/s, output: 13.11 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [02:34<00:02, 12.30it/s, est. speed input: 13410.69 toks/s, output: 13.10 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [02:35<00:01, 12.82it/s, est. speed input: 13419.10 toks/s, output: 13.10 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:35<00:00, 12.82it/s, est. speed input: 13511.43 toks/s, output: 13.19 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:35<00:00, 13.19it/s, est. speed input: 13511.43 toks/s, output: 13.19 toks/s]
[rank0]:[W126 04:30:32.817553734 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 04:30:35
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:31:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:31:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=178350) WARNING 01-26 04:31:20 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=178350) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=178350) WARNING 01-26 04:31:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 5.33 requests/s, 5466.83 total tokens/s, 5.33 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 04:31:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:31:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:31:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:31:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:31:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:31:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:31:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:31:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:31:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:31:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:31:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:31:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:31:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:31:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:31:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:31:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:31:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=178350) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=178350) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.06it/s]
(EngineCore_DP0 pid=178350) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.20it/s]
(EngineCore_DP0 pid=178350) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.89it/s]
(EngineCore_DP0 pid=178350) 
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=178350) [2026-01-26 04:31:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=178350) [rank0]:W0126 04:31:39.980000 178350 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=178350) [rank0]:W0126 04:31:40.089000 178350 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=178350) [rank0]:W0126 04:31:41.585000 178350 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=178350) [rank0]:W0126 04:31:41.759000 178350 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=178350) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  6.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  6.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  6.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  6.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  6.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.19it/s]
(EngineCore_DP0 pid=178350) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.17it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  7.82it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  8.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.71it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   0%|          | 20/4096 [00:00<00:20, 199.36it/s]
Adding requests:   1%|          | 43/4096 [00:00<00:18, 217.26it/s]
Adding requests:   2%|▏         | 69/4096 [00:00<00:17, 235.61it/s]
Adding requests:   2%|▏         | 93/4096 [00:00<00:17, 232.01it/s]
Adding requests:   3%|▎         | 118/4096 [00:00<00:16, 237.93it/s]
Adding requests:   3%|▎         | 143/4096 [00:00<00:16, 239.75it/s]
Adding requests:   4%|▍         | 169/4096 [00:00<00:16, 243.19it/s]
Adding requests:   5%|▍         | 194/4096 [00:00<00:16, 234.31it/s]
Adding requests:   5%|▌         | 219/4096 [00:00<00:16, 236.71it/s]
Adding requests:   6%|▌         | 244/4096 [00:01<00:16, 238.76it/s]
Adding requests:   7%|▋         | 269/4096 [00:01<00:15, 241.91it/s]
Adding requests:   7%|▋         | 294/4096 [00:01<00:16, 229.18it/s]
Adding requests:   8%|▊         | 320/4096 [00:01<00:15, 236.68it/s]
Adding requests:   8%|▊         | 348/4096 [00:01<00:15, 246.31it/s]
Adding requests:   9%|▉         | 376/4096 [00:01<00:14, 254.28it/s]
Adding requests:  10%|▉         | 402/4096 [00:01<00:15, 245.41it/s]
Adding requests:  10%|█         | 427/4096 [00:01<00:14, 245.64it/s]
Adding requests:  11%|█         | 454/4096 [00:01<00:14, 250.98it/s]
Adding requests:  12%|█▏        | 482/4096 [00:01<00:14, 256.50it/s]
Adding requests:  12%|█▏        | 508/4096 [00:02<00:14, 242.23it/s]
Adding requests:  13%|█▎        | 535/4096 [00:02<00:14, 247.07it/s]
Adding requests:  14%|█▍        | 564/4096 [00:02<00:13, 257.83it/s]
Adding requests:  14%|█▍        | 590/4096 [00:02<00:13, 255.70it/s]
Adding requests:  15%|█▌        | 616/4096 [00:02<00:14, 242.26it/s]
Adding requests:  24%|██▍       | 978/4096 [00:02<00:02, 1181.71it/s]
Adding requests:  27%|██▋       | 1102/4096 [00:03<00:05, 548.77it/s]
Adding requests:  29%|██▉       | 1196/4096 [00:03<00:06, 422.21it/s]
Adding requests:  31%|███       | 1269/4096 [00:03<00:07, 361.20it/s]
Adding requests:  32%|███▏      | 1328/4096 [00:04<00:08, 329.31it/s]
Adding requests:  34%|███▎      | 1376/4096 [00:04<00:08, 308.74it/s]
Adding requests:  35%|███▍      | 1417/4096 [00:04<00:09, 292.92it/s]
Adding requests:  35%|███▌      | 1453/4096 [00:04<00:09, 278.26it/s]
Adding requests:  36%|███▋      | 1485/4096 [00:04<00:09, 275.34it/s]
Adding requests:  37%|███▋      | 1516/4096 [00:04<00:09, 274.31it/s]
Adding requests:  38%|███▊      | 1546/4096 [00:04<00:09, 259.79it/s]
Adding requests:  38%|███▊      | 1574/4096 [00:05<00:10, 251.50it/s]
Adding requests:  39%|███▉      | 1600/4096 [00:05<00:09, 249.76it/s]
Adding requests:  40%|███▉      | 1626/4096 [00:05<00:09, 247.12it/s]
Adding requests:  40%|████      | 1651/4096 [00:05<00:10, 234.64it/s]
Adding requests:  41%|████      | 1675/4096 [00:05<00:10, 231.65it/s]
Adding requests:  42%|████▏     | 1703/4096 [00:05<00:09, 241.86it/s]
Adding requests:  42%|████▏     | 1730/4096 [00:05<00:09, 247.24it/s]
Adding requests:  43%|████▎     | 1755/4096 [00:05<00:09, 240.35it/s]
Adding requests:  43%|████▎     | 1781/4096 [00:05<00:09, 245.62it/s]
Adding requests:  44%|████▍     | 1807/4096 [00:06<00:09, 248.47it/s]
Adding requests:  45%|████▍     | 1834/4096 [00:06<00:08, 253.97it/s]
Adding requests:  45%|████▌     | 1860/4096 [00:06<00:09, 243.37it/s]
Adding requests:  46%|████▌     | 1886/4096 [00:06<00:08, 246.82it/s]
Adding requests:  47%|████▋     | 1912/4096 [00:06<00:08, 249.46it/s]
Adding requests:  47%|████▋     | 1940/4096 [00:06<00:08, 258.01it/s]
Adding requests:  48%|████▊     | 1966/4096 [00:06<00:08, 245.31it/s]
Adding requests:  49%|████▊     | 1991/4096 [00:06<00:08, 242.96it/s]
Adding requests:  49%|████▉     | 2016/4096 [00:06<00:08, 240.45it/s]
Adding requests:  50%|████▉     | 2041/4096 [00:07<00:08, 242.18it/s]
Adding requests:  50%|█████     | 2066/4096 [00:07<00:09, 223.44it/s]
Adding requests:  51%|█████     | 2089/4096 [00:07<00:08, 223.86it/s]
Adding requests:  52%|█████▏    | 2117/4096 [00:07<00:08, 237.80it/s]
Adding requests:  52%|█████▏    | 2142/4096 [00:07<00:08, 235.22it/s]
Adding requests:  53%|█████▎    | 2166/4096 [00:07<00:08, 221.64it/s]
Adding requests:  53%|█████▎    | 2190/4096 [00:07<00:08, 226.59it/s]
Adding requests:  54%|█████▍    | 2215/4096 [00:07<00:08, 232.06it/s]
Adding requests:  55%|█████▍    | 2240/4096 [00:07<00:07, 235.77it/s]
Adding requests:  55%|█████▌    | 2264/4096 [00:08<00:08, 227.79it/s]
Adding requests:  56%|█████▌    | 2291/4096 [00:08<00:07, 238.35it/s]
Adding requests:  57%|█████▋    | 2317/4096 [00:08<00:07, 243.47it/s]
Adding requests:  57%|█████▋    | 2344/4096 [00:08<00:06, 250.96it/s]
Adding requests:  58%|█████▊    | 2370/4096 [00:08<00:07, 235.32it/s]
Adding requests:  59%|█████▊    | 2397/4096 [00:08<00:06, 244.94it/s]
Adding requests:  59%|█████▉    | 2424/4096 [00:08<00:06, 251.55it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:08<00:06, 241.22it/s]
Adding requests:  60%|██████    | 2475/4096 [00:08<00:06, 233.35it/s]
Adding requests:  61%|██████    | 2502/4096 [00:08<00:06, 243.50it/s]
Adding requests:  62%|██████▏   | 2531/4096 [00:09<00:06, 255.26it/s]
Adding requests:  62%|██████▎   | 2560/4096 [00:09<00:05, 263.35it/s]
Adding requests:  63%|██████▎   | 2587/4096 [00:09<00:05, 255.01it/s]
Adding requests:  64%|██████▍   | 2613/4096 [00:09<00:05, 254.97it/s]
Adding requests:  64%|██████▍   | 2639/4096 [00:09<00:05, 254.42it/s]
Adding requests:  65%|██████▌   | 2666/4096 [00:09<00:05, 258.19it/s]
Adding requests:  66%|██████▌   | 2692/4096 [00:09<00:05, 243.97it/s]
Adding requests:  66%|██████▋   | 2717/4096 [00:09<00:05, 244.78it/s]
Adding requests:  67%|██████▋   | 2745/4096 [00:09<00:05, 252.24it/s]
Adding requests:  68%|██████▊   | 2773/4096 [00:10<00:05, 256.41it/s]
Adding requests:  68%|██████▊   | 2799/4096 [00:10<00:05, 249.89it/s]
Adding requests:  69%|██████▉   | 2825/4096 [00:10<00:05, 251.58it/s]
Adding requests:  70%|██████▉   | 2852/4096 [00:10<00:04, 256.57it/s]
Adding requests:  70%|███████   | 2879/4096 [00:10<00:04, 260.06it/s]
Adding requests:  71%|███████   | 2906/4096 [00:10<00:04, 245.00it/s]
Adding requests:  72%|███████▏  | 2931/4096 [00:10<00:04, 244.19it/s]
Adding requests:  72%|███████▏  | 2959/4096 [00:10<00:04, 253.77it/s]
Adding requests:  73%|███████▎  | 2985/4096 [00:10<00:04, 251.40it/s]
Adding requests:  74%|███████▎  | 3011/4096 [00:11<00:04, 241.94it/s]
Adding requests:  74%|███████▍  | 3037/4096 [00:11<00:04, 244.27it/s]
Adding requests:  75%|███████▍  | 3064/4096 [00:11<00:04, 250.93it/s]
Adding requests:  75%|███████▌  | 3091/4096 [00:11<00:03, 254.27it/s]
Adding requests:  76%|███████▌  | 3117/4096 [00:11<00:03, 245.33it/s]
Adding requests:  77%|███████▋  | 3142/4096 [00:11<00:03, 242.48it/s]
Adding requests:  77%|███████▋  | 3168/4096 [00:11<00:03, 247.23it/s]
Adding requests:  78%|███████▊  | 3195/4096 [00:11<00:03, 253.81it/s]
Adding requests:  79%|███████▊  | 3221/4096 [00:11<00:03, 247.50it/s]
Adding requests:  79%|███████▉  | 3246/4096 [00:11<00:03, 245.78it/s]
Adding requests:  80%|███████▉  | 3273/4096 [00:12<00:03, 252.01it/s]
Adding requests:  81%|████████  | 3299/4096 [00:12<00:03, 247.05it/s]
Adding requests:  81%|████████  | 3324/4096 [00:12<00:03, 239.73it/s]
Adding requests:  82%|████████▏ | 3349/4096 [00:12<00:03, 238.97it/s]
Adding requests:  82%|████████▏ | 3377/4096 [00:12<00:02, 250.14it/s]
Adding requests:  83%|████████▎ | 3405/4096 [00:12<00:02, 257.23it/s]
Adding requests:  84%|████████▍ | 3431/4096 [00:12<00:02, 245.69it/s]
Adding requests:  84%|████████▍ | 3456/4096 [00:12<00:02, 245.40it/s]
Adding requests:  85%|████████▌ | 3482/4096 [00:12<00:02, 249.19it/s]
Adding requests:  86%|████████▌ | 3511/4096 [00:13<00:02, 259.16it/s]
Adding requests:  86%|████████▋ | 3537/4096 [00:13<00:02, 258.66it/s]
Adding requests:  87%|████████▋ | 3563/4096 [00:13<00:02, 254.55it/s]
Adding requests:  88%|████████▊ | 3589/4096 [00:13<00:01, 255.03it/s]
Adding requests:  88%|████████▊ | 3616/4096 [00:13<00:01, 258.79it/s]
Adding requests:  89%|████████▉ | 3642/4096 [00:13<00:01, 257.05it/s]
Adding requests:  90%|████████▉ | 3668/4096 [00:13<00:01, 245.77it/s]
Adding requests:  90%|█████████ | 3694/4096 [00:13<00:01, 248.43it/s]
Adding requests:  91%|█████████ | 3720/4096 [00:13<00:01, 247.78it/s]
Adding requests:  91%|█████████▏| 3745/4096 [00:13<00:01, 233.09it/s]
Adding requests:  92%|█████████▏| 3769/4096 [00:14<00:01, 225.93it/s]
Adding requests:  93%|█████████▎| 3792/4096 [00:14<00:01, 224.11it/s]
Adding requests:  93%|█████████▎| 3816/4096 [00:14<00:01, 228.37it/s]
Adding requests:  94%|█████████▎| 3839/4096 [00:14<00:01, 219.20it/s]
Adding requests:  94%|█████████▍| 3862/4096 [00:14<00:01, 217.73it/s]
Adding requests:  95%|█████████▍| 3889/4096 [00:14<00:00, 229.57it/s]
Adding requests:  96%|█████████▌| 3913/4096 [00:14<00:00, 230.97it/s]
Adding requests:  96%|█████████▌| 3937/4096 [00:14<00:00, 228.31it/s]
Adding requests:  97%|█████████▋| 3961/4096 [00:14<00:00, 229.10it/s]
Adding requests:  97%|█████████▋| 3987/4096 [00:15<00:00, 237.27it/s]
Adding requests:  98%|█████████▊| 4014/4096 [00:15<00:00, 244.60it/s]
Adding requests:  99%|█████████▊| 4039/4096 [00:15<00:00, 240.46it/s]
Adding requests:  99%|█████████▉| 4064/4096 [00:15<00:00, 232.20it/s]
Adding requests: 100%|█████████▉| 4089/4096 [00:15<00:00, 236.92it/s]
Adding requests: 100%|██████████| 4096/4096 [00:15<00:00, 264.48it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 66/4096 [00:01<01:46, 37.71it/s, est. speed input: 38620.48 toks/s, output: 37.71 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:07<06:09, 10.82it/s, est. speed input: 12940.52 toks/s, output: 12.64 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:13<08:25,  7.84it/s, est. speed input: 9675.06 toks/s, output: 9.45 toks/s] 
Processed prompts:   4%|▍         | 162/4096 [00:18<08:44,  7.50it/s, est. speed input: 9030.50 toks/s, output: 8.82 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:24<09:50,  6.61it/s, est. speed input: 8150.44 toks/s, output: 7.96 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:30<10:30,  6.14it/s, est. speed input: 7617.78 toks/s, output: 7.44 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:36<10:55,  5.86it/s, est. speed input: 7261.12 toks/s, output: 7.09 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:42<11:09,  5.68it/s, est. speed input: 7004.50 toks/s, output: 6.84 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:48<11:17,  5.57it/s, est. speed input: 6811.94 toks/s, output: 6.65 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:53<10:31,  5.92it/s, est. speed input: 6836.89 toks/s, output: 6.68 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:59<10:47,  5.73it/s, est. speed input: 6695.86 toks/s, output: 6.54 toks/s]
Processed prompts:  10%|█         | 418/4096 [01:05<10:56,  5.60it/s, est. speed input: 6580.80 toks/s, output: 6.43 toks/s]
Processed prompts:  11%|█         | 450/4096 [01:11<11:01,  5.51it/s, est. speed input: 6485.39 toks/s, output: 6.33 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [01:17<11:02,  5.46it/s, est. speed input: 6404.89 toks/s, output: 6.25 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [01:21<10:14,  5.83it/s, est. speed input: 6444.01 toks/s, output: 6.29 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [01:27<10:26,  5.67it/s, est. speed input: 6376.12 toks/s, output: 6.23 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [01:33<10:32,  5.56it/s, est. speed input: 6316.88 toks/s, output: 6.17 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [01:39<10:35,  5.49it/s, est. speed input: 6264.84 toks/s, output: 6.12 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [01:45<10:35,  5.44it/s, est. speed input: 6218.62 toks/s, output: 6.07 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [01:50<09:48,  5.81it/s, est. speed input: 6255.63 toks/s, output: 6.11 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [01:56<09:58,  5.66it/s, est. speed input: 6214.38 toks/s, output: 6.07 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [02:02<10:04,  5.55it/s, est. speed input: 6176.96 toks/s, output: 6.03 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [02:08<10:00,  5.54it/s, est. speed input: 6152.97 toks/s, output: 6.01 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [02:14<10:01,  5.47it/s, est. speed input: 6121.42 toks/s, output: 5.98 toks/s]
Processed prompts:  20%|██        | 834/4096 [02:20<10:00,  5.43it/s, est. speed input: 6092.62 toks/s, output: 5.95 toks/s]
Processed prompts:  21%|██        | 866/4096 [02:24<09:16,  5.81it/s, est. speed input: 6124.92 toks/s, output: 5.98 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [02:30<09:25,  5.66it/s, est. speed input: 6098.50 toks/s, output: 5.96 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [02:36<09:30,  5.55it/s, est. speed input: 6073.25 toks/s, output: 5.93 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [02:42<09:32,  5.48it/s, est. speed input: 6049.81 toks/s, output: 5.91 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [02:48<09:31,  5.43it/s, est. speed input: 6028.15 toks/s, output: 5.89 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [02:54<09:28,  5.40it/s, est. speed input: 6008.37 toks/s, output: 5.87 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [03:00<09:22,  5.41it/s, est. speed input: 5993.67 toks/s, output: 5.85 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [03:06<09:17,  5.39it/s, est. speed input: 5977.40 toks/s, output: 5.84 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [03:11<08:26,  5.88it/s, est. speed input: 6014.34 toks/s, output: 5.87 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [03:17<08:36,  5.70it/s, est. speed input: 5997.42 toks/s, output: 5.86 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [03:23<08:41,  5.58it/s, est. speed input: 5981.47 toks/s, output: 5.84 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [03:29<08:42,  5.51it/s, est. speed input: 5966.43 toks/s, output: 5.83 toks/s]
Processed prompts:  31%|███       | 1250/4096 [03:35<08:42,  5.45it/s, est. speed input: 5952.14 toks/s, output: 5.81 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [03:39<07:57,  5.90it/s, est. speed input: 5982.52 toks/s, output: 5.84 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [03:45<08:06,  5.72it/s, est. speed input: 5969.17 toks/s, output: 5.83 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [03:51<08:11,  5.60it/s, est. speed input: 5955.97 toks/s, output: 5.82 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [03:57<08:12,  5.52it/s, est. speed input: 5943.38 toks/s, output: 5.80 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [04:03<08:12,  5.46it/s, est. speed input: 5931.42 toks/s, output: 5.79 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [04:09<08:09,  5.42it/s, est. speed input: 5920.06 toks/s, output: 5.78 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [04:14<07:33,  5.78it/s, est. speed input: 5940.11 toks/s, output: 5.80 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [04:20<07:39,  5.64it/s, est. speed input: 5928.96 toks/s, output: 5.79 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [04:26<07:41,  5.54it/s, est. speed input: 5918.30 toks/s, output: 5.78 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [04:32<07:41,  5.48it/s, est. speed input: 5908.09 toks/s, output: 5.77 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [04:37<07:34,  5.49it/s, est. speed input: 5902.76 toks/s, output: 5.76 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [04:42<07:01,  5.85it/s, est. speed input: 5921.82 toks/s, output: 5.78 toks/s]
Processed prompts:  41%|████      | 1666/4096 [04:48<07:07,  5.68it/s, est. speed input: 5912.16 toks/s, output: 5.77 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [04:54<07:10,  5.57it/s, est. speed input: 5902.88 toks/s, output: 5.76 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [05:00<07:10,  5.50it/s, est. speed input: 5894.00 toks/s, output: 5.76 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [05:06<07:08,  5.44it/s, est. speed input: 5885.41 toks/s, output: 5.75 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [05:12<07:05,  5.41it/s, est. speed input: 5877.20 toks/s, output: 5.74 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [05:17<06:32,  5.78it/s, est. speed input: 5894.59 toks/s, output: 5.76 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [05:23<06:36,  5.64it/s, est. speed input: 5886.46 toks/s, output: 5.75 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [05:29<06:37,  5.54it/s, est. speed input: 5878.62 toks/s, output: 5.74 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [05:35<06:36,  5.48it/s, est. speed input: 5871.08 toks/s, output: 5.73 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [05:41<06:34,  5.43it/s, est. speed input: 5863.78 toks/s, output: 5.73 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [05:45<06:03,  5.80it/s, est. speed input: 5879.90 toks/s, output: 5.74 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [05:51<06:07,  5.65it/s, est. speed input: 5872.73 toks/s, output: 5.74 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [05:57<06:08,  5.55it/s, est. speed input: 5865.70 toks/s, output: 5.73 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [06:03<06:07,  5.48it/s, est. speed input: 5858.93 toks/s, output: 5.72 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [06:09<06:04,  5.43it/s, est. speed input: 5852.37 toks/s, output: 5.72 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [06:14<05:35,  5.81it/s, est. speed input: 5867.47 toks/s, output: 5.73 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [06:20<05:35,  5.72it/s, est. speed input: 5864.28 toks/s, output: 5.73 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [06:26<05:36,  5.60it/s, est. speed input: 5857.98 toks/s, output: 5.72 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [06:32<05:36,  5.51it/s, est. speed input: 5851.80 toks/s, output: 5.71 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [06:38<05:34,  5.45it/s, est. speed input: 5845.71 toks/s, output: 5.71 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [06:44<05:30,  5.41it/s, est. speed input: 5839.87 toks/s, output: 5.70 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [06:48<05:03,  5.79it/s, est. speed input: 5854.00 toks/s, output: 5.72 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [06:54<05:05,  5.65it/s, est. speed input: 5848.69 toks/s, output: 5.71 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [07:00<05:05,  5.55it/s, est. speed input: 5842.75 toks/s, output: 5.71 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [07:07<05:03,  5.47it/s, est. speed input: 5836.86 toks/s, output: 5.70 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [07:13<05:00,  5.42it/s, est. speed input: 5831.25 toks/s, output: 5.69 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [07:17<04:35,  5.81it/s, est. speed input: 5845.06 toks/s, output: 5.71 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [07:23<04:36,  5.66it/s, est. speed input: 5839.99 toks/s, output: 5.70 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [07:29<04:35,  5.56it/s, est. speed input: 5835.09 toks/s, output: 5.70 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [07:35<04:33,  5.49it/s, est. speed input: 5830.18 toks/s, output: 5.69 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [07:41<04:30,  5.44it/s, est. speed input: 5825.30 toks/s, output: 5.69 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [07:47<04:25,  5.41it/s, est. speed input: 5820.57 toks/s, output: 5.68 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [07:52<04:03,  5.77it/s, est. speed input: 5832.42 toks/s, output: 5.70 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [07:58<04:03,  5.63it/s, est. speed input: 5827.72 toks/s, output: 5.69 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [08:04<04:02,  5.54it/s, est. speed input: 5823.12 toks/s, output: 5.69 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [08:10<03:59,  5.47it/s, est. speed input: 5818.65 toks/s, output: 5.68 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [08:16<03:55,  5.43it/s, est. speed input: 5814.27 toks/s, output: 5.68 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [08:20<03:34,  5.80it/s, est. speed input: 5825.81 toks/s, output: 5.69 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [08:26<03:34,  5.65it/s, est. speed input: 5821.45 toks/s, output: 5.69 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [08:32<03:32,  5.55it/s, est. speed input: 5817.19 toks/s, output: 5.68 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [08:38<03:29,  5.48it/s, est. speed input: 5813.02 toks/s, output: 5.68 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [08:44<03:25,  5.44it/s, est. speed input: 5808.95 toks/s, output: 5.67 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [08:50<03:21,  5.40it/s, est. speed input: 5804.95 toks/s, output: 5.67 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [08:55<03:02,  5.78it/s, est. speed input: 5815.88 toks/s, output: 5.68 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [09:01<03:01,  5.64it/s, est. speed input: 5811.89 toks/s, output: 5.68 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [09:07<02:58,  5.54it/s, est. speed input: 5808.00 toks/s, output: 5.67 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [09:13<02:54,  5.48it/s, est. speed input: 5804.19 toks/s, output: 5.67 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [09:19<02:50,  5.43it/s, est. speed input: 5800.44 toks/s, output: 5.66 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [09:24<02:34,  5.80it/s, est. speed input: 5810.84 toks/s, output: 5.67 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [09:30<02:32,  5.65it/s, est. speed input: 5807.13 toks/s, output: 5.67 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [09:36<02:29,  5.55it/s, est. speed input: 5803.48 toks/s, output: 5.67 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [09:42<02:25,  5.48it/s, est. speed input: 5799.90 toks/s, output: 5.66 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [09:48<02:20,  5.44it/s, est. speed input: 5796.40 toks/s, output: 5.66 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [09:52<02:06,  5.81it/s, est. speed input: 5806.33 toks/s, output: 5.67 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [09:58<02:04,  5.65it/s, est. speed input: 5802.83 toks/s, output: 5.67 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [10:04<02:00,  5.55it/s, est. speed input: 5799.40 toks/s, output: 5.66 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [10:10<01:56,  5.48it/s, est. speed input: 5796.01 toks/s, output: 5.66 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [10:16<01:51,  5.44it/s, est. speed input: 5792.69 toks/s, output: 5.66 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [10:22<01:46,  5.40it/s, est. speed input: 5789.44 toks/s, output: 5.65 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [10:27<01:33,  5.78it/s, est. speed input: 5798.93 toks/s, output: 5.66 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [10:33<01:30,  5.64it/s, est. speed input: 5795.71 toks/s, output: 5.66 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [10:39<01:26,  5.54it/s, est. speed input: 5792.44 toks/s, output: 5.66 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [10:45<01:21,  5.47it/s, est. speed input: 5789.14 toks/s, output: 5.65 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [10:51<01:15,  5.48it/s, est. speed input: 5787.81 toks/s, output: 5.65 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [10:56<01:05,  5.85it/s, est. speed input: 5797.18 toks/s, output: 5.66 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [11:02<01:01,  5.69it/s, est. speed input: 5794.32 toks/s, output: 5.66 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [11:08<00:57,  5.57it/s, est. speed input: 5791.05 toks/s, output: 5.66 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [11:14<00:52,  5.48it/s, est. speed input: 5787.49 toks/s, output: 5.65 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [11:20<00:46,  5.43it/s, est. speed input: 5784.51 toks/s, output: 5.65 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [11:26<00:41,  5.41it/s, est. speed input: 5781.83 toks/s, output: 5.65 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [11:30<00:32,  5.87it/s, est. speed input: 5792.77 toks/s, output: 5.66 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [11:36<00:27,  5.70it/s, est. speed input: 5789.88 toks/s, output: 5.65 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [11:42<00:22,  5.58it/s, est. speed input: 5787.04 toks/s, output: 5.65 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [11:48<00:17,  5.50it/s, est. speed input: 5784.25 toks/s, output: 5.65 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [11:54<00:11,  5.45it/s, est. speed input: 5781.49 toks/s, output: 5.65 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [11:58<00:05,  5.88it/s, est. speed input: 5791.42 toks/s, output: 5.66 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [11:58<00:00,  5.88it/s, est. speed input: 5834.14 toks/s, output: 5.70 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [11:58<00:00,  5.70it/s, est. speed input: 5834.14 toks/s, output: 5.70 toks/s]
[rank0]:[W126 04:44:06.224482967 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 04:44:09
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:45:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:45:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=190917) WARNING 01-26 04:45:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=190917) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=190917) WARNING 01-26 04:45:45 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     def forward(
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     raise e
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/tmp/torchinductor_root/ft/cftm23efk4phtqrn4bvlv2q3rwl6hdgmtnki6iioztkhpj2rkjah.py", line 1093, in call
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 8)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/RTX4090_cc89_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 263, in quant_slide_fp8_triton
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) ERROR 01-26 04:45:56 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered

STDERR:
[2026-01-26 04:45:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:45:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:45:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:45:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:45:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:45:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:45:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:45:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:45:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:45:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:45:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:45:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:45:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:45:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:45:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:45:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:45:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 04:45:32.791908659 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=190917) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=190917) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=190917) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]
(EngineCore_DP0 pid=190917) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.18s/it]
(EngineCore_DP0 pid=190917) 
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=190917) [2026-01-26 04:45:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=190917) [rank0]:W0126 04:45:53.486000 190917 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=190917) [rank0]:W0126 04:45:53.595000 190917 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=190917) [rank0]:W0126 04:45:55.201000 190917 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=190917) [rank0]:W0126 04:45:55.368000 190917 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=190917) Process EngineCore_DP0:
(EngineCore_DP0 pid=190917) Traceback (most recent call last):
(EngineCore_DP0 pid=190917)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=190917)     self.run()
(EngineCore_DP0 pid=190917)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=190917)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=190917)     raise e
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=190917)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=190917)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=190917)     super().__init__(
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=190917)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=190917)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=190917)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=190917)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=190917)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=190917)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=190917)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=190917)     return func(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=190917)     return func(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=190917)     self.model_runner.profile_run()
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=190917)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=190917)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=190917)     return func(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=190917)     outputs = self.model(
(EngineCore_DP0 pid=190917)               ^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=190917)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=190917)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=190917)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=190917)     hidden_states = self.model(
(EngineCore_DP0 pid=190917)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=190917)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=190917)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=190917)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=190917)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=190917)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=190917)     def forward(
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=190917)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=190917)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=190917)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=190917)     raise e
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=190917)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=190917)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=190917)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=190917)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=190917)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=190917)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=190917)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=190917)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=190917)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=190917)     return compiled_fn(full_args)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=190917)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=190917)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=190917)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=190917)                             ^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=190917)     outs = compiled_fn(args)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=190917)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=190917)     return self.current_callable(inputs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=190917)     out = model(new_inputs)
(EngineCore_DP0 pid=190917)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/tmp/torchinductor_root/ft/cftm23efk4phtqrn4bvlv2q3rwl6hdgmtnki6iioztkhpj2rkjah.py", line 1093, in call
(EngineCore_DP0 pid=190917)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 8)
(EngineCore_DP0 pid=190917)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=190917)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=190917)     return fn(input, L)
(EngineCore_DP0 pid=190917)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/RTX4090_cc89_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 263, in quant_slide_fp8_triton
(EngineCore_DP0 pid=190917)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=190917)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=190917)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=190917)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=190917)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=190917)     self._init_handles()
(EngineCore_DP0 pid=190917)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=190917)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=190917)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=190917) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 04:45:57.804488310 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 07:58:17
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 07:58:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 07:58:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=374506) WARNING 01-26 07:58:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=374506) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=374506) WARNING 01-26 07:59:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 9.53 requests/s, 4888.64 total tokens/s, 9.53 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 07:58:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 07:58:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 07:58:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 07:58:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 07:58:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 07:58:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 07:58:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 07:58:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 07:58:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 07:58:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 07:58:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:58:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 07:58:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 07:58:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 07:58:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 07:58:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 07:58:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=374506) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=374506) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:16,  5.51s/it]
(EngineCore_DP0 pid=374506) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:10<00:10,  5.25s/it]
(EngineCore_DP0 pid=374506) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.43s/it]
(EngineCore_DP0 pid=374506) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:17<00:00,  4.30s/it]
(EngineCore_DP0 pid=374506) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:17<00:00,  4.37s/it]
(EngineCore_DP0 pid=374506) 
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=374506) [2026-01-26 07:58:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=374506) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=374506) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  4.70it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  33%|███▎      | 42/128 [00:00<00:00, 413.59it/s]
Adding requests:  69%|██████▉   | 88/128 [00:00<00:00, 437.81it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 438.04it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:27,  4.69it/s, est. speed input: 2401.64 toks/s, output: 4.69 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:18,  6.65it/s, est. speed input: 3203.07 toks/s, output: 6.26 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:16,  7.73it/s, est. speed input: 3623.52 toks/s, output: 7.08 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:14,  8.33it/s, est. speed input: 3866.97 toks/s, output: 7.55 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:14,  8.72it/s, est. speed input: 4031.51 toks/s, output: 7.87 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:13,  8.97it/s, est. speed input: 4150.72 toks/s, output: 8.11 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:13,  9.15it/s, est. speed input: 4241.80 toks/s, output: 8.28 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:12,  9.30it/s, est. speed input: 4318.18 toks/s, output: 8.43 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:01<00:12,  9.41it/s, est. speed input: 4378.90 toks/s, output: 8.55 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:01<00:12,  9.49it/s, est. speed input: 4430.32 toks/s, output: 8.65 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:01<00:12,  9.56it/s, est. speed input: 4475.47 toks/s, output: 8.74 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:01<00:12,  9.62it/s, est. speed input: 4514.54 toks/s, output: 8.82 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:11,  9.66it/s, est. speed input: 4548.01 toks/s, output: 8.88 toks/s]
Processed prompts:  11%|█         | 14/128 [00:01<00:11,  9.66it/s, est. speed input: 4574.27 toks/s, output: 8.93 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:11,  9.64it/s, est. speed input: 4595.06 toks/s, output: 8.97 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:01<00:11,  9.69it/s, est. speed input: 4620.07 toks/s, output: 9.02 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:11,  9.79it/s, est. speed input: 4666.66 toks/s, output: 9.11 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:02<00:11,  9.79it/s, est. speed input: 4683.23 toks/s, output: 9.15 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:02<00:11,  9.81it/s, est. speed input: 4700.87 toks/s, output: 9.18 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:02<00:10,  9.82it/s, est. speed input: 4716.02 toks/s, output: 9.21 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:02<00:10,  9.84it/s, est. speed input: 4730.65 toks/s, output: 9.24 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:02<00:10,  9.80it/s, est. speed input: 4740.16 toks/s, output: 9.26 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:02<00:10,  9.79it/s, est. speed input: 4750.59 toks/s, output: 9.28 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:02<00:10,  9.79it/s, est. speed input: 4760.90 toks/s, output: 9.30 toks/s]
Processed prompts:  20%|██        | 26/128 [00:02<00:10,  9.78it/s, est. speed input: 4769.52 toks/s, output: 9.32 toks/s]
Processed prompts:  21%|██        | 27/128 [00:02<00:10,  9.78it/s, est. speed input: 4777.57 toks/s, output: 9.33 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:02<00:10,  9.79it/s, est. speed input: 4785.97 toks/s, output: 9.35 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:03<00:10,  9.80it/s, est. speed input: 4794.14 toks/s, output: 9.36 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:03<00:10,  9.78it/s, est. speed input: 4800.27 toks/s, output: 9.38 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:03<00:09,  9.76it/s, est. speed input: 4805.37 toks/s, output: 9.39 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:03<00:09,  9.81it/s, est. speed input: 4813.90 toks/s, output: 9.40 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:03<00:09,  9.84it/s, est. speed input: 4821.47 toks/s, output: 9.42 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:03<00:09,  9.84it/s, est. speed input: 4827.63 toks/s, output: 9.43 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:03<00:09,  9.88it/s, est. speed input: 4835.11 toks/s, output: 9.44 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:03<00:09,  9.92it/s, est. speed input: 4842.59 toks/s, output: 9.46 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:03<00:09,  9.93it/s, est. speed input: 4849.27 toks/s, output: 9.47 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:04<00:09,  9.90it/s, est. speed input: 4854.04 toks/s, output: 9.48 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:04<00:01, 45.01it/s, est. speed input: 6614.51 toks/s, output: 12.92 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:04<00:02, 24.41it/s, est. speed input: 6477.58 toks/s, output: 12.65 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:04<00:03, 18.80it/s, est. speed input: 6389.44 toks/s, output: 12.48 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:05<00:04, 15.64it/s, est. speed input: 6312.53 toks/s, output: 12.33 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:05<00:04, 14.16it/s, est. speed input: 6264.01 toks/s, output: 12.23 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:05<00:04, 13.02it/s, est. speed input: 6220.12 toks/s, output: 12.15 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:05<00:04, 12.20it/s, est. speed input: 6182.52 toks/s, output: 12.08 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:05<00:04, 11.61it/s, est. speed input: 6148.26 toks/s, output: 12.01 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:06<00:04, 11.11it/s, est. speed input: 6113.03 toks/s, output: 11.94 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:06<00:04, 10.79it/s, est. speed input: 6081.82 toks/s, output: 11.88 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:06<00:04, 10.53it/s, est. speed input: 6051.22 toks/s, output: 11.82 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:06<00:04, 10.32it/s, est. speed input: 6020.35 toks/s, output: 11.76 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:07<00:04, 10.20it/s, est. speed input: 5993.11 toks/s, output: 11.71 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:07<00:04, 10.08it/s, est. speed input: 5965.80 toks/s, output: 11.65 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:07<00:04,  9.90it/s, est. speed input: 5934.48 toks/s, output: 11.59 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:07<00:04,  9.88it/s, est. speed input: 5921.93 toks/s, output: 11.57 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:07<00:04,  9.87it/s, est. speed input: 5909.87 toks/s, output: 11.54 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:07<00:03,  9.86it/s, est. speed input: 5898.19 toks/s, output: 11.52 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:07<00:03,  9.74it/s, est. speed input: 5883.48 toks/s, output: 11.49 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:07<00:03,  9.71it/s, est. speed input: 5870.89 toks/s, output: 11.47 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:08<00:03,  9.69it/s, est. speed input: 5858.71 toks/s, output: 11.44 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:08<00:03,  9.66it/s, est. speed input: 5846.50 toks/s, output: 11.42 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:08<00:03,  9.73it/s, est. speed input: 5836.98 toks/s, output: 11.40 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:08<00:03,  9.79it/s, est. speed input: 5828.11 toks/s, output: 11.38 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:08<00:03,  9.84it/s, est. speed input: 5819.47 toks/s, output: 11.37 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:08<00:03,  9.84it/s, est. speed input: 5810.21 toks/s, output: 11.35 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:08<00:03,  9.84it/s, est. speed input: 5801.15 toks/s, output: 11.33 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:08<00:02,  9.91it/s, est. speed input: 5785.66 toks/s, output: 11.30 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:09<00:02,  9.96it/s, est. speed input: 5771.30 toks/s, output: 11.27 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:09<00:02,  9.94it/s, est. speed input: 5763.43 toks/s, output: 11.26 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:09<00:02,  9.94it/s, est. speed input: 5756.10 toks/s, output: 11.24 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:09<00:02,  9.95it/s, est. speed input: 5742.16 toks/s, output: 11.22 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:09<00:02,  9.93it/s, est. speed input: 5734.79 toks/s, output: 11.20 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:09<00:02,  9.89it/s, est. speed input: 5726.97 toks/s, output: 11.19 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:09<00:01,  9.89it/s, est. speed input: 5720.07 toks/s, output: 11.17 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:09<00:01,  9.91it/s, est. speed input: 5713.80 toks/s, output: 11.16 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:09<00:01,  9.90it/s, est. speed input: 5707.05 toks/s, output: 11.15 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:10<00:01,  9.91it/s, est. speed input: 5700.86 toks/s, output: 11.13 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:10<00:01,  9.89it/s, est. speed input: 5694.17 toks/s, output: 11.12 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:10<00:01,  9.89it/s, est. speed input: 5688.03 toks/s, output: 11.11 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:10<00:01,  9.91it/s, est. speed input: 5682.25 toks/s, output: 11.10 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:10<00:01,  9.92it/s, est. speed input: 5676.59 toks/s, output: 11.09 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:10<00:01,  9.93it/s, est. speed input: 5671.00 toks/s, output: 11.08 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:10<00:01,  9.91it/s, est. speed input: 5665.19 toks/s, output: 11.06 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:10<00:00,  9.87it/s, est. speed input: 5658.98 toks/s, output: 11.05 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:10<00:00,  9.85it/s, est. speed input: 5652.90 toks/s, output: 11.04 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:10<00:00,  9.87it/s, est. speed input: 5647.54 toks/s, output: 11.03 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:11<00:00,  9.85it/s, est. speed input: 5641.82 toks/s, output: 11.02 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:11<00:00,  9.83it/s, est. speed input: 5635.98 toks/s, output: 11.01 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:11<00:00,  9.81it/s, est. speed input: 5630.24 toks/s, output: 11.00 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:11<00:00,  9.75it/s, est. speed input: 5623.82 toks/s, output: 10.98 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:11<00:00,  9.71it/s, est. speed input: 5617.45 toks/s, output: 10.97 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:11<00:00,  9.67it/s, est. speed input: 5611.01 toks/s, output: 10.96 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:11<00:00,  9.65it/s, est. speed input: 5604.74 toks/s, output: 10.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:11<00:00,  9.65it/s, est. speed input: 5604.74 toks/s, output: 10.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:11<00:00, 10.95it/s, est. speed input: 5604.74 toks/s, output: 10.95 toks/s]
[rank0]:[W126 07:59:43.665580114 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 07:59:46
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 07:59:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 07:59:56 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=376008) WARNING 01-26 08:00:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=376008) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=376008) WARNING 01-26 08:00:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 9.34 requests/s, 9574.02 total tokens/s, 9.34 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 07:59:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 07:59:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 07:59:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 07:59:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:59:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:59:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:59:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:59:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 07:59:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 07:59:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 07:59:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 07:59:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 07:59:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 07:59:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:00:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:00:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:00:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:00:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:00:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:00:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:00:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:00:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:00:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:00:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:00:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:00:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:00:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:00:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:05] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=376008) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=376008) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.15it/s]
(EngineCore_DP0 pid=376008) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.25it/s]
(EngineCore_DP0 pid=376008) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.04s/it]
(EngineCore_DP0 pid=376008) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.03it/s]
(EngineCore_DP0 pid=376008) 
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=376008) [2026-01-26 08:00:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=376008) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.95it/s]
(EngineCore_DP0 pid=376008) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  17%|█▋        | 22/128 [00:00<00:00, 215.95it/s]
Adding requests:  37%|███▋      | 47/128 [00:00<00:00, 230.56it/s]
Adding requests:  57%|█████▋    | 73/128 [00:00<00:00, 241.54it/s]
Adding requests:  77%|███████▋  | 99/128 [00:00<00:00, 245.47it/s]
Adding requests:  98%|█████████▊| 125/128 [00:00<00:00, 250.49it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 243.81it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:07, 17.42it/s, est. speed input: 17840.10 toks/s, output: 17.42 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:09, 12.40it/s, est. speed input: 13396.75 toks/s, output: 13.08 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:10, 11.06it/s, est. speed input: 12124.30 toks/s, output: 11.84 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:11, 10.50it/s, est. speed input: 11546.06 toks/s, output: 11.28 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:01<00:11, 10.16it/s, est. speed input: 11185.34 toks/s, output: 10.92 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:11, 10.00it/s, est. speed input: 10970.70 toks/s, output: 10.71 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:11,  9.84it/s, est. speed input: 10787.57 toks/s, output: 10.53 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:01<00:11,  9.77it/s, est. speed input: 10715.65 toks/s, output: 10.46 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:11,  9.72it/s, est. speed input: 10653.65 toks/s, output: 10.40 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:11,  9.64it/s, est. speed input: 10588.62 toks/s, output: 10.34 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:11,  9.63it/s, est. speed input: 10546.19 toks/s, output: 10.30 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:11,  9.52it/s, est. speed input: 10486.10 toks/s, output: 10.24 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:02<00:11,  9.53it/s, est. speed input: 10449.83 toks/s, output: 10.20 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:02<00:11,  9.54it/s, est. speed input: 10417.75 toks/s, output: 10.17 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:02<00:10,  9.55it/s, est. speed input: 10390.08 toks/s, output: 10.15 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:02<00:10,  9.56it/s, est. speed input: 10363.78 toks/s, output: 10.12 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:02<00:10,  9.58it/s, est. speed input: 10342.81 toks/s, output: 10.10 toks/s]
Processed prompts:  20%|██        | 26/128 [00:02<00:10,  9.57it/s, est. speed input: 10319.83 toks/s, output: 10.08 toks/s]
Processed prompts:  21%|██        | 27/128 [00:02<00:10,  9.59it/s, est. speed input: 10302.49 toks/s, output: 10.06 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:02<00:10,  9.60it/s, est. speed input: 10286.28 toks/s, output: 10.05 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:10,  9.62it/s, est. speed input: 10272.66 toks/s, output: 10.03 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:02<00:10,  9.63it/s, est. speed input: 10259.29 toks/s, output: 10.02 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:03<00:10,  9.63it/s, est. speed input: 10245.23 toks/s, output: 10.01 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:03<00:09,  9.63it/s, est. speed input: 10233.29 toks/s, output: 9.99 toks/s] 
Processed prompts:  26%|██▌       | 33/128 [00:03<00:09,  9.64it/s, est. speed input: 10222.22 toks/s, output: 9.98 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:03<00:09,  9.59it/s, est. speed input: 10205.96 toks/s, output: 9.97 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:03<00:09,  9.60it/s, est. speed input: 10196.18 toks/s, output: 9.96 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:03<00:09,  9.62it/s, est. speed input: 10187.67 toks/s, output: 9.95 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:03<00:09,  9.58it/s, est. speed input: 10174.31 toks/s, output: 9.94 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:03<00:09,  9.60it/s, est. speed input: 10165.93 toks/s, output: 9.93 toks/s]
Processed prompts:  30%|███       | 39/128 [00:03<00:09,  9.62it/s, est. speed input: 10159.21 toks/s, output: 9.92 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:04<00:09,  9.64it/s, est. speed input: 10152.78 toks/s, output: 9.91 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:04<00:09,  9.62it/s, est. speed input: 10144.02 toks/s, output: 9.91 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:04<00:08,  9.61it/s, est. speed input: 10136.31 toks/s, output: 9.90 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:04<00:08,  9.62it/s, est. speed input: 10130.29 toks/s, output: 9.89 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:04<00:08,  9.60it/s, est. speed input: 10121.93 toks/s, output: 9.88 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:04<00:08,  9.60it/s, est. speed input: 10114.91 toks/s, output: 9.88 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:04<00:08,  9.59it/s, est. speed input: 10107.64 toks/s, output: 9.87 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:04<00:08,  9.58it/s, est. speed input: 10100.78 toks/s, output: 9.86 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:04<00:08,  9.60it/s, est. speed input: 10095.89 toks/s, output: 9.86 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:04<00:08,  9.61it/s, est. speed input: 10091.14 toks/s, output: 9.85 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:05<00:08,  9.59it/s, est. speed input: 10084.89 toks/s, output: 9.85 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:05<00:08,  9.60it/s, est. speed input: 10080.18 toks/s, output: 9.84 toks/s]
Processed prompts:  41%|████      | 52/128 [00:05<00:07,  9.61it/s, est. speed input: 10076.16 toks/s, output: 9.84 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:05<00:07,  9.63it/s, est. speed input: 10072.47 toks/s, output: 9.84 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:05<00:07,  9.63it/s, est. speed input: 10068.62 toks/s, output: 9.83 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:05<00:07,  9.64it/s, est. speed input: 10065.19 toks/s, output: 9.83 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:05<00:07,  9.63it/s, est. speed input: 10061.47 toks/s, output: 9.83 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:05<00:07,  9.58it/s, est. speed input: 10054.33 toks/s, output: 9.82 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:05<00:07,  9.60it/s, est. speed input: 10051.52 toks/s, output: 9.82 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:06<00:07,  9.62it/s, est. speed input: 10048.96 toks/s, output: 9.81 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:06<00:07,  9.63it/s, est. speed input: 10046.35 toks/s, output: 9.81 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:06<00:06,  9.64it/s, est. speed input: 10043.68 toks/s, output: 9.81 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:06<00:06,  9.64it/s, est. speed input: 10040.86 toks/s, output: 9.81 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:06<00:06,  9.65it/s, est. speed input: 10038.39 toks/s, output: 9.80 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:06<00:06,  9.65it/s, est. speed input: 10035.84 toks/s, output: 9.80 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:06<00:06,  9.65it/s, est. speed input: 10033.51 toks/s, output: 9.80 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:06<00:06,  9.64it/s, est. speed input: 10030.51 toks/s, output: 9.80 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:06<00:06,  9.64it/s, est. speed input: 10028.28 toks/s, output: 9.79 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:06<00:06,  9.65it/s, est. speed input: 10026.22 toks/s, output: 9.79 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:07<00:06,  9.65it/s, est. speed input: 10024.34 toks/s, output: 9.79 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:07<00:06,  9.66it/s, est. speed input: 10022.74 toks/s, output: 9.79 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:07<00:05,  9.66it/s, est. speed input: 10020.79 toks/s, output: 9.79 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:07<00:05,  9.64it/s, est. speed input: 10018.18 toks/s, output: 9.78 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:07<00:05,  9.65it/s, est. speed input: 10016.61 toks/s, output: 9.78 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:07<00:05,  9.64it/s, est. speed input: 10014.41 toks/s, output: 9.78 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:07<00:05,  9.64it/s, est. speed input: 10012.30 toks/s, output: 9.78 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:07<00:05,  9.64it/s, est. speed input: 10010.61 toks/s, output: 9.78 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:07<00:05,  9.64it/s, est. speed input: 10008.67 toks/s, output: 9.77 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:07<00:05,  9.64it/s, est. speed input: 10006.93 toks/s, output: 9.77 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:08<00:05,  9.65it/s, est. speed input: 10005.49 toks/s, output: 9.77 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:08<00:04,  9.64it/s, est. speed input: 10003.52 toks/s, output: 9.77 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:08<00:04,  9.64it/s, est. speed input: 10001.98 toks/s, output: 9.77 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:08<00:04,  9.64it/s, est. speed input: 10000.09 toks/s, output: 9.77 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:08<00:04,  9.64it/s, est. speed input: 9998.54 toks/s, output: 9.76 toks/s] 
Processed prompts:  66%|██████▌   | 84/128 [00:08<00:04,  9.63it/s, est. speed input: 9996.85 toks/s, output: 9.76 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:08<00:04,  9.63it/s, est. speed input: 9995.19 toks/s, output: 9.76 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:08<00:04,  9.63it/s, est. speed input: 9993.74 toks/s, output: 9.76 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:08<00:04,  9.63it/s, est. speed input: 9992.11 toks/s, output: 9.76 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:09<00:04,  9.63it/s, est. speed input: 9990.62 toks/s, output: 9.76 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:09<00:04,  9.61it/s, est. speed input: 9988.49 toks/s, output: 9.75 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:09<00:03,  9.63it/s, est. speed input: 9987.31 toks/s, output: 9.75 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:09<00:03,  9.63it/s, est. speed input: 9986.14 toks/s, output: 9.75 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:09<00:03,  9.64it/s, est. speed input: 9984.94 toks/s, output: 9.75 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:09<00:03,  9.64it/s, est. speed input: 9983.90 toks/s, output: 9.75 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:09<00:03,  9.64it/s, est. speed input: 9982.65 toks/s, output: 9.75 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:09<00:03,  9.65it/s, est. speed input: 9981.65 toks/s, output: 9.75 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:09<00:03,  9.65it/s, est. speed input: 9980.55 toks/s, output: 9.75 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:09<00:03,  9.64it/s, est. speed input: 9979.27 toks/s, output: 9.75 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:10<00:03,  9.64it/s, est. speed input: 9978.28 toks/s, output: 9.74 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:10<00:03,  9.64it/s, est. speed input: 9977.23 toks/s, output: 9.74 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:10<00:02,  9.65it/s, est. speed input: 9976.29 toks/s, output: 9.74 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:10<00:02,  9.65it/s, est. speed input: 9975.37 toks/s, output: 9.74 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:10<00:02,  9.65it/s, est. speed input: 9974.59 toks/s, output: 9.74 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:10<00:02,  9.65it/s, est. speed input: 9973.57 toks/s, output: 9.74 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:10<00:02,  9.64it/s, est. speed input: 9972.33 toks/s, output: 9.74 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:10<00:02,  9.64it/s, est. speed input: 9971.31 toks/s, output: 9.74 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:10<00:02,  9.64it/s, est. speed input: 9970.31 toks/s, output: 9.74 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:10<00:02,  9.48it/s, est. speed input: 9964.25 toks/s, output: 9.73 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:11<00:02,  9.53it/s, est. speed input: 9963.32 toks/s, output: 9.73 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:11<00:01,  9.56it/s, est. speed input: 9962.48 toks/s, output: 9.73 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:11<00:01,  9.58it/s, est. speed input: 9961.55 toks/s, output: 9.73 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:11<00:01,  9.60it/s, est. speed input: 9960.60 toks/s, output: 9.73 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:11<00:01,  9.60it/s, est. speed input: 9959.62 toks/s, output: 9.73 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:11<00:01,  9.61it/s, est. speed input: 9958.67 toks/s, output: 9.73 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:11<00:01,  9.61it/s, est. speed input: 9957.64 toks/s, output: 9.72 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:11<00:01,  9.61it/s, est. speed input: 9956.66 toks/s, output: 9.72 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:11<00:01,  9.61it/s, est. speed input: 9955.71 toks/s, output: 9.72 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:12<00:01,  9.61it/s, est. speed input: 9954.75 toks/s, output: 9.72 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:12<00:01,  9.62it/s, est. speed input: 9953.96 toks/s, output: 9.72 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:12<00:00,  9.62it/s, est. speed input: 9953.06 toks/s, output: 9.72 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:12<00:00,  9.63it/s, est. speed input: 9952.42 toks/s, output: 9.72 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:12<00:00,  9.63it/s, est. speed input: 9951.69 toks/s, output: 9.72 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:12<00:00,  9.63it/s, est. speed input: 9950.84 toks/s, output: 9.72 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:12<00:00,  9.63it/s, est. speed input: 9950.09 toks/s, output: 9.72 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:12<00:00,  9.63it/s, est. speed input: 9949.41 toks/s, output: 9.72 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:12<00:00,  9.63it/s, est. speed input: 9948.73 toks/s, output: 9.72 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:12<00:00,  9.63it/s, est. speed input: 9948.01 toks/s, output: 9.71 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:13<00:00,  9.63it/s, est. speed input: 9947.35 toks/s, output: 9.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:13<00:00,  9.64it/s, est. speed input: 9947.02 toks/s, output: 9.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:13<00:00,  9.64it/s, est. speed input: 9947.02 toks/s, output: 9.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:13<00:00,  9.71it/s, est. speed input: 9947.02 toks/s, output: 9.71 toks/s]
[rank0]:[W126 08:01:02.991664356 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 08:01:05
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:01:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:01:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=377352) WARNING 01-26 08:01:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=377352) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=377352) WARNING 01-26 08:01:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 10.01 requests/s, 10257.64 total tokens/s, 10.01 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 08:01:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:01:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:01:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:01:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:01:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:01:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:01:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:01:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:01:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:01:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:01:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:01:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:01:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:01:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:01:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:01:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:01:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=377352) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=377352) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.44s/it]
(EngineCore_DP0 pid=377352) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.43s/it]
(EngineCore_DP0 pid=377352) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.04it/s]
(EngineCore_DP0 pid=377352) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
(EngineCore_DP0 pid=377352) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
(EngineCore_DP0 pid=377352) 
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=377352) [2026-01-26 08:01:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=377352) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  2.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  3.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  3.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  3.61it/s]
(EngineCore_DP0 pid=377352) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  4.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  5.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  5.06it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   9%|▉         | 23/256 [00:00<00:01, 228.46it/s]
Adding requests:  19%|█▉        | 48/256 [00:00<00:00, 239.77it/s]
Adding requests:  29%|██▉       | 74/256 [00:00<00:00, 248.43it/s]
Adding requests:  39%|███▊      | 99/256 [00:00<00:00, 241.42it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 243.65it/s]
Adding requests:  58%|█████▊    | 149/256 [00:00<00:00, 245.48it/s]
Adding requests:  68%|██████▊   | 175/256 [00:00<00:00, 249.36it/s]
Adding requests:  79%|███████▊  | 201/256 [00:00<00:00, 250.61it/s]
Adding requests:  89%|████████▊ | 227/256 [00:00<00:00, 253.36it/s]
Adding requests:  99%|█████████▉| 253/256 [00:01<00:00, 252.40it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 248.00it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 10/256 [00:00<00:05, 42.74it/s, est. speed input: 43770.06 toks/s, output: 42.74 toks/s]
Processed prompts:   6%|▌         | 15/256 [00:00<00:11, 21.52it/s, est. speed input: 24463.31 toks/s, output: 23.89 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:01<00:16, 14.54it/s, est. speed input: 18042.36 toks/s, output: 17.62 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:01<00:17, 13.38it/s, est. speed input: 16804.34 toks/s, output: 16.41 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:01<00:18, 12.49it/s, est. speed input: 15912.35 toks/s, output: 15.54 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:01<00:19, 11.83it/s, est. speed input: 15235.78 toks/s, output: 14.88 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:01<00:06, 33.93it/s, est. speed input: 23205.66 toks/s, output: 22.66 toks/s]
Processed prompts:  18%|█▊        | 45/256 [00:02<00:08, 24.00it/s, est. speed input: 21336.41 toks/s, output: 20.84 toks/s]
Processed prompts:  19%|█▉        | 49/256 [00:02<00:11, 18.46it/s, est. speed input: 19648.20 toks/s, output: 19.19 toks/s]
Processed prompts:  20%|██        | 52/256 [00:02<00:14, 14.51it/s, est. speed input: 18060.30 toks/s, output: 17.64 toks/s]
Processed prompts:  21%|██▏       | 55/256 [00:03<00:13, 14.65it/s, est. speed input: 17905.00 toks/s, output: 17.49 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:03<00:14, 13.63it/s, est. speed input: 17463.62 toks/s, output: 17.05 toks/s]
Processed prompts:  23%|██▎       | 59/256 [00:03<00:15, 12.78it/s, est. speed input: 17071.24 toks/s, output: 16.67 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:03<00:16, 12.10it/s, est. speed input: 16720.16 toks/s, output: 16.33 toks/s]
Processed prompts:  25%|██▍       | 63/256 [00:03<00:16, 11.57it/s, est. speed input: 16400.87 toks/s, output: 16.02 toks/s]
Processed prompts:  25%|██▌       | 65/256 [00:04<00:17, 11.16it/s, est. speed input: 16110.72 toks/s, output: 15.73 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:04<00:17, 10.86it/s, est. speed input: 15848.19 toks/s, output: 15.48 toks/s]
Processed prompts:  27%|██▋       | 69/256 [00:04<00:17, 10.65it/s, est. speed input: 15610.42 toks/s, output: 15.24 toks/s]
Processed prompts:  28%|██▊       | 71/256 [00:04<00:17, 10.49it/s, est. speed input: 15390.20 toks/s, output: 15.03 toks/s]
Processed prompts:  29%|██▊       | 73/256 [00:04<00:17, 10.38it/s, est. speed input: 15188.19 toks/s, output: 14.83 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:05<00:17, 10.30it/s, est. speed input: 15001.69 toks/s, output: 14.65 toks/s]
Processed prompts:  30%|███       | 77/256 [00:05<00:17, 10.25it/s, est. speed input: 14829.12 toks/s, output: 14.48 toks/s]
Processed prompts:  31%|███       | 79/256 [00:05<00:17, 10.21it/s, est. speed input: 14669.42 toks/s, output: 14.33 toks/s]
Processed prompts:  32%|███▏      | 81/256 [00:05<00:17, 10.19it/s, est. speed input: 14520.72 toks/s, output: 14.18 toks/s]
Processed prompts:  32%|███▏      | 83/256 [00:05<00:17, 10.16it/s, est. speed input: 14380.77 toks/s, output: 14.04 toks/s]
Processed prompts:  33%|███▎      | 85/256 [00:06<00:16, 10.14it/s, est. speed input: 14249.79 toks/s, output: 13.92 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:06<00:16, 10.13it/s, est. speed input: 14127.20 toks/s, output: 13.80 toks/s]
Processed prompts:  35%|███▍      | 89/256 [00:06<00:16, 10.12it/s, est. speed input: 14011.91 toks/s, output: 13.68 toks/s]
Processed prompts:  36%|███▌      | 91/256 [00:06<00:16, 10.12it/s, est. speed input: 13904.09 toks/s, output: 13.58 toks/s]
Processed prompts:  36%|███▋      | 93/256 [00:06<00:16, 10.12it/s, est. speed input: 13802.36 toks/s, output: 13.48 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:07<00:15, 10.12it/s, est. speed input: 13706.90 toks/s, output: 13.39 toks/s]
Processed prompts:  38%|███▊      | 97/256 [00:07<00:15, 10.12it/s, est. speed input: 13616.19 toks/s, output: 13.30 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:07<00:15, 10.12it/s, est. speed input: 13530.28 toks/s, output: 13.21 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:07<00:15, 10.11it/s, est. speed input: 13448.40 toks/s, output: 13.13 toks/s]
Processed prompts:  40%|████      | 103/256 [00:07<00:15, 10.12it/s, est. speed input: 13371.79 toks/s, output: 13.06 toks/s]
Processed prompts:  41%|████      | 105/256 [00:08<00:14, 10.12it/s, est. speed input: 13297.60 toks/s, output: 12.99 toks/s]
Processed prompts:  42%|████▏     | 107/256 [00:08<00:14, 10.11it/s, est. speed input: 13226.72 toks/s, output: 12.92 toks/s]
Processed prompts:  43%|████▎     | 109/256 [00:08<00:14, 10.10it/s, est. speed input: 13158.81 toks/s, output: 12.85 toks/s]
Processed prompts:  43%|████▎     | 111/256 [00:08<00:14, 10.10it/s, est. speed input: 13094.32 toks/s, output: 12.79 toks/s]
Processed prompts:  44%|████▍     | 113/256 [00:08<00:14, 10.09it/s, est. speed input: 13032.21 toks/s, output: 12.73 toks/s]
Processed prompts:  45%|████▍     | 115/256 [00:09<00:13, 10.09it/s, est. speed input: 12973.25 toks/s, output: 12.67 toks/s]
Processed prompts:  46%|████▌     | 117/256 [00:09<00:13, 10.09it/s, est. speed input: 12916.96 toks/s, output: 12.61 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:09<00:13, 10.08it/s, est. speed input: 12862.18 toks/s, output: 12.56 toks/s]
Processed prompts:  47%|████▋     | 121/256 [00:09<00:13, 10.08it/s, est. speed input: 12810.09 toks/s, output: 12.51 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:09<00:13, 10.09it/s, est. speed input: 12760.47 toks/s, output: 12.46 toks/s]
Processed prompts:  49%|████▉     | 125/256 [00:10<00:12, 10.08it/s, est. speed input: 12712.18 toks/s, output: 12.41 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:10<00:12, 10.09it/s, est. speed input: 12666.53 toks/s, output: 12.37 toks/s]
Processed prompts:  50%|█████     | 129/256 [00:10<00:12, 10.09it/s, est. speed input: 12622.59 toks/s, output: 12.33 toks/s]
Processed prompts:  51%|█████     | 131/256 [00:10<00:12, 10.09it/s, est. speed input: 12580.28 toks/s, output: 12.29 toks/s]
Processed prompts:  52%|█████▏    | 133/256 [00:10<00:12, 10.09it/s, est. speed input: 12539.10 toks/s, output: 12.25 toks/s]
Processed prompts:  53%|█████▎    | 135/256 [00:11<00:11, 10.09it/s, est. speed input: 12499.67 toks/s, output: 12.21 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:11<00:11, 10.08it/s, est. speed input: 12460.80 toks/s, output: 12.17 toks/s]
Processed prompts:  54%|█████▍    | 139/256 [00:11<00:11, 10.07it/s, est. speed input: 12423.24 toks/s, output: 12.13 toks/s]
Processed prompts:  55%|█████▌    | 141/256 [00:11<00:11, 10.07it/s, est. speed input: 12387.18 toks/s, output: 12.10 toks/s]
Processed prompts:  56%|█████▌    | 143/256 [00:11<00:11, 10.07it/s, est. speed input: 12351.99 toks/s, output: 12.06 toks/s]
Processed prompts:  57%|█████▋    | 145/256 [00:12<00:11, 10.06it/s, est. speed input: 12318.10 toks/s, output: 12.03 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:12<00:10, 10.06it/s, est. speed input: 12284.99 toks/s, output: 12.00 toks/s]
Processed prompts:  58%|█████▊    | 149/256 [00:12<00:10, 10.05it/s, est. speed input: 12252.94 toks/s, output: 11.97 toks/s]
Processed prompts:  59%|█████▉    | 151/256 [00:12<00:10, 10.05it/s, est. speed input: 12222.02 toks/s, output: 11.94 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:12<00:10, 10.05it/s, est. speed input: 12191.85 toks/s, output: 11.91 toks/s]
Processed prompts:  61%|██████    | 155/256 [00:13<00:10, 10.05it/s, est. speed input: 12162.93 toks/s, output: 11.88 toks/s]
Processed prompts:  61%|██████▏   | 157/256 [00:13<00:09, 10.05it/s, est. speed input: 12134.80 toks/s, output: 11.85 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:13<00:09, 10.06it/s, est. speed input: 12108.39 toks/s, output: 11.82 toks/s]
Processed prompts:  63%|██████▎   | 161/256 [00:13<00:09, 10.07it/s, est. speed input: 12082.29 toks/s, output: 11.80 toks/s]
Processed prompts:  64%|██████▎   | 163/256 [00:13<00:09, 10.07it/s, est. speed input: 12056.88 toks/s, output: 11.77 toks/s]
Processed prompts:  64%|██████▍   | 165/256 [00:14<00:09, 10.07it/s, est. speed input: 12032.41 toks/s, output: 11.75 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:14<00:08, 10.07it/s, est. speed input: 12008.29 toks/s, output: 11.73 toks/s]
Processed prompts:  66%|██████▌   | 169/256 [00:14<00:08, 10.06it/s, est. speed input: 11984.57 toks/s, output: 11.70 toks/s]
Processed prompts:  67%|██████▋   | 171/256 [00:14<00:08, 10.05it/s, est. speed input: 11961.35 toks/s, output: 11.68 toks/s]
Processed prompts:  68%|██████▊   | 173/256 [00:14<00:08, 10.05it/s, est. speed input: 11938.74 toks/s, output: 11.66 toks/s]
Processed prompts:  68%|██████▊   | 175/256 [00:15<00:08, 10.04it/s, est. speed input: 11916.63 toks/s, output: 11.64 toks/s]
Processed prompts:  69%|██████▉   | 177/256 [00:15<00:07, 10.04it/s, est. speed input: 11895.01 toks/s, output: 11.62 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:15<00:07, 10.04it/s, est. speed input: 11874.00 toks/s, output: 11.60 toks/s]
Processed prompts:  71%|███████   | 181/256 [00:15<00:07, 10.04it/s, est. speed input: 11853.99 toks/s, output: 11.58 toks/s]
Processed prompts:  71%|███████▏  | 183/256 [00:15<00:07, 10.05it/s, est. speed input: 11834.45 toks/s, output: 11.56 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:16<00:07, 10.04it/s, est. speed input: 11815.16 toks/s, output: 11.54 toks/s]
Processed prompts:  73%|███████▎  | 187/256 [00:16<00:06, 10.04it/s, est. speed input: 11796.22 toks/s, output: 11.52 toks/s]
Processed prompts:  74%|███████▍  | 189/256 [00:16<00:06, 10.04it/s, est. speed input: 11778.01 toks/s, output: 11.50 toks/s]
Processed prompts:  75%|███████▍  | 191/256 [00:16<00:06, 10.05it/s, est. speed input: 11760.31 toks/s, output: 11.48 toks/s]
Processed prompts:  75%|███████▌  | 193/256 [00:16<00:06, 10.05it/s, est. speed input: 11742.89 toks/s, output: 11.47 toks/s]
Processed prompts:  76%|███████▌  | 195/256 [00:17<00:06, 10.04it/s, est. speed input: 11725.46 toks/s, output: 11.45 toks/s]
Processed prompts:  77%|███████▋  | 197/256 [00:17<00:05, 10.03it/s, est. speed input: 11708.59 toks/s, output: 11.43 toks/s]
Processed prompts:  78%|███████▊  | 199/256 [00:17<00:05, 10.03it/s, est. speed input: 11692.03 toks/s, output: 11.42 toks/s]
Processed prompts:  79%|███████▊  | 201/256 [00:17<00:05, 10.03it/s, est. speed input: 11676.03 toks/s, output: 11.40 toks/s]
Processed prompts:  79%|███████▉  | 203/256 [00:17<00:05, 10.03it/s, est. speed input: 11660.12 toks/s, output: 11.39 toks/s]
Processed prompts:  80%|████████  | 205/256 [00:18<00:05, 10.02it/s, est. speed input: 11644.49 toks/s, output: 11.37 toks/s]
Processed prompts:  81%|████████  | 207/256 [00:18<00:04, 10.03it/s, est. speed input: 11629.57 toks/s, output: 11.36 toks/s]
Processed prompts:  82%|████████▏ | 209/256 [00:18<00:04, 10.03it/s, est. speed input: 11614.75 toks/s, output: 11.34 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:18<00:04, 10.02it/s, est. speed input: 11600.13 toks/s, output: 11.33 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:18<00:04, 10.02it/s, est. speed input: 11585.80 toks/s, output: 11.31 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:19<00:04, 10.01it/s, est. speed input: 11571.69 toks/s, output: 11.30 toks/s]
Processed prompts:  85%|████████▍ | 217/256 [00:19<00:03, 10.01it/s, est. speed input: 11558.05 toks/s, output: 11.29 toks/s]
Processed prompts:  86%|████████▌ | 219/256 [00:19<00:03, 10.02it/s, est. speed input: 11544.79 toks/s, output: 11.27 toks/s]
Processed prompts:  86%|████████▋ | 221/256 [00:19<00:03, 10.03it/s, est. speed input: 11531.98 toks/s, output: 11.26 toks/s]
Processed prompts:  87%|████████▋ | 223/256 [00:19<00:03, 10.03it/s, est. speed input: 11519.28 toks/s, output: 11.25 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:20<00:03, 10.02it/s, est. speed input: 11506.73 toks/s, output: 11.24 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:20<00:02, 10.02it/s, est. speed input: 11494.47 toks/s, output: 11.23 toks/s]
Processed prompts:  89%|████████▉ | 229/256 [00:20<00:02, 10.03it/s, est. speed input: 11482.68 toks/s, output: 11.21 toks/s]
Processed prompts:  90%|█████████ | 231/256 [00:20<00:02, 10.03it/s, est. speed input: 11470.86 toks/s, output: 11.20 toks/s]
Processed prompts:  91%|█████████ | 233/256 [00:20<00:02, 10.02it/s, est. speed input: 11458.94 toks/s, output: 11.19 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:21<00:02, 10.02it/s, est. speed input: 11447.57 toks/s, output: 11.18 toks/s]
Processed prompts:  93%|█████████▎| 237/256 [00:21<00:01, 10.01it/s, est. speed input: 11436.20 toks/s, output: 11.17 toks/s]
Processed prompts:  93%|█████████▎| 239/256 [00:21<00:01, 10.02it/s, est. speed input: 11425.37 toks/s, output: 11.16 toks/s]
Processed prompts:  94%|█████████▍| 241/256 [00:21<00:01, 10.02it/s, est. speed input: 11414.49 toks/s, output: 11.15 toks/s]
Processed prompts:  95%|█████████▍| 243/256 [00:21<00:01, 10.01it/s, est. speed input: 11403.78 toks/s, output: 11.14 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:22<00:01, 10.01it/s, est. speed input: 11393.31 toks/s, output: 11.13 toks/s]
Processed prompts:  96%|█████████▋| 247/256 [00:22<00:00, 10.01it/s, est. speed input: 11383.14 toks/s, output: 11.12 toks/s]
Processed prompts:  97%|█████████▋| 249/256 [00:22<00:00, 10.02it/s, est. speed input: 11373.24 toks/s, output: 11.11 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:22<00:00, 10.02it/s, est. speed input: 11363.58 toks/s, output: 11.10 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:22<00:00, 10.02it/s, est. speed input: 11353.91 toks/s, output: 11.09 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:23<00:00, 10.02it/s, est. speed input: 11344.48 toks/s, output: 11.08 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 10.02it/s, est. speed input: 11336.31 toks/s, output: 11.07 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 11.07it/s, est. speed input: 11336.31 toks/s, output: 11.07 toks/s]
[rank0]:[W126 08:02:32.123128115 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 08:02:35
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:02:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:02:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=378832) WARNING 01-26 08:02:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=378832) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=378832) WARNING 01-26 08:03:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 10.07 requests/s, 10324.46 total tokens/s, 10.07 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 08:02:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:02:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:02:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:02:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:02:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:02:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:02:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:02:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:02:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:02:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:02:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:02:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:02:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:02:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:02:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:02:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:02:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=378832) [2026-01-26 08:02:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=378832) [2026-01-26 08:02:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=378832) [2026-01-26 08:02:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=378832) [2026-01-26 08:02:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=378832) [2026-01-26 08:02:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=378832) [2026-01-26 08:02:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=378832) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=378832) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.40s/it]
(EngineCore_DP0 pid=378832) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.38s/it]
(EngineCore_DP0 pid=378832) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.08it/s]
(EngineCore_DP0 pid=378832) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
(EngineCore_DP0 pid=378832) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
(EngineCore_DP0 pid=378832) 
(EngineCore_DP0 pid=378832) [2026-01-26 08:03:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=378832) [2026-01-26 08:03:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=378832) [2026-01-26 08:03:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=378832) [2026-01-26 08:03:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=378832) [2026-01-26 08:03:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=378832) [2026-01-26 08:03:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=378832) [2026-01-26 08:03:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=378832) [2026-01-26 08:03:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=378832) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  3.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  4.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  4.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:01<00:00,  2.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:01<00:00,  3.15it/s]
(EngineCore_DP0 pid=378832) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  4.63it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  3.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  4.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  4.22it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 22/512 [00:00<00:02, 217.07it/s]
Adding requests:   9%|▉         | 47/512 [00:00<00:01, 235.11it/s]
Adding requests:  14%|█▍        | 73/512 [00:00<00:01, 246.23it/s]
Adding requests:  19%|█▉        | 98/512 [00:00<00:01, 245.52it/s]
Adding requests:  24%|██▍       | 123/512 [00:00<00:01, 243.58it/s]
Adding requests:  29%|██▉       | 149/512 [00:00<00:01, 246.33it/s]
Adding requests:  34%|███▍      | 176/512 [00:00<00:01, 252.16it/s]
Adding requests:  39%|███▉      | 202/512 [00:00<00:01, 254.27it/s]
Adding requests:  45%|████▍     | 228/512 [00:00<00:01, 255.20it/s]
Adding requests:  50%|████▉     | 254/512 [00:01<00:01, 252.22it/s]
Adding requests:  55%|█████▍    | 280/512 [00:01<00:00, 252.72it/s]
Adding requests:  60%|█████▉    | 306/512 [00:01<00:00, 252.98it/s]
Adding requests:  65%|██████▌   | 334/512 [00:01<00:00, 260.90it/s]
Adding requests:  71%|███████   | 362/512 [00:01<00:00, 263.93it/s]
Adding requests:  76%|███████▌  | 389/512 [00:01<00:00, 265.52it/s]
Adding requests:  82%|████████▏ | 418/512 [00:01<00:00, 272.10it/s]
Adding requests:  87%|████████▋ | 446/512 [00:01<00:00, 265.61it/s]
Adding requests:  92%|█████████▏| 473/512 [00:01<00:00, 266.32it/s]
Adding requests:  98%|█████████▊| 502/512 [00:01<00:00, 271.40it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 258.46it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▎         | 18/512 [00:00<00:10, 46.89it/s, est. speed input: 48021.28 toks/s, output: 46.89 toks/s]
Processed prompts:   4%|▍         | 23/512 [00:00<00:18, 26.69it/s, est. speed input: 30406.54 toks/s, output: 29.69 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:01<00:27, 17.99it/s, est. speed input: 22860.20 toks/s, output: 22.32 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:01<00:32, 14.93it/s, est. speed input: 19757.76 toks/s, output: 19.29 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:01<00:36, 13.24it/s, est. speed input: 17897.43 toks/s, output: 17.48 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:02<00:38, 12.22it/s, est. speed input: 16663.83 toks/s, output: 16.27 toks/s]
Processed prompts:  11%|█         | 54/512 [00:02<00:16, 27.92it/s, est. speed input: 22506.33 toks/s, output: 21.98 toks/s]
Processed prompts:  12%|█▏        | 60/512 [00:02<00:19, 23.18it/s, est. speed input: 21573.28 toks/s, output: 21.07 toks/s]
Processed prompts:  12%|█▎        | 64/512 [00:03<00:24, 18.66it/s, est. speed input: 20234.92 toks/s, output: 19.76 toks/s]
Processed prompts:  13%|█▎        | 68/512 [00:03<00:28, 15.85it/s, est. speed input: 19182.52 toks/s, output: 18.73 toks/s]
Processed prompts:  14%|█▍        | 71/512 [00:04<00:33, 13.20it/s, est. speed input: 18079.00 toks/s, output: 17.66 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:04<00:38, 11.46it/s, est. speed input: 17172.01 toks/s, output: 16.77 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:04<00:39, 11.07it/s, est. speed input: 16624.45 toks/s, output: 16.23 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:05<00:39, 10.81it/s, est. speed input: 16160.32 toks/s, output: 15.78 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:05<00:40, 10.62it/s, est. speed input: 15759.63 toks/s, output: 15.39 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:05<00:40, 10.50it/s, est. speed input: 15411.76 toks/s, output: 15.05 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:06<00:40, 10.41it/s, est. speed input: 15106.52 toks/s, output: 14.75 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:06<00:40, 10.34it/s, est. speed input: 14836.06 toks/s, output: 14.49 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:07<00:39, 10.30it/s, est. speed input: 14596.19 toks/s, output: 14.25 toks/s]
Processed prompts:  21%|██        | 106/512 [00:07<00:39, 10.27it/s, est. speed input: 14380.53 toks/s, output: 14.04 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:07<00:39, 10.25it/s, est. speed input: 14185.13 toks/s, output: 13.85 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:08<00:38, 10.23it/s, est. speed input: 14008.85 toks/s, output: 13.68 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:08<00:38, 10.22it/s, est. speed input: 13848.06 toks/s, output: 13.52 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:09<00:38, 10.21it/s, est. speed input: 13700.63 toks/s, output: 13.38 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:09<00:37, 10.20it/s, est. speed input: 13565.20 toks/s, output: 13.25 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:09<00:37, 10.19it/s, est. speed input: 13439.98 toks/s, output: 13.12 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:10<00:37, 10.18it/s, est. speed input: 13324.39 toks/s, output: 13.01 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:10<00:36, 10.18it/s, est. speed input: 13217.32 toks/s, output: 12.91 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:11<00:36, 10.17it/s, est. speed input: 13117.37 toks/s, output: 12.81 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:11<00:35, 10.17it/s, est. speed input: 13024.39 toks/s, output: 12.72 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:11<00:35, 10.17it/s, est. speed input: 12937.23 toks/s, output: 12.63 toks/s]
Processed prompts:  30%|███       | 154/512 [00:12<00:35, 10.16it/s, est. speed input: 12855.90 toks/s, output: 12.55 toks/s]
Processed prompts:  31%|███       | 158/512 [00:12<00:34, 10.16it/s, est. speed input: 12779.32 toks/s, output: 12.48 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:13<00:34, 10.16it/s, est. speed input: 12707.32 toks/s, output: 12.41 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:13<00:34, 10.15it/s, est. speed input: 12639.43 toks/s, output: 12.34 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:13<00:33, 10.15it/s, est. speed input: 12574.95 toks/s, output: 12.28 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:14<00:33, 10.14it/s, est. speed input: 12513.99 toks/s, output: 12.22 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:14<00:32, 10.14it/s, est. speed input: 12456.45 toks/s, output: 12.16 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:15<00:32, 10.14it/s, est. speed input: 12402.00 toks/s, output: 12.11 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:15<00:32, 10.14it/s, est. speed input: 12350.14 toks/s, output: 12.06 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:15<00:31, 10.14it/s, est. speed input: 12300.87 toks/s, output: 12.01 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:16<00:31, 10.14it/s, est. speed input: 12253.94 toks/s, output: 11.97 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:16<00:30, 10.13it/s, est. speed input: 12209.04 toks/s, output: 11.92 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:17<00:30, 10.13it/s, est. speed input: 12166.20 toks/s, output: 11.88 toks/s]
Processed prompts:  40%|████      | 206/512 [00:17<00:30, 10.13it/s, est. speed input: 12125.57 toks/s, output: 11.84 toks/s]
Processed prompts:  41%|████      | 210/512 [00:17<00:29, 10.13it/s, est. speed input: 12086.41 toks/s, output: 11.80 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:18<00:29, 10.12it/s, est. speed input: 12048.94 toks/s, output: 11.77 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:18<00:29, 10.12it/s, est. speed input: 12012.98 toks/s, output: 11.73 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:18<00:28, 10.12it/s, est. speed input: 11978.33 toks/s, output: 11.70 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:19<00:28, 10.12it/s, est. speed input: 11945.13 toks/s, output: 11.67 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:19<00:27, 10.11it/s, est. speed input: 11913.28 toks/s, output: 11.63 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:20<00:27, 10.11it/s, est. speed input: 11882.48 toks/s, output: 11.60 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:20<00:27, 10.11it/s, est. speed input: 11853.14 toks/s, output: 11.58 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:20<00:26, 10.11it/s, est. speed input: 11824.71 toks/s, output: 11.55 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:21<00:26, 10.11it/s, est. speed input: 11797.23 toks/s, output: 11.52 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:21<00:25, 10.10it/s, est. speed input: 11770.64 toks/s, output: 11.49 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:22<00:25, 10.10it/s, est. speed input: 11744.97 toks/s, output: 11.47 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:22<00:25, 10.10it/s, est. speed input: 11720.24 toks/s, output: 11.45 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:22<00:24, 10.10it/s, est. speed input: 11696.39 toks/s, output: 11.42 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:23<00:24, 10.10it/s, est. speed input: 11673.27 toks/s, output: 11.40 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:23<00:23, 10.10it/s, est. speed input: 11650.89 toks/s, output: 11.38 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:24<00:23, 10.10it/s, est. speed input: 11629.31 toks/s, output: 11.36 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:24<00:23, 10.09it/s, est. speed input: 11608.39 toks/s, output: 11.34 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:24<00:22, 10.09it/s, est. speed input: 11588.04 toks/s, output: 11.32 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:25<00:22, 10.09it/s, est. speed input: 11568.42 toks/s, output: 11.30 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:25<00:22, 10.09it/s, est. speed input: 11549.30 toks/s, output: 11.28 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:26<00:21, 10.09it/s, est. speed input: 11530.97 toks/s, output: 11.26 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:26<00:21, 10.10it/s, est. speed input: 11513.31 toks/s, output: 11.24 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:26<00:20, 10.10it/s, est. speed input: 11496.25 toks/s, output: 11.23 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:27<00:20, 10.10it/s, est. speed input: 11479.62 toks/s, output: 11.21 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:27<00:19, 10.10it/s, est. speed input: 11463.47 toks/s, output: 11.19 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:28<00:19, 10.11it/s, est. speed input: 11447.79 toks/s, output: 11.18 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:28<00:19, 10.10it/s, est. speed input: 11432.48 toks/s, output: 11.16 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:28<00:18, 10.10it/s, est. speed input: 11417.49 toks/s, output: 11.15 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:29<00:18, 10.11it/s, est. speed input: 11403.20 toks/s, output: 11.14 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:29<00:18, 10.10it/s, est. speed input: 11388.98 toks/s, output: 11.12 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:30<00:17, 10.10it/s, est. speed input: 11375.30 toks/s, output: 11.11 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:30<00:17, 10.11it/s, est. speed input: 11361.97 toks/s, output: 11.10 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:30<00:16, 10.10it/s, est. speed input: 11348.79 toks/s, output: 11.08 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:31<00:16, 10.10it/s, est. speed input: 11336.20 toks/s, output: 11.07 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:31<00:16, 10.10it/s, est. speed input: 11323.85 toks/s, output: 11.06 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:32<00:15, 10.10it/s, est. speed input: 11311.73 toks/s, output: 11.05 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:32<00:15, 10.10it/s, est. speed input: 11299.91 toks/s, output: 11.04 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:32<00:14, 10.10it/s, est. speed input: 11288.30 toks/s, output: 11.02 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:33<00:14, 10.10it/s, est. speed input: 11277.03 toks/s, output: 11.01 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:33<00:14, 10.10it/s, est. speed input: 11266.10 toks/s, output: 11.00 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:33<00:05, 23.90it/s, est. speed input: 11705.91 toks/s, output: 11.43 toks/s]
Processed prompts:  76%|███████▋  | 391/512 [00:34<00:06, 19.98it/s, est. speed input: 11720.15 toks/s, output: 11.45 toks/s]
Processed prompts:  77%|███████▋  | 395/512 [00:34<00:07, 16.71it/s, est. speed input: 11704.62 toks/s, output: 11.43 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:34<00:08, 13.78it/s, est. speed input: 11660.15 toks/s, output: 11.39 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:35<00:08, 12.62it/s, est. speed input: 11645.45 toks/s, output: 11.37 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:35<00:08, 11.84it/s, est. speed input: 11631.06 toks/s, output: 11.36 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:36<00:09, 11.30it/s, est. speed input: 11616.78 toks/s, output: 11.34 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:36<00:08, 10.93it/s, est. speed input: 11602.97 toks/s, output: 11.33 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:36<00:08, 10.68it/s, est. speed input: 11589.51 toks/s, output: 11.32 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:37<00:08, 10.51it/s, est. speed input: 11576.37 toks/s, output: 11.31 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:37<00:08, 10.39it/s, est. speed input: 11563.48 toks/s, output: 11.29 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:38<00:07, 10.30it/s, est. speed input: 11550.85 toks/s, output: 11.28 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:38<00:07, 10.24it/s, est. speed input: 11538.49 toks/s, output: 11.27 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:38<00:07, 10.20it/s, est. speed input: 11526.25 toks/s, output: 11.26 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:39<00:06, 10.17it/s, est. speed input: 11514.23 toks/s, output: 11.24 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:39<00:06, 10.15it/s, est. speed input: 11502.49 toks/s, output: 11.23 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:40<00:06, 10.14it/s, est. speed input: 11491.31 toks/s, output: 11.22 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:40<00:05, 10.13it/s, est. speed input: 11480.13 toks/s, output: 11.21 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:40<00:05, 10.12it/s, est. speed input: 11468.96 toks/s, output: 11.20 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:41<00:04, 10.11it/s, est. speed input: 11458.10 toks/s, output: 11.19 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:41<00:04, 10.10it/s, est. speed input: 11447.31 toks/s, output: 11.18 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:42<00:04, 10.09it/s, est. speed input: 11436.64 toks/s, output: 11.17 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:42<00:03, 10.09it/s, est. speed input: 11426.26 toks/s, output: 11.16 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:42<00:03, 10.08it/s, est. speed input: 11415.92 toks/s, output: 11.15 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:43<00:02, 10.08it/s, est. speed input: 11405.82 toks/s, output: 11.14 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:43<00:02, 10.08it/s, est. speed input: 11396.03 toks/s, output: 11.13 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:44<00:02, 10.08it/s, est. speed input: 11386.45 toks/s, output: 11.12 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:44<00:01, 10.09it/s, est. speed input: 11377.05 toks/s, output: 11.11 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:44<00:01, 10.08it/s, est. speed input: 11367.65 toks/s, output: 11.10 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:45<00:00, 10.08it/s, est. speed input: 11358.39 toks/s, output: 11.09 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:45<00:00, 10.08it/s, est. speed input: 11349.44 toks/s, output: 11.08 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:45<00:00, 10.82it/s, est. speed input: 11362.95 toks/s, output: 11.10 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:45<00:00, 10.82it/s, est. speed input: 11407.45 toks/s, output: 11.14 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:45<00:00, 11.14it/s, est. speed input: 11407.45 toks/s, output: 11.14 toks/s]
[rank0]:[W126 08:04:27.969808880 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 08:04:30
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:04:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:04:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=380708) WARNING 01-26 08:04:53 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=380708) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=380708) WARNING 01-26 08:05:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 10.06 requests/s, 10311.56 total tokens/s, 10.06 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 08:04:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:04:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:04:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:04:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:04:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:04:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:04:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:04:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:04:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:04:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:04:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:04:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:04:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:04:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:04:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:04:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:04:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=380708) [2026-01-26 08:04:54] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=380708) [2026-01-26 08:04:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=380708) [2026-01-26 08:04:54] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=380708) [2026-01-26 08:04:54] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=380708) [2026-01-26 08:04:54] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=380708) [2026-01-26 08:04:54] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=380708) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=380708) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.40s/it]
(EngineCore_DP0 pid=380708) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.38s/it]
(EngineCore_DP0 pid=380708) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.07it/s]
(EngineCore_DP0 pid=380708) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
(EngineCore_DP0 pid=380708) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
(EngineCore_DP0 pid=380708) 
(EngineCore_DP0 pid=380708) [2026-01-26 08:05:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=380708) [2026-01-26 08:05:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=380708) [2026-01-26 08:05:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=380708) [2026-01-26 08:05:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=380708) [2026-01-26 08:05:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=380708) [2026-01-26 08:05:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=380708) [2026-01-26 08:05:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=380708) [2026-01-26 08:05:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=380708) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  5.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  5.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  5.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.78it/s]
(EngineCore_DP0 pid=380708) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:01,  2.31it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  3.08it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:01<00:00,  3.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:01<00:00,  3.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:01<00:00,  3.35it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 23/1024 [00:00<00:04, 226.10it/s]
Adding requests:   5%|▍         | 48/1024 [00:00<00:04, 238.34it/s]
Adding requests:   7%|▋         | 74/1024 [00:00<00:03, 248.05it/s]
Adding requests:  10%|▉         | 99/1024 [00:00<00:03, 247.24it/s]
Adding requests:  12%|█▏        | 124/1024 [00:00<00:03, 246.29it/s]
Adding requests:  15%|█▍        | 149/1024 [00:00<00:03, 240.10it/s]
Adding requests:  17%|█▋        | 176/1024 [00:00<00:03, 247.18it/s]
Adding requests:  20%|█▉        | 203/1024 [00:00<00:03, 253.37it/s]
Adding requests:  22%|██▏       | 230/1024 [00:00<00:03, 257.41it/s]
Adding requests:  25%|██▌       | 256/1024 [00:01<00:03, 251.29it/s]
Adding requests:  28%|██▊       | 282/1024 [00:01<00:02, 252.23it/s]
Adding requests:  30%|███       | 309/1024 [00:01<00:02, 254.16it/s]
Adding requests:  33%|███▎      | 337/1024 [00:01<00:02, 260.52it/s]
Adding requests:  36%|███▌      | 364/1024 [00:01<00:02, 261.66it/s]
Adding requests:  38%|███▊      | 392/1024 [00:01<00:02, 264.75it/s]
Adding requests:  41%|████      | 420/1024 [00:01<00:02, 268.57it/s]
Adding requests:  44%|████▎     | 447/1024 [00:01<00:02, 258.99it/s]
Adding requests:  46%|████▋     | 476/1024 [00:01<00:02, 266.43it/s]
Adding requests:  49%|████▉     | 505/1024 [00:01<00:01, 271.85it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:02<00:01, 277.65it/s]
Adding requests:  55%|█████▍    | 563/1024 [00:02<00:01, 275.09it/s]
Adding requests:  58%|█████▊    | 591/1024 [00:02<00:01, 266.13it/s]
Adding requests:  60%|██████    | 618/1024 [00:02<00:01, 264.74it/s]
Adding requests:  63%|██████▎   | 645/1024 [00:02<00:01, 255.19it/s]
Adding requests:  66%|██████▌   | 671/1024 [00:02<00:01, 249.84it/s]
Adding requests:  68%|██████▊   | 698/1024 [00:02<00:01, 254.31it/s]
Adding requests:  71%|███████   | 724/1024 [00:02<00:01, 253.93it/s]
Adding requests:  73%|███████▎  | 750/1024 [00:02<00:01, 249.21it/s]
Adding requests:  76%|███████▌  | 776/1024 [00:03<00:00, 250.38it/s]
Adding requests:  78%|███████▊  | 802/1024 [00:03<00:00, 251.69it/s]
Adding requests:  81%|████████  | 828/1024 [00:03<00:00, 253.36it/s]
Adding requests:  83%|████████▎ | 854/1024 [00:03<00:00, 252.89it/s]
Adding requests:  86%|████████▌ | 880/1024 [00:03<00:00, 252.82it/s]
Adding requests:  89%|████████▊ | 908/1024 [00:03<00:00, 258.03it/s]
Adding requests:  91%|█████████ | 934/1024 [00:03<00:00, 249.53it/s]
Adding requests:  94%|█████████▍| 960/1024 [00:03<00:00, 250.55it/s]
Adding requests:  96%|█████████▋| 986/1024 [00:03<00:00, 249.33it/s]
Adding requests:  99%|█████████▊| 1011/1024 [00:03<00:00, 247.56it/s]
Adding requests: 100%|██████████| 1024/1024 [00:04<00:00, 254.99it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▎         | 38/1024 [00:00<00:09, 99.33it/s, est. speed input: 101726.42 toks/s, output: 99.34 toks/s]
Processed prompts:   5%|▍         | 48/1024 [00:01<00:28, 34.85it/s, est. speed input: 42195.39 toks/s, output: 41.21 toks/s] 
Processed prompts:   5%|▌         | 53/1024 [00:01<00:35, 27.19it/s, est. speed input: 34877.54 toks/s, output: 34.06 toks/s]
Processed prompts:   6%|▌         | 57/1024 [00:01<00:44, 21.57it/s, est. speed input: 29978.67 toks/s, output: 29.28 toks/s]
Processed prompts:   6%|▌         | 60/1024 [00:02<00:56, 17.10it/s, est. speed input: 26276.57 toks/s, output: 25.66 toks/s]
Processed prompts:   6%|▌         | 62/1024 [00:02<01:12, 13.32it/s, est. speed input: 23259.18 toks/s, output: 22.71 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:03<01:17, 12.36it/s, est. speed input: 21653.39 toks/s, output: 21.15 toks/s]
Processed prompts:   7%|▋         | 70/1024 [00:03<01:21, 11.70it/s, est. speed input: 20405.42 toks/s, output: 19.93 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:03<01:24, 11.25it/s, est. speed input: 19410.32 toks/s, output: 18.96 toks/s]
Processed prompts:   8%|▊         | 78/1024 [00:04<01:26, 10.94it/s, est. speed input: 18594.55 toks/s, output: 18.16 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:04<01:27, 10.72it/s, est. speed input: 17915.27 toks/s, output: 17.50 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:04<00:37, 24.60it/s, est. speed input: 20794.28 toks/s, output: 20.31 toks/s]
Processed prompts:  10%|█         | 103/1024 [00:05<00:44, 20.47it/s, est. speed input: 20213.45 toks/s, output: 19.74 toks/s]
Processed prompts:  10%|█         | 107/1024 [00:05<00:53, 17.05it/s, est. speed input: 19530.88 toks/s, output: 19.07 toks/s]
Processed prompts:  11%|█         | 110/1024 [00:06<01:05, 14.01it/s, est. speed input: 18764.14 toks/s, output: 18.32 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:06<01:11, 12.81it/s, est. speed input: 18253.22 toks/s, output: 17.83 toks/s]
Processed prompts:  12%|█▏        | 118/1024 [00:06<01:15, 11.99it/s, est. speed input: 17800.32 toks/s, output: 17.38 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:07<01:18, 11.43it/s, est. speed input: 17395.83 toks/s, output: 16.99 toks/s]
Processed prompts:  12%|█▏        | 126/1024 [00:07<01:21, 11.05it/s, est. speed input: 17034.36 toks/s, output: 16.64 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:07<01:22, 10.78it/s, est. speed input: 16707.91 toks/s, output: 16.32 toks/s]
Processed prompts:  13%|█▎        | 134/1024 [00:08<01:24, 10.59it/s, est. speed input: 16410.22 toks/s, output: 16.03 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:08<01:24, 10.46it/s, est. speed input: 16140.74 toks/s, output: 15.76 toks/s]
Processed prompts:  14%|█▍        | 142/1024 [00:09<01:25, 10.37it/s, est. speed input: 15893.65 toks/s, output: 15.52 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:09<01:25, 10.30it/s, est. speed input: 15666.17 toks/s, output: 15.30 toks/s]
Processed prompts:  15%|█▍        | 150/1024 [00:09<01:25, 10.26it/s, est. speed input: 15458.16 toks/s, output: 15.10 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:10<01:25, 10.23it/s, est. speed input: 15265.18 toks/s, output: 14.91 toks/s]
Processed prompts:  15%|█▌        | 158/1024 [00:10<01:24, 10.20it/s, est. speed input: 15085.46 toks/s, output: 14.73 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:11<01:24, 10.19it/s, est. speed input: 14919.70 toks/s, output: 14.57 toks/s]
Processed prompts:  16%|█▌        | 166/1024 [00:11<01:24, 10.17it/s, est. speed input: 14763.70 toks/s, output: 14.42 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:11<01:24, 10.16it/s, est. speed input: 14618.77 toks/s, output: 14.28 toks/s]
Processed prompts:  17%|█▋        | 174/1024 [00:12<01:23, 10.16it/s, est. speed input: 14483.30 toks/s, output: 14.14 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:12<01:23, 10.15it/s, est. speed input: 14356.05 toks/s, output: 14.02 toks/s]
Processed prompts:  18%|█▊        | 182/1024 [00:13<01:22, 10.15it/s, est. speed input: 14236.27 toks/s, output: 13.90 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:13<01:22, 10.14it/s, est. speed input: 14123.24 toks/s, output: 13.79 toks/s]
Processed prompts:  19%|█▊        | 190/1024 [00:13<01:22, 10.14it/s, est. speed input: 14016.57 toks/s, output: 13.69 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:14<01:21, 10.14it/s, est. speed input: 13916.43 toks/s, output: 13.59 toks/s]
Processed prompts:  19%|█▉        | 198/1024 [00:14<01:21, 10.14it/s, est. speed input: 13821.05 toks/s, output: 13.50 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:15<01:21, 10.14it/s, est. speed input: 13730.79 toks/s, output: 13.41 toks/s]
Processed prompts:  20%|██        | 206/1024 [00:15<01:20, 10.14it/s, est. speed input: 13645.29 toks/s, output: 13.33 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:15<01:20, 10.13it/s, est. speed input: 13563.70 toks/s, output: 13.25 toks/s]
Processed prompts:  21%|██        | 214/1024 [00:16<01:19, 10.14it/s, est. speed input: 13486.44 toks/s, output: 13.17 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:16<01:19, 10.13it/s, est. speed input: 13412.54 toks/s, output: 13.10 toks/s]
Processed prompts:  22%|██▏       | 222/1024 [00:17<01:19, 10.13it/s, est. speed input: 13342.34 toks/s, output: 13.03 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:17<01:18, 10.13it/s, est. speed input: 13274.95 toks/s, output: 12.96 toks/s]
Processed prompts:  22%|██▏       | 230/1024 [00:17<01:18, 10.13it/s, est. speed input: 13210.64 toks/s, output: 12.90 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:18<01:17, 10.13it/s, est. speed input: 13149.05 toks/s, output: 12.84 toks/s]
Processed prompts:  23%|██▎       | 238/1024 [00:18<01:17, 10.13it/s, est. speed input: 13090.25 toks/s, output: 12.78 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:19<01:17, 10.13it/s, est. speed input: 13033.66 toks/s, output: 12.73 toks/s]
Processed prompts:  24%|██▍       | 246/1024 [00:19<01:16, 10.13it/s, est. speed input: 12979.34 toks/s, output: 12.68 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:19<01:16, 10.13it/s, est. speed input: 12927.23 toks/s, output: 12.62 toks/s]
Processed prompts:  25%|██▍       | 254/1024 [00:20<01:16, 10.12it/s, est. speed input: 12877.09 toks/s, output: 12.58 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:20<01:15, 10.12it/s, est. speed input: 12828.91 toks/s, output: 12.53 toks/s]
Processed prompts:  26%|██▌       | 262/1024 [00:20<01:15, 10.12it/s, est. speed input: 12782.53 toks/s, output: 12.48 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:21<01:14, 10.12it/s, est. speed input: 12737.87 toks/s, output: 12.44 toks/s]
Processed prompts:  26%|██▋       | 270/1024 [00:21<01:14, 10.12it/s, est. speed input: 12694.65 toks/s, output: 12.40 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:22<01:14, 10.12it/s, est. speed input: 12653.11 toks/s, output: 12.36 toks/s]
Processed prompts:  27%|██▋       | 278/1024 [00:22<01:13, 10.12it/s, est. speed input: 12613.11 toks/s, output: 12.32 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:22<01:13, 10.12it/s, est. speed input: 12574.11 toks/s, output: 12.28 toks/s]
Processed prompts:  28%|██▊       | 286/1024 [00:23<01:12, 10.12it/s, est. speed input: 12536.56 toks/s, output: 12.24 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:23<01:12, 10.12it/s, est. speed input: 12500.24 toks/s, output: 12.21 toks/s]
Processed prompts:  29%|██▊       | 294/1024 [00:24<01:12, 10.11it/s, est. speed input: 12465.13 toks/s, output: 12.17 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:24<01:11, 10.11it/s, est. speed input: 12431.19 toks/s, output: 12.14 toks/s]
Processed prompts:  29%|██▉       | 302/1024 [00:24<01:11, 10.11it/s, est. speed input: 12398.22 toks/s, output: 12.11 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:25<01:11, 10.11it/s, est. speed input: 12366.19 toks/s, output: 12.08 toks/s]
Processed prompts:  30%|███       | 310/1024 [00:25<01:10, 10.11it/s, est. speed input: 12335.16 toks/s, output: 12.05 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:26<01:10, 10.11it/s, est. speed input: 12305.25 toks/s, output: 12.02 toks/s]
Processed prompts:  31%|███       | 318/1024 [00:26<01:09, 10.11it/s, est. speed input: 12276.09 toks/s, output: 11.99 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:26<01:09, 10.11it/s, est. speed input: 12247.61 toks/s, output: 11.96 toks/s]
Processed prompts:  32%|███▏      | 326/1024 [00:27<01:09, 10.11it/s, est. speed input: 12220.15 toks/s, output: 11.93 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:27<01:08, 10.10it/s, est. speed input: 12193.25 toks/s, output: 11.91 toks/s]
Processed prompts:  33%|███▎      | 334/1024 [00:28<01:08, 10.11it/s, est. speed input: 12167.43 toks/s, output: 11.88 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:28<01:07, 10.11it/s, est. speed input: 12142.26 toks/s, output: 11.86 toks/s]
Processed prompts:  33%|███▎      | 342/1024 [00:28<01:07, 10.11it/s, est. speed input: 12117.58 toks/s, output: 11.83 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:29<01:07, 10.11it/s, est. speed input: 12093.65 toks/s, output: 11.81 toks/s]
Processed prompts:  34%|███▍      | 350/1024 [00:29<01:06, 10.10it/s, est. speed input: 12070.35 toks/s, output: 11.79 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:30<01:06, 10.10it/s, est. speed input: 12047.71 toks/s, output: 11.77 toks/s]
Processed prompts:  35%|███▍      | 358/1024 [00:30<01:05, 10.11it/s, est. speed input: 12025.65 toks/s, output: 11.74 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:30<01:05, 10.11it/s, est. speed input: 12004.16 toks/s, output: 11.72 toks/s]
Processed prompts:  36%|███▌      | 366/1024 [00:31<01:05, 10.11it/s, est. speed input: 11983.17 toks/s, output: 11.70 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:31<01:04, 10.11it/s, est. speed input: 11962.78 toks/s, output: 11.68 toks/s]
Processed prompts:  37%|███▋      | 374/1024 [00:32<01:04, 10.11it/s, est. speed input: 11942.81 toks/s, output: 11.66 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:32<01:03, 10.10it/s, est. speed input: 11923.33 toks/s, output: 11.64 toks/s]
Processed prompts:  37%|███▋      | 382/1024 [00:32<01:03, 10.11it/s, est. speed input: 11904.41 toks/s, output: 11.63 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:33<01:03, 10.10it/s, est. speed input: 11885.71 toks/s, output: 11.61 toks/s]
Processed prompts:  38%|███▊      | 390/1024 [00:33<01:02, 10.10it/s, est. speed input: 11867.56 toks/s, output: 11.59 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:34<01:02, 10.10it/s, est. speed input: 11849.83 toks/s, output: 11.57 toks/s]
Processed prompts:  39%|███▉      | 398/1024 [00:34<01:01, 10.10it/s, est. speed input: 11832.52 toks/s, output: 11.56 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:34<01:01, 10.10it/s, est. speed input: 11815.55 toks/s, output: 11.54 toks/s]
Processed prompts:  40%|███▉      | 406/1024 [00:35<01:01, 10.10it/s, est. speed input: 11798.85 toks/s, output: 11.52 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:35<01:00, 10.09it/s, est. speed input: 11782.37 toks/s, output: 11.51 toks/s]
Processed prompts:  40%|████      | 414/1024 [00:36<01:00, 10.09it/s, est. speed input: 11766.48 toks/s, output: 11.49 toks/s]
Processed prompts:  42%|████▏     | 430/1024 [00:36<00:24, 24.02it/s, est. speed input: 12177.32 toks/s, output: 11.89 toks/s]
Processed prompts:  42%|████▏     | 435/1024 [00:36<00:29, 20.03it/s, est. speed input: 12185.15 toks/s, output: 11.90 toks/s]
Processed prompts:  43%|████▎     | 439/1024 [00:36<00:35, 16.71it/s, est. speed input: 12165.12 toks/s, output: 11.88 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:37<00:42, 13.76it/s, est. speed input: 12118.06 toks/s, output: 11.83 toks/s]
Processed prompts:  44%|████▎     | 446/1024 [00:37<00:45, 12.60it/s, est. speed input: 12099.12 toks/s, output: 11.82 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:38<00:48, 11.81it/s, est. speed input: 12080.47 toks/s, output: 11.80 toks/s]
Processed prompts:  44%|████▍     | 454/1024 [00:38<00:50, 11.28it/s, est. speed input: 12062.43 toks/s, output: 11.78 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:38<00:51, 10.91it/s, est. speed input: 12044.66 toks/s, output: 11.76 toks/s]
Processed prompts:  45%|████▌     | 462/1024 [00:39<00:52, 10.66it/s, est. speed input: 12027.21 toks/s, output: 11.75 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:39<00:53, 10.49it/s, est. speed input: 12010.29 toks/s, output: 11.73 toks/s]
Processed prompts:  46%|████▌     | 470/1024 [00:40<00:53, 10.36it/s, est. speed input: 11993.38 toks/s, output: 11.71 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:40<00:53, 10.28it/s, est. speed input: 11977.09 toks/s, output: 11.70 toks/s]
Processed prompts:  47%|████▋     | 478/1024 [00:40<00:53, 10.21it/s, est. speed input: 11960.91 toks/s, output: 11.68 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:41<00:53, 10.17it/s, est. speed input: 11945.04 toks/s, output: 11.67 toks/s]
Processed prompts:  47%|████▋     | 486/1024 [00:41<00:53, 10.14it/s, est. speed input: 11929.59 toks/s, output: 11.65 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:42<00:52, 10.12it/s, est. speed input: 11914.29 toks/s, output: 11.64 toks/s]
Processed prompts:  48%|████▊     | 494/1024 [00:42<00:52, 10.11it/s, est. speed input: 11899.32 toks/s, output: 11.62 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:42<00:52, 10.10it/s, est. speed input: 11884.68 toks/s, output: 11.61 toks/s]
Processed prompts:  49%|████▉     | 502/1024 [00:43<00:51, 10.09it/s, est. speed input: 11870.13 toks/s, output: 11.59 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:43<00:51, 10.08it/s, est. speed input: 11855.96 toks/s, output: 11.58 toks/s]
Processed prompts:  50%|████▉     | 510/1024 [00:44<00:51, 10.08it/s, est. speed input: 11842.03 toks/s, output: 11.56 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:44<00:50, 10.07it/s, est. speed input: 11828.29 toks/s, output: 11.55 toks/s]
Processed prompts:  51%|█████     | 518/1024 [00:44<00:50, 10.07it/s, est. speed input: 11814.94 toks/s, output: 11.54 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:45<00:49, 10.07it/s, est. speed input: 11801.67 toks/s, output: 11.53 toks/s]
Processed prompts:  51%|█████▏    | 526/1024 [00:45<00:49, 10.07it/s, est. speed input: 11788.66 toks/s, output: 11.51 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:46<00:49, 10.07it/s, est. speed input: 11775.87 toks/s, output: 11.50 toks/s]
Processed prompts:  52%|█████▏    | 534/1024 [00:46<00:48, 10.07it/s, est. speed input: 11763.32 toks/s, output: 11.49 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:46<00:48, 10.06it/s, est. speed input: 11750.91 toks/s, output: 11.48 toks/s]
Processed prompts:  53%|█████▎    | 542/1024 [00:47<00:47, 10.07it/s, est. speed input: 11738.82 toks/s, output: 11.46 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:47<00:47, 10.06it/s, est. speed input: 11726.82 toks/s, output: 11.45 toks/s]
Processed prompts:  54%|█████▎    | 550/1024 [00:48<00:47, 10.06it/s, est. speed input: 11715.10 toks/s, output: 11.44 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:48<00:46, 10.07it/s, est. speed input: 11703.58 toks/s, output: 11.43 toks/s]
Processed prompts:  54%|█████▍    | 558/1024 [00:48<00:46, 10.07it/s, est. speed input: 11692.30 toks/s, output: 11.42 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:49<00:45, 10.07it/s, est. speed input: 11681.11 toks/s, output: 11.41 toks/s]
Processed prompts:  55%|█████▌    | 566/1024 [00:49<00:45, 10.07it/s, est. speed input: 11670.11 toks/s, output: 11.40 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:50<00:45, 10.07it/s, est. speed input: 11659.32 toks/s, output: 11.39 toks/s]
Processed prompts:  56%|█████▌    | 574/1024 [00:50<00:44, 10.07it/s, est. speed input: 11648.68 toks/s, output: 11.38 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:50<00:44, 10.07it/s, est. speed input: 11638.21 toks/s, output: 11.37 toks/s]
Processed prompts:  57%|█████▋    | 582/1024 [00:51<00:43, 10.07it/s, est. speed input: 11627.88 toks/s, output: 11.36 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:51<00:43, 10.07it/s, est. speed input: 11617.71 toks/s, output: 11.35 toks/s]
Processed prompts:  58%|█████▊    | 590/1024 [00:52<00:43, 10.07it/s, est. speed input: 11607.77 toks/s, output: 11.34 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:52<00:42, 10.07it/s, est. speed input: 11597.92 toks/s, output: 11.33 toks/s]
Processed prompts:  58%|█████▊    | 598/1024 [00:52<00:42, 10.07it/s, est. speed input: 11588.27 toks/s, output: 11.32 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:53<00:41, 10.07it/s, est. speed input: 11578.66 toks/s, output: 11.31 toks/s]
Processed prompts:  59%|█████▉    | 606/1024 [00:53<00:41, 10.06it/s, est. speed input: 11569.17 toks/s, output: 11.30 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:54<00:41, 10.06it/s, est. speed input: 11559.90 toks/s, output: 11.29 toks/s]
Processed prompts:  60%|█████▉    | 614/1024 [00:54<00:40, 10.06it/s, est. speed input: 11550.73 toks/s, output: 11.28 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:54<00:40, 10.06it/s, est. speed input: 11541.67 toks/s, output: 11.27 toks/s]
Processed prompts:  61%|██████    | 622/1024 [00:55<00:39, 10.06it/s, est. speed input: 11532.78 toks/s, output: 11.26 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:55<00:39, 10.07it/s, est. speed input: 11524.06 toks/s, output: 11.25 toks/s]
Processed prompts:  62%|██████▏   | 630/1024 [00:56<00:39, 10.06it/s, est. speed input: 11515.40 toks/s, output: 11.25 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:56<00:38, 10.07it/s, est. speed input: 11506.91 toks/s, output: 11.24 toks/s]
Processed prompts:  62%|██████▏   | 638/1024 [00:56<00:38, 10.07it/s, est. speed input: 11498.53 toks/s, output: 11.23 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:57<00:37, 10.06it/s, est. speed input: 11490.15 toks/s, output: 11.22 toks/s]
Processed prompts:  63%|██████▎   | 646/1024 [00:57<00:37, 10.06it/s, est. speed input: 11481.93 toks/s, output: 11.21 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:58<00:37, 10.06it/s, est. speed input: 11473.95 toks/s, output: 11.21 toks/s]
Processed prompts:  64%|██████▍   | 654/1024 [00:58<00:36, 10.07it/s, est. speed input: 11466.06 toks/s, output: 11.20 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:58<00:36, 10.07it/s, est. speed input: 11458.24 toks/s, output: 11.19 toks/s]
Processed prompts:  65%|██████▍   | 662/1024 [00:59<00:35, 10.07it/s, est. speed input: 11450.54 toks/s, output: 11.18 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:59<00:35, 10.07it/s, est. speed input: 11442.93 toks/s, output: 11.17 toks/s]
Processed prompts:  65%|██████▌   | 670/1024 [00:59<00:35, 10.07it/s, est. speed input: 11435.41 toks/s, output: 11.17 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [01:00<00:34, 10.07it/s, est. speed input: 11428.00 toks/s, output: 11.16 toks/s]
Processed prompts:  66%|██████▌   | 678/1024 [01:00<00:34, 10.07it/s, est. speed input: 11420.64 toks/s, output: 11.15 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [01:01<00:33, 10.06it/s, est. speed input: 11413.37 toks/s, output: 11.15 toks/s]
Processed prompts:  67%|██████▋   | 686/1024 [01:01<00:33, 10.06it/s, est. speed input: 11406.23 toks/s, output: 11.14 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [01:01<00:33, 10.06it/s, est. speed input: 11399.14 toks/s, output: 11.13 toks/s]
Processed prompts:  68%|██████▊   | 694/1024 [01:02<00:32, 10.06it/s, est. speed input: 11392.20 toks/s, output: 11.13 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [01:02<00:32, 10.06it/s, est. speed input: 11385.32 toks/s, output: 11.12 toks/s]
Processed prompts:  69%|██████▊   | 702/1024 [01:03<00:32, 10.06it/s, est. speed input: 11378.43 toks/s, output: 11.11 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [01:03<00:31, 10.06it/s, est. speed input: 11371.74 toks/s, output: 11.11 toks/s]
Processed prompts:  69%|██████▉   | 710/1024 [01:03<00:31, 10.06it/s, est. speed input: 11365.10 toks/s, output: 11.10 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [01:04<00:30, 10.06it/s, est. speed input: 11358.61 toks/s, output: 11.09 toks/s]
Processed prompts:  70%|███████   | 718/1024 [01:04<00:30, 10.06it/s, est. speed input: 11352.15 toks/s, output: 11.09 toks/s]
Processed prompts:  71%|███████   | 722/1024 [01:05<00:30, 10.06it/s, est. speed input: 11345.74 toks/s, output: 11.08 toks/s]
Processed prompts:  71%|███████   | 726/1024 [01:05<00:29, 10.06it/s, est. speed input: 11339.42 toks/s, output: 11.07 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [01:05<00:29, 10.06it/s, est. speed input: 11333.19 toks/s, output: 11.07 toks/s]
Processed prompts:  72%|███████▏  | 734/1024 [01:06<00:28, 10.07it/s, est. speed input: 11327.08 toks/s, output: 11.06 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [01:06<00:28, 10.07it/s, est. speed input: 11321.01 toks/s, output: 11.06 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [01:06<00:11, 23.71it/s, est. speed input: 11542.00 toks/s, output: 11.27 toks/s]
Processed prompts:  74%|███████▍  | 759/1024 [01:07<00:13, 19.85it/s, est. speed input: 11549.92 toks/s, output: 11.28 toks/s]
Processed prompts:  75%|███████▍  | 763/1024 [01:07<00:15, 16.60it/s, est. speed input: 11542.61 toks/s, output: 11.27 toks/s]
Processed prompts:  75%|███████▍  | 766/1024 [01:08<00:18, 13.70it/s, est. speed input: 11520.42 toks/s, output: 11.25 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [01:08<00:20, 12.56it/s, est. speed input: 11513.41 toks/s, output: 11.24 toks/s]
Processed prompts:  76%|███████▌  | 774/1024 [01:08<00:21, 11.79it/s, est. speed input: 11506.48 toks/s, output: 11.24 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [01:09<00:21, 11.26it/s, est. speed input: 11499.65 toks/s, output: 11.23 toks/s]
Processed prompts:  76%|███████▋  | 782/1024 [01:09<00:22, 10.89it/s, est. speed input: 11492.81 toks/s, output: 11.22 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [01:10<00:22, 10.64it/s, est. speed input: 11486.07 toks/s, output: 11.22 toks/s]
Processed prompts:  77%|███████▋  | 790/1024 [01:10<00:22, 10.47it/s, est. speed input: 11479.44 toks/s, output: 11.21 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [01:10<00:22, 10.35it/s, est. speed input: 11472.86 toks/s, output: 11.20 toks/s]
Processed prompts:  78%|███████▊  | 798/1024 [01:11<00:22, 10.26it/s, est. speed input: 11466.35 toks/s, output: 11.20 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [01:11<00:21, 10.20it/s, est. speed input: 11459.89 toks/s, output: 11.19 toks/s]
Processed prompts:  79%|███████▊  | 806/1024 [01:12<00:21, 10.16it/s, est. speed input: 11453.50 toks/s, output: 11.19 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [01:12<00:21, 10.13it/s, est. speed input: 11447.19 toks/s, output: 11.18 toks/s]
Processed prompts:  79%|███████▉  | 814/1024 [01:12<00:20, 10.11it/s, est. speed input: 11441.00 toks/s, output: 11.17 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [01:13<00:20, 10.10it/s, est. speed input: 11434.86 toks/s, output: 11.17 toks/s]
Processed prompts:  80%|████████  | 822/1024 [01:13<00:20, 10.08it/s, est. speed input: 11428.70 toks/s, output: 11.16 toks/s]
Processed prompts:  81%|████████  | 826/1024 [01:14<00:19, 10.08it/s, est. speed input: 11422.64 toks/s, output: 11.15 toks/s]
Processed prompts:  81%|████████  | 830/1024 [01:14<00:19, 10.07it/s, est. speed input: 11416.67 toks/s, output: 11.15 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [01:14<00:18, 10.07it/s, est. speed input: 11410.76 toks/s, output: 11.14 toks/s]
Processed prompts:  82%|████████▏ | 838/1024 [01:15<00:18, 10.07it/s, est. speed input: 11404.87 toks/s, output: 11.14 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [01:15<00:18, 10.06it/s, est. speed input: 11399.05 toks/s, output: 11.13 toks/s]
Processed prompts:  83%|████████▎ | 846/1024 [01:16<00:17, 10.06it/s, est. speed input: 11393.35 toks/s, output: 11.13 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [01:16<00:17, 10.06it/s, est. speed input: 11387.61 toks/s, output: 11.12 toks/s]
Processed prompts:  83%|████████▎ | 854/1024 [01:16<00:16, 10.06it/s, est. speed input: 11381.98 toks/s, output: 11.12 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [01:17<00:16, 10.06it/s, est. speed input: 11376.46 toks/s, output: 11.11 toks/s]
Processed prompts:  84%|████████▍ | 862/1024 [01:17<00:16, 10.06it/s, est. speed input: 11370.89 toks/s, output: 11.10 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [01:18<00:15, 10.06it/s, est. speed input: 11365.48 toks/s, output: 11.10 toks/s]
Processed prompts:  85%|████████▍ | 870/1024 [01:18<00:15, 10.06it/s, est. speed input: 11360.07 toks/s, output: 11.09 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [01:18<00:14, 10.06it/s, est. speed input: 11354.76 toks/s, output: 11.09 toks/s]
Processed prompts:  86%|████████▌ | 878/1024 [01:19<00:14, 10.07it/s, est. speed input: 11349.58 toks/s, output: 11.08 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [01:19<00:14, 10.06it/s, est. speed input: 11344.30 toks/s, output: 11.08 toks/s]
Processed prompts:  87%|████████▋ | 886/1024 [01:20<00:13, 10.07it/s, est. speed input: 11339.19 toks/s, output: 11.07 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [01:20<00:13, 10.06it/s, est. speed input: 11334.03 toks/s, output: 11.07 toks/s]
Processed prompts:  87%|████████▋ | 894/1024 [01:20<00:12, 10.06it/s, est. speed input: 11328.94 toks/s, output: 11.06 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [01:21<00:12, 10.06it/s, est. speed input: 11323.91 toks/s, output: 11.06 toks/s]
Processed prompts:  88%|████████▊ | 902/1024 [01:21<00:12, 10.06it/s, est. speed input: 11318.93 toks/s, output: 11.05 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [01:21<00:11, 10.06it/s, est. speed input: 11314.00 toks/s, output: 11.05 toks/s]
Processed prompts:  89%|████████▉ | 910/1024 [01:22<00:11, 10.06it/s, est. speed input: 11309.11 toks/s, output: 11.04 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [01:22<00:10, 10.06it/s, est. speed input: 11304.30 toks/s, output: 11.04 toks/s]
Processed prompts:  90%|████████▉ | 918/1024 [01:23<00:10, 10.06it/s, est. speed input: 11299.51 toks/s, output: 11.03 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [01:23<00:10, 10.06it/s, est. speed input: 11294.77 toks/s, output: 11.03 toks/s]
Processed prompts:  90%|█████████ | 926/1024 [01:23<00:09, 10.06it/s, est. speed input: 11290.06 toks/s, output: 11.03 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [01:24<00:09, 10.06it/s, est. speed input: 11285.43 toks/s, output: 11.02 toks/s]
Processed prompts:  91%|█████████ | 934/1024 [01:24<00:08, 10.06it/s, est. speed input: 11280.86 toks/s, output: 11.02 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [01:25<00:08, 10.07it/s, est. speed input: 11276.35 toks/s, output: 11.01 toks/s]
Processed prompts:  92%|█████████▏| 942/1024 [01:25<00:08, 10.06it/s, est. speed input: 11271.80 toks/s, output: 11.01 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [01:25<00:07, 10.06it/s, est. speed input: 11267.28 toks/s, output: 11.00 toks/s]
Processed prompts:  93%|█████████▎| 950/1024 [01:26<00:07, 10.06it/s, est. speed input: 11262.84 toks/s, output: 11.00 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [01:26<00:06, 10.06it/s, est. speed input: 11258.46 toks/s, output: 10.99 toks/s]
Processed prompts:  94%|█████████▎| 958/1024 [01:27<00:06, 10.06it/s, est. speed input: 11254.08 toks/s, output: 10.99 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [01:27<00:06, 10.06it/s, est. speed input: 11249.76 toks/s, output: 10.99 toks/s]
Processed prompts:  94%|█████████▍| 966/1024 [01:27<00:05, 10.06it/s, est. speed input: 11245.50 toks/s, output: 10.98 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [01:28<00:05, 10.06it/s, est. speed input: 11241.24 toks/s, output: 10.98 toks/s]
Processed prompts:  95%|█████████▌| 974/1024 [01:28<00:04, 10.06it/s, est. speed input: 11237.09 toks/s, output: 10.97 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [01:29<00:04, 10.06it/s, est. speed input: 11232.95 toks/s, output: 10.97 toks/s]
Processed prompts:  96%|█████████▌| 982/1024 [01:29<00:04, 10.06it/s, est. speed input: 11228.81 toks/s, output: 10.97 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [01:29<00:03, 10.06it/s, est. speed input: 11224.70 toks/s, output: 10.96 toks/s]
Processed prompts:  97%|█████████▋| 990/1024 [01:30<00:03, 10.06it/s, est. speed input: 11220.66 toks/s, output: 10.96 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [01:30<00:02, 10.07it/s, est. speed input: 11216.71 toks/s, output: 10.95 toks/s]
Processed prompts:  97%|█████████▋| 998/1024 [01:31<00:02, 10.06it/s, est. speed input: 11212.70 toks/s, output: 10.95 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [01:31<00:02, 10.06it/s, est. speed input: 11208.71 toks/s, output: 10.95 toks/s]
Processed prompts:  98%|█████████▊| 1006/1024 [01:31<00:01, 10.06it/s, est. speed input: 11204.79 toks/s, output: 10.94 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [01:32<00:01, 10.06it/s, est. speed input: 11200.90 toks/s, output: 10.94 toks/s]
Processed prompts:  99%|█████████▉| 1014/1024 [01:32<00:00, 10.06it/s, est. speed input: 11197.11 toks/s, output: 10.93 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [01:33<00:00, 10.06it/s, est. speed input: 11193.28 toks/s, output: 10.93 toks/s]
Processed prompts: 100%|█████████▉| 1022/1024 [01:33<00:00, 10.80it/s, est. speed input: 11200.31 toks/s, output: 10.94 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:33<00:00, 10.80it/s, est. speed input: 11222.20 toks/s, output: 10.96 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:33<00:00, 10.96it/s, est. speed input: 11222.20 toks/s, output: 10.96 toks/s]
[rank0]:[W126 08:07:17.543185149 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 08:07:20
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:07:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:07:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=383407) WARNING 01-26 08:07:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=383407) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=383407) WARNING 01-26 08:08:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=383407) ERROR 01-26 08:08:32 [core.py:866] ValueError: To serve at least one request with the models's max seq len (1025), (0.19 GiB KV cache is needed, which is larger than the available KV cache memory (0.18 GiB). Based on the available memory, the estimated maximum model length is 960. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:07:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:07:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:07:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:07:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:07:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:07:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:07:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:07:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:07:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:07:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:07:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:07:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:07:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:07:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:07:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:07:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:07:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=383407) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=383407) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.40s/it]
(EngineCore_DP0 pid=383407) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.38s/it]
(EngineCore_DP0 pid=383407) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.08it/s]
(EngineCore_DP0 pid=383407) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
(EngineCore_DP0 pid=383407) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
(EngineCore_DP0 pid=383407) 
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=383407) [2026-01-26 08:07:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=383407) [rank0]:W0126 08:08:23.861000 383407 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383407) [rank0]:W0126 08:08:23.941000 383407 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383407) [rank0]:W0126 08:08:25.598000 383407 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383407) [rank0]:W0126 08:08:25.718000 383407 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383407) Process EngineCore_DP0:
(EngineCore_DP0 pid=383407) Traceback (most recent call last):
(EngineCore_DP0 pid=383407)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=383407)     self.run()
(EngineCore_DP0 pid=383407)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=383407)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=383407)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=383407)     raise e
(EngineCore_DP0 pid=383407)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=383407)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=383407)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=383407)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=383407)     super().__init__(
(EngineCore_DP0 pid=383407)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=383407)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=383407)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=383407)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=383407)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=383407)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=383407)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=383407)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=383407)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=383407)     raise ValueError(
(EngineCore_DP0 pid=383407) ValueError: To serve at least one request with the models's max seq len (1025), (0.19 GiB KV cache is needed, which is larger than the available KV cache memory (0.18 GiB). Based on the available memory, the estimated maximum model length is 960. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:08:34.865543788 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=32768 ==========
Time: 2026-01-26 08:08:38
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:09:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:09:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=384906) WARNING 01-26 08:09:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=384906) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=384906) WARNING 01-26 08:09:54 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=384906) ERROR 01-26 08:11:36 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:09:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:09:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:09:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:09:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:09:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:09:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:09:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:09:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:09:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:09:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:09:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:09:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:09:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:09:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:09:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:09:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:09:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 08:09:32.012671433 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=384906) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=384906) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.46s/it]
(EngineCore_DP0 pid=384906) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.41s/it]
(EngineCore_DP0 pid=384906) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.06it/s]
(EngineCore_DP0 pid=384906) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
(EngineCore_DP0 pid=384906) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
(EngineCore_DP0 pid=384906) 
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=384906) [2026-01-26 08:09:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=384906) [rank0]:W0126 08:10:06.483000 384906 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=384906) [rank0]:W0126 08:10:06.562000 384906 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=384906) [rank0]:W0126 08:10:08.672000 384906 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=384906) [rank0]:W0126 08:10:08.789000 384906 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=384906) Process EngineCore_DP0:
(EngineCore_DP0 pid=384906) Traceback (most recent call last):
(EngineCore_DP0 pid=384906)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=384906)     self.run()
(EngineCore_DP0 pid=384906)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=384906)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=384906)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=384906)     raise e
(EngineCore_DP0 pid=384906)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=384906)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=384906)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=384906)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=384906)     super().__init__(
(EngineCore_DP0 pid=384906)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=384906)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=384906)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=384906)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=384906)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=384906)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=384906)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=384906)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=384906)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=384906)     raise ValueError(
(EngineCore_DP0 pid=384906) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:11:37.727815867 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=65536 ==========
Time: 2026-01-26 08:11:41
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:12:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:12:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=388096) WARNING 01-26 08:12:59 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=388096) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=388096) WARNING 01-26 08:13:20 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=388096) ERROR 01-26 08:18:23 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:12:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:12:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:12:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:12:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:12:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:12:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:12:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:12:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:12:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:12:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:12:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:12:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:12:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:12:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:12:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:12:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:12:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:00] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:00] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:00] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:00] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=388096) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=388096) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.46s/it]
(EngineCore_DP0 pid=388096) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.93it/s]
(EngineCore_DP0 pid=388096) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.21it/s]
(EngineCore_DP0 pid=388096) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.23it/s]
(EngineCore_DP0 pid=388096) 
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=388096) [2026-01-26 08:13:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=388096) [rank0]:W0126 08:13:33.290000 388096 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=388096) [rank0]:W0126 08:13:33.369000 388096 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=388096) [rank0]:W0126 08:13:34.358000 388096 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=388096) [rank0]:W0126 08:13:34.478000 388096 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=388096) Process EngineCore_DP0:
(EngineCore_DP0 pid=388096) Traceback (most recent call last):
(EngineCore_DP0 pid=388096)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=388096)     self.run()
(EngineCore_DP0 pid=388096)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=388096)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=388096)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=388096)     raise e
(EngineCore_DP0 pid=388096)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=388096)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=388096)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=388096)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=388096)     super().__init__(
(EngineCore_DP0 pid=388096)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=388096)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=388096)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=388096)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=388096)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=388096)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=388096)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=388096)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=388096)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=388096)     raise ValueError(
(EngineCore_DP0 pid=388096) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:18:24.615289427 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=16384 ==========
Time: 2026-01-26 21:21:52
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:22:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:22:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1087740) ERROR 01-26 21:22:25 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:22:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:22:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:22:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:22:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:22:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:22:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:22:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:22:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:22:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:22:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:22:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:22:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:22:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:22:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:22:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:22:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:22:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1087740) Process EngineCore_DP0:
(EngineCore_DP0 pid=1087740) Traceback (most recent call last):
(EngineCore_DP0 pid=1087740)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1087740)     self.run()
(EngineCore_DP0 pid=1087740)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1087740)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1087740)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1087740)     raise e
(EngineCore_DP0 pid=1087740)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1087740)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1087740)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1087740)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1087740)     super().__init__(
(EngineCore_DP0 pid=1087740)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1087740)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1087740)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1087740)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1087740)     self._init_executor()
(EngineCore_DP0 pid=1087740)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1087740)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1087740)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1087740)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1087740)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1087740)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1087740)     raise ValueError(
(EngineCore_DP0 pid=1087740) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:22:26.694841035 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=16384 ==========
Time: 2026-01-26 21:22:51
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:23:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:23:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1088766) ERROR 01-26 21:23:24 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:23:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:23:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:23:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:23:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:23:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:23:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:23:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:23:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:23:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:23:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:23:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:23:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:23:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:23:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:23:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:23:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:23:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1088766) Process EngineCore_DP0:
(EngineCore_DP0 pid=1088766) Traceback (most recent call last):
(EngineCore_DP0 pid=1088766)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1088766)     self.run()
(EngineCore_DP0 pid=1088766)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1088766)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1088766)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1088766)     raise e
(EngineCore_DP0 pid=1088766)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1088766)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1088766)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1088766)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1088766)     super().__init__(
(EngineCore_DP0 pid=1088766)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1088766)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1088766)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1088766)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1088766)     self._init_executor()
(EngineCore_DP0 pid=1088766)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1088766)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1088766)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1088766)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1088766)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1088766)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1088766)     raise ValueError(
(EngineCore_DP0 pid=1088766) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:23:25.248084460 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=32768 ==========
Time: 2026-01-26 21:23:48
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:24:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:24:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1089937) ERROR 01-26 21:24:37 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:24:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:24:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:24:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:24:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:24:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:24:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:24:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:24:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:24:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:24:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:24:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:24:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:24:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:24:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:24:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:24:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:24:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1089937) Process EngineCore_DP0:
(EngineCore_DP0 pid=1089937) Traceback (most recent call last):
(EngineCore_DP0 pid=1089937)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1089937)     self.run()
(EngineCore_DP0 pid=1089937)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1089937)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1089937)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1089937)     raise e
(EngineCore_DP0 pid=1089937)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1089937)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1089937)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1089937)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1089937)     super().__init__(
(EngineCore_DP0 pid=1089937)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1089937)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1089937)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1089937)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1089937)     self._init_executor()
(EngineCore_DP0 pid=1089937)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1089937)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1089937)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1089937)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1089937)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1089937)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1089937)     raise ValueError(
(EngineCore_DP0 pid=1089937) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:24:37.095462331 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=32768 ==========
Time: 2026-01-26 21:25:01
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:25:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:25:40 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1091170) ERROR 01-26 21:25:49 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:25:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:25:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:25:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:25:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:25:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:25:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:25:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:25:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:25:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:25:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:25:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:25:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:25:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:25:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:25:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:25:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:25:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1091170) Process EngineCore_DP0:
(EngineCore_DP0 pid=1091170) Traceback (most recent call last):
(EngineCore_DP0 pid=1091170)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1091170)     self.run()
(EngineCore_DP0 pid=1091170)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1091170)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1091170)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1091170)     raise e
(EngineCore_DP0 pid=1091170)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1091170)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1091170)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1091170)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1091170)     super().__init__(
(EngineCore_DP0 pid=1091170)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1091170)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1091170)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1091170)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1091170)     self._init_executor()
(EngineCore_DP0 pid=1091170)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1091170)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1091170)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1091170)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1091170)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1091170)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1091170)     raise ValueError(
(EngineCore_DP0 pid=1091170) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:25:50.666533673 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=65536 ==========
Time: 2026-01-26 21:26:13
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:27:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:27:21 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1092781) ERROR 01-26 21:27:32 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:27:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:27:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:27:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:27:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:27:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:27:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:27:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:27:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:27:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:27:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:27:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:27:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:27:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:27:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:27:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:27:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:27:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1092781) Process EngineCore_DP0:
(EngineCore_DP0 pid=1092781) Traceback (most recent call last):
(EngineCore_DP0 pid=1092781)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1092781)     self.run()
(EngineCore_DP0 pid=1092781)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1092781)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1092781)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1092781)     raise e
(EngineCore_DP0 pid=1092781)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1092781)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1092781)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1092781)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1092781)     super().__init__(
(EngineCore_DP0 pid=1092781)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1092781)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1092781)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1092781)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1092781)     self._init_executor()
(EngineCore_DP0 pid=1092781)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1092781)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1092781)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1092781)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1092781)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1092781)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1092781)     raise ValueError(
(EngineCore_DP0 pid=1092781) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:27:33.428203472 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=65536 ==========
Time: 2026-01-26 21:27:58
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:29:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:29:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1094410) ERROR 01-26 21:29:15 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:29:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:29:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:29:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:29:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:29:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:29:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:29:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:29:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:29:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:29:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:29:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:29:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:29:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:29:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:29:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:29:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1094410) Process EngineCore_DP0:
(EngineCore_DP0 pid=1094410) Traceback (most recent call last):
(EngineCore_DP0 pid=1094410)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1094410)     self.run()
(EngineCore_DP0 pid=1094410)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1094410)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1094410)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1094410)     raise e
(EngineCore_DP0 pid=1094410)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1094410)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1094410)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1094410)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1094410)     super().__init__(
(EngineCore_DP0 pid=1094410)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1094410)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1094410)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1094410)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1094410)     self._init_executor()
(EngineCore_DP0 pid=1094410)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1094410)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1094410)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1094410)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1094410)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1094410)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1094410)     raise ValueError(
(EngineCore_DP0 pid=1094410) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:29:16.410698725 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536
