
========== M=16 ==========
Time: 2026-01-25 17:34:29
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:34:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:34:39 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=198001) WARNING 01-25 17:34:47 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=198001) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=198001) WARNING 01-25 17:34:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.67 requests/s, 266.46 total tokens/s, 15.67 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 17:34:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:34:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:34:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:34:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:34:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:34:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:34:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:34:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:34:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:34:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:34:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:34:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:34:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:34:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:34:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:34:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:34:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:34:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:34:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:34:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:34:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:34:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:34:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:34:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:34:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:34:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:34:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:34:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:47] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=198001) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=198001) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.74s/it]
(EngineCore_DP0 pid=198001) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.74s/it]
(EngineCore_DP0 pid=198001) 
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=198001) [2026-01-25 17:34:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=198001) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.22it/s]
(EngineCore_DP0 pid=198001) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2759.59it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:38,  3.34it/s, est. speed input: 53.41 toks/s, output: 3.34 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:15,  8.02it/s, est. speed input: 112.53 toks/s, output: 7.03 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:11, 10.85it/s, est. speed input: 145.59 toks/s, output: 9.10 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:09, 12.63it/s, est. speed input: 166.59 toks/s, output: 10.41 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 13.75it/s, est. speed input: 180.81 toks/s, output: 11.30 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:08, 14.61it/s, est. speed input: 191.83 toks/s, output: 11.99 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:07, 15.27it/s, est. speed input: 200.66 toks/s, output: 12.54 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 15.76it/s, est. speed input: 207.79 toks/s, output: 12.99 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 15.92it/s, est. speed input: 212.88 toks/s, output: 13.30 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 16.02it/s, est. speed input: 217.00 toks/s, output: 13.56 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 16.03it/s, est. speed input: 220.26 toks/s, output: 13.77 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 15.94it/s, est. speed input: 222.69 toks/s, output: 13.92 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 16.16it/s, est. speed input: 225.67 toks/s, output: 14.10 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 16.11it/s, est. speed input: 227.67 toks/s, output: 14.23 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:06, 16.23it/s, est. speed input: 229.88 toks/s, output: 14.37 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:05, 16.36it/s, est. speed input: 231.94 toks/s, output: 14.50 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 16.45it/s, est. speed input: 233.78 toks/s, output: 14.61 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.54it/s, est. speed input: 235.50 toks/s, output: 14.72 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.73it/s, est. speed input: 237.35 toks/s, output: 14.83 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.10it/s, est. speed input: 239.53 toks/s, output: 14.97 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 17.26it/s, est. speed input: 241.31 toks/s, output: 15.08 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.41it/s, est. speed input: 243.02 toks/s, output: 15.19 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.37it/s, est. speed input: 244.33 toks/s, output: 15.27 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:04, 17.55it/s, est. speed input: 245.90 toks/s, output: 15.37 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:04, 17.62it/s, est. speed input: 247.28 toks/s, output: 15.45 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 17.56it/s, est. speed input: 248.38 toks/s, output: 15.52 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 17.57it/s, est. speed input: 249.48 toks/s, output: 15.59 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 17.53it/s, est. speed input: 250.45 toks/s, output: 15.65 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 17.54it/s, est. speed input: 251.40 toks/s, output: 15.71 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 17.55it/s, est. speed input: 252.32 toks/s, output: 15.77 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 17.54it/s, est. speed input: 253.14 toks/s, output: 15.82 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 17.45it/s, est. speed input: 253.80 toks/s, output: 15.86 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:03, 17.50it/s, est. speed input: 254.58 toks/s, output: 15.91 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 17.70it/s, est. speed input: 255.53 toks/s, output: 15.97 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 17.87it/s, est. speed input: 256.47 toks/s, output: 16.03 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 17.91it/s, est. speed input: 257.27 toks/s, output: 16.08 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 17.86it/s, est. speed input: 257.94 toks/s, output: 16.12 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:02, 17.91it/s, est. speed input: 258.67 toks/s, output: 16.17 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.46it/s, est. speed input: 258.80 toks/s, output: 16.17 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 17.32it/s, est. speed input: 259.11 toks/s, output: 16.19 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 17.08it/s, est. speed input: 259.25 toks/s, output: 16.20 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 16.88it/s, est. speed input: 259.34 toks/s, output: 16.21 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.79it/s, est. speed input: 259.48 toks/s, output: 16.22 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.66it/s, est. speed input: 259.53 toks/s, output: 16.22 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.58it/s, est. speed input: 259.60 toks/s, output: 16.22 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.60it/s, est. speed input: 259.74 toks/s, output: 16.23 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.60it/s, est. speed input: 259.87 toks/s, output: 16.24 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.61it/s, est. speed input: 259.99 toks/s, output: 16.25 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.44it/s, est. speed input: 259.93 toks/s, output: 16.25 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 15.67it/s, est. speed input: 259.15 toks/s, output: 16.20 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 15.30it/s, est. speed input: 258.55 toks/s, output: 16.16 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 15.11it/s, est. speed input: 258.04 toks/s, output: 16.13 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 14.89it/s, est. speed input: 257.45 toks/s, output: 16.09 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 14.79it/s, est. speed input: 256.95 toks/s, output: 16.06 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 14.74it/s, est. speed input: 256.48 toks/s, output: 16.03 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 14.67it/s, est. speed input: 256.00 toks/s, output: 16.00 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:01, 14.58it/s, est. speed input: 255.49 toks/s, output: 15.97 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 14.49it/s, est. speed input: 254.97 toks/s, output: 15.94 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 14.56it/s, est. speed input: 254.61 toks/s, output: 15.91 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 14.52it/s, est. speed input: 254.17 toks/s, output: 15.89 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 14.44it/s, est. speed input: 253.69 toks/s, output: 15.86 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 14.39it/s, est. speed input: 253.23 toks/s, output: 15.83 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 14.46it/s, est. speed input: 252.90 toks/s, output: 15.81 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:08<00:00, 14.42it/s, est. speed input: 252.49 toks/s, output: 15.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.42it/s, est. speed input: 252.27 toks/s, output: 15.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.77it/s, est. speed input: 252.27 toks/s, output: 15.77 toks/s]
[rank0]:[W125 17:35:16.576939921 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 17:35:19
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:35:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:35:29 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=198936) WARNING 01-25 17:35:35 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=198936) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=198936) WARNING 01-25 17:35:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.44 requests/s, 2120.58 total tokens/s, 16.44 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 17:35:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:35:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:35:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:35:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:35:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:35:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:35:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:35:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:35:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:35:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:35:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:35:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:35:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:35:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:35:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:35:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:35:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:35:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:35:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:35:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:35:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:35:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:35:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:35:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:35:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:35:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:35:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:35:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=198936) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=198936) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=198936) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=198936) 
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=198936) [2026-01-25 17:35:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=198936) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.18it/s]
(EngineCore_DP0 pid=198936) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.31it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1270.84it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1268.31it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:30,  4.23it/s, est. speed input: 541.87 toks/s, output: 4.23 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:13,  9.46it/s, est. speed input: 1077.94 toks/s, output: 8.42 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:10, 12.23it/s, est. speed input: 1348.93 toks/s, output: 10.54 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 13.64it/s, est. speed input: 1497.72 toks/s, output: 11.70 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 14.54it/s, est. speed input: 1598.40 toks/s, output: 12.49 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 15.25it/s, est. speed input: 1676.15 toks/s, output: 13.09 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:07, 15.76it/s, est. speed input: 1736.11 toks/s, output: 13.56 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 15.99it/s, est. speed input: 1778.31 toks/s, output: 13.89 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 16.20it/s, est. speed input: 1813.96 toks/s, output: 14.17 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 16.35it/s, est. speed input: 1843.29 toks/s, output: 14.40 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 16.21it/s, est. speed input: 1860.02 toks/s, output: 14.53 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 16.33it/s, est. speed input: 1880.40 toks/s, output: 14.69 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 16.49it/s, est. speed input: 1900.01 toks/s, output: 14.84 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 16.56it/s, est. speed input: 1915.96 toks/s, output: 14.97 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 16.50it/s, est. speed input: 1927.42 toks/s, output: 15.06 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:05, 16.43it/s, est. speed input: 1936.74 toks/s, output: 15.13 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 16.40it/s, est. speed input: 1945.34 toks/s, output: 15.20 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.49it/s, est. speed input: 1955.41 toks/s, output: 15.28 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.93it/s, est. speed input: 1971.81 toks/s, output: 15.40 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.23it/s, est. speed input: 1986.33 toks/s, output: 15.52 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:04, 17.41it/s, est. speed input: 1998.99 toks/s, output: 15.62 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.61it/s, est. speed input: 2011.86 toks/s, output: 15.72 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.65it/s, est. speed input: 2022.07 toks/s, output: 15.80 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 17.67it/s, est. speed input: 2031.44 toks/s, output: 15.87 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:04, 17.72it/s, est. speed input: 2040.62 toks/s, output: 15.94 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 17.72it/s, est. speed input: 2048.64 toks/s, output: 16.00 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 17.75it/s, est. speed input: 2056.55 toks/s, output: 16.07 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 17.64it/s, est. speed input: 2062.27 toks/s, output: 16.11 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:03, 17.77it/s, est. speed input: 2070.13 toks/s, output: 16.17 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 17.89it/s, est. speed input: 2077.90 toks/s, output: 16.23 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 18.00it/s, est. speed input: 2085.46 toks/s, output: 16.29 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 18.07it/s, est. speed input: 2092.59 toks/s, output: 16.35 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 18.16it/s, est. speed input: 2099.75 toks/s, output: 16.40 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 18.24it/s, est. speed input: 2106.59 toks/s, output: 16.46 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 18.00it/s, est. speed input: 2110.10 toks/s, output: 16.49 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 17.92it/s, est. speed input: 2114.34 toks/s, output: 16.52 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 17.91it/s, est. speed input: 2118.80 toks/s, output: 16.55 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:02, 17.80it/s, est. speed input: 2121.99 toks/s, output: 16.58 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.72it/s, est. speed input: 2125.01 toks/s, output: 16.60 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 17.78it/s, est. speed input: 2129.01 toks/s, output: 16.63 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 17.75it/s, est. speed input: 2132.11 toks/s, output: 16.66 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 17.60it/s, est. speed input: 2133.93 toks/s, output: 16.67 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 17.65it/s, est. speed input: 2137.05 toks/s, output: 16.70 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 17.78it/s, est. speed input: 2140.78 toks/s, output: 16.72 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 17.70it/s, est. speed input: 2142.97 toks/s, output: 16.74 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 17.30it/s, est. speed input: 2142.10 toks/s, output: 16.74 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.98it/s, est. speed input: 2140.78 toks/s, output: 16.72 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.95it/s, est. speed input: 2141.19 toks/s, output: 16.73 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.92it/s, est. speed input: 2141.50 toks/s, output: 16.73 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 16.91it/s, est. speed input: 2141.93 toks/s, output: 16.73 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.90it/s, est. speed input: 2142.30 toks/s, output: 16.74 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.85it/s, est. speed input: 2142.25 toks/s, output: 16.74 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.80it/s, est. speed input: 2142.11 toks/s, output: 16.74 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.70it/s, est. speed input: 2141.46 toks/s, output: 16.73 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 16.70it/s, est. speed input: 2717.48 toks/s, output: 21.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 21.23it/s, est. speed input: 2717.48 toks/s, output: 21.23 toks/s]
[rank0]:[W125 17:36:02.434489431 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 17:36:05
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:36:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:36:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=199851) WARNING 01-25 17:36:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=199851) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=199851) WARNING 01-25 17:36:30 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.06 requests/s, 4127.96 total tokens/s, 16.06 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 17:36:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:36:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:36:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:36:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:36:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:36:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:36:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:36:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:36:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:36:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:36:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:36:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:36:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:36:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:36:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:36:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:36:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:36:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:36:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:36:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:36:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:36:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:36:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:36:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:36:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:36:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:36:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:36:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=199851) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=199851) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=199851) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=199851) 
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=199851) [2026-01-25 17:36:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=199851) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.85it/s]
(EngineCore_DP0 pid=199851) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.51it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  70%|███████   | 90/128 [00:00<00:00, 892.51it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 904.49it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:23,  5.40it/s, est. speed input: 1382.34 toks/s, output: 5.40 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:11, 10.54it/s, est. speed input: 2464.79 toks/s, output: 9.63 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:09, 12.83it/s, est. speed input: 2939.16 toks/s, output: 11.48 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 14.13it/s, est. speed input: 3213.95 toks/s, output: 12.55 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 14.93it/s, est. speed input: 3393.37 toks/s, output: 13.25 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 15.42it/s, est. speed input: 3516.44 toks/s, output: 13.74 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:07, 15.76it/s, est. speed input: 3608.81 toks/s, output: 14.10 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 15.70it/s, est. speed input: 3655.21 toks/s, output: 14.28 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 15.95it/s, est. speed input: 3714.24 toks/s, output: 14.51 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 16.01it/s, est. speed input: 3754.75 toks/s, output: 14.67 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 16.11it/s, est. speed input: 3791.88 toks/s, output: 14.81 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 16.17it/s, est. speed input: 3822.40 toks/s, output: 14.93 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 16.31it/s, est. speed input: 3853.89 toks/s, output: 15.05 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 16.36it/s, est. speed input: 3878.50 toks/s, output: 15.15 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 16.39it/s, est. speed input: 3900.05 toks/s, output: 15.23 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:05, 16.37it/s, est. speed input: 3917.02 toks/s, output: 15.30 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 16.37it/s, est. speed input: 3932.67 toks/s, output: 15.36 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.36it/s, est. speed input: 3946.09 toks/s, output: 15.41 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.38it/s, est. speed input: 3959.27 toks/s, output: 15.47 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.40it/s, est. speed input: 3971.40 toks/s, output: 15.51 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.45it/s, est. speed input: 3983.81 toks/s, output: 15.56 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.40it/s, est. speed input: 3992.00 toks/s, output: 15.59 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 16.38it/s, est. speed input: 4000.16 toks/s, output: 15.63 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:04, 16.37it/s, est. speed input: 4007.59 toks/s, output: 15.65 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:04, 16.42it/s, est. speed input: 4016.24 toks/s, output: 15.69 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 16.37it/s, est. speed input: 4021.89 toks/s, output: 15.71 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 16.38it/s, est. speed input: 4028.25 toks/s, output: 15.74 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.32it/s, est. speed input: 4032.28 toks/s, output: 15.75 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.29it/s, est. speed input: 4036.52 toks/s, output: 15.77 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.35it/s, est. speed input: 4042.36 toks/s, output: 15.79 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.34it/s, est. speed input: 4046.78 toks/s, output: 15.81 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.37it/s, est. speed input: 4051.67 toks/s, output: 15.83 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:03, 16.41it/s, est. speed input: 4056.69 toks/s, output: 15.85 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.47it/s, est. speed input: 4062.43 toks/s, output: 15.87 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.51it/s, est. speed input: 4067.61 toks/s, output: 15.89 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.54it/s, est. speed input: 4072.60 toks/s, output: 15.91 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.38it/s, est. speed input: 4073.41 toks/s, output: 15.91 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.30it/s, est. speed input: 4074.82 toks/s, output: 15.92 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.22it/s, est. speed input: 4075.61 toks/s, output: 15.92 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:03, 16.33it/s, est. speed input: 4079.70 toks/s, output: 15.94 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:02, 16.29it/s, est. speed input: 4081.30 toks/s, output: 15.94 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 16.47it/s, est. speed input: 4086.94 toks/s, output: 15.96 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.52it/s, est. speed input: 4090.80 toks/s, output: 15.98 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.54it/s, est. speed input: 4094.29 toks/s, output: 15.99 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.69it/s, est. speed input: 4099.94 toks/s, output: 16.02 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.51it/s, est. speed input: 4100.51 toks/s, output: 16.02 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.50it/s, est. speed input: 4102.99 toks/s, output: 16.03 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:02, 16.45it/s, est. speed input: 4104.62 toks/s, output: 16.03 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:01, 16.15it/s, est. speed input: 4101.65 toks/s, output: 16.02 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.29it/s, est. speed input: 4104.72 toks/s, output: 16.03 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.45it/s, est. speed input: 4108.59 toks/s, output: 16.05 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.86it/s, est. speed input: 4116.77 toks/s, output: 16.08 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 17.10it/s, est. speed input: 4123.88 toks/s, output: 16.11 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 17.29it/s, est. speed input: 4131.07 toks/s, output: 16.14 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 17.44it/s, est. speed input: 4138.16 toks/s, output: 16.16 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:00, 17.44it/s, est. speed input: 4143.59 toks/s, output: 16.19 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 17.47it/s, est. speed input: 4149.22 toks/s, output: 16.21 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 17.31it/s, est. speed input: 4152.39 toks/s, output: 16.22 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 17.37it/s, est. speed input: 4157.70 toks/s, output: 16.24 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 17.33it/s, est. speed input: 4161.74 toks/s, output: 16.26 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 17.54it/s, est. speed input: 4168.51 toks/s, output: 16.28 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 17.53it/s, est. speed input: 4173.25 toks/s, output: 16.30 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 17.52it/s, est. speed input: 4177.79 toks/s, output: 16.32 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 17.76it/s, est. speed input: 4185.14 toks/s, output: 16.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.76it/s, est. speed input: 4187.12 toks/s, output: 16.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.36it/s, est. speed input: 4187.12 toks/s, output: 16.36 toks/s]
[rank0]:[W125 17:36:49.237906078 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-25 19:25:08
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:25:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:25:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=309856) WARNING 01-25 19:25:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=309856) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=309856) WARNING 01-25 19:25:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.47 requests/s, 8450.67 total tokens/s, 16.47 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 19:25:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:25:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:25:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:25:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:25:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:25:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:25:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:25:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:25:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:25:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:25:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:25:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:25:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:25:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:25:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:25:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:25:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:25:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:25:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:25:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:25:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:25:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:25:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:25:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:25:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:25:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:25:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:25:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=309856) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=309856) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.77it/s]
(EngineCore_DP0 pid=309856) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.77it/s]
(EngineCore_DP0 pid=309856) 
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=309856) [2026-01-25 19:25:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=309856) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.69it/s]
(EngineCore_DP0 pid=309856) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  44%|████▍     | 56/128 [00:00<00:00, 554.19it/s]
Adding requests:  89%|████████▉ | 114/128 [00:00<00:00, 568.72it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 570.27it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 32.74it/s, est. speed input: 16766.79 toks/s, output: 32.74 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 22.11it/s, est. speed input: 11969.00 toks/s, output: 23.38 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:05, 20.01it/s, est. speed input: 10974.89 toks/s, output: 21.43 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:05, 19.03it/s, est. speed input: 10486.93 toks/s, output: 20.48 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:05, 18.72it/s, est. speed input: 10305.50 toks/s, output: 20.13 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:05, 18.31it/s, est. speed input: 10125.91 toks/s, output: 19.78 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:05, 18.19it/s, est. speed input: 10025.99 toks/s, output: 19.58 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 18.12it/s, est. speed input: 9946.45 toks/s, output: 19.43 toks/s] 
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 18.07it/s, est. speed input: 9881.39 toks/s, output: 19.30 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 18.06it/s, est. speed input: 9830.66 toks/s, output: 19.20 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.86it/s, est. speed input: 9760.44 toks/s, output: 19.06 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.68it/s, est. speed input: 9695.55 toks/s, output: 18.94 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 17.55it/s, est. speed input: 9638.07 toks/s, output: 18.82 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 17.50it/s, est. speed input: 9593.04 toks/s, output: 18.74 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:05, 17.58it/s, est. speed input: 9564.35 toks/s, output: 18.68 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.66it/s, est. speed input: 9541.50 toks/s, output: 18.64 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 17.38it/s, est. speed input: 9490.14 toks/s, output: 18.54 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.77it/s, est. speed input: 9404.21 toks/s, output: 18.37 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 16.46it/s, est. speed input: 9336.23 toks/s, output: 18.23 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 16.38it/s, est. speed input: 9286.26 toks/s, output: 18.14 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 16.28it/s, est. speed input: 9236.94 toks/s, output: 18.04 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 16.36it/s, est. speed input: 9204.81 toks/s, output: 17.98 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:04, 16.43it/s, est. speed input: 9176.17 toks/s, output: 17.92 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.51it/s, est. speed input: 9151.73 toks/s, output: 17.87 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.43it/s, est. speed input: 9119.64 toks/s, output: 17.81 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.50it/s, est. speed input: 9098.54 toks/s, output: 17.77 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.58it/s, est. speed input: 9080.38 toks/s, output: 17.74 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.56it/s, est. speed input: 9059.19 toks/s, output: 17.69 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 16.48it/s, est. speed input: 9035.58 toks/s, output: 17.65 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 16.32it/s, est. speed input: 9007.15 toks/s, output: 17.59 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:03, 16.27it/s, est. speed input: 8983.84 toks/s, output: 17.55 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.28it/s, est. speed input: 8964.44 toks/s, output: 17.51 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.28it/s, est. speed input: 8945.85 toks/s, output: 17.47 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.28it/s, est. speed input: 8928.59 toks/s, output: 17.44 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.35it/s, est. speed input: 8915.44 toks/s, output: 17.41 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 16.41it/s, est. speed input: 8903.87 toks/s, output: 17.39 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.48it/s, est. speed input: 8894.11 toks/s, output: 17.37 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 16.32it/s, est. speed input: 8875.32 toks/s, output: 17.33 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 16.30it/s, est. speed input: 8861.45 toks/s, output: 17.31 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.34it/s, est. speed input: 8850.44 toks/s, output: 17.29 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.36it/s, est. speed input: 8840.02 toks/s, output: 17.27 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.42it/s, est. speed input: 8831.55 toks/s, output: 17.25 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.44it/s, est. speed input: 8822.77 toks/s, output: 17.23 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:02, 16.41it/s, est. speed input: 8812.67 toks/s, output: 17.21 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.33it/s, est. speed input: 8800.73 toks/s, output: 17.19 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 16.34it/s, est. speed input: 8791.84 toks/s, output: 17.17 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 16.21it/s, est. speed input: 8777.98 toks/s, output: 17.14 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.17it/s, est. speed input: 8766.64 toks/s, output: 17.12 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.23it/s, est. speed input: 8759.03 toks/s, output: 17.11 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.21it/s, est. speed input: 8749.60 toks/s, output: 17.09 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.02it/s, est. speed input: 8734.01 toks/s, output: 17.06 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.21it/s, est. speed input: 8730.38 toks/s, output: 17.05 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.15it/s, est. speed input: 8720.26 toks/s, output: 17.03 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 16.12it/s, est. speed input: 8711.03 toks/s, output: 17.01 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 16.24it/s, est. speed input: 8706.81 toks/s, output: 17.01 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 16.28it/s, est. speed input: 8701.21 toks/s, output: 16.99 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 16.40it/s, est. speed input: 8698.53 toks/s, output: 16.99 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 16.49it/s, est. speed input: 8695.99 toks/s, output: 16.98 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.45it/s, est. speed input: 8690.85 toks/s, output: 16.97 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.54it/s, est. speed input: 8688.84 toks/s, output: 16.97 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.54it/s, est. speed input: 8686.90 toks/s, output: 16.97 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.97it/s, est. speed input: 8686.90 toks/s, output: 16.97 toks/s]
[rank0]:[W125 19:25:53.901746552 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 19:25:56
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:26:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:26:06 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=310739) WARNING 01-25 19:26:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=310739) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=310739) WARNING 01-25 19:26:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.07 requests/s, 16471.99 total tokens/s, 16.07 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 19:26:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:26:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:26:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:26:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:26:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:26:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:26:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:26:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:26:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:26:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:26:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:26:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:26:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:26:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:26:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:26:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:26:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:26:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=310739) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=310739) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.06it/s]
(EngineCore_DP0 pid=310739) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.06it/s]
(EngineCore_DP0 pid=310739) 
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=310739) [2026-01-25 19:26:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=310739) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.96it/s]
(EngineCore_DP0 pid=310739) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  23%|██▎       | 29/128 [00:00<00:00, 280.21it/s]
Adding requests:  46%|████▌     | 59/128 [00:00<00:00, 291.66it/s]
Adding requests:  70%|██████▉   | 89/128 [00:00<00:00, 291.07it/s]
Adding requests:  93%|█████████▎| 119/128 [00:00<00:00, 291.65it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 291.32it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 55.95it/s, est. speed input: 57309.53 toks/s, output: 55.96 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 24.07it/s, est. speed input: 26958.24 toks/s, output: 26.33 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 21.06it/s, est. speed input: 23808.62 toks/s, output: 23.25 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:05, 19.88it/s, est. speed input: 22596.78 toks/s, output: 22.07 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:05, 19.01it/s, est. speed input: 21740.49 toks/s, output: 21.23 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 18.42it/s, est. speed input: 21127.93 toks/s, output: 20.63 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 18.12it/s, est. speed input: 20810.51 toks/s, output: 20.32 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.98it/s, est. speed input: 20585.78 toks/s, output: 20.10 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.96it/s, est. speed input: 20422.72 toks/s, output: 19.94 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 17.93it/s, est. speed input: 20279.08 toks/s, output: 19.80 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 17.79it/s, est. speed input: 20122.37 toks/s, output: 19.65 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:05, 17.32it/s, est. speed input: 19894.94 toks/s, output: 19.43 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.92it/s, est. speed input: 19679.10 toks/s, output: 19.22 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.59it/s, est. speed input: 19475.17 toks/s, output: 19.02 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.45it/s, est. speed input: 19313.47 toks/s, output: 18.86 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 16.27it/s, est. speed input: 19151.80 toks/s, output: 18.70 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:05, 16.08it/s, est. speed input: 18994.72 toks/s, output: 18.55 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 16.00it/s, est. speed input: 18860.77 toks/s, output: 18.42 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 15.83it/s, est. speed input: 18719.64 toks/s, output: 18.28 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:04, 15.88it/s, est. speed input: 18620.38 toks/s, output: 18.18 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.01it/s, est. speed input: 18542.20 toks/s, output: 18.11 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.13it/s, est. speed input: 18476.27 toks/s, output: 18.04 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.16it/s, est. speed input: 18406.30 toks/s, output: 17.97 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 15.95it/s, est. speed input: 18309.22 toks/s, output: 17.88 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:04, 16.04it/s, est. speed input: 18251.64 toks/s, output: 17.82 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 16.16it/s, est. speed input: 18204.06 toks/s, output: 17.78 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 16.19it/s, est. speed input: 18153.80 toks/s, output: 17.73 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:03, 16.34it/s, est. speed input: 18121.37 toks/s, output: 17.70 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.46it/s, est. speed input: 18092.47 toks/s, output: 17.67 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:04<00:00, 79.26it/s, est. speed input: 24445.27 toks/s, output: 23.87 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:04<00:00, 39.45it/s, est. speed input: 23609.01 toks/s, output: 23.06 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:05<00:00, 30.19it/s, est. speed input: 23106.65 toks/s, output: 22.56 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:05<00:00, 25.85it/s, est. speed input: 22762.27 toks/s, output: 22.23 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:05<00:00, 23.44it/s, est. speed input: 22522.65 toks/s, output: 21.99 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:05<00:00, 21.58it/s, est. speed input: 22297.81 toks/s, output: 21.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 21.58it/s, est. speed input: 22187.12 toks/s, output: 21.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 21.67it/s, est. speed input: 22187.12 toks/s, output: 21.67 toks/s]
[rank0]:[W125 19:26:40.196426711 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 19:26:43
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:26:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:26:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=311649) WARNING 01-25 19:27:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=311649) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=311649) WARNING 01-25 19:27:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.14 requests/s, 32939.61 total tokens/s, 32.14 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 19:26:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:26:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:26:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:26:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:26:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:26:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:26:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:26:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:26:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:26:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:27:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:27:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:27:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:27:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:27:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:27:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:27:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:27:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:27:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=311649) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=311649) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
(EngineCore_DP0 pid=311649) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
(EngineCore_DP0 pid=311649) 
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=311649) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=311649) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  6.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  7.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.75it/s]
(EngineCore_DP0 pid=311649) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  8.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.98it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 27/256 [00:00<00:00, 263.34it/s]
Adding requests:  23%|██▎       | 58/256 [00:00<00:00, 286.74it/s]
Adding requests:  35%|███▍      | 89/256 [00:00<00:00, 295.70it/s]
Adding requests:  46%|████▋     | 119/256 [00:00<00:00, 297.37it/s]
Adding requests:  58%|█████▊    | 149/256 [00:00<00:00, 295.36it/s]
Adding requests:  71%|███████   | 182/256 [00:00<00:00, 304.33it/s]
Adding requests:  83%|████████▎ | 213/256 [00:00<00:00, 302.34it/s]
Adding requests:  96%|█████████▌| 246/256 [00:00<00:00, 309.89it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 302.32it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 26/256 [00:00<00:01, 177.04it/s, est. speed input: 181319.06 toks/s, output: 177.05 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:03, 59.51it/s, est. speed input: 69089.62 toks/s, output: 67.45 toks/s]   
Processed prompts:  21%|██        | 54/256 [00:00<00:04, 49.82it/s, est. speed input: 59085.62 toks/s, output: 57.70 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:01<00:04, 47.33it/s, est. speed input: 56277.45 toks/s, output: 54.96 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:01<00:04, 44.24it/s, est. speed input: 53669.72 toks/s, output: 52.41 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:04, 39.91it/s, est. speed input: 50779.40 toks/s, output: 49.59 toks/s]
Processed prompts:  30%|███       | 77/256 [00:01<00:04, 40.76it/s, est. speed input: 50362.07 toks/s, output: 49.18 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:04, 36.57it/s, est. speed input: 48084.09 toks/s, output: 46.96 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:04, 35.27it/s, est. speed input: 46986.13 toks/s, output: 45.88 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:04, 34.50it/s, est. speed input: 46120.14 toks/s, output: 45.04 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 33.88it/s, est. speed input: 45347.34 toks/s, output: 44.28 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 33.45it/s, est. speed input: 44671.51 toks/s, output: 43.62 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 33.25it/s, est. speed input: 44095.75 toks/s, output: 43.06 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 32.78it/s, est. speed input: 43503.92 toks/s, output: 42.48 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:04, 32.77it/s, est. speed input: 43037.16 toks/s, output: 42.03 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:04, 32.64it/s, est. speed input: 42589.58 toks/s, output: 41.59 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:04, 32.55it/s, est. speed input: 42180.83 toks/s, output: 41.19 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:04, 32.42it/s, est. speed input: 41793.52 toks/s, output: 40.81 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:04, 32.34it/s, est. speed input: 41440.27 toks/s, output: 40.47 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 32.22it/s, est. speed input: 41101.42 toks/s, output: 40.14 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 32.14it/s, est. speed input: 40791.09 toks/s, output: 39.83 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 32.04it/s, est. speed input: 40492.96 toks/s, output: 39.54 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:03, 32.43it/s, est. speed input: 40283.83 toks/s, output: 39.34 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:03, 32.49it/s, est. speed input: 40059.01 toks/s, output: 39.12 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:03, 32.29it/s, est. speed input: 39815.26 toks/s, output: 38.88 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:03, 32.21it/s, est. speed input: 39594.78 toks/s, output: 38.67 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:04<00:03, 32.21it/s, est. speed input: 39395.23 toks/s, output: 38.47 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:04<00:02, 32.17it/s, est. speed input: 39201.66 toks/s, output: 38.28 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 32.18it/s, est. speed input: 39023.90 toks/s, output: 38.11 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 32.17it/s, est. speed input: 38855.41 toks/s, output: 37.94 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 32.07it/s, est. speed input: 38685.15 toks/s, output: 37.78 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 32.18it/s, est. speed input: 38542.26 toks/s, output: 37.64 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:02, 32.41it/s, est. speed input: 38421.73 toks/s, output: 37.52 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:02, 32.38it/s, est. speed input: 38289.44 toks/s, output: 37.39 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:05<00:02, 32.38it/s, est. speed input: 38164.65 toks/s, output: 37.27 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:05<00:01, 32.28it/s, est. speed input: 38036.74 toks/s, output: 37.15 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:05<00:01, 32.46it/s, est. speed input: 37937.50 toks/s, output: 37.05 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:05<00:01, 32.45it/s, est. speed input: 37830.98 toks/s, output: 36.94 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:05<00:01, 32.38it/s, est. speed input: 37723.33 toks/s, output: 36.84 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:05<00:01, 32.01it/s, est. speed input: 37593.34 toks/s, output: 36.71 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:05<00:01, 31.93it/s, est. speed input: 37483.62 toks/s, output: 36.60 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:05<00:01, 32.03it/s, est. speed input: 37391.55 toks/s, output: 36.51 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:06<00:01, 32.15it/s, est. speed input: 37306.91 toks/s, output: 36.43 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:06<00:00, 32.28it/s, est. speed input: 37229.22 toks/s, output: 36.36 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:06<00:00, 32.39it/s, est. speed input: 37155.73 toks/s, output: 36.28 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:06<00:00, 32.29it/s, est. speed input: 37072.64 toks/s, output: 36.20 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:06<00:00, 32.44it/s, est. speed input: 37008.30 toks/s, output: 36.14 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:06<00:00, 32.34it/s, est. speed input: 36931.20 toks/s, output: 36.07 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:06<00:00, 32.20it/s, est. speed input: 36852.17 toks/s, output: 35.99 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:06<00:00, 33.11it/s, est. speed input: 36843.40 toks/s, output: 35.98 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:07<00:00, 33.79it/s, est. speed input: 36835.18 toks/s, output: 35.97 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 33.79it/s, est. speed input: 36830.08 toks/s, output: 35.97 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 35.97it/s, est. speed input: 36830.08 toks/s, output: 35.97 toks/s]
[rank0]:[W125 19:27:30.453758937 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 19:27:33
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:27:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:27:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=312574) WARNING 01-25 19:28:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=312574) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=312574) WARNING 01-25 19:28:09 [backends.py:609] Failed to read file <frozen os>
Throughput: 59.94 requests/s, 61433.62 total tokens/s, 59.94 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 19:27:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:27:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:27:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:27:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:27:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:27:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:27:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:27:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:27:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:27:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:27:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:27:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:27:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:27:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:27:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:27:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:27:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:27:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:27:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W125 19:28:01.183363036 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=312574) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=312574) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.91it/s]
(EngineCore_DP0 pid=312574) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.91it/s]
(EngineCore_DP0 pid=312574) 
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=312574) [2026-01-25 19:28:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=312574) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  9.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 10.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.86it/s]
(EngineCore_DP0 pid=312574) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  8.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 10.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 10.15it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 25/512 [00:00<00:02, 241.68it/s]
Adding requests:  11%|█         | 54/512 [00:00<00:01, 265.92it/s]
Adding requests:  16%|█▋        | 84/512 [00:00<00:01, 280.23it/s]
Adding requests:  22%|██▏       | 114/512 [00:00<00:01, 287.68it/s]
Adding requests:  28%|██▊       | 144/512 [00:00<00:01, 288.84it/s]
Adding requests:  34%|███▍      | 174/512 [00:00<00:01, 289.90it/s]
Adding requests:  40%|███▉      | 204/512 [00:00<00:01, 291.26it/s]
Adding requests:  46%|████▌     | 235/512 [00:00<00:00, 295.51it/s]
Adding requests:  52%|█████▏    | 266/512 [00:00<00:00, 297.82it/s]
Adding requests:  58%|█████▊    | 296/512 [00:01<00:00, 295.26it/s]
Adding requests:  64%|██████▎   | 326/512 [00:01<00:00, 288.36it/s]
Adding requests:  70%|██████▉   | 357/512 [00:01<00:00, 291.91it/s]
Adding requests:  76%|███████▌  | 387/512 [00:01<00:00, 292.06it/s]
Adding requests:  81%|████████▏ | 417/512 [00:01<00:00, 285.07it/s]
Adding requests:  87%|████████▋ | 446/512 [00:01<00:00, 279.10it/s]
Adding requests:  93%|█████████▎| 474/512 [00:01<00:00, 272.73it/s]
Adding requests:  98%|█████████▊| 502/512 [00:01<00:00, 265.23it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 281.69it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:00<00:00, 609.21it/s, est. speed input: 623949.07 toks/s, output: 609.25 toks/s]
Processed prompts:  29%|██▉       | 151/512 [00:01<00:03, 117.39it/s, est. speed input: 140492.25 toks/s, output: 137.20 toks/s]
Processed prompts:  35%|███▌      | 180/512 [00:01<00:03, 96.93it/s, est. speed input: 118466.02 toks/s, output: 115.69 toks/s] 
Processed prompts:  39%|███▉      | 199/512 [00:01<00:03, 86.67it/s, est. speed input: 108732.33 toks/s, output: 106.18 toks/s]
Processed prompts:  42%|████▏     | 213/512 [00:02<00:03, 84.30it/s, est. speed input: 105733.50 toks/s, output: 103.26 toks/s]
Processed prompts:  44%|████▍     | 225/512 [00:02<00:03, 79.80it/s, est. speed input: 102283.20 toks/s, output: 99.89 toks/s] 
Processed prompts:  46%|████▌     | 235/512 [00:02<00:03, 73.02it/s, est. speed input: 98329.49 toks/s, output: 96.02 toks/s] 
Processed prompts:  48%|████▊     | 244/512 [00:02<00:03, 72.05it/s, est. speed input: 96848.09 toks/s, output: 94.58 toks/s]
Processed prompts:  49%|████▉     | 252/512 [00:02<00:03, 70.14it/s, est. speed input: 95314.04 toks/s, output: 93.08 toks/s]
Processed prompts:  51%|█████     | 260/512 [00:02<00:03, 68.70it/s, est. speed input: 93981.12 toks/s, output: 91.78 toks/s]
Processed prompts:  52%|█████▏    | 267/512 [00:02<00:03, 65.76it/s, est. speed input: 92464.93 toks/s, output: 90.30 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:03<00:03, 63.39it/s, est. speed input: 91073.70 toks/s, output: 88.94 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:03<00:03, 64.63it/s, est. speed input: 90295.82 toks/s, output: 88.18 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:03<00:03, 65.53it/s, est. speed input: 89561.28 toks/s, output: 87.46 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:03<00:03, 66.18it/s, est. speed input: 88873.16 toks/s, output: 86.79 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:03<00:03, 66.33it/s, est. speed input: 88178.47 toks/s, output: 86.11 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:03<00:02, 66.32it/s, est. speed input: 87512.43 toks/s, output: 85.46 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:03<00:02, 66.07it/s, est. speed input: 86853.95 toks/s, output: 84.82 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:03<00:02, 66.18it/s, est. speed input: 86275.93 toks/s, output: 84.25 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:04<00:02, 66.40it/s, est. speed input: 85749.66 toks/s, output: 83.74 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:04<00:02, 65.22it/s, est. speed input: 85085.85 toks/s, output: 83.09 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:04<00:02, 65.16it/s, est. speed input: 84555.26 toks/s, output: 82.57 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:04<00:02, 61.66it/s, est. speed input: 83616.04 toks/s, output: 81.66 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:04<00:02, 61.12it/s, est. speed input: 82965.07 toks/s, output: 81.02 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:04<00:02, 62.36it/s, est. speed input: 82549.18 toks/s, output: 80.61 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:04<00:02, 62.86it/s, est. speed input: 82109.06 toks/s, output: 80.18 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:04<00:01, 63.69it/s, est. speed input: 81744.71 toks/s, output: 79.83 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:05<00:01, 64.63it/s, est. speed input: 81431.70 toks/s, output: 79.52 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:05<00:01, 65.65it/s, est. speed input: 81168.17 toks/s, output: 79.27 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:05<00:01, 66.65it/s, est. speed input: 80940.30 toks/s, output: 79.04 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:05<00:01, 66.73it/s, est. speed input: 80666.32 toks/s, output: 78.78 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:05<00:01, 67.47it/s, est. speed input: 80462.30 toks/s, output: 78.58 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:05<00:01, 67.51it/s, est. speed input: 80226.11 toks/s, output: 78.35 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:05<00:00, 67.04it/s, est. speed input: 79959.90 toks/s, output: 78.09 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:05<00:00, 65.35it/s, est. speed input: 79591.35 toks/s, output: 77.73 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:06<00:00, 64.68it/s, est. speed input: 79277.88 toks/s, output: 77.42 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:06<00:00, 64.42it/s, est. speed input: 78993.44 toks/s, output: 77.14 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:06<00:00, 64.35it/s, est. speed input: 78729.63 toks/s, output: 76.88 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:06<00:00, 63.86it/s, est. speed input: 78441.44 toks/s, output: 76.60 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:06<00:00, 63.54it/s, est. speed input: 78165.35 toks/s, output: 76.33 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:06<00:00, 62.64it/s, est. speed input: 77846.79 toks/s, output: 76.02 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 62.64it/s, est. speed input: 77983.89 toks/s, output: 76.16 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 76.15it/s, est. speed input: 77983.89 toks/s, output: 76.16 toks/s]
[rank0]:[W125 19:28:29.552514096 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 19:28:32
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:28:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:28:47 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=313683) WARNING 01-25 19:28:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=313683) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=313683) WARNING 01-25 19:29:03 [backends.py:609] Failed to read file <frozen os>
Throughput: 83.31 requests/s, 85389.81 total tokens/s, 83.31 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-25 19:28:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:28:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:28:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:28:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:28:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:28:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:28:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:28:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:28:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:28:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:28:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:28:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:28:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:28:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:28:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:28:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:28:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:28:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:28:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:28:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:28:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:28:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:28:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:28:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:28:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:28:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:28:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:28:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=313683) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=313683) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.08it/s]
(EngineCore_DP0 pid=313683) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=313683) 
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=313683) [2026-01-25 19:28:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=313683) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  9.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.93it/s]
(EngineCore_DP0 pid=313683) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  8.06it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  9.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.57it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.40it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 30/1024 [00:00<00:03, 293.23it/s]
Adding requests:   6%|▌         | 62/1024 [00:00<00:03, 304.96it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 314.76it/s]
Adding requests:  13%|█▎        | 129/1024 [00:00<00:02, 323.34it/s]
Adding requests:  16%|█▌        | 162/1024 [00:00<00:02, 319.60it/s]
Adding requests:  19%|█▉        | 194/1024 [00:00<00:02, 317.71it/s]
Adding requests:  22%|██▏       | 228/1024 [00:00<00:02, 322.83it/s]
Adding requests:  25%|██▌       | 261/1024 [00:00<00:02, 323.13it/s]
Adding requests:  29%|██▊       | 294/1024 [00:00<00:02, 320.85it/s]
Adding requests:  32%|███▏      | 327/1024 [00:01<00:02, 315.73it/s]
Adding requests:  35%|███▌      | 359/1024 [00:01<00:02, 312.73it/s]
Adding requests:  38%|███▊      | 391/1024 [00:01<00:02, 313.51it/s]
Adding requests:  41%|████▏     | 423/1024 [00:01<00:01, 307.63it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 307.79it/s]
Adding requests:  47%|████▋     | 485/1024 [00:01<00:01, 307.23it/s]
Adding requests:  50%|█████     | 516/1024 [00:01<00:01, 295.07it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 286.16it/s]
Adding requests:  56%|█████▋    | 576/1024 [00:01<00:01, 287.84it/s]
Adding requests:  59%|█████▉    | 605/1024 [00:01<00:01, 284.15it/s]
Adding requests:  62%|██████▏   | 634/1024 [00:02<00:01, 280.56it/s]
Adding requests:  65%|██████▍   | 663/1024 [00:02<00:01, 279.55it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:02<00:01, 282.03it/s]
Adding requests:  70%|███████   | 721/1024 [00:02<00:01, 278.58it/s]
Adding requests:  73%|███████▎  | 749/1024 [00:02<00:00, 276.02it/s]
Adding requests:  76%|███████▌  | 777/1024 [00:02<00:00, 275.42it/s]
Adding requests:  79%|███████▊  | 805/1024 [00:02<00:00, 267.43it/s]
Adding requests:  81%|████████▏ | 832/1024 [00:02<00:00, 265.61it/s]
Adding requests:  84%|████████▍ | 862/1024 [00:02<00:00, 274.64it/s]
Adding requests:  87%|████████▋ | 891/1024 [00:03<00:00, 278.13it/s]
Adding requests:  90%|████████▉ | 920/1024 [00:03<00:00, 280.43it/s]
Adding requests:  93%|█████████▎| 949/1024 [00:03<00:00, 279.97it/s]
Adding requests:  96%|█████████▌| 978/1024 [00:03<00:00, 282.62it/s]
Adding requests:  98%|█████████▊| 1007/1024 [00:03<00:00, 276.66it/s]
Adding requests: 100%|██████████| 1024/1024 [00:03<00:00, 292.83it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:00<00:00, 1846.91it/s, est. speed input: 1891539.08 toks/s, output: 1847.00 toks/s]
Processed prompts:  45%|████▍     | 459/1024 [00:02<00:03, 164.93it/s, est. speed input: 201802.53 toks/s, output: 197.07 toks/s]   
Processed prompts:  53%|█████▎    | 541/1024 [00:03<00:03, 135.60it/s, est. speed input: 168915.55 toks/s, output: 164.96 toks/s]
Processed prompts:  58%|█████▊    | 589/1024 [00:03<00:03, 123.39it/s, est. speed input: 156848.66 toks/s, output: 153.17 toks/s]
Processed prompts:  61%|██████    | 622/1024 [00:04<00:03, 118.14it/s, est. speed input: 151727.71 toks/s, output: 148.17 toks/s]
Processed prompts:  63%|██████▎   | 647/1024 [00:04<00:03, 114.41it/s, est. speed input: 148484.52 toks/s, output: 145.00 toks/s]
Processed prompts:  65%|██████▌   | 667/1024 [00:04<00:03, 107.19it/s, est. speed input: 144493.34 toks/s, output: 141.11 toks/s]
Processed prompts:  67%|██████▋   | 683/1024 [00:04<00:03, 104.68it/s, est. speed input: 142624.28 toks/s, output: 139.28 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:05<00:03, 101.19it/s, est. speed input: 140688.70 toks/s, output: 137.39 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:05<00:03, 98.97it/s, est. speed input: 139059.76 toks/s, output: 135.80 toks/s] 
Processed prompts:  71%|███████▏  | 730/1024 [00:05<00:03, 97.15it/s, est. speed input: 137565.18 toks/s, output: 134.34 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:05<00:02, 95.60it/s, est. speed input: 136162.52 toks/s, output: 132.97 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:05<00:02, 94.22it/s, est. speed input: 134821.39 toks/s, output: 131.66 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:05<00:02, 93.46it/s, est. speed input: 133609.37 toks/s, output: 130.48 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:06<00:02, 91.92it/s, est. speed input: 132323.32 toks/s, output: 129.22 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:06<00:02, 89.47it/s, est. speed input: 130917.06 toks/s, output: 127.85 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:06<00:02, 87.89it/s, est. speed input: 129615.09 toks/s, output: 126.58 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:06<00:02, 86.66it/s, est. speed input: 128369.20 toks/s, output: 125.36 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:06<00:01, 85.70it/s, est. speed input: 127178.67 toks/s, output: 124.20 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:07<00:01, 85.25it/s, est. speed input: 126082.26 toks/s, output: 123.13 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:07<00:01, 84.91it/s, est. speed input: 125039.16 toks/s, output: 122.11 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:07<00:01, 84.75it/s, est. speed input: 124059.89 toks/s, output: 121.15 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:07<00:01, 84.41it/s, est. speed input: 123100.13 toks/s, output: 120.21 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:07<00:01, 85.23it/s, est. speed input: 122311.68 toks/s, output: 119.44 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:08<00:00, 84.80it/s, est. speed input: 121444.85 toks/s, output: 118.60 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:08<00:00, 84.48it/s, est. speed input: 120616.64 toks/s, output: 117.79 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:08<00:00, 85.42it/s, est. speed input: 119949.03 toks/s, output: 117.14 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:08<00:00, 84.79it/s, est. speed input: 119178.20 toks/s, output: 116.38 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:08<00:00, 85.50it/s, est. speed input: 118554.45 toks/s, output: 115.78 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:08<00:00, 85.50it/s, est. speed input: 119249.39 toks/s, output: 116.45 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:08<00:00, 116.45it/s, est. speed input: 119249.39 toks/s, output: 116.45 toks/s]
[rank0]:[W125 19:29:27.823883434 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-25 19:29:30
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:29:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:29:52 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=314802) WARNING 01-25 19:30:00 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=314802) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=314802) WARNING 01-25 19:30:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 83.32 requests/s, 85402.08 total tokens/s, 83.32 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-25 19:29:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:29:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:29:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:29:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:29:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:29:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:29:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:29:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:29:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:29:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:29:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:29:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:29:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:29:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:29:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:29:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:29:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:29:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:29:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:29:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:29:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:29:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:29:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:29:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:29:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:29:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:29:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:29:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:00] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:00] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:00] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:00] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=314802) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=314802) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.13it/s]
(EngineCore_DP0 pid=314802) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.13it/s]
(EngineCore_DP0 pid=314802) 
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=314802) [2026-01-25 19:30:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=314802) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  5.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:01,  3.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  5.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  4.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  5.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  6.12it/s]
(EngineCore_DP0 pid=314802) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 10.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.41it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 278.58it/s]
Adding requests:   3%|▎         | 60/2048 [00:00<00:06, 300.82it/s]
Adding requests:   4%|▍         | 91/2048 [00:00<00:06, 304.65it/s]
Adding requests:   6%|▌         | 122/2048 [00:00<00:06, 305.38it/s]
Adding requests:   7%|▋         | 153/2048 [00:00<00:06, 301.99it/s]
Adding requests:   9%|▉         | 184/2048 [00:00<00:06, 301.27it/s]
Adding requests:  10%|█         | 215/2048 [00:00<00:06, 299.08it/s]
Adding requests:  12%|█▏        | 247/2048 [00:00<00:05, 303.19it/s]
Adding requests:  14%|█▎        | 278/2048 [00:00<00:05, 297.23it/s]
Adding requests:  15%|█▌        | 308/2048 [00:01<00:06, 288.65it/s]
Adding requests:  16%|█▋        | 337/2048 [00:01<00:06, 284.24it/s]
Adding requests:  18%|█▊        | 368/2048 [00:01<00:05, 289.24it/s]
Adding requests:  19%|█▉        | 398/2048 [00:01<00:05, 292.29it/s]
Adding requests:  21%|██        | 428/2048 [00:01<00:05, 294.52it/s]
Adding requests:  22%|██▏       | 458/2048 [00:01<00:05, 293.17it/s]
Adding requests:  24%|██▍       | 489/2048 [00:01<00:05, 293.07it/s]
Adding requests:  25%|██▌       | 519/2048 [00:01<00:05, 286.49it/s]
Adding requests:  27%|██▋       | 548/2048 [00:01<00:05, 285.54it/s]
Adding requests:  28%|██▊       | 579/2048 [00:01<00:05, 290.02it/s]
Adding requests:  30%|██▉       | 609/2048 [00:02<00:04, 290.96it/s]
Adding requests:  31%|███       | 639/2048 [00:02<00:04, 290.51it/s]
Adding requests:  33%|███▎      | 669/2048 [00:02<00:04, 289.96it/s]
Adding requests:  34%|███▍      | 700/2048 [00:02<00:04, 294.69it/s]
Adding requests:  36%|███▌      | 730/2048 [00:02<00:04, 287.85it/s]
Adding requests:  37%|███▋      | 759/2048 [00:02<00:04, 287.35it/s]
Adding requests:  38%|███▊      | 788/2048 [00:02<00:04, 283.41it/s]
Adding requests:  40%|███▉      | 817/2048 [00:02<00:04, 279.14it/s]
Adding requests:  41%|████▏     | 845/2048 [00:02<00:04, 274.09it/s]
Adding requests:  43%|████▎     | 873/2048 [00:03<00:04, 275.11it/s]
Adding requests:  44%|████▍     | 901/2048 [00:03<00:04, 276.02it/s]
Adding requests:  45%|████▌     | 929/2048 [00:03<00:04, 272.90it/s]
Adding requests:  47%|████▋     | 957/2048 [00:03<00:03, 274.27it/s]
Adding requests:  48%|████▊     | 986/2048 [00:03<00:03, 276.76it/s]
Adding requests:  50%|████▉     | 1014/2048 [00:03<00:03, 274.57it/s]
Adding requests:  51%|█████     | 1042/2048 [00:03<00:03, 275.85it/s]
Adding requests:  52%|█████▏    | 1070/2048 [00:03<00:03, 273.25it/s]
Adding requests:  54%|█████▎    | 1098/2048 [00:03<00:03, 268.95it/s]
Adding requests:  55%|█████▍    | 1126/2048 [00:03<00:03, 271.10it/s]
Adding requests:  56%|█████▋    | 1156/2048 [00:04<00:03, 276.98it/s]
Adding requests:  58%|█████▊    | 1188/2048 [00:04<00:03, 286.60it/s]
Adding requests:  59%|█████▉    | 1217/2048 [00:04<00:02, 286.79it/s]
Adding requests:  61%|██████    | 1246/2048 [00:04<00:02, 278.94it/s]
Adding requests:  62%|██████▏   | 1275/2048 [00:04<00:02, 279.76it/s]
Adding requests:  64%|██████▎   | 1305/2048 [00:04<00:02, 284.02it/s]
Adding requests:  65%|██████▌   | 1335/2048 [00:04<00:02, 288.11it/s]
Adding requests:  67%|██████▋   | 1364/2048 [00:04<00:02, 288.40it/s]
Adding requests:  68%|██████▊   | 1395/2048 [00:04<00:02, 294.09it/s]
Adding requests:  70%|██████▉   | 1425/2048 [00:04<00:02, 289.04it/s]
Adding requests:  71%|███████   | 1455/2048 [00:05<00:02, 290.08it/s]
Adding requests:  73%|███████▎  | 1485/2048 [00:05<00:01, 287.17it/s]
Adding requests:  74%|███████▍  | 1516/2048 [00:05<00:01, 292.43it/s]
Adding requests:  75%|███████▌  | 1546/2048 [00:05<00:01, 287.39it/s]
Adding requests:  77%|███████▋  | 1576/2048 [00:05<00:01, 288.49it/s]
Adding requests:  78%|███████▊  | 1607/2048 [00:05<00:01, 292.57it/s]
Adding requests:  80%|███████▉  | 1638/2048 [00:05<00:01, 297.52it/s]
Adding requests:  81%|████████▏ | 1668/2048 [00:05<00:01, 290.01it/s]
Adding requests:  83%|████████▎ | 1698/2048 [00:05<00:01, 280.74it/s]
Adding requests:  84%|████████▍ | 1728/2048 [00:06<00:01, 283.80it/s]
Adding requests:  86%|████████▌ | 1757/2048 [00:06<00:01, 279.83it/s]
Adding requests:  87%|████████▋ | 1786/2048 [00:06<00:00, 279.38it/s]
Adding requests:  89%|████████▊ | 1815/2048 [00:06<00:00, 281.95it/s]
Adding requests:  90%|█████████ | 1844/2048 [00:06<00:00, 283.11it/s]
Adding requests:  91%|█████████▏| 1873/2048 [00:06<00:00, 268.32it/s]
Adding requests:  93%|█████████▎| 1903/2048 [00:06<00:00, 274.88it/s]
Adding requests:  94%|█████████▍| 1932/2048 [00:06<00:00, 276.64it/s]
Adding requests:  96%|█████████▌| 1961/2048 [00:06<00:00, 278.90it/s]
Adding requests:  97%|█████████▋| 1989/2048 [00:06<00:00, 278.54it/s]
Adding requests:  99%|█████████▊| 2019/2048 [00:07<00:00, 284.24it/s]
Adding requests: 100%|██████████| 2048/2048 [00:07<00:00, 285.80it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:00<00:00, 2522.89it/s, est. speed input: 2583740.40 toks/s, output: 2522.98 toks/s]
Processed prompts:  40%|███▉      | 815/2048 [00:03<00:05, 219.09it/s, est. speed input: 276595.07 toks/s, output: 270.11 toks/s]   
Processed prompts:  45%|████▌     | 925/2048 [00:04<00:06, 164.62it/s, est. speed input: 216815.44 toks/s, output: 211.73 toks/s]
Processed prompts:  48%|████▊     | 989/2048 [00:05<00:07, 145.51it/s, est. speed input: 197697.63 toks/s, output: 193.06 toks/s]
Processed prompts:  50%|█████     | 1031/2048 [00:05<00:07, 130.28it/s, est. speed input: 185134.26 toks/s, output: 180.79 toks/s]
Processed prompts:  52%|█████▏    | 1061/2048 [00:06<00:08, 121.56it/s, est. speed input: 178429.62 toks/s, output: 174.25 toks/s]
Processed prompts:  53%|█████▎    | 1083/2048 [00:06<00:07, 120.73it/s, est. speed input: 176520.22 toks/s, output: 172.38 toks/s]
Processed prompts:  54%|█████▍    | 1102/2048 [00:06<00:08, 117.85it/s, est. speed input: 174278.80 toks/s, output: 170.19 toks/s]
Processed prompts:  55%|█████▍    | 1118/2048 [00:06<00:08, 112.18it/s, est. speed input: 171647.18 toks/s, output: 167.62 toks/s]
Processed prompts:  55%|█████▌    | 1132/2048 [00:06<00:08, 105.89it/s, est. speed input: 169153.11 toks/s, output: 165.19 toks/s]
Processed prompts:  56%|█████▌    | 1144/2048 [00:07<00:09, 98.27it/s, est. speed input: 166603.90 toks/s, output: 162.70 toks/s] 
Processed prompts:  56%|█████▋    | 1155/2048 [00:07<00:09, 91.19it/s, est. speed input: 164237.49 toks/s, output: 160.39 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:07<00:09, 89.47it/s, est. speed input: 162347.90 toks/s, output: 158.54 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:07<00:09, 89.52it/s, est. speed input: 160681.93 toks/s, output: 156.92 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:07<00:09, 89.54it/s, est. speed input: 159090.54 toks/s, output: 155.36 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:07<00:09, 89.53it/s, est. speed input: 157567.50 toks/s, output: 153.87 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:08<00:09, 89.55it/s, est. speed input: 156115.84 toks/s, output: 152.46 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:08<00:08, 89.61it/s, est. speed input: 154732.13 toks/s, output: 151.11 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:08<00:08, 89.55it/s, est. speed input: 153394.07 toks/s, output: 149.80 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:08<00:08, 89.48it/s, est. speed input: 152108.81 toks/s, output: 148.54 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:08<00:08, 89.51it/s, est. speed input: 150884.67 toks/s, output: 147.35 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:08<00:08, 89.51it/s, est. speed input: 149706.54 toks/s, output: 146.20 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:09<00:08, 89.48it/s, est. speed input: 148571.70 toks/s, output: 145.09 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:09<00:07, 89.46it/s, est. speed input: 147479.42 toks/s, output: 144.02 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:09<00:07, 89.27it/s, est. speed input: 146411.23 toks/s, output: 142.98 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:09<00:07, 87.09it/s, est. speed input: 145170.95 toks/s, output: 141.77 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:09<00:07, 85.68it/s, est. speed input: 143985.57 toks/s, output: 140.61 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:10<00:07, 84.78it/s, est. speed input: 142851.80 toks/s, output: 139.50 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:10<00:07, 84.11it/s, est. speed input: 141755.64 toks/s, output: 138.43 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:10<00:07, 83.61it/s, est. speed input: 140696.47 toks/s, output: 137.40 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:10<00:07, 83.29it/s, est. speed input: 139678.45 toks/s, output: 136.40 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:10<00:06, 83.09it/s, est. speed input: 138698.66 toks/s, output: 135.45 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:11<00:06, 82.90it/s, est. speed input: 137748.25 toks/s, output: 134.52 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:11<00:06, 82.65it/s, est. speed input: 136818.82 toks/s, output: 133.61 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:11<00:06, 82.56it/s, est. speed input: 135929.31 toks/s, output: 132.74 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:11<00:06, 82.52it/s, est. speed input: 135071.91 toks/s, output: 131.91 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:11<00:05, 82.47it/s, est. speed input: 134239.95 toks/s, output: 131.09 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:12<00:05, 82.45it/s, est. speed input: 133436.20 toks/s, output: 130.31 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:12<00:05, 82.39it/s, est. speed input: 132654.54 toks/s, output: 129.55 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:12<00:05, 82.37it/s, est. speed input: 131899.11 toks/s, output: 128.81 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:12<00:05, 82.48it/s, est. speed input: 131176.70 toks/s, output: 128.10 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:12<00:05, 82.33it/s, est. speed input: 130457.83 toks/s, output: 127.40 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:13<00:04, 82.32it/s, est. speed input: 129768.44 toks/s, output: 126.73 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:13<00:04, 82.31it/s, est. speed input: 129098.67 toks/s, output: 126.07 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:13<00:04, 82.31it/s, est. speed input: 128449.40 toks/s, output: 125.44 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:13<00:04, 82.33it/s, est. speed input: 127819.44 toks/s, output: 124.82 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:13<00:04, 82.37it/s, est. speed input: 127209.51 toks/s, output: 124.23 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:13<00:03, 82.31it/s, est. speed input: 126610.58 toks/s, output: 123.64 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:14<00:03, 82.26it/s, est. speed input: 126026.75 toks/s, output: 123.07 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:14<00:03, 82.27it/s, est. speed input: 125462.07 toks/s, output: 122.52 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:14<00:03, 83.70it/s, est. speed input: 125006.86 toks/s, output: 122.08 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:14<00:02, 85.21it/s, est. speed input: 124593.23 toks/s, output: 121.67 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:14<00:02, 86.32it/s, est. speed input: 124190.47 toks/s, output: 121.28 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:15<00:02, 87.10it/s, est. speed input: 123797.10 toks/s, output: 120.90 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:15<00:02, 87.65it/s, est. speed input: 123411.86 toks/s, output: 120.52 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:15<00:02, 88.05it/s, est. speed input: 123036.98 toks/s, output: 120.15 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:15<00:01, 89.63it/s, est. speed input: 122738.52 toks/s, output: 119.86 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:15<00:01, 89.47it/s, est. speed input: 122380.85 toks/s, output: 119.51 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:15<00:01, 89.41it/s, est. speed input: 122033.80 toks/s, output: 119.17 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:16<00:01, 89.43it/s, est. speed input: 121697.38 toks/s, output: 118.84 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:16<00:01, 89.28it/s, est. speed input: 121360.48 toks/s, output: 118.52 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:16<00:01, 90.57it/s, est. speed input: 121098.42 toks/s, output: 118.26 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:16<00:00, 90.11it/s, est. speed input: 120776.48 toks/s, output: 117.95 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:16<00:00, 89.85it/s, est. speed input: 120464.41 toks/s, output: 117.64 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:17<00:00, 89.60it/s, est. speed input: 120155.56 toks/s, output: 117.34 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:17<00:00, 89.42it/s, est. speed input: 119853.42 toks/s, output: 117.04 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:17<00:00, 90.73it/s, est. speed input: 119621.93 toks/s, output: 116.82 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 90.73it/s, est. speed input: 120442.50 toks/s, output: 117.62 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 117.62it/s, est. speed input: 120442.50 toks/s, output: 117.62 toks/s]
[rank0]:[W125 19:30:45.604926400 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-25 19:30:46
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:31:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:31:21 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=316266) WARNING 01-25 19:31:29 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=316266) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=316266) WARNING 01-25 19:31:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 84.63 requests/s, 86743.69 total tokens/s, 84.63 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-25 19:31:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:31:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:31:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:31:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:31:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:31:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:31:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:31:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:31:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:31:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:31:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:31:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:31:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:31:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:31:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:31:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:31:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:31:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:31:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:31:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:31:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:31:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:31:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:31:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:31:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:31:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:31:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:31:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=316266) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=316266) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=316266) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=316266) 
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=316266) [2026-01-25 19:31:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=316266) [rank0]:W0125 19:31:42.863000 316266 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=316266) [rank0]:W0125 19:31:42.946000 316266 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=316266) [rank0]:W0125 19:31:44.133000 316266 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=316266) [rank0]:W0125 19:31:44.244000 316266 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=316266) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:03,  2.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:02,  4.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  4.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:01,  4.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:01<00:00,  6.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  7.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  6.71it/s]
(EngineCore_DP0 pid=316266) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.86it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.94it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 10.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 10.02it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/4096 [00:00<00:16, 242.17it/s]
Adding requests:   1%|▏         | 54/4096 [00:00<00:15, 262.22it/s]
Adding requests:   2%|▏         | 82/4096 [00:00<00:15, 267.15it/s]
Adding requests:   3%|▎         | 110/4096 [00:00<00:14, 270.80it/s]
Adding requests:   3%|▎         | 140/4096 [00:00<00:14, 280.88it/s]
Adding requests:   4%|▍         | 169/4096 [00:00<00:13, 283.28it/s]
Adding requests:   5%|▍         | 198/4096 [00:00<00:13, 280.65it/s]
Adding requests:   6%|▌         | 229/4096 [00:00<00:13, 287.01it/s]
Adding requests:   6%|▋         | 259/4096 [00:00<00:13, 288.50it/s]
Adding requests:   7%|▋         | 290/4096 [00:01<00:12, 292.94it/s]
Adding requests:   8%|▊         | 320/4096 [00:01<00:12, 292.00it/s]
Adding requests:   9%|▊         | 351/4096 [00:01<00:12, 297.20it/s]
Adding requests:   9%|▉         | 381/4096 [00:01<00:12, 297.53it/s]
Adding requests:  10%|█         | 411/4096 [00:01<00:12, 292.85it/s]
Adding requests:  11%|█         | 441/4096 [00:01<00:12, 289.64it/s]
Adding requests:  12%|█▏        | 472/4096 [00:01<00:12, 293.54it/s]
Adding requests:  12%|█▏        | 502/4096 [00:01<00:12, 288.28it/s]
Adding requests:  13%|█▎        | 531/4096 [00:01<00:12, 275.22it/s]
Adding requests:  14%|█▎        | 560/4096 [00:01<00:12, 278.35it/s]
Adding requests:  14%|█▍        | 589/4096 [00:02<00:12, 281.03it/s]
Adding requests:  15%|█▌        | 618/4096 [00:02<00:12, 281.96it/s]
Adding requests:  16%|█▌        | 648/4096 [00:02<00:12, 284.90it/s]
Adding requests:  17%|█▋        | 678/4096 [00:02<00:11, 287.91it/s]
Adding requests:  17%|█▋        | 708/4096 [00:02<00:11, 289.73it/s]
Adding requests:  18%|█▊        | 738/4096 [00:02<00:11, 292.43it/s]
Adding requests:  19%|█▉        | 768/4096 [00:02<00:11, 284.24it/s]
Adding requests:  19%|█▉        | 797/4096 [00:02<00:11, 280.11it/s]
Adding requests:  20%|██        | 826/4096 [00:02<00:11, 276.01it/s]
Adding requests:  21%|██        | 854/4096 [00:03<00:11, 270.73it/s]
Adding requests:  22%|██▏       | 883/4096 [00:03<00:11, 274.51it/s]
Adding requests:  22%|██▏       | 915/4096 [00:03<00:11, 286.59it/s]
Adding requests:  23%|██▎       | 947/4096 [00:03<00:10, 293.80it/s]
Adding requests:  24%|██▍       | 977/4096 [00:03<00:10, 294.88it/s]
Adding requests:  25%|██▍       | 1007/4096 [00:03<00:10, 295.35it/s]
Adding requests:  25%|██▌       | 1038/4096 [00:03<00:10, 297.27it/s]
Adding requests:  26%|██▌       | 1069/4096 [00:03<00:10, 297.74it/s]
Adding requests:  27%|██▋       | 1099/4096 [00:03<00:10, 293.02it/s]
Adding requests:  28%|██▊       | 1129/4096 [00:03<00:10, 288.26it/s]
Adding requests:  28%|██▊       | 1161/4096 [00:04<00:09, 295.76it/s]
Adding requests:  29%|██▉       | 1191/4096 [00:04<00:09, 291.36it/s]
Adding requests:  30%|██▉       | 1224/4096 [00:04<00:09, 302.51it/s]
Adding requests:  31%|███       | 1255/4096 [00:04<00:09, 302.47it/s]
Adding requests:  31%|███▏      | 1287/4096 [00:04<00:09, 305.36it/s]
Adding requests:  32%|███▏      | 1320/4096 [00:04<00:08, 310.74it/s]
Adding requests:  33%|███▎      | 1352/4096 [00:04<00:08, 311.39it/s]
Adding requests:  34%|███▍      | 1384/4096 [00:04<00:08, 312.44it/s]
Adding requests:  35%|███▍      | 1417/4096 [00:04<00:08, 317.25it/s]
Adding requests:  35%|███▌      | 1450/4096 [00:04<00:08, 318.69it/s]
Adding requests:  36%|███▌      | 1482/4096 [00:05<00:08, 316.04it/s]
Adding requests:  37%|███▋      | 1514/4096 [00:05<00:08, 315.79it/s]
Adding requests:  38%|███▊      | 1546/4096 [00:05<00:08, 309.81it/s]
Adding requests:  39%|███▊      | 1578/4096 [00:05<00:08, 308.35it/s]
Adding requests:  39%|███▉      | 1611/4096 [00:05<00:07, 314.41it/s]
Adding requests:  40%|████      | 1643/4096 [00:05<00:07, 313.48it/s]
Adding requests:  41%|████      | 1675/4096 [00:05<00:07, 309.12it/s]
Adding requests:  42%|████▏     | 1706/4096 [00:05<00:07, 308.67it/s]
Adding requests:  42%|████▏     | 1737/4096 [00:05<00:07, 302.57it/s]
Adding requests:  43%|████▎     | 1768/4096 [00:06<00:07, 298.60it/s]
Adding requests:  44%|████▍     | 1799/4096 [00:06<00:07, 301.09it/s]
Adding requests:  45%|████▍     | 1831/4096 [00:06<00:07, 305.46it/s]
Adding requests:  45%|████▌     | 1863/4096 [00:06<00:07, 308.06it/s]
Adding requests:  46%|████▋     | 1896/4096 [00:06<00:07, 313.75it/s]
Adding requests:  47%|████▋     | 1929/4096 [00:06<00:06, 317.83it/s]
Adding requests:  48%|████▊     | 1961/4096 [00:06<00:06, 308.53it/s]
Adding requests:  49%|████▊     | 1992/4096 [00:06<00:06, 304.45it/s]
Adding requests:  49%|████▉     | 2023/4096 [00:06<00:06, 301.91it/s]
Adding requests:  50%|█████     | 2055/4096 [00:06<00:06, 304.32it/s]
Adding requests:  51%|█████     | 2086/4096 [00:07<00:06, 302.66it/s]
Adding requests:  52%|█████▏    | 2117/4096 [00:07<00:06, 299.70it/s]
Adding requests:  52%|█████▏    | 2147/4096 [00:07<00:06, 295.03it/s]
Adding requests:  53%|█████▎    | 2177/4096 [00:07<00:06, 291.05it/s]
Adding requests:  54%|█████▍    | 2207/4096 [00:07<00:06, 290.63it/s]
Adding requests:  55%|█████▍    | 2239/4096 [00:07<00:06, 296.38it/s]
Adding requests:  55%|█████▌    | 2269/4096 [00:07<00:06, 290.54it/s]
Adding requests:  56%|█████▌    | 2300/4096 [00:07<00:06, 292.46it/s]
Adding requests:  57%|█████▋    | 2330/4096 [00:07<00:06, 290.10it/s]
Adding requests:  58%|█████▊    | 2360/4096 [00:08<00:06, 281.05it/s]
Adding requests:  58%|█████▊    | 2389/4096 [00:08<00:06, 278.90it/s]
Adding requests:  59%|█████▉    | 2417/4096 [00:08<00:06, 273.62it/s]
Adding requests:  60%|█████▉    | 2445/4096 [00:08<00:06, 274.95it/s]
Adding requests:  60%|██████    | 2473/4096 [00:08<00:05, 276.19it/s]
Adding requests:  61%|██████    | 2502/4096 [00:08<00:05, 278.74it/s]
Adding requests:  62%|██████▏   | 2530/4096 [00:08<00:05, 274.03it/s]
Adding requests:  62%|██████▏   | 2559/4096 [00:08<00:05, 277.00it/s]
Adding requests:  63%|██████▎   | 2587/4096 [00:08<00:05, 276.28it/s]
Adding requests:  64%|██████▍   | 2615/4096 [00:08<00:05, 275.79it/s]
Adding requests:  65%|██████▍   | 2644/4096 [00:09<00:05, 277.41it/s]
Adding requests:  65%|██████▌   | 2673/4096 [00:09<00:05, 280.55it/s]
Adding requests:  66%|██████▌   | 2702/4096 [00:09<00:05, 275.29it/s]
Adding requests:  67%|██████▋   | 2730/4096 [00:09<00:04, 275.59it/s]
Adding requests:  67%|██████▋   | 2759/4096 [00:09<00:04, 279.59it/s]
Adding requests:  68%|██████▊   | 2790/4096 [00:09<00:04, 284.65it/s]
Adding requests:  69%|██████▉   | 2820/4096 [00:09<00:04, 286.03it/s]
Adding requests:  70%|██████▉   | 2851/4096 [00:09<00:04, 290.51it/s]
Adding requests:  70%|███████   | 2881/4096 [00:09<00:04, 287.19it/s]
Adding requests:  71%|███████   | 2912/4096 [00:09<00:04, 292.04it/s]
Adding requests:  72%|███████▏  | 2942/4096 [00:10<00:03, 289.31it/s]
Adding requests:  73%|███████▎  | 2971/4096 [00:10<00:03, 289.10it/s]
Adding requests:  73%|███████▎  | 3002/4096 [00:10<00:03, 293.42it/s]
Adding requests:  74%|███████▍  | 3032/4096 [00:10<00:03, 291.39it/s]
Adding requests:  75%|███████▍  | 3062/4096 [00:10<00:03, 290.42it/s]
Adding requests:  75%|███████▌  | 3092/4096 [00:10<00:03, 290.09it/s]
Adding requests:  76%|███████▌  | 3123/4096 [00:10<00:03, 292.98it/s]
Adding requests:  77%|███████▋  | 3153/4096 [00:10<00:03, 293.51it/s]
Adding requests:  78%|███████▊  | 3183/4096 [00:10<00:03, 290.57it/s]
Adding requests:  78%|███████▊  | 3213/4096 [00:11<00:03, 285.83it/s]
Adding requests:  79%|███████▉  | 3244/4096 [00:11<00:02, 291.88it/s]
Adding requests:  80%|███████▉  | 3276/4096 [00:11<00:02, 297.53it/s]
Adding requests:  81%|████████  | 3307/4096 [00:11<00:02, 298.39it/s]
Adding requests:  81%|████████▏ | 3337/4096 [00:11<00:02, 290.59it/s]
Adding requests:  82%|████████▏ | 3367/4096 [00:11<00:02, 290.74it/s]
Adding requests:  83%|████████▎ | 3397/4096 [00:11<00:02, 286.04it/s]
Adding requests:  84%|████████▎ | 3427/4096 [00:11<00:02, 288.07it/s]
Adding requests:  84%|████████▍ | 3456/4096 [00:11<00:02, 283.85it/s]
Adding requests:  85%|████████▌ | 3486/4096 [00:11<00:02, 286.63it/s]
Adding requests:  86%|████████▌ | 3517/4096 [00:12<00:01, 291.50it/s]
Adding requests:  87%|████████▋ | 3547/4096 [00:12<00:01, 292.84it/s]
Adding requests:  87%|████████▋ | 3577/4096 [00:12<00:01, 294.20it/s]
Adding requests:  88%|████████▊ | 3609/4096 [00:12<00:01, 298.47it/s]
Adding requests:  89%|████████▉ | 3639/4096 [00:12<00:01, 296.14it/s]
Adding requests:  90%|████████▉ | 3669/4096 [00:12<00:01, 287.37it/s]
Adding requests:  90%|█████████ | 3702/4096 [00:12<00:01, 299.23it/s]
Adding requests:  91%|█████████ | 3734/4096 [00:12<00:01, 302.88it/s]
Adding requests:  92%|█████████▏| 3767/4096 [00:12<00:01, 310.63it/s]
Adding requests:  93%|█████████▎| 3799/4096 [00:12<00:00, 310.50it/s]
Adding requests:  94%|█████████▎| 3831/4096 [00:13<00:00, 309.25it/s]
Adding requests:  94%|█████████▍| 3865/4096 [00:13<00:00, 315.45it/s]
Adding requests:  95%|█████████▌| 3899/4096 [00:13<00:00, 319.87it/s]
Adding requests:  96%|█████████▌| 3935/4096 [00:13<00:00, 328.05it/s]
Adding requests:  97%|█████████▋| 3969/4096 [00:13<00:00, 329.32it/s]
Adding requests:  98%|█████████▊| 4002/4096 [00:13<00:00, 325.88it/s]
Adding requests:  99%|█████████▊| 4035/4096 [00:13<00:00, 322.74it/s]
Adding requests:  99%|█████████▉| 4068/4096 [00:13<00:00, 320.74it/s]
Adding requests: 100%|██████████| 4096/4096 [00:13<00:00, 294.83it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:00<00:00, 4959.35it/s, est. speed input: 5079030.53 toks/s, output: 4959.56 toks/s]
Processed prompts:  40%|████      | 1650/4096 [00:05<00:11, 221.25it/s, est. speed input: 283361.16 toks/s, output: 276.72 toks/s]   
Processed prompts:  45%|████▌     | 1860/4096 [00:08<00:13, 169.63it/s, est. speed input: 225243.83 toks/s, output: 219.96 toks/s]
Processed prompts:  48%|████▊     | 1978/4096 [00:09<00:13, 156.56it/s, est. speed input: 211067.38 toks/s, output: 206.12 toks/s]
Processed prompts:  50%|█████     | 2054/4096 [00:10<00:14, 136.14it/s, est. speed input: 195459.79 toks/s, output: 190.88 toks/s]
Processed prompts:  51%|█████▏    | 2105/4096 [00:11<00:14, 135.66it/s, est. speed input: 193326.74 toks/s, output: 188.80 toks/s]
Processed prompts:  52%|█████▏    | 2143/4096 [00:11<00:14, 131.09it/s, est. speed input: 190194.74 toks/s, output: 185.74 toks/s]
Processed prompts:  53%|█████▎    | 2172/4096 [00:11<00:15, 122.81it/s, est. speed input: 186499.07 toks/s, output: 182.13 toks/s]
Processed prompts:  54%|█████▎    | 2194/4096 [00:12<00:17, 111.31it/s, est. speed input: 182448.20 toks/s, output: 178.17 toks/s]
Processed prompts:  54%|█████▍    | 2211/4096 [00:12<00:19, 98.83it/s, est. speed input: 178508.71 toks/s, output: 174.32 toks/s] 
Processed prompts:  58%|█████▊    | 2370/4096 [00:12<00:09, 181.15it/s, est. speed input: 187458.26 toks/s, output: 183.06 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:13<00:10, 155.79it/s, est. speed input: 184440.46 toks/s, output: 180.12 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:13<00:12, 136.12it/s, est. speed input: 181601.45 toks/s, output: 177.34 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:14<00:13, 121.86it/s, est. speed input: 179011.29 toks/s, output: 174.82 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:14<00:14, 113.85it/s, est. speed input: 176943.95 toks/s, output: 172.80 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:14<00:14, 106.97it/s, est. speed input: 174867.68 toks/s, output: 170.77 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:15<00:14, 102.60it/s, est. speed input: 172989.74 toks/s, output: 168.93 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:15<00:15, 98.68it/s, est. speed input: 171098.36 toks/s, output: 167.09 toks/s] 
Processed prompts:  64%|██████▍   | 2626/4096 [00:15<00:15, 95.85it/s, est. speed input: 169290.18 toks/s, output: 165.32 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:16<00:15, 93.87it/s, est. speed input: 167566.44 toks/s, output: 163.64 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:16<00:15, 92.46it/s, est. speed input: 165914.98 toks/s, output: 162.03 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:16<00:15, 89.95it/s, est. speed input: 164138.29 toks/s, output: 160.29 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:17<00:15, 87.56it/s, est. speed input: 162350.01 toks/s, output: 158.54 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:17<00:15, 85.92it/s, est. speed input: 160637.47 toks/s, output: 156.87 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:18<00:15, 84.77it/s, est. speed input: 158996.12 toks/s, output: 155.27 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:18<00:14, 83.98it/s, est. speed input: 157424.02 toks/s, output: 153.73 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:18<00:14, 83.44it/s, est. speed input: 155917.60 toks/s, output: 152.26 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:19<00:14, 83.06it/s, est. speed input: 154470.63 toks/s, output: 150.85 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:19<00:13, 82.78it/s, est. speed input: 153078.99 toks/s, output: 149.49 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:20<00:13, 82.59it/s, est. speed input: 151742.34 toks/s, output: 148.19 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:20<00:13, 82.49it/s, est. speed input: 150460.10 toks/s, output: 146.93 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:20<00:12, 82.40it/s, est. speed input: 149223.98 toks/s, output: 145.73 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:21<00:12, 82.34it/s, est. speed input: 148033.20 toks/s, output: 144.56 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:21<00:12, 82.30it/s, est. speed input: 146884.64 toks/s, output: 143.44 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:22<00:11, 82.88it/s, est. speed input: 145840.07 toks/s, output: 142.42 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:22<00:11, 82.88it/s, est. speed input: 144790.20 toks/s, output: 141.40 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:22<00:10, 84.64it/s, est. speed input: 143944.56 toks/s, output: 140.57 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:23<00:10, 85.90it/s, est. speed input: 143124.18 toks/s, output: 139.77 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:23<00:09, 86.82it/s, est. speed input: 142329.46 toks/s, output: 138.99 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:23<00:09, 87.45it/s, est. speed input: 141557.00 toks/s, output: 138.24 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:24<00:08, 87.97it/s, est. speed input: 140812.98 toks/s, output: 137.51 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:24<00:08, 88.25it/s, est. speed input: 140083.99 toks/s, output: 136.80 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:24<00:08, 87.47it/s, est. speed input: 139301.01 toks/s, output: 136.04 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:25<00:07, 85.76it/s, est. speed input: 138449.58 toks/s, output: 135.20 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:25<00:07, 84.68it/s, est. speed input: 137630.04 toks/s, output: 134.40 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:26<00:07, 83.86it/s, est. speed input: 136828.38 toks/s, output: 133.62 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:26<00:06, 83.34it/s, est. speed input: 136054.04 toks/s, output: 132.87 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:26<00:06, 83.02it/s, est. speed input: 135304.60 toks/s, output: 132.13 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:27<00:06, 82.80it/s, est. speed input: 134577.50 toks/s, output: 131.42 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:27<00:05, 82.65it/s, est. speed input: 133870.37 toks/s, output: 130.73 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:28<00:05, 82.54it/s, est. speed input: 133183.38 toks/s, output: 130.06 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:28<00:05, 82.46it/s, est. speed input: 132513.94 toks/s, output: 129.41 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:28<00:04, 83.07it/s, est. speed input: 131910.56 toks/s, output: 128.82 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:29<00:04, 82.88it/s, est. speed input: 131280.01 toks/s, output: 128.20 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:29<00:03, 82.75it/s, est. speed input: 130665.48 toks/s, output: 127.60 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:29<00:03, 82.68it/s, est. speed input: 130068.65 toks/s, output: 127.02 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:30<00:03, 84.58it/s, est. speed input: 129613.81 toks/s, output: 126.58 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:30<00:02, 85.96it/s, est. speed input: 129169.39 toks/s, output: 126.14 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:31<00:02, 86.97it/s, est. speed input: 128736.09 toks/s, output: 125.72 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [00:31<00:01, 87.67it/s, est. speed input: 128311.75 toks/s, output: 125.30 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [00:31<00:01, 88.19it/s, est. speed input: 127898.06 toks/s, output: 124.90 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [00:32<00:01, 88.55it/s, est. speed input: 127493.27 toks/s, output: 124.51 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [00:32<00:00, 89.46it/s, est. speed input: 127132.25 toks/s, output: 124.15 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [00:32<00:00, 90.15it/s, est. speed input: 126780.21 toks/s, output: 123.81 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:32<00:00, 90.15it/s, est. speed input: 127713.07 toks/s, output: 124.72 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:32<00:00, 124.72it/s, est. speed input: 127713.07 toks/s, output: 124.72 toks/s]
[rank0]:[W125 19:32:38.154893022 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-25 19:32:41
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:33:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 19:33:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=318492) WARNING 01-25 19:33:49 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=318492) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=318492) WARNING 01-25 19:33:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 12.17 requests/s, 12471.10 total tokens/s, 12.17 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-25 19:33:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:33:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:33:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:33:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:33:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:33:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:33:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:33:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:33:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:33:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:33:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:33:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:33:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:33:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:33:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:33:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 19:33:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 19:33:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:33:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:33:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:33:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:33:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 19:33:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 19:33:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:33:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:33:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:33:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:33:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=318492) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=318492) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=318492) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=318492) 
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7618560 bytes
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5079040 bytes
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 40632320 bytes
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=318492) [2026-01-25 19:33:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20185088 bytes
(EngineCore_DP0 pid=318492) [rank0]:W0125 19:34:01.214000 318492 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=318492) [rank0]:W0125 19:34:01.297000 318492 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=318492) [rank0]:W0125 19:34:02.436000 318492 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=318492) [rank0]:W0125 19:34:02.552000 318492 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=318492) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:01,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:01,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  9.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  9.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  9.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  9.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:01,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:01,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  9.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  9.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00,  9.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  5.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  6.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.06it/s]
(EngineCore_DP0 pid=318492) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:03,  3.01it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  5.00it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:01,  6.33it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  8.08it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:01<00:00,  9.24it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  9.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.08it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 28/8192 [00:00<00:29, 277.66it/s]
Adding requests:   1%|          | 59/8192 [00:00<00:27, 295.10it/s]
Adding requests:   1%|          | 89/8192 [00:00<00:28, 286.53it/s]
Adding requests:   1%|▏         | 121/8192 [00:00<00:27, 296.18it/s]
Adding requests:   2%|▏         | 152/8192 [00:00<00:26, 299.31it/s]
Adding requests:   2%|▏         | 184/8192 [00:00<00:26, 304.70it/s]
Adding requests:   3%|▎         | 216/8192 [00:00<00:25, 309.45it/s]
Adding requests:   3%|▎         | 248/8192 [00:00<00:25, 310.79it/s]
Adding requests:   3%|▎         | 280/8192 [00:00<00:26, 304.10it/s]
Adding requests:   4%|▍         | 311/8192 [00:01<00:26, 301.85it/s]
Adding requests:   4%|▍         | 342/8192 [00:01<00:26, 300.41it/s]
Adding requests:   5%|▍         | 375/8192 [00:01<00:25, 307.20it/s]
Adding requests:   5%|▍         | 406/8192 [00:01<00:25, 306.85it/s]
Adding requests:   5%|▌         | 438/8192 [00:01<00:25, 309.75it/s]
Adding requests:   6%|▌         | 470/8192 [00:01<00:24, 311.76it/s]
Adding requests:   6%|▌         | 502/8192 [00:01<00:24, 313.41it/s]
Adding requests:   7%|▋         | 534/8192 [00:01<00:24, 309.29it/s]
Adding requests:   7%|▋         | 565/8192 [00:01<00:24, 309.48it/s]
Adding requests:   7%|▋         | 597/8192 [00:01<00:24, 310.59it/s]
Adding requests:   8%|▊         | 629/8192 [00:02<00:24, 308.04it/s]
Adding requests:   8%|▊         | 660/8192 [00:02<00:25, 296.53it/s]
Adding requests:   8%|▊         | 690/8192 [00:02<00:25, 294.95it/s]
Adding requests:   9%|▉         | 720/8192 [00:02<00:26, 283.75it/s]
Adding requests:   9%|▉         | 749/8192 [00:02<00:26, 280.34it/s]
Adding requests:   9%|▉         | 778/8192 [00:02<00:26, 281.81it/s]
Adding requests:  10%|▉         | 807/8192 [00:02<00:26, 278.35it/s]
Adding requests:  10%|█         | 835/8192 [00:02<00:26, 275.40it/s]
Adding requests:  11%|█         | 864/8192 [00:02<00:26, 277.76it/s]
Adding requests:  11%|█         | 892/8192 [00:03<00:26, 278.05it/s]
Adding requests:  11%|█         | 920/8192 [00:03<00:26, 272.48it/s]
Adding requests:  12%|█▏        | 949/8192 [00:03<00:26, 274.17it/s]
Adding requests:  12%|█▏        | 978/8192 [00:03<00:26, 276.28it/s]
Adding requests:  12%|█▏        | 1006/8192 [00:03<00:26, 273.04it/s]
Adding requests:  13%|█▎        | 1034/8192 [00:03<00:26, 271.37it/s]
Adding requests:  13%|█▎        | 1063/8192 [00:03<00:25, 275.45it/s]
Adding requests:  13%|█▎        | 1093/8192 [00:03<00:25, 280.33it/s]
Adding requests:  14%|█▎        | 1122/8192 [00:03<00:25, 280.62it/s]
Adding requests:  14%|█▍        | 1152/8192 [00:03<00:24, 284.97it/s]
Adding requests:  14%|█▍        | 1183/8192 [00:04<00:24, 290.31it/s]
Adding requests:  15%|█▍        | 1213/8192 [00:04<00:23, 291.85it/s]
Adding requests:  15%|█▌        | 1243/8192 [00:04<00:23, 292.52it/s]
Adding requests:  16%|█▌        | 1273/8192 [00:04<00:23, 293.70it/s]
Adding requests:  16%|█▌        | 1303/8192 [00:04<00:23, 288.53it/s]
Adding requests:  16%|█▋        | 1333/8192 [00:04<00:23, 289.56it/s]
Adding requests:  17%|█▋        | 1362/8192 [00:04<00:23, 287.83it/s]
Adding requests:  17%|█▋        | 1394/8192 [00:04<00:23, 294.63it/s]
Adding requests:  17%|█▋        | 1425/8192 [00:04<00:22, 296.25it/s]
Adding requests:  18%|█▊        | 1456/8192 [00:04<00:22, 299.63it/s]
Adding requests:  18%|█▊        | 1486/8192 [00:05<00:22, 298.51it/s]
Adding requests:  19%|█▊        | 1516/8192 [00:05<00:22, 290.73it/s]
Adding requests:  19%|█▉        | 1546/8192 [00:05<00:22, 292.95it/s]
Adding requests:  19%|█▉        | 1576/8192 [00:05<00:22, 288.02it/s]
Adding requests:  20%|█▉        | 1605/8192 [00:05<00:23, 277.93it/s]
Adding requests:  20%|█▉        | 1634/8192 [00:05<00:23, 278.89it/s]
Adding requests:  20%|██        | 1662/8192 [00:05<00:24, 268.56it/s]
Adding requests:  21%|██        | 1689/8192 [00:05<00:24, 268.90it/s]
Adding requests:  21%|██        | 1718/8192 [00:05<00:23, 273.92it/s]
Adding requests:  21%|██▏       | 1747/8192 [00:06<00:23, 277.62it/s]
Adding requests:  22%|██▏       | 1775/8192 [00:06<00:23, 276.06it/s]
Adding requests:  22%|██▏       | 1803/8192 [00:06<00:23, 275.03it/s]
Adding requests:  22%|██▏       | 1832/8192 [00:06<00:22, 277.67it/s]
Adding requests:  23%|██▎       | 1861/8192 [00:06<00:22, 279.06it/s]
Adding requests:  23%|██▎       | 1889/8192 [00:06<00:23, 264.22it/s]
Adding requests:  23%|██▎       | 1917/8192 [00:06<00:23, 267.55it/s]
Adding requests:  24%|██▎       | 1945/8192 [00:06<00:23, 270.61it/s]
Adding requests:  24%|██▍       | 1974/8192 [00:06<00:22, 274.42it/s]
Adding requests:  24%|██▍       | 2003/8192 [00:06<00:22, 276.85it/s]
Adding requests:  25%|██▍       | 2037/8192 [00:07<00:20, 293.17it/s]
Adding requests:  25%|██▌       | 2068/8192 [00:07<00:20, 296.44it/s]
Adding requests:  26%|██▌       | 2098/8192 [00:07<00:20, 294.61it/s]
Adding requests:  26%|██▌       | 2131/8192 [00:07<00:19, 303.37it/s]
Adding requests:  26%|██▋       | 2163/8192 [00:07<00:19, 306.17it/s]
Adding requests:  27%|██▋       | 2194/8192 [00:07<00:19, 304.92it/s]
Adding requests:  27%|██▋       | 2227/8192 [00:07<00:19, 311.42it/s]
Adding requests:  28%|██▊       | 2261/8192 [00:07<00:18, 316.84it/s]
Adding requests:  28%|██▊       | 2293/8192 [00:07<00:18, 311.47it/s]
Adding requests:  28%|██▊       | 2325/8192 [00:07<00:19, 307.40it/s]
Adding requests:  29%|██▉       | 2358/8192 [00:08<00:18, 313.73it/s]
Adding requests:  29%|██▉       | 2390/8192 [00:08<00:18, 309.72it/s]
Adding requests:  30%|██▉       | 2422/8192 [00:08<00:18, 312.28it/s]
Adding requests:  30%|██▉       | 2454/8192 [00:08<00:18, 312.12it/s]
Adding requests:  36%|███▋      | 2971/8192 [00:08<00:03, 1730.67it/s]
Adding requests:  38%|███▊      | 3146/8192 [00:09<00:06, 747.77it/s] 
Adding requests:  40%|████      | 3278/8192 [00:09<00:08, 551.56it/s]
Adding requests:  41%|████▏     | 3380/8192 [00:09<00:10, 478.97it/s]
Adding requests:  42%|████▏     | 3461/8192 [00:10<00:11, 420.94it/s]
Adding requests:  43%|████▎     | 3526/8192 [00:10<00:12, 382.60it/s]
Adding requests:  44%|████▎     | 3580/8192 [00:10<00:12, 358.16it/s]
Adding requests:  44%|████▍     | 3626/8192 [00:10<00:13, 340.40it/s]
Adding requests:  45%|████▍     | 3667/8192 [00:10<00:13, 328.67it/s]
Adding requests:  45%|████▌     | 3704/8192 [00:10<00:14, 318.31it/s]
Adding requests:  46%|████▌     | 3739/8192 [00:11<00:14, 305.81it/s]
Adding requests:  46%|████▌     | 3771/8192 [00:11<00:14, 301.02it/s]
Adding requests:  46%|████▋     | 3802/8192 [00:11<00:14, 300.48it/s]
Adding requests:  47%|████▋     | 3833/8192 [00:11<00:14, 299.84it/s]
Adding requests:  47%|████▋     | 3864/8192 [00:11<00:14, 300.59it/s]
Adding requests:  48%|████▊     | 3895/8192 [00:11<00:14, 298.11it/s]
Adding requests:  48%|████▊     | 3925/8192 [00:11<00:14, 292.37it/s]
Adding requests:  48%|████▊     | 3955/8192 [00:11<00:14, 290.27it/s]
Adding requests:  49%|████▊     | 3986/8192 [00:11<00:14, 294.88it/s]
Adding requests:  49%|████▉     | 4016/8192 [00:12<00:14, 296.31it/s]
Adding requests:  49%|████▉     | 4046/8192 [00:12<00:14, 285.39it/s]
Adding requests:  50%|████▉     | 4077/8192 [00:12<00:14, 290.17it/s]
Adding requests:  50%|█████     | 4108/8192 [00:12<00:13, 293.58it/s]
Adding requests:  51%|█████     | 4139/8192 [00:12<00:13, 296.24it/s]
Adding requests:  51%|█████     | 4169/8192 [00:12<00:13, 294.34it/s]
Adding requests:  51%|█████▏    | 4201/8192 [00:12<00:13, 300.82it/s]
Adding requests:  52%|█████▏    | 4232/8192 [00:12<00:13, 291.86it/s]
Adding requests:  52%|█████▏    | 4263/8192 [00:12<00:13, 295.16it/s]
Adding requests:  52%|█████▏    | 4295/8192 [00:12<00:12, 300.81it/s]
Adding requests:  53%|█████▎    | 4326/8192 [00:13<00:12, 298.73it/s]
Adding requests:  53%|█████▎    | 4356/8192 [00:13<00:13, 293.66it/s]
Adding requests:  54%|█████▎    | 4387/8192 [00:13<00:12, 297.12it/s]
Adding requests:  54%|█████▍    | 4417/8192 [00:13<00:13, 283.41it/s]
Adding requests:  54%|█████▍    | 4446/8192 [00:13<00:13, 282.57it/s]
Adding requests:  55%|█████▍    | 4476/8192 [00:13<00:13, 285.30it/s]
Adding requests:  55%|█████▍    | 4505/8192 [00:13<00:13, 282.47it/s]
Adding requests:  55%|█████▌    | 4534/8192 [00:13<00:13, 270.78it/s]
Adding requests:  56%|█████▌    | 4562/8192 [00:13<00:13, 272.77it/s]
Adding requests:  56%|█████▌    | 4591/8192 [00:14<00:13, 274.88it/s]
Adding requests:  56%|█████▋    | 4619/8192 [00:14<00:13, 259.14it/s]
Adding requests:  57%|█████▋    | 4648/8192 [00:14<00:13, 267.17it/s]
Adding requests:  57%|█████▋    | 4676/8192 [00:14<00:13, 269.96it/s]
Adding requests:  57%|█████▋    | 4704/8192 [00:14<00:12, 270.09it/s]
Adding requests:  58%|█████▊    | 4733/8192 [00:14<00:12, 275.55it/s]
Adding requests:  58%|█████▊    | 4764/8192 [00:14<00:12, 283.42it/s]
Adding requests:  59%|█████▊    | 4795/8192 [00:14<00:11, 288.49it/s]
Adding requests:  59%|█████▉    | 4824/8192 [00:14<00:11, 285.69it/s]
Adding requests:  59%|█████▉    | 4853/8192 [00:15<00:11, 282.41it/s]
Adding requests:  60%|█████▉    | 4884/8192 [00:15<00:11, 288.44it/s]
Adding requests:  60%|█████▉    | 4914/8192 [00:15<00:11, 291.75it/s]
Adding requests:  60%|██████    | 4945/8192 [00:15<00:10, 296.00it/s]
Adding requests:  61%|██████    | 4978/8192 [00:15<00:10, 303.53it/s]
Adding requests:  61%|██████    | 5009/8192 [00:15<00:10, 303.85it/s]
Adding requests:  62%|██████▏   | 5041/8192 [00:15<00:10, 306.54it/s]
Adding requests:  62%|██████▏   | 5074/8192 [00:15<00:09, 312.70it/s]
Adding requests:  62%|██████▏   | 5107/8192 [00:15<00:09, 317.30it/s]
Adding requests:  63%|██████▎   | 5139/8192 [00:15<00:09, 313.08it/s]
Adding requests:  63%|██████▎   | 5171/8192 [00:16<00:09, 311.54it/s]
Adding requests:  64%|██████▎   | 5203/8192 [00:16<00:09, 313.03it/s]
Adding requests:  64%|██████▍   | 5235/8192 [00:16<00:09, 312.74it/s]
Adding requests:  64%|██████▍   | 5267/8192 [00:16<00:09, 306.47it/s]
Adding requests:  65%|██████▍   | 5298/8192 [00:16<00:09, 305.88it/s]
Adding requests:  65%|██████▌   | 5330/8192 [00:16<00:09, 307.06it/s]
Adding requests:  65%|██████▌   | 5362/8192 [00:16<00:09, 310.06it/s]
Adding requests:  66%|██████▌   | 5394/8192 [00:16<00:09, 305.00it/s]
Adding requests:  66%|██████▌   | 5425/8192 [00:16<00:09, 300.94it/s]
Adding requests:  67%|██████▋   | 5456/8192 [00:16<00:09, 299.95it/s]
Adding requests:  67%|██████▋   | 5487/8192 [00:17<00:09, 294.77it/s]
Adding requests:  67%|██████▋   | 5517/8192 [00:17<00:09, 291.29it/s]
Adding requests:  68%|██████▊   | 5547/8192 [00:17<00:09, 292.47it/s]
Adding requests:  68%|██████▊   | 5577/8192 [00:17<00:08, 291.47it/s]
Adding requests:  68%|██████▊   | 5607/8192 [00:17<00:08, 288.31it/s]
Adding requests:  69%|██████▉   | 5636/8192 [00:17<00:08, 284.70it/s]
Adding requests:  69%|██████▉   | 5666/8192 [00:17<00:08, 287.93it/s]
Adding requests:  70%|██████▉   | 5698/8192 [00:17<00:08, 296.52it/s]
Adding requests:  70%|██████▉   | 5730/8192 [00:17<00:08, 301.39it/s]
Adding requests:  70%|███████   | 5761/8192 [00:18<00:08, 297.84it/s]
Adding requests:  71%|███████   | 5791/8192 [00:18<00:08, 292.93it/s]
Adding requests:  71%|███████   | 5821/8192 [00:18<00:08, 290.76it/s]
Adding requests:  71%|███████▏  | 5851/8192 [00:18<00:08, 285.47it/s]
Adding requests:  72%|███████▏  | 5880/8192 [00:18<00:08, 282.06it/s]
Adding requests:  72%|███████▏  | 5910/8192 [00:18<00:08, 284.52it/s]
Adding requests:  72%|███████▏  | 5939/8192 [00:18<00:08, 273.93it/s]
Adding requests:  73%|███████▎  | 5967/8192 [00:18<00:08, 258.96it/s]
Adding requests:  73%|███████▎  | 5997/8192 [00:18<00:08, 267.66it/s]
Adding requests:  74%|███████▎  | 6025/8192 [00:18<00:08, 269.56it/s]
Adding requests:  74%|███████▍  | 6054/8192 [00:19<00:07, 273.43it/s]
Adding requests:  74%|███████▍  | 6082/8192 [00:19<00:07, 272.67it/s]
Adding requests:  75%|███████▍  | 6110/8192 [00:19<00:07, 272.75it/s]
Adding requests:  75%|███████▍  | 6138/8192 [00:19<00:07, 270.04it/s]
Adding requests:  75%|███████▌  | 6166/8192 [00:19<00:07, 268.65it/s]
Adding requests:  76%|███████▌  | 6195/8192 [00:19<00:07, 272.56it/s]
Adding requests:  76%|███████▌  | 6223/8192 [00:19<00:07, 269.93it/s]
Adding requests:  76%|███████▋  | 6251/8192 [00:19<00:07, 267.36it/s]
Adding requests:  77%|███████▋  | 6279/8192 [00:19<00:07, 269.25it/s]
Adding requests:  77%|███████▋  | 6306/8192 [00:20<00:07, 265.79it/s]
Adding requests:  77%|███████▋  | 6333/8192 [00:20<00:07, 264.34it/s]
Adding requests:  78%|███████▊  | 6362/8192 [00:20<00:06, 269.85it/s]
Adding requests:  78%|███████▊  | 6389/8192 [00:20<00:06, 267.65it/s]
Adding requests:  78%|███████▊  | 6416/8192 [00:20<00:06, 263.41it/s]
Adding requests:  79%|███████▊  | 6443/8192 [00:20<00:06, 264.94it/s]
Adding requests:  79%|███████▉  | 6470/8192 [00:20<00:06, 264.34it/s]
Adding requests:  79%|███████▉  | 6497/8192 [00:20<00:06, 264.73it/s]
Adding requests:  80%|███████▉  | 6525/8192 [00:20<00:06, 266.69it/s]
Adding requests:  80%|████████  | 6554/8192 [00:20<00:06, 272.20it/s]
Adding requests:  80%|████████  | 6583/8192 [00:21<00:05, 275.99it/s]
Adding requests:  81%|████████  | 6612/8192 [00:21<00:05, 277.66it/s]
Adding requests:  81%|████████  | 6641/8192 [00:21<00:05, 279.58it/s]
Adding requests:  81%|████████▏ | 6669/8192 [00:21<00:05, 278.97it/s]
Adding requests:  82%|████████▏ | 6698/8192 [00:21<00:05, 282.04it/s]
Adding requests:  82%|████████▏ | 6729/8192 [00:21<00:05, 288.51it/s]
Adding requests:  82%|████████▏ | 6758/8192 [00:21<00:04, 288.62it/s]
Adding requests:  83%|████████▎ | 6787/8192 [00:21<00:04, 286.76it/s]
Adding requests:  83%|████████▎ | 6817/8192 [00:21<00:04, 290.32it/s]
Adding requests:  84%|████████▎ | 6849/8192 [00:21<00:04, 296.19it/s]
Adding requests:  84%|████████▍ | 6880/8192 [00:22<00:04, 297.90it/s]
Adding requests:  84%|████████▍ | 6910/8192 [00:22<00:04, 294.62it/s]
Adding requests:  85%|████████▍ | 6940/8192 [00:22<00:04, 295.76it/s]
Adding requests:  85%|████████▌ | 6970/8192 [00:22<00:04, 294.64it/s]
Adding requests:  85%|████████▌ | 7000/8192 [00:22<00:04, 282.25it/s]
Adding requests:  86%|████████▌ | 7029/8192 [00:22<00:04, 283.63it/s]
Adding requests:  86%|████████▌ | 7058/8192 [00:22<00:04, 282.85it/s]
Adding requests:  87%|████████▋ | 7087/8192 [00:22<00:03, 280.75it/s]
Adding requests:  87%|████████▋ | 7116/8192 [00:22<00:03, 283.09it/s]
Adding requests:  87%|████████▋ | 7145/8192 [00:23<00:03, 279.67it/s]
Adding requests:  88%|████████▊ | 7173/8192 [00:23<00:03, 278.16it/s]
Adding requests:  88%|████████▊ | 7202/8192 [00:23<00:03, 281.43it/s]
Adding requests:  88%|████████▊ | 7231/8192 [00:23<00:03, 282.22it/s]
Adding requests:  89%|████████▊ | 7261/8192 [00:23<00:03, 285.05it/s]
Adding requests:  89%|████████▉ | 7290/8192 [00:23<00:03, 286.50it/s]
Adding requests:  89%|████████▉ | 7321/8192 [00:23<00:02, 292.11it/s]
Adding requests:  90%|████████▉ | 7351/8192 [00:23<00:02, 280.84it/s]
Adding requests:  90%|█████████ | 7382/8192 [00:23<00:02, 286.99it/s]
Adding requests:  90%|█████████ | 7413/8192 [00:23<00:02, 291.83it/s]
Adding requests:  91%|█████████ | 7446/8192 [00:24<00:02, 302.40it/s]
Adding requests:  91%|█████████▏| 7477/8192 [00:24<00:02, 303.09it/s]
Adding requests:  92%|█████████▏| 7509/8192 [00:24<00:02, 306.40it/s]
Adding requests:  92%|█████████▏| 7542/8192 [00:24<00:02, 312.67it/s]
Adding requests:  92%|█████████▏| 7574/8192 [00:24<00:01, 309.68it/s]
Adding requests:  93%|█████████▎| 7606/8192 [00:24<00:01, 310.50it/s]
Adding requests:  93%|█████████▎| 7638/8192 [00:24<00:01, 310.34it/s]
Adding requests:  94%|█████████▎| 7673/8192 [00:24<00:01, 321.03it/s]
Adding requests:  94%|█████████▍| 7706/8192 [00:24<00:01, 319.29it/s]
Adding requests:  94%|█████████▍| 7738/8192 [00:24<00:01, 310.25it/s]
Adding requests:  95%|█████████▍| 7770/8192 [00:25<00:01, 310.24it/s]
Adding requests:  95%|█████████▌| 7803/8192 [00:25<00:01, 315.60it/s]
Adding requests:  96%|█████████▌| 7837/8192 [00:25<00:01, 322.71it/s]
Adding requests:  96%|█████████▌| 7870/8192 [00:25<00:01, 321.15it/s]
Adding requests:  96%|█████████▋| 7904/8192 [00:25<00:00, 324.39it/s]
Adding requests:  97%|█████████▋| 7937/8192 [00:25<00:00, 324.00it/s]
Adding requests:  97%|█████████▋| 7970/8192 [00:25<00:00, 315.00it/s]
Adding requests:  98%|█████████▊| 8002/8192 [00:25<00:00, 309.47it/s]
Adding requests:  98%|█████████▊| 8034/8192 [00:25<00:00, 300.88it/s]
Adding requests:  98%|█████████▊| 8065/8192 [00:26<00:00, 300.12it/s]
Adding requests:  99%|█████████▉| 8096/8192 [00:26<00:00, 299.19it/s]
Adding requests:  99%|█████████▉| 8126/8192 [00:26<00:00, 293.50it/s]
Adding requests: 100%|█████████▉| 8156/8192 [00:26<00:00, 287.68it/s]
Adding requests: 100%|█████████▉| 8186/8192 [00:26<00:00, 288.01it/s]
Adding requests: 100%|██████████| 8192/8192 [00:26<00:00, 309.51it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 369/8192 [00:04<01:42, 76.52it/s, est. speed input: 78355.26 toks/s, output: 76.52 toks/s]
Processed prompts:   5%|▌         | 433/8192 [00:09<03:24, 37.95it/s, est. speed input: 44611.24 toks/s, output: 43.57 toks/s]
Processed prompts:   6%|▌         | 497/8192 [00:13<04:14, 30.18it/s, est. speed input: 37450.67 toks/s, output: 36.57 toks/s]
Processed prompts:   7%|▋         | 561/8192 [00:18<05:43, 22.22it/s, est. speed input: 30430.45 toks/s, output: 29.72 toks/s]
Processed prompts:   8%|▊         | 625/8192 [00:24<06:52, 18.33it/s, est. speed input: 26501.92 toks/s, output: 25.88 toks/s]
Processed prompts:   8%|▊         | 689/8192 [00:29<07:49, 15.96it/s, est. speed input: 23852.84 toks/s, output: 23.29 toks/s]
Processed prompts:   9%|▉         | 753/8192 [00:34<08:26, 14.68it/s, est. speed input: 22123.81 toks/s, output: 21.61 toks/s]
Processed prompts:  10%|▉         | 817/8192 [00:40<08:50, 13.90it/s, est. speed input: 20871.83 toks/s, output: 20.38 toks/s]
Processed prompts:  11%|█         | 881/8192 [00:43<08:18, 14.66it/s, est. speed input: 20568.52 toks/s, output: 20.09 toks/s]
Processed prompts:  12%|█▏        | 945/8192 [00:48<08:39, 13.96it/s, est. speed input: 19757.57 toks/s, output: 19.29 toks/s]
Processed prompts:  12%|█▏        | 1009/8192 [00:54<09:01, 13.27it/s, est. speed input: 19001.05 toks/s, output: 18.56 toks/s]
Processed prompts:  13%|█▎        | 1073/8192 [00:59<09:13, 12.86it/s, est. speed input: 18399.94 toks/s, output: 17.97 toks/s]
Processed prompts:  14%|█▍        | 1137/8192 [01:04<09:13, 12.74it/s, est. speed input: 17953.58 toks/s, output: 17.53 toks/s]
Processed prompts:  15%|█▍        | 1201/8192 [01:10<09:19, 12.50it/s, est. speed input: 17519.47 toks/s, output: 17.11 toks/s]
Processed prompts:  15%|█▌        | 1265/8192 [01:13<08:26, 13.68it/s, est. speed input: 17543.66 toks/s, output: 17.13 toks/s]
Processed prompts:  16%|█▌        | 1329/8192 [01:19<08:41, 13.17it/s, est. speed input: 17199.74 toks/s, output: 16.80 toks/s]
Processed prompts:  17%|█▋        | 1393/8192 [01:24<08:53, 12.74it/s, est. speed input: 16875.55 toks/s, output: 16.48 toks/s]
Processed prompts:  18%|█▊        | 1457/8192 [01:29<08:58, 12.52it/s, est. speed input: 16605.27 toks/s, output: 16.22 toks/s]
Processed prompts:  19%|█▊        | 1521/8192 [01:35<08:58, 12.38it/s, est. speed input: 16368.82 toks/s, output: 15.99 toks/s]
Processed prompts:  19%|█▉        | 1585/8192 [01:40<09:00, 12.23it/s, est. speed input: 16143.75 toks/s, output: 15.77 toks/s]
Processed prompts:  20%|██        | 1649/8192 [01:44<08:02, 13.55it/s, est. speed input: 16225.39 toks/s, output: 15.85 toks/s]
Processed prompts:  21%|██        | 1713/8192 [01:49<08:18, 13.00it/s, est. speed input: 16025.58 toks/s, output: 15.65 toks/s]
Processed prompts:  22%|██▏       | 1777/8192 [01:54<08:23, 12.75it/s, est. speed input: 15863.65 toks/s, output: 15.49 toks/s]
Processed prompts:  22%|██▏       | 1841/8192 [01:59<08:25, 12.58it/s, est. speed input: 15715.87 toks/s, output: 15.35 toks/s]
Processed prompts:  23%|██▎       | 1905/8192 [02:05<08:27, 12.39it/s, est. speed input: 15568.31 toks/s, output: 15.20 toks/s]
Processed prompts:  24%|██▍       | 1969/8192 [02:10<08:24, 12.34it/s, est. speed input: 15446.60 toks/s, output: 15.08 toks/s]
Processed prompts:  25%|██▍       | 2033/8192 [02:15<08:20, 12.31it/s, est. speed input: 15334.65 toks/s, output: 14.98 toks/s]
Processed prompts:  26%|██▌       | 2097/8192 [02:19<07:34, 13.40it/s, est. speed input: 15387.98 toks/s, output: 15.03 toks/s]
Processed prompts:  26%|██▋       | 2161/8192 [02:24<07:40, 13.10it/s, est. speed input: 15294.04 toks/s, output: 14.94 toks/s]
Processed prompts:  27%|██▋       | 2225/8192 [02:29<07:45, 12.82it/s, est. speed input: 15196.62 toks/s, output: 14.84 toks/s]
Processed prompts:  28%|██▊       | 2289/8192 [02:35<07:43, 12.72it/s, est. speed input: 15117.38 toks/s, output: 14.76 toks/s]
Processed prompts:  29%|██▊       | 2353/8192 [02:40<07:44, 12.57it/s, est. speed input: 15033.17 toks/s, output: 14.68 toks/s]
Processed prompts:  30%|██▉       | 2417/8192 [02:45<07:47, 12.35it/s, est. speed input: 14938.78 toks/s, output: 14.59 toks/s]
Processed prompts:  30%|███       | 2481/8192 [02:48<06:51, 13.86it/s, est. speed input: 15035.41 toks/s, output: 14.68 toks/s]
Processed prompts:  31%|███       | 2545/8192 [02:54<07:07, 13.20it/s, est. speed input: 14946.42 toks/s, output: 14.60 toks/s]
Processed prompts:  32%|███▏      | 2609/8192 [02:59<07:14, 12.84it/s, est. speed input: 14870.32 toks/s, output: 14.52 toks/s]
Processed prompts:  33%|███▎      | 2673/8192 [03:04<07:17, 12.63it/s, est. speed input: 14801.38 toks/s, output: 14.45 toks/s]
Processed prompts:  33%|███▎      | 2737/8192 [03:10<07:20, 12.38it/s, est. speed input: 14725.15 toks/s, output: 14.38 toks/s]
Processed prompts:  34%|███▍      | 2801/8192 [03:15<07:18, 12.28it/s, est. speed input: 14660.87 toks/s, output: 14.32 toks/s]
Processed prompts:  35%|███▍      | 2865/8192 [03:19<06:32, 13.56it/s, est. speed input: 14726.35 toks/s, output: 14.38 toks/s]
Processed prompts:  36%|███▌      | 2929/8192 [03:24<06:44, 13.02it/s, est. speed input: 14660.30 toks/s, output: 14.32 toks/s]
Processed prompts:  37%|███▋      | 2993/8192 [03:29<06:47, 12.75it/s, est. speed input: 14605.29 toks/s, output: 14.26 toks/s]
Processed prompts:  37%|███▋      | 3057/8192 [03:35<06:50, 12.51it/s, est. speed input: 14547.27 toks/s, output: 14.21 toks/s]
Processed prompts:  38%|███▊      | 3121/8192 [03:40<06:48, 12.41it/s, est. speed input: 14497.52 toks/s, output: 14.16 toks/s]
Processed prompts:  39%|███▉      | 3185/8192 [03:45<06:44, 12.37it/s, est. speed input: 14452.85 toks/s, output: 14.11 toks/s]
Processed prompts:  40%|███▉      | 3249/8192 [03:49<06:10, 13.35it/s, est. speed input: 14492.53 toks/s, output: 14.15 toks/s]
Processed prompts:  40%|████      | 3313/8192 [03:54<06:15, 12.99it/s, est. speed input: 14448.19 toks/s, output: 14.11 toks/s]
Processed prompts:  41%|████      | 3377/8192 [04:00<06:19, 12.70it/s, est. speed input: 14402.34 toks/s, output: 14.06 toks/s]
Processed prompts:  42%|████▏     | 3441/8192 [04:05<06:22, 12.41it/s, est. speed input: 14350.42 toks/s, output: 14.01 toks/s]
Processed prompts:  43%|████▎     | 3505/8192 [04:10<06:19, 12.34it/s, est. speed input: 14311.37 toks/s, output: 13.98 toks/s]
Processed prompts:  44%|████▎     | 3569/8192 [04:16<06:16, 12.29it/s, est. speed input: 14273.10 toks/s, output: 13.94 toks/s]
Processed prompts:  44%|████▍     | 3633/8192 [04:19<05:39, 13.42it/s, est. speed input: 14319.75 toks/s, output: 13.98 toks/s]
Processed prompts:  45%|████▌     | 3697/8192 [04:24<05:42, 13.12it/s, est. speed input: 14289.47 toks/s, output: 13.95 toks/s]
Processed prompts:  46%|████▌     | 3761/8192 [04:30<05:49, 12.67it/s, est. speed input: 14243.64 toks/s, output: 13.91 toks/s]
Processed prompts:  47%|████▋     | 3825/8192 [04:35<05:49, 12.49it/s, est. speed input: 14207.88 toks/s, output: 13.87 toks/s]
Processed prompts:  47%|████▋     | 3889/8192 [04:40<05:46, 12.41it/s, est. speed input: 14176.70 toks/s, output: 13.84 toks/s]
Processed prompts:  48%|████▊     | 3953/8192 [04:46<05:46, 12.25it/s, est. speed input: 14138.86 toks/s, output: 13.81 toks/s]
Processed prompts:  49%|████▉     | 4017/8192 [04:49<05:07, 13.58it/s, est. speed input: 14193.63 toks/s, output: 13.86 toks/s]
Processed prompts:  50%|████▉     | 4081/8192 [04:55<05:15, 13.03it/s, est. speed input: 14157.26 toks/s, output: 13.83 toks/s]
Processed prompts:  51%|█████     | 4145/8192 [05:00<05:18, 12.71it/s, est. speed input: 14124.05 toks/s, output: 13.79 toks/s]
Processed prompts:  51%|█████▏    | 4209/8192 [05:05<05:13, 12.69it/s, est. speed input: 14104.62 toks/s, output: 13.77 toks/s]
Processed prompts:  52%|█████▏    | 4273/8192 [05:10<05:14, 12.45it/s, est. speed input: 14071.79 toks/s, output: 13.74 toks/s]
Processed prompts:  53%|█████▎    | 4337/8192 [05:16<05:10, 12.41it/s, est. speed input: 14047.77 toks/s, output: 13.72 toks/s]
Processed prompts:  54%|█████▎    | 4401/8192 [05:21<05:06, 12.37it/s, est. speed input: 14024.04 toks/s, output: 13.70 toks/s]
Processed prompts:  55%|█████▍    | 4465/8192 [05:25<04:37, 13.43it/s, est. speed input: 14061.32 toks/s, output: 13.73 toks/s]
Processed prompts:  55%|█████▌    | 4529/8192 [05:30<04:39, 13.09it/s, est. speed input: 14039.37 toks/s, output: 13.71 toks/s]
Processed prompts:  56%|█████▌    | 4593/8192 [05:35<04:43, 12.68it/s, est. speed input: 14008.09 toks/s, output: 13.68 toks/s]
Processed prompts:  57%|█████▋    | 4657/8192 [05:41<04:43, 12.45it/s, est. speed input: 13980.00 toks/s, output: 13.65 toks/s]
Processed prompts:  58%|█████▊    | 4721/8192 [05:46<04:40, 12.38it/s, est. speed input: 13958.03 toks/s, output: 13.63 toks/s]
Processed prompts:  58%|█████▊    | 4785/8192 [05:51<04:37, 12.28it/s, est. speed input: 13933.61 toks/s, output: 13.61 toks/s]
Processed prompts:  59%|█████▉    | 4849/8192 [05:55<04:04, 13.69it/s, est. speed input: 13983.83 toks/s, output: 13.66 toks/s]
Processed prompts:  60%|█████▉    | 4913/8192 [06:00<04:08, 13.17it/s, est. speed input: 13960.55 toks/s, output: 13.63 toks/s]
Processed prompts:  61%|██████    | 4977/8192 [06:05<04:10, 12.82it/s, est. speed input: 13937.26 toks/s, output: 13.61 toks/s]
Processed prompts:  62%|██████▏   | 5041/8192 [06:10<04:09, 12.63it/s, est. speed input: 13916.93 toks/s, output: 13.59 toks/s]
Processed prompts:  62%|██████▏   | 5105/8192 [06:16<04:07, 12.45it/s, est. speed input: 13894.56 toks/s, output: 13.57 toks/s]
Processed prompts:  63%|██████▎   | 5169/8192 [06:21<04:06, 12.26it/s, est. speed input: 13869.46 toks/s, output: 13.54 toks/s]
Processed prompts:  64%|██████▍   | 5233/8192 [06:25<03:37, 13.63it/s, est. speed input: 13914.73 toks/s, output: 13.59 toks/s]
Processed prompts:  65%|██████▍   | 5297/8192 [06:30<03:42, 13.01it/s, est. speed input: 13888.51 toks/s, output: 13.56 toks/s]
Processed prompts:  65%|██████▌   | 5361/8192 [06:35<03:42, 12.74it/s, est. speed input: 13869.50 toks/s, output: 13.54 toks/s]
Processed prompts:  66%|██████▌   | 5425/8192 [06:41<03:39, 12.58it/s, est. speed input: 13851.77 toks/s, output: 13.53 toks/s]
Processed prompts:  67%|██████▋   | 5489/8192 [06:46<03:39, 12.32it/s, est. speed input: 13827.64 toks/s, output: 13.50 toks/s]
Processed prompts:  68%|██████▊   | 5553/8192 [06:51<03:33, 12.37it/s, est. speed input: 13814.65 toks/s, output: 13.49 toks/s]
Processed prompts:  69%|██████▊   | 5617/8192 [06:55<03:11, 13.43it/s, est. speed input: 13845.73 toks/s, output: 13.52 toks/s]
Processed prompts:  69%|██████▉   | 5681/8192 [07:00<03:13, 12.98it/s, est. speed input: 13826.47 toks/s, output: 13.50 toks/s]
Processed prompts:  70%|███████   | 5745/8192 [07:05<03:12, 12.73it/s, est. speed input: 13809.66 toks/s, output: 13.49 toks/s]
Processed prompts:  71%|███████   | 5809/8192 [07:11<03:11, 12.46it/s, est. speed input: 13788.95 toks/s, output: 13.47 toks/s]
Processed prompts:  72%|███████▏  | 5873/8192 [07:16<03:08, 12.33it/s, est. speed input: 13771.28 toks/s, output: 13.45 toks/s]
Processed prompts:  72%|███████▏  | 5937/8192 [07:21<03:02, 12.33it/s, est. speed input: 13757.75 toks/s, output: 13.44 toks/s]
Processed prompts:  73%|███████▎  | 6001/8192 [07:25<02:43, 13.40it/s, est. speed input: 13787.21 toks/s, output: 13.46 toks/s]
Processed prompts:  74%|███████▍  | 6065/8192 [07:30<02:43, 13.02it/s, est. speed input: 13772.35 toks/s, output: 13.45 toks/s]
Processed prompts:  75%|███████▍  | 6129/8192 [07:36<02:42, 12.67it/s, est. speed input: 13753.89 toks/s, output: 13.43 toks/s]
Processed prompts:  76%|███████▌  | 6193/8192 [07:41<02:40, 12.42it/s, est. speed input: 13735.42 toks/s, output: 13.41 toks/s]
Processed prompts:  76%|███████▋  | 6257/8192 [07:46<02:36, 12.39it/s, est. speed input: 13722.95 toks/s, output: 13.40 toks/s]
Processed prompts:  77%|███████▋  | 6321/8192 [07:52<02:32, 12.31it/s, est. speed input: 13708.18 toks/s, output: 13.39 toks/s]
Processed prompts:  78%|███████▊  | 6385/8192 [07:55<02:14, 13.45it/s, est. speed input: 13738.52 toks/s, output: 13.42 toks/s]
Processed prompts:  79%|███████▊  | 6449/8192 [08:01<02:13, 13.03it/s, est. speed input: 13724.26 toks/s, output: 13.40 toks/s]
Processed prompts:  80%|███████▉  | 6513/8192 [08:06<02:13, 12.62it/s, est. speed input: 13705.26 toks/s, output: 13.38 toks/s]
Processed prompts:  80%|████████  | 6577/8192 [08:11<02:08, 12.55it/s, est. speed input: 13694.61 toks/s, output: 13.37 toks/s]
Processed prompts:  81%|████████  | 6641/8192 [08:16<02:03, 12.52it/s, est. speed input: 13684.75 toks/s, output: 13.36 toks/s]
Processed prompts:  82%|████████▏ | 6705/8192 [08:22<02:00, 12.29it/s, est. speed input: 13667.45 toks/s, output: 13.35 toks/s]
Processed prompts:  83%|████████▎ | 6769/8192 [08:25<01:44, 13.58it/s, est. speed input: 13700.84 toks/s, output: 13.38 toks/s]
Processed prompts:  83%|████████▎ | 6833/8192 [08:31<01:44, 13.03it/s, est. speed input: 13684.97 toks/s, output: 13.36 toks/s]
Processed prompts:  84%|████████▍ | 6897/8192 [08:36<01:41, 12.74it/s, est. speed input: 13671.94 toks/s, output: 13.35 toks/s]
Processed prompts:  85%|████████▍ | 6961/8192 [08:41<01:38, 12.54it/s, est. speed input: 13658.71 toks/s, output: 13.34 toks/s]
Processed prompts:  86%|████████▌ | 7025/8192 [08:47<01:34, 12.31it/s, est. speed input: 13642.62 toks/s, output: 13.32 toks/s]
Processed prompts:  87%|████████▋ | 7089/8192 [08:52<01:29, 12.34it/s, est. speed input: 13633.45 toks/s, output: 13.31 toks/s]
Processed prompts:  87%|████████▋ | 7153/8192 [08:56<01:16, 13.62it/s, est. speed input: 13665.28 toks/s, output: 13.34 toks/s]
Processed prompts:  88%|████████▊ | 7217/8192 [09:01<01:13, 13.18it/s, est. speed input: 13654.45 toks/s, output: 13.33 toks/s]
Processed prompts:  89%|████████▉ | 7281/8192 [09:06<01:10, 12.95it/s, est. speed input: 13645.98 toks/s, output: 13.33 toks/s]
Processed prompts:  90%|████████▉ | 7345/8192 [09:11<01:07, 12.57it/s, est. speed input: 13630.20 toks/s, output: 13.31 toks/s]
Processed prompts:  90%|█████████ | 7409/8192 [09:17<01:03, 12.41it/s, est. speed input: 13617.86 toks/s, output: 13.30 toks/s]
Processed prompts:  91%|█████████ | 7473/8192 [09:22<00:58, 12.34it/s, est. speed input: 13607.10 toks/s, output: 13.29 toks/s]
Processed prompts:  92%|█████████▏| 7537/8192 [09:27<00:53, 12.20it/s, est. speed input: 13593.37 toks/s, output: 13.27 toks/s]
Processed prompts:  93%|█████████▎| 7601/8192 [09:31<00:43, 13.49it/s, est. speed input: 13623.25 toks/s, output: 13.30 toks/s]
Processed prompts:  94%|█████████▎| 7665/8192 [09:36<00:40, 13.01it/s, est. speed input: 13610.88 toks/s, output: 13.29 toks/s]
Processed prompts:  94%|█████████▍| 7729/8192 [09:42<00:36, 12.65it/s, est. speed input: 13597.63 toks/s, output: 13.28 toks/s]
Processed prompts:  95%|█████████▌| 7793/8192 [09:47<00:31, 12.50it/s, est. speed input: 13587.50 toks/s, output: 13.27 toks/s]
Processed prompts:  96%|█████████▌| 7857/8192 [09:52<00:27, 12.36it/s, est. speed input: 13576.09 toks/s, output: 13.26 toks/s]
Processed prompts:  97%|█████████▋| 7921/8192 [09:57<00:22, 12.22it/s, est. speed input: 13563.77 toks/s, output: 13.25 toks/s]
Processed prompts:  97%|█████████▋| 7985/8192 [10:01<00:15, 13.46it/s, est. speed input: 13590.94 toks/s, output: 13.27 toks/s]
Processed prompts:  98%|█████████▊| 8049/8192 [10:06<00:11, 12.99it/s, est. speed input: 13579.51 toks/s, output: 13.26 toks/s]
Processed prompts:  99%|█████████▉| 8113/8192 [10:12<00:06, 12.73it/s, est. speed input: 13569.83 toks/s, output: 13.25 toks/s]
Processed prompts: 100%|█████████▉| 8177/8192 [10:12<00:00, 17.87it/s, est. speed input: 13672.20 toks/s, output: 13.35 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [10:12<00:00, 17.87it/s, est. speed input: 13697.27 toks/s, output: 13.38 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [10:12<00:00, 13.38it/s, est. speed input: 13697.27 toks/s, output: 13.38 toks/s]
[rank0]:[W125 19:44:52.387222934 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-25 22:51:42
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:51:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:51:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=503544) WARNING 01-25 22:52:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=503544) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=503544) WARNING 01-25 22:52:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 13.88 requests/s, 7120.39 total tokens/s, 13.88 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 22:51:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:51:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:51:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:51:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:51:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:51:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:51:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:51:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:51:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:51:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:51:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:51:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:51:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:51:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:51:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:51:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:51:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:51:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:51:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:51:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:51:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:51:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:51:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:51:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:51:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:51:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:51:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:51:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:04] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=503544) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=503544) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.06s/it]
(EngineCore_DP0 pid=503544) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.06s/it]
(EngineCore_DP0 pid=503544) 
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19005440 bytes
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11403264 bytes
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 60817408 bytes
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=503544) [2026-01-25 22:52:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30277632 bytes
(EngineCore_DP0 pid=503544) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]
(EngineCore_DP0 pid=503544) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.30it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 398.35it/s]
Adding requests:  67%|██████▋   | 86/128 [00:00<00:00, 431.75it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 410.09it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:21,  5.90it/s, est. speed input: 3020.58 toks/s, output: 5.90 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:19,  6.41it/s, est. speed input: 3239.12 toks/s, output: 6.33 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:13,  9.16it/s, est. speed input: 4276.79 toks/s, output: 8.35 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:11, 10.49it/s, est. speed input: 4810.26 toks/s, output: 9.39 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:10, 11.81it/s, est. speed input: 5277.05 toks/s, output: 10.31 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:09, 12.60it/s, est. speed input: 5586.97 toks/s, output: 10.91 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:01<00:08, 13.18it/s, est. speed input: 5823.60 toks/s, output: 11.37 toks/s]
Processed prompts:  11%|█         | 14/128 [00:01<00:08, 13.60it/s, est. speed input: 6011.94 toks/s, output: 11.74 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:01<00:08, 13.82it/s, est. speed input: 6149.45 toks/s, output: 12.01 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:07, 13.89it/s, est. speed input: 6249.95 toks/s, output: 12.21 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:07, 14.08it/s, est. speed input: 6351.23 toks/s, output: 12.40 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:07, 14.19it/s, est. speed input: 6434.42 toks/s, output: 12.57 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:07, 14.23it/s, est. speed input: 6500.46 toks/s, output: 12.70 toks/s]
Processed prompts:  20%|██        | 26/128 [00:02<00:07, 14.18it/s, est. speed input: 6549.92 toks/s, output: 12.79 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:02<00:07, 14.21it/s, est. speed input: 6598.81 toks/s, output: 12.89 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:02<00:07, 13.86it/s, est. speed input: 6606.86 toks/s, output: 12.90 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:02<00:06, 14.19it/s, est. speed input: 6664.96 toks/s, output: 13.02 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:02<00:06, 14.33it/s, est. speed input: 6709.39 toks/s, output: 13.10 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:06, 13.96it/s, est. speed input: 6711.38 toks/s, output: 13.11 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:06, 14.07it/s, est. speed input: 6741.92 toks/s, output: 13.17 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:03<00:06, 13.88it/s, est. speed input: 6748.93 toks/s, output: 13.18 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:03<00:06, 13.66it/s, est. speed input: 6748.50 toks/s, output: 13.18 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:03<00:05, 14.07it/s, est. speed input: 6788.97 toks/s, output: 13.26 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:03<00:05, 14.47it/s, est. speed input: 6831.50 toks/s, output: 13.34 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:03<00:05, 14.83it/s, est. speed input: 6875.24 toks/s, output: 13.43 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:03<00:05, 14.97it/s, est. speed input: 6909.04 toks/s, output: 13.49 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:05, 14.88it/s, est. speed input: 6930.24 toks/s, output: 13.54 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 15.01it/s, est. speed input: 6960.72 toks/s, output: 13.60 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:04<00:04, 15.14it/s, est. speed input: 6990.46 toks/s, output: 13.65 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:04<00:04, 15.26it/s, est. speed input: 7020.07 toks/s, output: 13.71 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:04<00:04, 15.32it/s, est. speed input: 7046.78 toks/s, output: 13.76 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:04<00:04, 15.48it/s, est. speed input: 7076.79 toks/s, output: 13.82 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:04<00:04, 15.54it/s, est. speed input: 7103.36 toks/s, output: 13.87 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:04<00:03, 15.55it/s, est. speed input: 7126.88 toks/s, output: 13.92 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:03, 15.61it/s, est. speed input: 7151.47 toks/s, output: 13.97 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 15.67it/s, est. speed input: 7175.30 toks/s, output: 14.01 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:05<00:03, 15.49it/s, est. speed input: 7189.35 toks/s, output: 14.04 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:05<00:03, 15.48it/s, est. speed input: 7207.28 toks/s, output: 14.08 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:05<00:03, 15.60it/s, est. speed input: 7228.92 toks/s, output: 14.12 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:05<00:03, 15.64it/s, est. speed input: 7248.03 toks/s, output: 14.16 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:05<00:03, 15.52it/s, est. speed input: 7261.18 toks/s, output: 14.18 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:05<00:02, 15.50it/s, est. speed input: 7275.69 toks/s, output: 14.21 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:05<00:02, 15.49it/s, est. speed input: 7289.65 toks/s, output: 14.24 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:06<00:02, 15.49it/s, est. speed input: 7303.33 toks/s, output: 14.26 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:06<00:02, 15.44it/s, est. speed input: 7315.03 toks/s, output: 14.29 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:06<00:02, 15.04it/s, est. speed input: 7313.72 toks/s, output: 14.28 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:06<00:02, 14.90it/s, est. speed input: 7317.09 toks/s, output: 14.29 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:06<00:02, 14.84it/s, est. speed input: 7321.28 toks/s, output: 14.30 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:06<00:02, 14.77it/s, est. speed input: 7324.67 toks/s, output: 14.31 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:06<00:02, 14.66it/s, est. speed input: 7325.78 toks/s, output: 14.31 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:06<00:01, 14.68it/s, est. speed input: 7329.72 toks/s, output: 14.32 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:07<00:01, 14.70it/s, est. speed input: 7333.88 toks/s, output: 14.32 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:07<00:01, 14.66it/s, est. speed input: 7336.26 toks/s, output: 14.33 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:07<00:01, 14.69it/s, est. speed input: 7340.48 toks/s, output: 14.34 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:07<00:01, 14.69it/s, est. speed input: 7343.67 toks/s, output: 14.34 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:07<00:01, 14.68it/s, est. speed input: 7346.40 toks/s, output: 14.35 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:07<00:01, 14.69it/s, est. speed input: 7349.70 toks/s, output: 14.35 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:07<00:00, 14.64it/s, est. speed input: 7351.28 toks/s, output: 14.36 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:08<00:00, 14.61it/s, est. speed input: 7352.79 toks/s, output: 14.36 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:08<00:00, 14.67it/s, est. speed input: 7356.55 toks/s, output: 14.37 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:08<00:00, 14.56it/s, est. speed input: 7356.20 toks/s, output: 14.37 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:08<00:00, 14.60it/s, est. speed input: 7358.83 toks/s, output: 14.37 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:08<00:00, 14.50it/s, est. speed input: 7358.03 toks/s, output: 14.37 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:08<00:00, 14.42it/s, est. speed input: 7357.01 toks/s, output: 14.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.41it/s, est. speed input: 7357.09 toks/s, output: 14.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.41it/s, est. speed input: 7357.09 toks/s, output: 14.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.37it/s, est. speed input: 7357.09 toks/s, output: 14.37 toks/s]
[rank0]:[W125 22:52:49.050814850 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 22:52:51
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:53:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:53:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=505162) WARNING 01-25 22:53:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=505162) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=505162) WARNING 01-25 22:53:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 14.27 requests/s, 14631.84 total tokens/s, 14.27 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 22:53:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:53:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:53:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:53:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:53:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:53:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:53:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:53:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:53:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:53:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:53:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:53:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:53:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:53:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:53:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:53:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:53:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:53:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:53:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:53:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:53:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:53:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:53:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:53:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:53:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:53:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:53:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:53:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:11] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=505162) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=505162) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.14s/it]
(EngineCore_DP0 pid=505162) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.14s/it]
(EngineCore_DP0 pid=505162) 
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19005440 bytes
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11403264 bytes
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 60817408 bytes
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=505162) [2026-01-25 22:53:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30277632 bytes
(EngineCore_DP0 pid=505162) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=505162) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 276.40it/s]
Adding requests:  45%|████▍     | 57/128 [00:00<00:00, 279.19it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 276.76it/s]
Adding requests:  88%|████████▊ | 113/128 [00:00<00:00, 273.14it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 276.70it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:13,  9.02it/s, est. speed input: 9232.83 toks/s, output: 9.02 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:10, 11.60it/s, est. speed input: 11391.93 toks/s, output: 11.12 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:09, 12.90it/s, est. speed input: 12438.16 toks/s, output: 12.15 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:08, 13.60it/s, est. speed input: 13026.44 toks/s, output: 12.72 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:08, 14.04it/s, est. speed input: 13418.12 toks/s, output: 13.10 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:08, 14.36it/s, est. speed input: 13709.26 toks/s, output: 13.39 toks/s]
Processed prompts:  11%|█         | 14/128 [00:01<00:07, 14.32it/s, est. speed input: 13829.80 toks/s, output: 13.51 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:01<00:07, 14.45it/s, est. speed input: 13974.22 toks/s, output: 13.65 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:07, 14.53it/s, est. speed input: 14086.55 toks/s, output: 13.76 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:07, 14.57it/s, est. speed input: 14173.42 toks/s, output: 13.84 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:07, 14.54it/s, est. speed input: 14231.64 toks/s, output: 13.90 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:07, 14.49it/s, est. speed input: 14270.01 toks/s, output: 13.94 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:07, 14.53it/s, est. speed input: 14322.15 toks/s, output: 13.99 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:06, 14.58it/s, est. speed input: 14370.74 toks/s, output: 14.03 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:02<00:06, 14.58it/s, est. speed input: 14407.13 toks/s, output: 14.07 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:02<00:06, 14.62it/s, est. speed input: 14446.60 toks/s, output: 14.11 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:02<00:06, 14.57it/s, est. speed input: 14467.06 toks/s, output: 14.13 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:06, 14.56it/s, est. speed input: 14489.87 toks/s, output: 14.15 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:06, 14.60it/s, est. speed input: 14517.98 toks/s, output: 14.18 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:06, 14.45it/s, est. speed input: 14515.00 toks/s, output: 14.17 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 14.44it/s, est. speed input: 14525.82 toks/s, output: 14.19 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:03<00:05, 14.49it/s, est. speed input: 14545.02 toks/s, output: 14.20 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:03<00:05, 14.53it/s, est. speed input: 14562.98 toks/s, output: 14.22 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:03<00:05, 14.60it/s, est. speed input: 14586.06 toks/s, output: 14.24 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:03<00:05, 14.72it/s, est. speed input: 14615.96 toks/s, output: 14.27 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:05, 13.73it/s, est. speed input: 14503.17 toks/s, output: 14.16 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:05, 14.09it/s, est. speed input: 14532.96 toks/s, output: 14.19 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:05, 14.37it/s, est. speed input: 14562.74 toks/s, output: 14.22 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:04<00:04, 14.80it/s, est. speed input: 14616.96 toks/s, output: 14.27 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:04<00:04, 15.22it/s, est. speed input: 14677.14 toks/s, output: 14.33 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:04<00:04, 15.51it/s, est. speed input: 14733.44 toks/s, output: 14.39 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:04<00:04, 15.68it/s, est. speed input: 14781.76 toks/s, output: 14.44 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:04<00:03, 15.73it/s, est. speed input: 14822.15 toks/s, output: 14.47 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:03, 15.82it/s, est. speed input: 14864.69 toks/s, output: 14.52 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 15.87it/s, est. speed input: 14903.62 toks/s, output: 14.55 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 15.88it/s, est. speed input: 14938.58 toks/s, output: 14.59 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:05<00:03, 15.84it/s, est. speed input: 14968.33 toks/s, output: 14.62 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:05<00:03, 15.87it/s, est. speed input: 15001.29 toks/s, output: 14.65 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:05<00:03, 15.89it/s, est. speed input: 15032.49 toks/s, output: 14.68 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:05<00:03, 15.90it/s, est. speed input: 15061.74 toks/s, output: 14.71 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:05<00:02, 15.84it/s, est. speed input: 15084.98 toks/s, output: 14.73 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:05<00:02, 15.79it/s, est. speed input: 15106.68 toks/s, output: 14.75 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 15.89it/s, est. speed input: 15137.03 toks/s, output: 14.78 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 16.03it/s, est. speed input: 15170.13 toks/s, output: 14.81 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:06<00:02, 16.06it/s, est. speed input: 15197.76 toks/s, output: 14.84 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:06<00:02, 16.13it/s, est. speed input: 15227.14 toks/s, output: 14.87 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:06<00:02, 16.18it/s, est. speed input: 15255.40 toks/s, output: 14.90 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:06<00:01, 16.13it/s, est. speed input: 15278.03 toks/s, output: 14.92 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:06<00:01, 16.06it/s, est. speed input: 15297.09 toks/s, output: 14.94 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:06<00:01, 16.04it/s, est. speed input: 15317.08 toks/s, output: 14.96 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:06<00:01, 15.72it/s, est. speed input: 15318.26 toks/s, output: 14.96 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 15.68it/s, est. speed input: 15330.16 toks/s, output: 14.97 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:07<00:01, 15.53it/s, est. speed input: 15334.41 toks/s, output: 14.97 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:07<00:01, 15.63it/s, est. speed input: 15350.58 toks/s, output: 14.99 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:07<00:01, 15.73it/s, est. speed input: 15367.57 toks/s, output: 15.01 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:07<00:01, 15.90it/s, est. speed input: 15389.39 toks/s, output: 15.03 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:07<00:00, 16.04it/s, est. speed input: 15411.71 toks/s, output: 15.05 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:07<00:00, 15.94it/s, est. speed input: 15423.13 toks/s, output: 15.06 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:07<00:00, 15.61it/s, est. speed input: 15420.22 toks/s, output: 15.06 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:07<00:00, 15.43it/s, est. speed input: 15419.36 toks/s, output: 15.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 15.43it/s, est. speed input: 19067.49 toks/s, output: 18.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 18.62it/s, est. speed input: 19067.49 toks/s, output: 18.62 toks/s]
[rank0]:[W125 22:53:49.681383941 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 22:53:53
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:54:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:54:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=506487) WARNING 01-25 22:54:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=506487) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=506487) WARNING 01-25 22:54:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 29.31 requests/s, 30044.51 total tokens/s, 29.31 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 22:54:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:54:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:54:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:54:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:54:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:54:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:54:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:54:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:54:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:54:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:54:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:54:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:54:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:54:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:54:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:54:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:54:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:54:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:54:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:54:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:54:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:54:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:54:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:54:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:54:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:54:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:54:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:54:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=506487) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=506487) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.11s/it]
(EngineCore_DP0 pid=506487) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.11s/it]
(EngineCore_DP0 pid=506487) 
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19005440 bytes
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11403264 bytes
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 60817408 bytes
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=506487) [2026-01-25 22:54:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30277632 bytes
(EngineCore_DP0 pid=506487) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  2.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:01<00:00,  2.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]
(EngineCore_DP0 pid=506487) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.90it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  12%|█▏        | 30/256 [00:00<00:00, 294.46it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 307.27it/s]
Adding requests:  37%|███▋      | 94/256 [00:00<00:00, 311.17it/s]
Adding requests:  49%|████▉     | 126/256 [00:00<00:00, 310.17it/s]
Adding requests:  62%|██████▏   | 158/256 [00:00<00:00, 307.27it/s]
Adding requests:  74%|███████▍  | 189/256 [00:00<00:00, 301.64it/s]
Adding requests:  86%|████████▌ | 220/256 [00:00<00:00, 302.92it/s]
Adding requests:  98%|█████████▊| 251/256 [00:00<00:00, 301.43it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 303.61it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:01, 144.17it/s, est. speed input: 147660.19 toks/s, output: 144.18 toks/s]
Processed prompts:  13%|█▎        | 33/256 [00:00<00:04, 49.96it/s, est. speed input: 57286.23 toks/s, output: 55.94 toks/s]   
Processed prompts:  16%|█▌        | 41/256 [00:00<00:05, 41.49it/s, est. speed input: 48724.27 toks/s, output: 47.58 toks/s]
Processed prompts:  18%|█▊        | 47/256 [00:01<00:05, 37.90it/s, est. speed input: 45251.85 toks/s, output: 44.19 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:05, 34.01it/s, est. speed input: 42136.38 toks/s, output: 41.15 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:01<00:06, 33.09it/s, est. speed input: 41034.10 toks/s, output: 40.07 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:01<00:06, 32.22it/s, est. speed input: 40080.63 toks/s, output: 39.14 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:06, 31.40it/s, est. speed input: 39232.09 toks/s, output: 38.31 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:06, 30.91it/s, est. speed input: 38560.76 toks/s, output: 37.66 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:06, 30.44it/s, est. speed input: 37954.80 toks/s, output: 37.06 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:02<00:05, 30.19it/s, est. speed input: 37454.23 toks/s, output: 36.58 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:02<00:05, 30.06it/s, est. speed input: 37027.82 toks/s, output: 36.16 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:02<00:05, 29.94it/s, est. speed input: 36644.06 toks/s, output: 35.78 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:02<00:05, 29.88it/s, est. speed input: 36307.54 toks/s, output: 35.46 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:02<00:05, 29.80it/s, est. speed input: 35999.09 toks/s, output: 35.16 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:05, 29.74it/s, est. speed input: 35720.07 toks/s, output: 34.88 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:05, 29.54it/s, est. speed input: 35437.09 toks/s, output: 34.61 toks/s]
Processed prompts:  41%|████      | 104/256 [00:03<00:05, 29.51it/s, est. speed input: 35200.07 toks/s, output: 34.37 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:03<00:05, 29.47it/s, est. speed input: 34978.28 toks/s, output: 34.16 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:03<00:04, 29.50it/s, est. speed input: 34786.31 toks/s, output: 33.97 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:03<00:04, 29.56it/s, est. speed input: 34614.50 toks/s, output: 33.80 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:03<00:04, 29.73it/s, est. speed input: 34474.19 toks/s, output: 33.67 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:03<00:04, 29.78it/s, est. speed input: 34334.56 toks/s, output: 33.53 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:03<00:04, 29.79it/s, est. speed input: 34201.58 toks/s, output: 33.40 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:03<00:04, 29.70it/s, est. speed input: 34064.16 toks/s, output: 33.27 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:04<00:04, 29.75it/s, est. speed input: 33950.66 toks/s, output: 33.15 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:04<00:03, 29.80it/s, est. speed input: 33846.40 toks/s, output: 33.05 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:04<00:03, 29.78it/s, est. speed input: 33741.66 toks/s, output: 32.95 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:04<00:03, 29.58it/s, est. speed input: 33621.86 toks/s, output: 32.83 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:04<00:03, 29.55it/s, est. speed input: 33521.66 toks/s, output: 32.74 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:04<00:03, 29.64it/s, est. speed input: 33439.16 toks/s, output: 32.66 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:04<00:03, 29.85it/s, est. speed input: 33375.97 toks/s, output: 32.59 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:05<00:03, 29.92it/s, est. speed input: 33308.39 toks/s, output: 32.53 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:05<00:02, 30.05it/s, est. speed input: 33251.22 toks/s, output: 32.47 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:05<00:02, 30.19it/s, est. speed input: 33201.73 toks/s, output: 32.42 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:05<00:02, 30.12it/s, est. speed input: 33139.63 toks/s, output: 32.36 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:05<00:02, 30.39it/s, est. speed input: 33108.46 toks/s, output: 32.33 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:05<00:02, 31.05it/s, est. speed input: 33116.78 toks/s, output: 32.34 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:05<00:02, 31.58it/s, est. speed input: 33128.38 toks/s, output: 32.35 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:05<00:02, 31.94it/s, est. speed input: 33138.05 toks/s, output: 32.36 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:06<00:00, 162.20it/s, est. speed input: 42272.88 toks/s, output: 41.28 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 162.20it/s, est. speed input: 41975.72 toks/s, output: 40.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 40.99it/s, est. speed input: 41975.72 toks/s, output: 40.99 toks/s] 
[rank0]:[W125 22:54:52.062389584 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 22:54:55
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:55:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:55:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=507891) WARNING 01-25 22:55:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=507891) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=507891) WARNING 01-25 22:55:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.51 requests/s, 36397.21 total tokens/s, 35.51 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 22:55:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:55:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:55:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:55:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:55:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:55:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:55:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:55:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:55:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:55:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:55:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:55:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:55:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:55:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:55:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:55:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:55:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:55:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:55:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:55:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:55:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:55:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:55:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:55:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:55:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:55:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:55:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:55:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=507891) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=507891) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.16s/it]
(EngineCore_DP0 pid=507891) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.16s/it]
(EngineCore_DP0 pid=507891) 
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19005440 bytes
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11403264 bytes
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 60817408 bytes
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=507891) [2026-01-25 22:55:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30277632 bytes
(EngineCore_DP0 pid=507891) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.94it/s]
(EngineCore_DP0 pid=507891) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.67it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.60it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 30/512 [00:00<00:01, 297.28it/s]
Adding requests:  12%|█▏        | 63/512 [00:00<00:01, 311.93it/s]
Adding requests:  19%|█▉        | 97/512 [00:00<00:01, 321.48it/s]
Adding requests:  26%|██▌       | 132/512 [00:00<00:01, 331.29it/s]
Adding requests:  32%|███▏      | 166/512 [00:00<00:01, 321.80it/s]
Adding requests:  39%|███▉      | 199/512 [00:00<00:01, 299.66it/s]
Adding requests:  45%|████▌     | 231/512 [00:00<00:00, 303.52it/s]
Adding requests:  51%|█████     | 262/512 [00:00<00:00, 300.78it/s]
Adding requests:  57%|█████▋    | 293/512 [00:00<00:00, 293.59it/s]
Adding requests:  63%|██████▎   | 323/512 [00:01<00:00, 293.10it/s]
Adding requests:  69%|██████▉   | 353/512 [00:01<00:00, 292.46it/s]
Adding requests:  75%|███████▍  | 383/512 [00:01<00:00, 289.61it/s]
Adding requests:  80%|████████  | 412/512 [00:01<00:00, 285.33it/s]
Adding requests:  86%|████████▌ | 441/512 [00:01<00:00, 272.24it/s]
Adding requests:  92%|█████████▏| 469/512 [00:01<00:00, 274.04it/s]
Adding requests:  97%|█████████▋| 497/512 [00:01<00:00, 274.86it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 292.10it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 54/512 [00:00<00:01, 424.40it/s, est. speed input: 434678.26 toks/s, output: 424.43 toks/s]
Processed prompts:  19%|█▉        | 97/512 [00:01<00:06, 66.96it/s, est. speed input: 79792.96 toks/s, output: 77.92 toks/s]   
Processed prompts:  23%|██▎       | 117/512 [00:01<00:07, 54.59it/s, est. speed input: 66459.82 toks/s, output: 64.90 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:02<00:03, 99.57it/s, est. speed input: 92629.88 toks/s, output: 90.46 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:02<00:04, 73.87it/s, est. speed input: 80401.89 toks/s, output: 78.52 toks/s]
Processed prompts:  42%|████▏     | 216/512 [00:02<00:04, 65.61it/s, est. speed input: 76035.39 toks/s, output: 74.25 toks/s]
Processed prompts:  44%|████▍     | 227/512 [00:03<00:05, 56.79it/s, est. speed input: 71622.70 toks/s, output: 69.94 toks/s]
Processed prompts:  46%|████▌     | 236/512 [00:03<00:05, 53.41it/s, est. speed input: 69654.41 toks/s, output: 68.02 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:03<00:05, 48.42it/s, est. speed input: 67368.71 toks/s, output: 65.79 toks/s]
Processed prompts:  49%|████▊     | 249/512 [00:03<00:05, 49.14it/s, est. speed input: 67002.15 toks/s, output: 65.43 toks/s]
Processed prompts:  50%|████▉     | 255/512 [00:04<00:05, 43.18it/s, est. speed input: 64913.34 toks/s, output: 63.39 toks/s]
Processed prompts:  51%|█████     | 260/512 [00:04<00:05, 44.00it/s, est. speed input: 64526.62 toks/s, output: 63.01 toks/s]
Processed prompts:  52%|█████▏    | 265/512 [00:04<00:05, 44.80it/s, est. speed input: 64158.37 toks/s, output: 62.65 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:04<00:06, 37.64it/s, est. speed input: 62315.14 toks/s, output: 60.85 toks/s]
Processed prompts:  54%|█████▎    | 275/512 [00:04<00:05, 39.73it/s, est. speed input: 62017.86 toks/s, output: 60.56 toks/s]
Processed prompts:  55%|█████▍    | 280/512 [00:04<00:05, 41.58it/s, est. speed input: 61732.33 toks/s, output: 60.29 toks/s]
Processed prompts:  56%|█████▌    | 285/512 [00:04<00:05, 43.16it/s, est. speed input: 61461.46 toks/s, output: 60.02 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:04<00:06, 35.37it/s, est. speed input: 59913.16 toks/s, output: 58.51 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:05<00:06, 36.08it/s, est. speed input: 59493.23 toks/s, output: 58.10 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:05<00:05, 36.67it/s, est. speed input: 59090.60 toks/s, output: 57.71 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:05<00:05, 37.16it/s, est. speed input: 58704.93 toks/s, output: 57.33 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:05<00:05, 37.55it/s, est. speed input: 58335.11 toks/s, output: 56.97 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:05<00:05, 37.81it/s, est. speed input: 57976.11 toks/s, output: 56.62 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:05<00:05, 38.01it/s, est. speed input: 57630.59 toks/s, output: 56.28 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:05<00:05, 38.19it/s, est. speed input: 57301.70 toks/s, output: 55.96 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:05<00:04, 38.30it/s, est. speed input: 56982.71 toks/s, output: 55.65 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:05<00:04, 38.37it/s, est. speed input: 56673.86 toks/s, output: 55.35 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:05<00:04, 38.42it/s, est. speed input: 56375.47 toks/s, output: 55.05 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:06<00:04, 38.48it/s, est. speed input: 56089.84 toks/s, output: 54.78 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:06<00:04, 38.51it/s, est. speed input: 55812.76 toks/s, output: 54.50 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:06<00:04, 38.53it/s, est. speed input: 55544.21 toks/s, output: 54.24 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:06<00:04, 38.54it/s, est. speed input: 55284.72 toks/s, output: 53.99 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:06<00:04, 38.56it/s, est. speed input: 55033.75 toks/s, output: 53.74 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:06<00:04, 38.57it/s, est. speed input: 54790.94 toks/s, output: 53.51 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:06<00:03, 38.57it/s, est. speed input: 54554.50 toks/s, output: 53.28 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:06<00:03, 37.96it/s, est. speed input: 54281.58 toks/s, output: 53.01 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:06<00:03, 37.20it/s, est. speed input: 53991.25 toks/s, output: 52.73 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:07<00:03, 36.69it/s, est. speed input: 53710.94 toks/s, output: 52.45 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:07<00:03, 36.31it/s, est. speed input: 53437.15 toks/s, output: 52.18 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:07<00:03, 36.12it/s, est. speed input: 53176.41 toks/s, output: 51.93 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:07<00:03, 35.96it/s, est. speed input: 52922.16 toks/s, output: 51.68 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:07<00:03, 35.87it/s, est. speed input: 52677.02 toks/s, output: 51.44 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:07<00:03, 35.79it/s, est. speed input: 52437.97 toks/s, output: 51.21 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:07<00:03, 35.73it/s, est. speed input: 52205.16 toks/s, output: 50.98 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:07<00:03, 35.69it/s, est. speed input: 51979.31 toks/s, output: 50.76 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:07<00:03, 35.67it/s, est. speed input: 51760.42 toks/s, output: 50.55 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:08<00:02, 35.65it/s, est. speed input: 51547.09 toks/s, output: 50.34 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:08<00:02, 35.60it/s, est. speed input: 51337.71 toks/s, output: 50.13 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:08<00:02, 35.60it/s, est. speed input: 51135.74 toks/s, output: 49.94 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:08<00:02, 35.57it/s, est. speed input: 50937.91 toks/s, output: 49.74 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:08<00:02, 35.60it/s, est. speed input: 50748.11 toks/s, output: 49.56 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:08<00:02, 35.58it/s, est. speed input: 50560.89 toks/s, output: 49.38 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:08<00:02, 35.57it/s, est. speed input: 50378.34 toks/s, output: 49.20 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:08<00:02, 35.58it/s, est. speed input: 50201.77 toks/s, output: 49.03 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:08<00:02, 35.59it/s, est. speed input: 50029.81 toks/s, output: 48.86 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:09<00:01, 35.58it/s, est. speed input: 49860.94 toks/s, output: 48.69 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:09<00:01, 35.57it/s, est. speed input: 49695.86 toks/s, output: 48.53 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:09<00:01, 35.58it/s, est. speed input: 49536.19 toks/s, output: 48.38 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:09<00:01, 35.58it/s, est. speed input: 49379.43 toks/s, output: 48.22 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:09<00:01, 35.60it/s, est. speed input: 49228.00 toks/s, output: 48.07 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:09<00:01, 35.58it/s, est. speed input: 49077.79 toks/s, output: 47.93 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:09<00:01, 35.58it/s, est. speed input: 48931.90 toks/s, output: 47.78 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:09<00:01, 35.59it/s, est. speed input: 48790.06 toks/s, output: 47.65 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:09<00:01, 35.60it/s, est. speed input: 48651.38 toks/s, output: 47.51 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:10<00:00, 35.58it/s, est. speed input: 48514.64 toks/s, output: 47.38 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:10<00:00, 35.56it/s, est. speed input: 48380.54 toks/s, output: 47.25 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:10<00:00, 35.57it/s, est. speed input: 48250.48 toks/s, output: 47.12 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:10<00:00, 35.54it/s, est. speed input: 48121.30 toks/s, output: 46.99 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:10<00:00, 35.55it/s, est. speed input: 47996.43 toks/s, output: 46.87 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:10<00:00, 35.52it/s, est. speed input: 47872.71 toks/s, output: 46.75 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:10<00:00, 35.55it/s, est. speed input: 47753.71 toks/s, output: 46.63 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:10<00:00, 35.55it/s, est. speed input: 47636.24 toks/s, output: 46.52 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 35.55it/s, est. speed input: 47795.38 toks/s, output: 46.68 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 46.67it/s, est. speed input: 47795.38 toks/s, output: 46.68 toks/s]
[rank0]:[W125 22:56:03.742797477 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 22:56:06
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:56:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:56:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=509556) WARNING 01-25 22:56:29 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=509556) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=509556) WARNING 01-25 22:56:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.50 requests/s, 36388.20 total tokens/s, 35.50 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-25 22:56:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:56:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:56:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:56:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:56:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:56:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:56:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:56:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:56:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:56:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:56:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:56:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:56:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:56:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:56:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:56:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:56:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:56:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:56:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:56:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:56:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:56:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:56:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:56:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:56:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:56:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:56:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:56:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:30] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:30] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:30] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:30] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=509556) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=509556) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.16s/it]
(EngineCore_DP0 pid=509556) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.16s/it]
(EngineCore_DP0 pid=509556) 
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19005440 bytes
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11403264 bytes
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 60817408 bytes
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=509556) [2026-01-25 22:56:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30277632 bytes
(EngineCore_DP0 pid=509556) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  3.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  5.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  6.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  5.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.99it/s]
(EngineCore_DP0 pid=509556) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  5.49it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  7.15it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  4.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  5.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  5.10it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 31/1024 [00:00<00:03, 304.54it/s]
Adding requests:   6%|▌         | 62/1024 [00:00<00:03, 305.57it/s]
Adding requests:   9%|▉         | 93/1024 [00:00<00:03, 300.11it/s]
Adding requests:  12%|█▏        | 126/1024 [00:00<00:02, 311.20it/s]
Adding requests:  15%|█▌        | 158/1024 [00:00<00:02, 304.21it/s]
Adding requests:  18%|█▊        | 189/1024 [00:00<00:02, 304.16it/s]
Adding requests:  22%|██▏       | 221/1024 [00:00<00:02, 308.26it/s]
Adding requests:  25%|██▍       | 254/1024 [00:00<00:02, 313.67it/s]
Adding requests:  28%|██▊       | 287/1024 [00:00<00:02, 316.97it/s]
Adding requests:  31%|███       | 319/1024 [00:01<00:02, 317.59it/s]
Adding requests:  34%|███▍      | 352/1024 [00:01<00:02, 320.26it/s]
Adding requests:  38%|███▊      | 385/1024 [00:01<00:01, 320.54it/s]
Adding requests:  41%|████      | 418/1024 [00:01<00:01, 316.48it/s]
Adding requests:  44%|████▍     | 450/1024 [00:01<00:01, 307.33it/s]
Adding requests:  47%|████▋     | 481/1024 [00:01<00:01, 298.97it/s]
Adding requests:  50%|████▉     | 511/1024 [00:01<00:01, 281.04it/s]
Adding requests:  53%|█████▎    | 540/1024 [00:01<00:01, 267.44it/s]
Adding requests:  56%|█████▌    | 570/1024 [00:01<00:01, 275.02it/s]
Adding requests:  58%|█████▊    | 599/1024 [00:02<00:01, 277.28it/s]
Adding requests:  61%|██████    | 627/1024 [00:02<00:01, 276.19it/s]
Adding requests:  64%|██████▍   | 657/1024 [00:02<00:01, 280.57it/s]
Adding requests:  67%|██████▋   | 688/1024 [00:02<00:01, 288.38it/s]
Adding requests:  70%|███████   | 719/1024 [00:02<00:01, 292.14it/s]
Adding requests:  73%|███████▎  | 749/1024 [00:02<00:00, 293.47it/s]
Adding requests:  76%|███████▌  | 779/1024 [00:02<00:00, 294.60it/s]
Adding requests:  79%|███████▉  | 809/1024 [00:02<00:00, 294.41it/s]
Adding requests:  82%|████████▏ | 839/1024 [00:02<00:00, 288.06it/s]
Adding requests:  85%|████████▍ | 870/1024 [00:02<00:00, 291.39it/s]
Adding requests:  88%|████████▊ | 901/1024 [00:03<00:00, 295.35it/s]
Adding requests:  91%|█████████ | 932/1024 [00:03<00:00, 297.56it/s]
Adding requests:  94%|█████████▍| 962/1024 [00:03<00:00, 297.45it/s]
Adding requests:  97%|█████████▋| 993/1024 [00:03<00:00, 299.79it/s]
Adding requests: 100%|█████████▉| 1023/1024 [00:03<00:00, 298.75it/s]
Adding requests: 100%|██████████| 1024/1024 [00:03<00:00, 297.35it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:00<00:01, 499.67it/s, est. speed input: 511737.96 toks/s, output: 499.69 toks/s]
Processed prompts:  16%|█▌        | 164/1024 [00:01<00:10, 84.96it/s, est. speed input: 105211.40 toks/s, output: 102.75 toks/s] 
Processed prompts:  18%|█▊        | 187/1024 [00:02<00:12, 64.94it/s, est. speed input: 83981.71 toks/s, output: 82.01 toks/s]  
Processed prompts:  20%|█▉        | 201/1024 [00:02<00:12, 64.38it/s, est. speed input: 82058.65 toks/s, output: 80.14 toks/s]
Processed prompts:  21%|██        | 212/1024 [00:02<00:15, 51.95it/s, est. speed input: 73249.34 toks/s, output: 71.53 toks/s]
Processed prompts:  21%|██▏       | 220/1024 [00:03<00:16, 48.89it/s, est. speed input: 70580.16 toks/s, output: 68.93 toks/s]
Processed prompts:  22%|██▏       | 227/1024 [00:03<00:17, 45.16it/s, est. speed input: 67970.85 toks/s, output: 66.38 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:18, 41.87it/s, est. speed input: 65685.22 toks/s, output: 64.15 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:03<00:19, 40.20it/s, est. speed input: 63930.66 toks/s, output: 62.43 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:19, 38.88it/s, est. speed input: 62374.62 toks/s, output: 60.91 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:20, 37.94it/s, est. speed input: 61009.93 toks/s, output: 59.58 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:19, 37.93it/s, est. speed input: 59978.11 toks/s, output: 58.57 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:19, 37.92it/s, est. speed input: 59038.76 toks/s, output: 57.65 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:04<00:19, 37.94it/s, est. speed input: 58183.55 toks/s, output: 56.82 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:19, 37.92it/s, est. speed input: 57392.39 toks/s, output: 56.05 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:19, 37.92it/s, est. speed input: 56665.26 toks/s, output: 55.34 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:18, 37.94it/s, est. speed input: 55995.44 toks/s, output: 54.68 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:18, 37.95it/s, est. speed input: 55375.63 toks/s, output: 54.08 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:06<00:18, 37.14it/s, est. speed input: 54655.85 toks/s, output: 53.37 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:19, 36.49it/s, est. speed input: 53971.13 toks/s, output: 52.71 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:19, 36.03it/s, est. speed input: 53331.74 toks/s, output: 52.08 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:18, 35.70it/s, est. speed input: 52734.91 toks/s, output: 51.50 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:18, 35.47it/s, est. speed input: 52175.46 toks/s, output: 50.95 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:07<00:18, 35.32it/s, est. speed input: 51653.56 toks/s, output: 50.44 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:07<00:18, 35.23it/s, est. speed input: 51166.09 toks/s, output: 49.97 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:07<00:18, 35.11it/s, est. speed input: 50700.12 toks/s, output: 49.51 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:18, 35.06it/s, est. speed input: 50265.62 toks/s, output: 49.09 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:08<00:17, 35.03it/s, est. speed input: 49856.57 toks/s, output: 48.69 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:08<00:17, 35.00it/s, est. speed input: 49469.51 toks/s, output: 48.31 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:08<00:17, 34.98it/s, est. speed input: 49102.66 toks/s, output: 47.95 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:08<00:17, 34.98it/s, est. speed input: 48756.26 toks/s, output: 47.61 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:09<00:17, 34.98it/s, est. speed input: 48427.44 toks/s, output: 47.29 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:09<00:16, 34.96it/s, est. speed input: 48113.70 toks/s, output: 46.99 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:09<00:16, 34.96it/s, est. speed input: 47815.60 toks/s, output: 46.69 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:09<00:16, 34.96it/s, est. speed input: 47532.06 toks/s, output: 46.42 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:09<00:16, 34.94it/s, est. speed input: 47259.54 toks/s, output: 46.15 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:10<00:15, 34.93it/s, est. speed input: 46999.49 toks/s, output: 45.90 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:10<00:15, 34.92it/s, est. speed input: 46750.49 toks/s, output: 45.65 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:10<00:15, 34.91it/s, est. speed input: 46512.19 toks/s, output: 45.42 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:10<00:15, 34.89it/s, est. speed input: 46283.29 toks/s, output: 45.20 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:11<00:15, 34.89it/s, est. speed input: 46064.73 toks/s, output: 44.98 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:11<00:14, 35.61it/s, est. speed input: 45917.26 toks/s, output: 44.84 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:11<00:14, 36.24it/s, est. speed input: 45784.45 toks/s, output: 44.71 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:11<00:13, 36.69it/s, est. speed input: 45655.96 toks/s, output: 44.59 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:11<00:13, 36.98it/s, est. speed input: 45530.65 toks/s, output: 44.46 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:12<00:13, 37.23it/s, est. speed input: 45411.62 toks/s, output: 44.35 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:12<00:12, 37.39it/s, est. speed input: 45296.60 toks/s, output: 44.23 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:12<00:12, 37.50it/s, est. speed input: 45184.79 toks/s, output: 44.13 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:12<00:12, 37.56it/s, est. speed input: 45074.86 toks/s, output: 44.02 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:12<00:12, 37.61it/s, est. speed input: 44969.92 toks/s, output: 43.92 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:13<00:11, 37.62it/s, est. speed input: 44866.86 toks/s, output: 43.81 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:13<00:11, 37.64it/s, est. speed input: 44767.37 toks/s, output: 43.72 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:13<00:11, 37.65it/s, est. speed input: 44670.62 toks/s, output: 43.62 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:13<00:11, 37.65it/s, est. speed input: 44576.78 toks/s, output: 43.53 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:14<00:11, 36.82it/s, est. speed input: 44435.07 toks/s, output: 43.39 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:14<00:11, 36.17it/s, est. speed input: 44292.12 toks/s, output: 43.25 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:14<00:11, 35.75it/s, est. speed input: 44155.14 toks/s, output: 43.12 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:14<00:11, 35.44it/s, est. speed input: 44020.86 toks/s, output: 42.99 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:14<00:10, 35.26it/s, est. speed input: 43893.01 toks/s, output: 42.86 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:15<00:10, 35.12it/s, est. speed input: 43768.19 toks/s, output: 42.74 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:15<00:10, 35.01it/s, est. speed input: 43646.28 toks/s, output: 42.62 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:15<00:10, 34.94it/s, est. speed input: 43528.03 toks/s, output: 42.51 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:15<00:10, 34.88it/s, est. speed input: 43413.15 toks/s, output: 42.40 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:16<00:09, 34.84it/s, est. speed input: 43301.34 toks/s, output: 42.29 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:16<00:09, 34.80it/s, est. speed input: 43191.98 toks/s, output: 42.18 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:16<00:09, 34.73it/s, est. speed input: 43083.71 toks/s, output: 42.07 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:16<00:09, 34.75it/s, est. speed input: 42981.28 toks/s, output: 41.97 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:17<00:08, 34.72it/s, est. speed input: 42879.52 toks/s, output: 41.87 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:17<00:08, 34.72it/s, est. speed input: 42781.88 toks/s, output: 41.78 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:17<00:08, 34.71it/s, est. speed input: 42686.47 toks/s, output: 41.69 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:17<00:08, 34.70it/s, est. speed input: 42593.34 toks/s, output: 41.59 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:17<00:08, 34.70it/s, est. speed input: 42502.37 toks/s, output: 41.51 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:18<00:07, 34.70it/s, est. speed input: 42413.90 toks/s, output: 41.42 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:18<00:07, 34.70it/s, est. speed input: 42328.27 toks/s, output: 41.34 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:18<00:07, 34.70it/s, est. speed input: 42244.18 toks/s, output: 41.25 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:18<00:06, 35.23it/s, est. speed input: 42187.88 toks/s, output: 41.20 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:19<00:06, 35.87it/s, est. speed input: 42145.15 toks/s, output: 41.16 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:19<00:06, 36.38it/s, est. speed input: 42105.36 toks/s, output: 41.12 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:19<00:06, 36.72it/s, est. speed input: 42065.46 toks/s, output: 41.08 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:19<00:05, 36.96it/s, est. speed input: 42026.27 toks/s, output: 41.04 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:19<00:05, 37.15it/s, est. speed input: 41988.36 toks/s, output: 41.00 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:20<00:05, 37.27it/s, est. speed input: 41951.11 toks/s, output: 40.97 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:20<00:05, 37.35it/s, est. speed input: 41914.27 toks/s, output: 40.93 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:20<00:04, 37.41it/s, est. speed input: 41878.48 toks/s, output: 40.90 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:20<00:04, 37.44it/s, est. speed input: 41843.10 toks/s, output: 40.86 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:21<00:04, 37.47it/s, est. speed input: 41808.88 toks/s, output: 40.83 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:21<00:04, 37.48it/s, est. speed input: 41774.54 toks/s, output: 40.80 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:21<00:00, 124.87it/s, est. speed input: 44766.59 toks/s, output: 43.72 toks/s]
Processed prompts:  93%|█████████▎| 951/1024 [00:21<00:00, 105.92it/s, est. speed input: 44939.39 toks/s, output: 43.89 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:22<00:00, 67.90it/s, est. speed input: 44544.73 toks/s, output: 43.50 toks/s] 
Processed prompts:  95%|█████████▍| 970/1024 [00:22<00:00, 59.33it/s, est. speed input: 44450.90 toks/s, output: 43.41 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:22<00:00, 52.70it/s, est. speed input: 44360.02 toks/s, output: 43.32 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:22<00:00, 47.70it/s, est. speed input: 44270.26 toks/s, output: 43.23 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:23<00:00, 44.01it/s, est. speed input: 44182.51 toks/s, output: 43.15 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:23<00:00, 41.28it/s, est. speed input: 44095.59 toks/s, output: 43.06 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:23<00:00, 39.36it/s, est. speed input: 44011.48 toks/s, output: 42.98 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:23<00:00, 39.08it/s, est. speed input: 43969.22 toks/s, output: 42.94 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:23<00:00, 39.08it/s, est. speed input: 44227.85 toks/s, output: 43.19 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:23<00:00, 43.19it/s, est. speed input: 44227.85 toks/s, output: 43.19 toks/s]
[rank0]:[W125 22:57:29.757462121 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-25 22:57:32
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:57:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 22:57:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=511184) WARNING 01-25 22:58:00 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=511184) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=511184) WARNING 01-25 22:58:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.36 requests/s, 36239.35 total tokens/s, 35.36 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-25 22:57:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:57:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:57:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:57:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:57:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:57:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:57:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:57:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:57:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:57:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:57:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:57:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:57:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:57:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:57:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:58:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:58:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:58:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:58:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:58:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:58:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:58:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:58:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:58:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:58:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:58:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:58:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:58:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=511184) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=511184) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.11s/it]
(EngineCore_DP0 pid=511184) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.11s/it]
(EngineCore_DP0 pid=511184) 
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19005440 bytes
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11403264 bytes
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 60817408 bytes
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=511184) [2026-01-25 22:58:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30277632 bytes
(EngineCore_DP0 pid=511184) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.07it/s]
(EngineCore_DP0 pid=511184) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  5.04it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  6.37it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  3.67it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  4.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:01<00:00,  5.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:01<00:00,  4.88it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/2048 [00:00<00:08, 246.81it/s]
Adding requests:   3%|▎         | 55/2048 [00:00<00:07, 274.73it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:06, 280.64it/s]
Adding requests:   6%|▌         | 114/2048 [00:00<00:06, 284.42it/s]
Adding requests:   7%|▋         | 143/2048 [00:00<00:06, 284.82it/s]
Adding requests:   8%|▊         | 172/2048 [00:00<00:06, 280.68it/s]
Adding requests:  10%|▉         | 203/2048 [00:00<00:06, 288.14it/s]
Adding requests:  11%|█▏        | 234/2048 [00:00<00:06, 292.74it/s]
Adding requests:  13%|█▎        | 264/2048 [00:00<00:06, 292.84it/s]
Adding requests:  14%|█▍        | 294/2048 [00:01<00:05, 294.57it/s]
Adding requests:  16%|█▌        | 325/2048 [00:01<00:05, 296.64it/s]
Adding requests:  17%|█▋        | 355/2048 [00:01<00:05, 289.88it/s]
Adding requests:  19%|█▉        | 385/2048 [00:01<00:05, 287.62it/s]
Adding requests:  20%|██        | 416/2048 [00:01<00:05, 292.55it/s]
Adding requests:  22%|██▏       | 446/2048 [00:01<00:05, 292.65it/s]
Adding requests:  23%|██▎       | 476/2048 [00:01<00:05, 293.46it/s]
Adding requests:  25%|██▍       | 506/2048 [00:01<00:05, 287.61it/s]
Adding requests:  26%|██▌       | 535/2048 [00:01<00:05, 281.46it/s]
Adding requests:  28%|██▊       | 566/2048 [00:01<00:05, 287.07it/s]
Adding requests:  29%|██▉       | 596/2048 [00:02<00:05, 288.10it/s]
Adding requests:  31%|███       | 628/2048 [00:02<00:04, 295.40it/s]
Adding requests:  32%|███▏      | 659/2048 [00:02<00:04, 297.22it/s]
Adding requests:  34%|███▍      | 692/2048 [00:02<00:04, 305.75it/s]
Adding requests:  35%|███▌      | 725/2048 [00:02<00:04, 310.48it/s]
Adding requests:  37%|███▋      | 757/2048 [00:02<00:04, 302.18it/s]
Adding requests:  38%|███▊      | 788/2048 [00:02<00:04, 299.04it/s]
Adding requests:  40%|███▉      | 818/2048 [00:02<00:04, 295.91it/s]
Adding requests:  41%|████▏     | 848/2048 [00:02<00:04, 284.47it/s]
Adding requests:  43%|████▎     | 878/2048 [00:03<00:04, 288.50it/s]
Adding requests:  44%|████▍     | 909/2048 [00:03<00:03, 292.78it/s]
Adding requests:  46%|████▌     | 940/2048 [00:03<00:03, 296.23it/s]
Adding requests:  47%|████▋     | 971/2048 [00:03<00:03, 299.24it/s]
Adding requests:  49%|████▉     | 1001/2048 [00:03<00:03, 297.69it/s]
Adding requests:  50%|█████     | 1032/2048 [00:03<00:03, 299.08it/s]
Adding requests:  52%|█████▏    | 1062/2048 [00:03<00:03, 294.13it/s]
Adding requests:  53%|█████▎    | 1092/2048 [00:03<00:03, 291.76it/s]
Adding requests:  55%|█████▍    | 1122/2048 [00:03<00:03, 286.68it/s]
Adding requests:  56%|█████▋    | 1152/2048 [00:03<00:03, 289.44it/s]
Adding requests:  58%|█████▊    | 1181/2048 [00:04<00:02, 289.36it/s]
Adding requests:  59%|█████▉    | 1212/2048 [00:04<00:02, 294.19it/s]
Adding requests:  61%|██████    | 1242/2048 [00:04<00:02, 289.39it/s]
Adding requests:  62%|██████▏   | 1271/2048 [00:04<00:02, 287.74it/s]
Adding requests:  63%|██████▎   | 1300/2048 [00:04<00:02, 286.00it/s]
Adding requests:  65%|██████▍   | 1330/2048 [00:04<00:02, 287.48it/s]
Adding requests:  66%|██████▋   | 1359/2048 [00:04<00:02, 285.63it/s]
Adding requests:  68%|██████▊   | 1389/2048 [00:04<00:02, 289.17it/s]
Adding requests:  69%|██████▉   | 1419/2048 [00:04<00:02, 289.56it/s]
Adding requests:  71%|███████   | 1448/2048 [00:04<00:02, 282.81it/s]
Adding requests:  72%|███████▏  | 1477/2048 [00:05<00:02, 283.20it/s]
Adding requests:  74%|███████▎  | 1507/2048 [00:05<00:01, 287.43it/s]
Adding requests:  75%|███████▌  | 1537/2048 [00:05<00:01, 290.79it/s]
Adding requests:  77%|███████▋  | 1569/2048 [00:05<00:01, 297.41it/s]
Adding requests:  78%|███████▊  | 1604/2048 [00:05<00:01, 312.16it/s]
Adding requests:  80%|███████▉  | 1638/2048 [00:05<00:01, 318.80it/s]
Adding requests:  82%|████████▏ | 1672/2048 [00:05<00:01, 322.90it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:05<00:01, 327.03it/s]
Adding requests:  85%|████████▍ | 1740/2048 [00:05<00:00, 329.34it/s]
Adding requests:  87%|████████▋ | 1774/2048 [00:05<00:00, 330.37it/s]
Adding requests:  88%|████████▊ | 1808/2048 [00:06<00:00, 329.20it/s]
Adding requests:  90%|████████▉ | 1841/2048 [00:06<00:00, 325.84it/s]
Adding requests:  92%|█████████▏| 1874/2048 [00:06<00:00, 322.94it/s]
Adding requests:  93%|█████████▎| 1907/2048 [00:06<00:00, 324.90it/s]
Adding requests:  95%|█████████▍| 1940/2048 [00:06<00:00, 320.41it/s]
Adding requests:  96%|█████████▋| 1973/2048 [00:06<00:00, 318.98it/s]
Adding requests:  98%|█████████▊| 2006/2048 [00:06<00:00, 320.24it/s]
Adding requests: 100%|█████████▉| 2039/2048 [00:06<00:00, 302.27it/s]
Adding requests: 100%|██████████| 2048/2048 [00:06<00:00, 297.96it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:00<00:01, 1368.28it/s, est. speed input: 1401349.01 toks/s, output: 1368.35 toks/s]
Processed prompts:  18%|█▊        | 363/2048 [00:03<00:21, 78.84it/s, est. speed input: 97974.57 toks/s, output: 95.68 toks/s]      
Processed prompts:  21%|██        | 422/2048 [00:05<00:27, 59.47it/s, est. speed input: 76619.89 toks/s, output: 74.82 toks/s]
Processed prompts:  22%|██▏       | 456/2048 [00:06<00:29, 54.69it/s, est. speed input: 71457.68 toks/s, output: 69.78 toks/s]
Processed prompts:  23%|██▎       | 478/2048 [00:06<00:28, 54.27it/s, est. speed input: 70310.30 toks/s, output: 68.66 toks/s]
Processed prompts:  24%|██▍       | 494/2048 [00:07<00:30, 51.48it/s, est. speed input: 68400.72 toks/s, output: 66.80 toks/s]
Processed prompts:  25%|██▍       | 506/2048 [00:07<00:32, 47.17it/s, est. speed input: 66235.45 toks/s, output: 64.68 toks/s]
Processed prompts:  25%|██▌       | 515/2048 [00:08<00:36, 41.74it/s, est. speed input: 63923.03 toks/s, output: 62.42 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:08<00:37, 40.21it/s, est. speed input: 62545.71 toks/s, output: 61.08 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:09<00:38, 39.07it/s, est. speed input: 61272.22 toks/s, output: 59.84 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:09<00:39, 37.82it/s, est. speed input: 60019.40 toks/s, output: 58.61 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:10<00:39, 36.90it/s, est. speed input: 58883.63 toks/s, output: 57.50 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:40, 36.21it/s, est. speed input: 57844.97 toks/s, output: 56.49 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:10<00:40, 35.72it/s, est. speed input: 56894.10 toks/s, output: 55.56 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:11<00:40, 35.36it/s, est. speed input: 56019.66 toks/s, output: 54.71 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:11<00:40, 35.10it/s, est. speed input: 55213.44 toks/s, output: 53.92 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:12<00:39, 34.92it/s, est. speed input: 54466.76 toks/s, output: 53.19 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:39, 34.81it/s, est. speed input: 53777.91 toks/s, output: 52.52 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:13<00:39, 34.68it/s, est. speed input: 53128.72 toks/s, output: 51.88 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:13<00:38, 34.61it/s, est. speed input: 52526.12 toks/s, output: 51.29 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:14<00:37, 35.14it/s, est. speed input: 52056.69 toks/s, output: 50.84 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:14<00:36, 35.77it/s, est. speed input: 51651.42 toks/s, output: 50.44 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:15<00:35, 36.22it/s, est. speed input: 51268.82 toks/s, output: 50.07 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:15<00:34, 36.53it/s, est. speed input: 50906.20 toks/s, output: 49.71 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:15<00:34, 36.75it/s, est. speed input: 50562.98 toks/s, output: 49.38 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:16<00:33, 36.92it/s, est. speed input: 50239.01 toks/s, output: 49.06 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:16<00:33, 37.03it/s, est. speed input: 49930.39 toks/s, output: 48.76 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:17<00:33, 36.43it/s, est. speed input: 49560.74 toks/s, output: 48.40 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:17<00:33, 35.79it/s, est. speed input: 49183.68 toks/s, output: 48.03 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:18<00:33, 35.40it/s, est. speed input: 48829.86 toks/s, output: 47.69 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:18<00:33, 35.06it/s, est. speed input: 48486.95 toks/s, output: 47.35 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:19<00:32, 34.85it/s, est. speed input: 48162.92 toks/s, output: 47.03 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:19<00:32, 34.71it/s, est. speed input: 47854.39 toks/s, output: 46.73 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:20<00:32, 34.61it/s, est. speed input: 47560.84 toks/s, output: 46.45 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:20<00:31, 34.55it/s, est. speed input: 47280.71 toks/s, output: 46.17 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:20<00:12, 80.46it/s, est. speed input: 50042.89 toks/s, output: 48.87 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:21<00:15, 64.56it/s, est. speed input: 49717.22 toks/s, output: 48.55 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:21<00:18, 54.60it/s, est. speed input: 49405.07 toks/s, output: 48.25 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:22<00:20, 48.12it/s, est. speed input: 49106.03 toks/s, output: 47.96 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:22<00:22, 43.97it/s, est. speed input: 48831.20 toks/s, output: 47.69 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:22<00:22, 42.01it/s, est. speed input: 48632.14 toks/s, output: 47.49 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:23<00:23, 40.62it/s, est. speed input: 48440.48 toks/s, output: 47.31 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:23<00:23, 39.65it/s, est. speed input: 48257.40 toks/s, output: 47.13 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:24<00:23, 38.94it/s, est. speed input: 48078.26 toks/s, output: 46.95 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:24<00:22, 39.01it/s, est. speed input: 47946.28 toks/s, output: 46.82 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:25<00:23, 38.14it/s, est. speed input: 47754.35 toks/s, output: 46.64 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:25<00:23, 36.98it/s, est. speed input: 47528.38 toks/s, output: 46.41 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:26<00:23, 36.21it/s, est. speed input: 47311.06 toks/s, output: 46.20 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:26<00:23, 35.68it/s, est. speed input: 47101.16 toks/s, output: 46.00 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:26<00:23, 35.33it/s, est. speed input: 46898.81 toks/s, output: 45.80 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:27<00:22, 35.08it/s, est. speed input: 46703.24 toks/s, output: 45.61 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:27<00:22, 34.91it/s, est. speed input: 46514.26 toks/s, output: 45.42 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:28<00:22, 34.79it/s, est. speed input: 46331.63 toks/s, output: 45.25 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:28<00:21, 34.72it/s, est. speed input: 46155.26 toks/s, output: 45.07 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:29<00:21, 34.66it/s, est. speed input: 45984.27 toks/s, output: 44.91 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:29<00:20, 34.62it/s, est. speed input: 45818.44 toks/s, output: 44.74 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:30<00:20, 34.59it/s, est. speed input: 45657.83 toks/s, output: 44.59 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:30<00:19, 34.76it/s, est. speed input: 45514.46 toks/s, output: 44.45 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:31<00:18, 35.51it/s, est. speed input: 45415.02 toks/s, output: 44.35 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:31<00:18, 36.06it/s, est. speed input: 45318.27 toks/s, output: 44.26 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:31<00:17, 36.44it/s, est. speed input: 45223.97 toks/s, output: 44.16 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:32<00:16, 36.73it/s, est. speed input: 45132.56 toks/s, output: 44.07 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:32<00:16, 36.93it/s, est. speed input: 45043.41 toks/s, output: 43.99 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:33<00:16, 36.47it/s, est. speed input: 44924.42 toks/s, output: 43.87 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:33<00:16, 35.86it/s, est. speed input: 44792.87 toks/s, output: 43.74 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:34<00:15, 35.44it/s, est. speed input: 44664.38 toks/s, output: 43.62 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:34<00:15, 35.16it/s, est. speed input: 44539.67 toks/s, output: 43.50 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:35<00:15, 34.97it/s, est. speed input: 44418.31 toks/s, output: 43.38 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:35<00:14, 34.83it/s, est. speed input: 44299.70 toks/s, output: 43.26 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:36<00:14, 34.73it/s, est. speed input: 44184.15 toks/s, output: 43.15 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:36<00:13, 34.66it/s, est. speed input: 44071.78 toks/s, output: 43.04 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:36<00:13, 34.62it/s, est. speed input: 43962.27 toks/s, output: 42.93 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:37<00:12, 34.59it/s, est. speed input: 43855.36 toks/s, output: 42.83 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:37<00:12, 34.57it/s, est. speed input: 43751.26 toks/s, output: 42.73 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:38<00:11, 34.79it/s, est. speed input: 43661.71 toks/s, output: 42.64 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:38<00:11, 35.52it/s, est. speed input: 43601.94 toks/s, output: 42.58 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:39<00:10, 36.07it/s, est. speed input: 43544.05 toks/s, output: 42.52 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:39<00:10, 36.45it/s, est. speed input: 43487.07 toks/s, output: 42.47 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:40<00:09, 36.72it/s, est. speed input: 43431.02 toks/s, output: 42.41 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:40<00:09, 36.90it/s, est. speed input: 43376.08 toks/s, output: 42.36 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:40<00:08, 37.04it/s, est. speed input: 43322.54 toks/s, output: 42.31 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:41<00:08, 36.40it/s, est. speed input: 43239.47 toks/s, output: 42.23 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:41<00:07, 35.81it/s, est. speed input: 43152.07 toks/s, output: 42.14 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:42<00:07, 35.41it/s, est. speed input: 43066.32 toks/s, output: 42.06 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:42<00:07, 35.14it/s, est. speed input: 42982.59 toks/s, output: 41.98 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:43<00:06, 34.95it/s, est. speed input: 42900.72 toks/s, output: 41.90 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:43<00:06, 34.82it/s, est. speed input: 42820.52 toks/s, output: 41.82 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:44<00:05, 34.73it/s, est. speed input: 42742.05 toks/s, output: 41.74 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:44<00:05, 34.67it/s, est. speed input: 42665.36 toks/s, output: 41.67 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:45<00:04, 35.15it/s, est. speed input: 42611.87 toks/s, output: 41.61 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:45<00:04, 34.96it/s, est. speed input: 42537.92 toks/s, output: 41.54 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:45<00:04, 35.40it/s, est. speed input: 42488.41 toks/s, output: 41.49 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:46<00:03, 35.97it/s, est. speed input: 42449.31 toks/s, output: 41.45 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:46<00:03, 36.37it/s, est. speed input: 42410.64 toks/s, output: 41.42 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:47<00:02, 36.65it/s, est. speed input: 42372.80 toks/s, output: 41.38 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:47<00:02, 36.87it/s, est. speed input: 42335.98 toks/s, output: 41.34 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:48<00:01, 37.02it/s, est. speed input: 42299.70 toks/s, output: 41.31 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:48<00:01, 37.12it/s, est. speed input: 42264.07 toks/s, output: 41.27 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:48<00:00, 37.20it/s, est. speed input: 42229.10 toks/s, output: 41.24 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:49<00:00, 37.09it/s, est. speed input: 42189.60 toks/s, output: 41.20 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:49<00:00, 37.09it/s, est. speed input: 42479.62 toks/s, output: 41.48 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:49<00:00, 41.48it/s, est. speed input: 42479.62 toks/s, output: 41.48 toks/s]
[rank0]:[W125 22:59:28.761673379 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-25 22:59:29
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:00:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 23:00:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=513305) WARNING 01-25 23:00:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=513305) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=513305) WARNING 01-25 23:00:25 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.38 requests/s, 36265.11 total tokens/s, 35.38 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-25 23:00:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:00:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 23:00:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 23:00:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:00:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:00:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:00:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:00:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:00:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 23:00:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:00:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:00:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:00:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:00:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:00:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:00:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 23:00:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 23:00:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:00:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:00:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:00:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:00:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:00:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 23:00:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:00:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:00:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:00:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:00:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=513305) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=513305) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.05s/it]
(EngineCore_DP0 pid=513305) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.05s/it]
(EngineCore_DP0 pid=513305) 
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19005440 bytes
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11403264 bytes
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 60817408 bytes
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=513305) [2026-01-25 23:00:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30277632 bytes
(EngineCore_DP0 pid=513305) [rank0]:W0125 23:00:32.237000 513305 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=513305) [rank0]:W0125 23:00:32.364000 513305 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=513305) [rank0]:W0125 23:00:33.972000 513305 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=513305) [rank0]:W0125 23:00:34.154000 513305 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=513305) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  7.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  5.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  3.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:02<00:00,  3.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:02<00:00,  5.35it/s]
(EngineCore_DP0 pid=513305) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:01,  3.04it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:01,  3.48it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  4.75it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  5.61it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  6.41it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:01<00:00,  7.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  7.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  5.91it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 27/4096 [00:00<00:15, 263.62it/s]
Adding requests:   1%|▏         | 57/4096 [00:00<00:14, 280.89it/s]
Adding requests:   2%|▏         | 88/4096 [00:00<00:13, 290.86it/s]
Adding requests:   3%|▎         | 119/4096 [00:00<00:13, 294.80it/s]
Adding requests:   4%|▎         | 149/4096 [00:00<00:13, 294.31it/s]
Adding requests:   4%|▍         | 179/4096 [00:00<00:13, 291.77it/s]
Adding requests:   5%|▌         | 210/4096 [00:00<00:13, 295.65it/s]
Adding requests:   6%|▌         | 244/4096 [00:00<00:12, 308.17it/s]
Adding requests:   7%|▋         | 277/4096 [00:00<00:12, 313.55it/s]
Adding requests:   8%|▊         | 310/4096 [00:01<00:11, 316.15it/s]
Adding requests:   8%|▊         | 342/4096 [00:01<00:11, 317.06it/s]
Adding requests:   9%|▉         | 376/4096 [00:01<00:11, 323.87it/s]
Adding requests:  10%|█         | 410/4096 [00:01<00:11, 326.08it/s]
Adding requests:  11%|█         | 443/4096 [00:01<00:11, 322.64it/s]
Adding requests:  12%|█▏        | 477/4096 [00:01<00:11, 326.36it/s]
Adding requests:  12%|█▏        | 510/4096 [00:01<00:11, 321.96it/s]
Adding requests:  13%|█▎        | 543/4096 [00:01<00:11, 316.98it/s]
Adding requests:  14%|█▍        | 577/4096 [00:01<00:10, 322.63it/s]
Adding requests:  15%|█▍        | 610/4096 [00:01<00:10, 324.23it/s]
Adding requests:  16%|█▌        | 643/4096 [00:02<00:10, 321.05it/s]
Adding requests:  17%|█▋        | 676/4096 [00:02<00:10, 317.02it/s]
Adding requests:  17%|█▋        | 709/4096 [00:02<00:10, 319.09it/s]
Adding requests:  18%|█▊        | 741/4096 [00:02<00:10, 316.18it/s]
Adding requests:  19%|█▉        | 773/4096 [00:02<00:10, 311.98it/s]
Adding requests:  20%|█▉        | 805/4096 [00:02<00:10, 308.57it/s]
Adding requests:  20%|██        | 836/4096 [00:02<00:10, 304.63it/s]
Adding requests:  21%|██        | 867/4096 [00:02<00:10, 303.11it/s]
Adding requests:  22%|██▏       | 898/4096 [00:02<00:10, 305.08it/s]
Adding requests:  23%|██▎       | 929/4096 [00:02<00:10, 303.91it/s]
Adding requests:  23%|██▎       | 960/4096 [00:03<00:10, 299.38it/s]
Adding requests:  24%|██▍       | 990/4096 [00:03<00:10, 298.37it/s]
Adding requests:  25%|██▍       | 1020/4096 [00:03<00:10, 294.38it/s]
Adding requests:  26%|██▌       | 1050/4096 [00:03<00:10, 288.56it/s]
Adding requests:  26%|██▋       | 1079/4096 [00:03<00:10, 284.76it/s]
Adding requests:  27%|██▋       | 1108/4096 [00:03<00:10, 283.41it/s]
Adding requests:  28%|██▊       | 1137/4096 [00:03<00:10, 283.18it/s]
Adding requests:  28%|██▊       | 1167/4096 [00:03<00:10, 287.17it/s]
Adding requests:  29%|██▉       | 1196/4096 [00:03<00:10, 280.20it/s]
Adding requests:  30%|██▉       | 1228/4096 [00:04<00:09, 291.36it/s]
Adding requests:  31%|███       | 1258/4096 [00:04<00:09, 293.31it/s]
Adding requests:  31%|███▏      | 1289/4096 [00:04<00:09, 295.75it/s]
Adding requests:  32%|███▏      | 1320/4096 [00:04<00:09, 299.07it/s]
Adding requests:  33%|███▎      | 1351/4096 [00:04<00:09, 300.48it/s]
Adding requests:  34%|███▍      | 1383/4096 [00:04<00:08, 304.16it/s]
Adding requests:  35%|███▍      | 1415/4096 [00:04<00:08, 307.32it/s]
Adding requests:  35%|███▌      | 1446/4096 [00:04<00:08, 307.46it/s]
Adding requests:  36%|███▌      | 1478/4096 [00:04<00:08, 310.13it/s]
Adding requests:  37%|███▋      | 1510/4096 [00:04<00:08, 309.00it/s]
Adding requests:  38%|███▊      | 1542/4096 [00:05<00:08, 310.68it/s]
Adding requests:  38%|███▊      | 1574/4096 [00:05<00:08, 309.77it/s]
Adding requests:  39%|███▉      | 1605/4096 [00:05<00:08, 302.68it/s]
Adding requests:  40%|███▉      | 1636/4096 [00:05<00:08, 301.23it/s]
Adding requests:  41%|████      | 1667/4096 [00:05<00:08, 294.81it/s]
Adding requests:  41%|████▏     | 1697/4096 [00:05<00:08, 291.42it/s]
Adding requests:  42%|████▏     | 1728/4096 [00:05<00:08, 295.98it/s]
Adding requests:  43%|████▎     | 1758/4096 [00:05<00:07, 295.21it/s]
Adding requests:  44%|████▎     | 1788/4096 [00:05<00:07, 293.17it/s]
Adding requests:  44%|████▍     | 1818/4096 [00:05<00:07, 291.69it/s]
Adding requests:  45%|████▌     | 1848/4096 [00:06<00:07, 290.66it/s]
Adding requests:  46%|████▌     | 1878/4096 [00:06<00:07, 290.66it/s]
Adding requests:  47%|████▋     | 1908/4096 [00:06<00:07, 292.34it/s]
Adding requests:  47%|████▋     | 1939/4096 [00:06<00:07, 295.87it/s]
Adding requests:  48%|████▊     | 1969/4096 [00:06<00:07, 288.82it/s]
Adding requests:  49%|████▉     | 2000/4096 [00:06<00:07, 293.59it/s]
Adding requests:  50%|████▉     | 2031/4096 [00:06<00:06, 296.30it/s]
Adding requests:  50%|█████     | 2062/4096 [00:06<00:06, 297.76it/s]
Adding requests:  51%|█████     | 2092/4096 [00:06<00:06, 296.94it/s]
Adding requests:  52%|█████▏    | 2122/4096 [00:07<00:06, 289.94it/s]
Adding requests:  53%|█████▎    | 2152/4096 [00:07<00:06, 290.51it/s]
Adding requests:  53%|█████▎    | 2182/4096 [00:07<00:06, 287.55it/s]
Adding requests:  54%|█████▍    | 2213/4096 [00:07<00:06, 292.61it/s]
Adding requests:  55%|█████▍    | 2245/4096 [00:07<00:06, 299.96it/s]
Adding requests:  56%|█████▌    | 2276/4096 [00:07<00:06, 297.06it/s]
Adding requests:  56%|█████▋    | 2306/4096 [00:07<00:06, 297.06it/s]
Adding requests:  57%|█████▋    | 2336/4096 [00:07<00:05, 293.71it/s]
Adding requests:  58%|█████▊    | 2367/4096 [00:07<00:05, 295.93it/s]
Adding requests:  59%|█████▊    | 2397/4096 [00:07<00:05, 293.60it/s]
Adding requests:  59%|█████▉    | 2427/4096 [00:08<00:05, 285.53it/s]
Adding requests:  60%|██████    | 2461/4096 [00:08<00:05, 299.36it/s]
Adding requests:  61%|██████    | 2493/4096 [00:08<00:05, 304.51it/s]
Adding requests:  62%|██████▏   | 2526/4096 [00:08<00:05, 309.91it/s]
Adding requests:  62%|██████▏   | 2558/4096 [00:08<00:04, 309.90it/s]
Adding requests:  63%|██████▎   | 2591/4096 [00:08<00:04, 314.68it/s]
Adding requests:  64%|██████▍   | 2623/4096 [00:08<00:04, 314.87it/s]
Adding requests:  65%|██████▍   | 2655/4096 [00:08<00:04, 314.10it/s]
Adding requests:  66%|██████▌   | 2687/4096 [00:08<00:04, 311.74it/s]
Adding requests:  66%|██████▋   | 2719/4096 [00:08<00:04, 312.62it/s]
Adding requests:  67%|██████▋   | 2751/4096 [00:09<00:04, 314.51it/s]
Adding requests:  68%|██████▊   | 2783/4096 [00:09<00:04, 315.19it/s]
Adding requests:  69%|██████▊   | 2815/4096 [00:09<00:04, 313.73it/s]
Adding requests:  70%|██████▉   | 2847/4096 [00:09<00:03, 313.79it/s]
Adding requests:  70%|███████   | 2879/4096 [00:09<00:03, 311.84it/s]
Adding requests:  71%|███████   | 2912/4096 [00:09<00:03, 314.38it/s]
Adding requests:  72%|███████▏  | 2944/4096 [00:09<00:03, 306.91it/s]
Adding requests:  73%|███████▎  | 2976/4096 [00:09<00:03, 310.69it/s]
Adding requests:  73%|███████▎  | 3009/4096 [00:09<00:03, 314.10it/s]
Adding requests:  74%|███████▍  | 3041/4096 [00:10<00:03, 310.91it/s]
Adding requests:  75%|███████▌  | 3073/4096 [00:10<00:03, 308.65it/s]
Adding requests:  76%|███████▌  | 3105/4096 [00:10<00:03, 310.06it/s]
Adding requests:  77%|███████▋  | 3137/4096 [00:10<00:03, 310.39it/s]
Adding requests:  77%|███████▋  | 3169/4096 [00:10<00:02, 311.11it/s]
Adding requests:  78%|███████▊  | 3201/4096 [00:10<00:02, 310.10it/s]
Adding requests:  79%|███████▉  | 3236/4096 [00:10<00:02, 319.32it/s]
Adding requests:  80%|███████▉  | 3268/4096 [00:10<00:02, 318.42it/s]
Adding requests:  81%|████████  | 3300/4096 [00:10<00:02, 318.47it/s]
Adding requests:  81%|████████▏ | 3332/4096 [00:10<00:02, 314.71it/s]
Adding requests:  82%|████████▏ | 3365/4096 [00:11<00:02, 318.29it/s]
Adding requests:  83%|████████▎ | 3397/4096 [00:11<00:02, 315.61it/s]
Adding requests:  84%|████████▎ | 3429/4096 [00:11<00:02, 315.50it/s]
Adding requests:  84%|████████▍ | 3461/4096 [00:11<00:02, 305.40it/s]
Adding requests:  85%|████████▌ | 3492/4096 [00:11<00:02, 298.82it/s]
Adding requests:  86%|████████▌ | 3522/4096 [00:11<00:01, 294.12it/s]
Adding requests:  87%|████████▋ | 3552/4096 [00:11<00:01, 291.62it/s]
Adding requests:  87%|████████▋ | 3582/4096 [00:11<00:01, 291.08it/s]
Adding requests:  88%|████████▊ | 3612/4096 [00:11<00:01, 291.81it/s]
Adding requests:  89%|████████▉ | 3642/4096 [00:12<00:01, 291.41it/s]
Adding requests:  90%|████████▉ | 3672/4096 [00:12<00:01, 290.66it/s]
Adding requests:  90%|█████████ | 3702/4096 [00:12<00:01, 287.82it/s]
Adding requests:  91%|█████████ | 3731/4096 [00:12<00:01, 287.34it/s]
Adding requests:  92%|█████████▏| 3761/4096 [00:12<00:01, 290.14it/s]
Adding requests:  93%|█████████▎| 3791/4096 [00:12<00:01, 279.83it/s]
Adding requests:  93%|█████████▎| 3820/4096 [00:12<00:00, 281.26it/s]
Adding requests:  94%|█████████▍| 3850/4096 [00:12<00:00, 285.53it/s]
Adding requests:  95%|█████████▍| 3880/4096 [00:12<00:00, 289.06it/s]
Adding requests:  95%|█████████▌| 3909/4096 [00:12<00:00, 288.54it/s]
Adding requests:  96%|█████████▌| 3938/4096 [00:13<00:00, 287.93it/s]
Adding requests:  97%|█████████▋| 3968/4096 [00:13<00:00, 289.35it/s]
Adding requests:  98%|█████████▊| 3997/4096 [00:13<00:00, 283.70it/s]
Adding requests:  98%|█████████▊| 4027/4096 [00:13<00:00, 286.14it/s]
Adding requests:  99%|█████████▉| 4056/4096 [00:13<00:00, 282.32it/s]
Adding requests: 100%|█████████▉| 4088/4096 [00:13<00:00, 290.97it/s]
Adding requests: 100%|██████████| 4096/4096 [00:13<00:00, 301.51it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:00<00:04, 866.09it/s, est. speed input: 886933.44 toks/s, output: 866.10 toks/s]
Processed prompts:  13%|█▎        | 537/4096 [00:02<00:19, 183.65it/s, est. speed input: 234506.64 toks/s, output: 229.01 toks/s]
Processed prompts:  14%|█▍        | 576/4096 [00:03<00:26, 134.46it/s, est. speed input: 184586.61 toks/s, output: 180.26 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:04<00:31, 111.25it/s, est. speed input: 158924.61 toks/s, output: 155.20 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:04<00:38, 87.81it/s, est. speed input: 138348.95 toks/s, output: 135.11 toks/s] 
Processed prompts:  17%|█▋        | 706/4096 [00:05<00:48, 70.44it/s, est. speed input: 122419.78 toks/s, output: 119.55 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:06<00:56, 58.93it/s, est. speed input: 110645.56 toks/s, output: 108.05 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:07<01:04, 51.27it/s, est. speed input: 101672.10 toks/s, output: 99.29 toks/s] 
Processed prompts:  20%|█▉        | 802/4096 [00:08<01:11, 46.10it/s, est. speed input: 94608.97 toks/s, output: 92.39 toks/s] 
Processed prompts:  20%|██        | 834/4096 [00:09<01:16, 42.56it/s, est. speed input: 88908.38 toks/s, output: 86.82 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:10<01:20, 40.18it/s, est. speed input: 84238.80 toks/s, output: 82.26 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:11<01:21, 39.40it/s, est. speed input: 80794.08 toks/s, output: 78.90 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:12<01:21, 38.83it/s, est. speed input: 77829.15 toks/s, output: 76.00 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:13<01:21, 38.42it/s, est. speed input: 75249.55 toks/s, output: 73.49 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:14<01:23, 37.22it/s, est. speed input: 72620.77 toks/s, output: 70.92 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:14<01:24, 36.39it/s, est. speed input: 70309.49 toks/s, output: 68.66 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:15<01:24, 35.82it/s, est. speed input: 68268.02 toks/s, output: 66.67 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:16<01:24, 35.42it/s, est. speed input: 66449.52 toks/s, output: 64.89 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:17<01:24, 35.15it/s, est. speed input: 64823.71 toks/s, output: 63.30 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:18<01:23, 35.28it/s, est. speed input: 63452.78 toks/s, output: 61.97 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:19<01:21, 35.89it/s, est. speed input: 62348.51 toks/s, output: 60.89 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:20<01:19, 36.33it/s, est. speed input: 61335.94 toks/s, output: 59.90 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:21<01:17, 36.60it/s, est. speed input: 60397.52 toks/s, output: 58.98 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:22<01:18, 35.95it/s, est. speed input: 59347.09 toks/s, output: 57.96 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:23<01:18, 35.51it/s, est. speed input: 58382.28 toks/s, output: 57.01 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:23<01:18, 35.22it/s, est. speed input: 57493.89 toks/s, output: 56.15 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:24<01:17, 35.02it/s, est. speed input: 56672.26 toks/s, output: 55.34 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:25<01:17, 34.88it/s, est. speed input: 55909.32 toks/s, output: 54.60 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:26<01:15, 35.15it/s, est. speed input: 55264.84 toks/s, output: 53.97 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:27<01:13, 35.81it/s, est. speed input: 54740.67 toks/s, output: 53.46 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:28<01:11, 36.29it/s, est. speed input: 54249.01 toks/s, output: 52.98 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:29<01:10, 36.53it/s, est. speed input: 53770.63 toks/s, output: 52.51 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:30<01:10, 35.96it/s, est. speed input: 53213.13 toks/s, output: 51.97 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:31<01:10, 35.52it/s, est. speed input: 52682.50 toks/s, output: 51.45 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:32<01:09, 35.21it/s, est. speed input: 52180.83 toks/s, output: 50.96 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:32<01:09, 35.01it/s, est. speed input: 51708.36 toks/s, output: 50.50 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:33<01:08, 34.89it/s, est. speed input: 51265.42 toks/s, output: 50.06 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:34<00:39, 58.65it/s, est. speed input: 52848.25 toks/s, output: 51.61 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:35<00:45, 50.69it/s, est. speed input: 52439.47 toks/s, output: 51.21 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:35<00:48, 46.42it/s, est. speed input: 52105.29 toks/s, output: 50.88 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:36<00:50, 43.94it/s, est. speed input: 51816.26 toks/s, output: 50.60 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:37<00:53, 41.47it/s, est. speed input: 51470.38 toks/s, output: 50.26 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:38<00:55, 39.24it/s, est. speed input: 51085.09 toks/s, output: 49.89 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:39<00:56, 37.77it/s, est. speed input: 50717.68 toks/s, output: 49.53 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:40<00:57, 36.78it/s, est. speed input: 50367.24 toks/s, output: 49.19 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:41<00:57, 36.11it/s, est. speed input: 50032.88 toks/s, output: 48.86 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:42<00:57, 35.64it/s, est. speed input: 49712.48 toks/s, output: 48.55 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:43<00:56, 35.48it/s, est. speed input: 49421.01 toks/s, output: 48.26 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:43<00:54, 36.05it/s, est. speed input: 49206.40 toks/s, output: 48.05 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:44<00:53, 36.45it/s, est. speed input: 48999.32 toks/s, output: 47.85 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:45<00:52, 36.26it/s, est. speed input: 48758.84 toks/s, output: 47.62 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:46<00:52, 35.74it/s, est. speed input: 48493.56 toks/s, output: 47.36 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:47<00:52, 35.37it/s, est. speed input: 48238.52 toks/s, output: 47.11 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:48<00:51, 35.13it/s, est. speed input: 47993.59 toks/s, output: 46.87 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:49<00:51, 34.95it/s, est. speed input: 47756.90 toks/s, output: 46.64 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:50<00:50, 34.83it/s, est. speed input: 47529.30 toks/s, output: 46.42 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:51<00:48, 35.32it/s, est. speed input: 47355.97 toks/s, output: 46.25 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:52<00:47, 35.92it/s, est. speed input: 47207.71 toks/s, output: 46.10 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:52<00:45, 36.36it/s, est. speed input: 47063.94 toks/s, output: 45.96 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:53<00:45, 36.10it/s, est. speed input: 46885.20 toks/s, output: 45.79 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:54<00:44, 35.90it/s, est. speed input: 46710.72 toks/s, output: 45.62 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:55<00:44, 35.49it/s, est. speed input: 46522.33 toks/s, output: 45.43 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:56<00:43, 35.20it/s, est. speed input: 46339.75 toks/s, output: 45.25 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:57<00:42, 35.00it/s, est. speed input: 46163.36 toks/s, output: 45.08 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:58<00:41, 35.07it/s, est. speed input: 46006.87 toks/s, output: 44.93 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:59<00:40, 35.75it/s, est. speed input: 45896.10 toks/s, output: 44.82 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [01:00<00:38, 36.24it/s, est. speed input: 45788.58 toks/s, output: 44.72 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:01<00:37, 36.59it/s, est. speed input: 45684.10 toks/s, output: 44.61 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:01<00:37, 36.23it/s, est. speed input: 45546.71 toks/s, output: 44.48 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:02<00:36, 35.71it/s, est. speed input: 45396.88 toks/s, output: 44.33 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:03<00:36, 35.36it/s, est. speed input: 45251.50 toks/s, output: 44.19 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:04<00:35, 35.11it/s, est. speed input: 45109.95 toks/s, output: 44.05 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:04<00:19, 60.62it/s, est. speed input: 46008.77 toks/s, output: 44.93 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:05<00:22, 51.06it/s, est. speed input: 45859.00 toks/s, output: 44.78 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:06<00:24, 45.41it/s, est. speed input: 45714.21 toks/s, output: 44.64 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:07<00:25, 42.90it/s, est. speed input: 45619.80 toks/s, output: 44.55 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:08<00:25, 41.21it/s, est. speed input: 45528.48 toks/s, output: 44.46 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:09<00:25, 40.05it/s, est. speed input: 45439.45 toks/s, output: 44.37 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:10<00:25, 38.35it/s, est. speed input: 45309.99 toks/s, output: 44.25 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:11<00:25, 37.15it/s, est. speed input: 45180.66 toks/s, output: 44.12 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:12<00:25, 36.34it/s, est. speed input: 45054.49 toks/s, output: 44.00 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:12<00:24, 35.79it/s, est. speed input: 44931.42 toks/s, output: 43.88 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:13<00:24, 35.41it/s, est. speed input: 44811.49 toks/s, output: 43.76 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:14<00:23, 35.15it/s, est. speed input: 44694.69 toks/s, output: 43.65 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:15<00:22, 35.40it/s, est. speed input: 44602.83 toks/s, output: 43.56 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:16<00:21, 35.97it/s, est. speed input: 44532.29 toks/s, output: 43.49 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:17<00:20, 36.39it/s, est. speed input: 44463.62 toks/s, output: 43.42 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:18<00:19, 35.83it/s, est. speed input: 44356.59 toks/s, output: 43.32 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:19<00:18, 35.43it/s, est. speed input: 44251.51 toks/s, output: 43.21 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:20<00:18, 35.16it/s, est. speed input: 44148.90 toks/s, output: 43.11 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:21<00:17, 34.97it/s, est. speed input: 44048.67 toks/s, output: 43.02 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:22<00:16, 34.84it/s, est. speed input: 43950.55 toks/s, output: 42.92 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:22<00:15, 34.88it/s, est. speed input: 43860.73 toks/s, output: 42.83 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:23<00:14, 35.60it/s, est. speed input: 43804.17 toks/s, output: 42.78 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:24<00:13, 36.13it/s, est. speed input: 43748.63 toks/s, output: 42.72 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:25<00:12, 36.42it/s, est. speed input: 43690.92 toks/s, output: 42.67 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:26<00:11, 35.83it/s, est. speed input: 43601.61 toks/s, output: 42.58 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:27<00:10, 35.44it/s, est. speed input: 43514.46 toks/s, output: 42.49 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:28<00:09, 35.16it/s, est. speed input: 43428.85 toks/s, output: 42.41 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:29<00:09, 34.97it/s, est. speed input: 43345.09 toks/s, output: 42.33 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:30<00:08, 34.99it/s, est. speed input: 43269.40 toks/s, output: 42.26 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:31<00:07, 35.68it/s, est. speed input: 43222.69 toks/s, output: 42.21 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:31<00:06, 36.19it/s, est. speed input: 43177.08 toks/s, output: 42.17 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:32<00:05, 36.53it/s, est. speed input: 43131.67 toks/s, output: 42.12 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:33<00:04, 36.69it/s, est. speed input: 43083.95 toks/s, output: 42.07 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:34<00:03, 36.01it/s, est. speed input: 43008.14 toks/s, output: 42.00 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:35<00:02, 35.55it/s, est. speed input: 42933.88 toks/s, output: 41.93 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:35<00:00, 61.85it/s, est. speed input: 43555.69 toks/s, output: 42.53 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:35<00:00, 61.85it/s, est. speed input: 43876.78 toks/s, output: 42.85 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:35<00:00, 42.85it/s, est. speed input: 43876.78 toks/s, output: 42.85 toks/s]
[rank0]:[W125 23:02:34.047928572 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-25 23:02:38
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:03:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 23:03:36 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=516523) WARNING 01-25 23:03:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=516523) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=516523) WARNING 01-25 23:03:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 13.22 requests/s, 13554.93 total tokens/s, 13.22 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-25 23:03:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:03:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 23:03:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 23:03:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:03:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:03:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:03:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:03:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:03:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 23:03:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:03:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:03:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:03:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:03:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:03:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:03:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 23:03:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 23:03:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:03:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:03:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:03:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:03:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 23:03:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 23:03:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:03:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:03:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:03:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:03:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=516523) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=516523) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.15s/it]
(EngineCore_DP0 pid=516523) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.15s/it]
(EngineCore_DP0 pid=516523) 
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19005440 bytes
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11403264 bytes
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 60817408 bytes
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=516523) [2026-01-25 23:03:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30277632 bytes
(EngineCore_DP0 pid=516523) [rank0]:W0125 23:04:06.902000 516523 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=516523) [rank0]:W0125 23:04:07.027000 516523 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=516523) [rank0]:W0125 23:04:07.292000 516523 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=516523) [rank0]:W0125 23:04:07.460000 516523 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=516523) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  4.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  5.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:03,  4.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:03,  3.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:03,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:02<00:04,  2.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:02<00:03,  3.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:02<00:02,  3.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:02<00:01,  4.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:01,  5.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:03<00:00,  6.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:03<00:00,  6.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:03<00:00,  7.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:03<00:00,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:03<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:03<00:00,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  5.07it/s]
(EngineCore_DP0 pid=516523) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.50it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:01,  6.99it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:01,  3.57it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:01<00:01,  4.53it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:01<00:01,  4.81it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:01<00:00,  4.01it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:01<00:00,  4.35it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  5.09it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  5.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:02<00:00,  6.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:02<00:00,  5.27it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 26/8192 [00:00<00:31, 259.31it/s]
Adding requests:   1%|          | 56/8192 [00:00<00:29, 278.25it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:28, 282.85it/s]
Adding requests:   1%|▏         | 114/8192 [00:00<00:28, 281.44it/s]
Adding requests:   2%|▏         | 143/8192 [00:00<00:28, 280.85it/s]
Adding requests:   2%|▏         | 172/8192 [00:00<00:28, 280.97it/s]
Adding requests:   2%|▏         | 201/8192 [00:00<00:28, 282.39it/s]
Adding requests:   3%|▎         | 230/8192 [00:00<00:28, 284.28it/s]
Adding requests:   3%|▎         | 259/8192 [00:00<00:28, 282.11it/s]
Adding requests:   4%|▎         | 288/8192 [00:01<00:28, 276.58it/s]
Adding requests:   4%|▍         | 316/8192 [00:01<00:29, 266.86it/s]
Adding requests:   4%|▍         | 343/8192 [00:01<00:29, 266.30it/s]
Adding requests:   5%|▍         | 371/8192 [00:01<00:29, 269.19it/s]
Adding requests:   5%|▍         | 399/8192 [00:01<00:28, 270.75it/s]
Adding requests:   5%|▌         | 428/8192 [00:01<00:28, 275.95it/s]
Adding requests:   6%|▌         | 458/8192 [00:01<00:27, 281.57it/s]
Adding requests:   6%|▌         | 488/8192 [00:01<00:27, 284.70it/s]
Adding requests:   6%|▋         | 517/8192 [00:01<00:27, 275.20it/s]
Adding requests:   7%|▋         | 545/8192 [00:01<00:27, 273.81it/s]
Adding requests:   7%|▋         | 576/8192 [00:02<00:26, 283.42it/s]
Adding requests:   7%|▋         | 605/8192 [00:02<00:27, 279.20it/s]
Adding requests:   8%|▊         | 635/8192 [00:02<00:26, 284.03it/s]
Adding requests:   8%|▊         | 664/8192 [00:02<00:26, 282.70it/s]
Adding requests:   8%|▊         | 693/8192 [00:02<00:26, 284.23it/s]
Adding requests:   9%|▉         | 722/8192 [00:02<00:26, 279.83it/s]
Adding requests:   9%|▉         | 752/8192 [00:02<00:26, 284.88it/s]
Adding requests:  10%|▉         | 781/8192 [00:02<00:25, 286.06it/s]
Adding requests:  10%|▉         | 810/8192 [00:02<00:25, 284.89it/s]
Adding requests:  10%|█         | 840/8192 [00:02<00:25, 288.47it/s]
Adding requests:  11%|█         | 870/8192 [00:03<00:25, 289.79it/s]
Adding requests:  11%|█         | 900/8192 [00:03<00:24, 292.45it/s]
Adding requests:  11%|█▏        | 931/8192 [00:03<00:24, 296.37it/s]
Adding requests:  12%|█▏        | 962/8192 [00:03<00:24, 298.03it/s]
Adding requests:  12%|█▏        | 992/8192 [00:03<00:24, 297.19it/s]
Adding requests:  12%|█▎        | 1024/8192 [00:03<00:23, 302.79it/s]
Adding requests:  13%|█▎        | 1055/8192 [00:03<00:23, 304.59it/s]
Adding requests:  13%|█▎        | 1086/8192 [00:03<00:23, 303.25it/s]
Adding requests:  14%|█▎        | 1117/8192 [00:03<00:23, 302.22it/s]
Adding requests:  14%|█▍        | 1149/8192 [00:04<00:22, 307.23it/s]
Adding requests:  14%|█▍        | 1180/8192 [00:04<00:22, 306.51it/s]
Adding requests:  15%|█▍        | 1211/8192 [00:04<00:22, 303.90it/s]
Adding requests:  15%|█▌        | 1242/8192 [00:04<00:22, 304.20it/s]
Adding requests:  16%|█▌        | 1273/8192 [00:04<00:23, 297.62it/s]
Adding requests:  16%|█▌        | 1303/8192 [00:04<00:23, 296.57it/s]
Adding requests:  16%|█▋        | 1334/8192 [00:04<00:22, 299.81it/s]
Adding requests:  17%|█▋        | 1365/8192 [00:04<00:22, 302.77it/s]
Adding requests:  17%|█▋        | 1397/8192 [00:04<00:22, 306.88it/s]
Adding requests:  17%|█▋        | 1430/8192 [00:04<00:21, 311.82it/s]
Adding requests:  18%|█▊        | 1463/8192 [00:05<00:21, 315.59it/s]
Adding requests:  18%|█▊        | 1495/8192 [00:05<00:21, 304.92it/s]
Adding requests:  19%|█▊        | 1526/8192 [00:05<00:21, 306.29it/s]
Adding requests:  19%|█▉        | 1557/8192 [00:05<00:21, 305.34it/s]
Adding requests:  19%|█▉        | 1588/8192 [00:05<00:22, 294.72it/s]
Adding requests:  20%|█▉        | 1618/8192 [00:05<00:22, 289.60it/s]
Adding requests:  20%|██        | 1648/8192 [00:05<00:22, 291.60it/s]
Adding requests:  20%|██        | 1678/8192 [00:05<00:22, 290.09it/s]
Adding requests:  21%|██        | 1708/8192 [00:05<00:23, 274.50it/s]
Adding requests:  21%|██        | 1737/8192 [00:06<00:23, 278.00it/s]
Adding requests:  22%|██▏       | 1765/8192 [00:06<00:23, 274.51it/s]
Adding requests:  22%|██▏       | 1794/8192 [00:06<00:23, 276.81it/s]
Adding requests:  22%|██▏       | 1823/8192 [00:06<00:22, 278.99it/s]
Adding requests:  23%|██▎       | 1853/8192 [00:06<00:22, 282.16it/s]
Adding requests:  23%|██▎       | 1882/8192 [00:06<00:22, 280.58it/s]
Adding requests:  23%|██▎       | 1911/8192 [00:06<00:22, 279.20it/s]
Adding requests:  24%|██▎       | 1941/8192 [00:06<00:22, 284.04it/s]
Adding requests:  24%|██▍       | 1971/8192 [00:06<00:21, 286.29it/s]
Adding requests:  24%|██▍       | 2000/8192 [00:06<00:22, 279.71it/s]
Adding requests:  25%|██▍       | 2030/8192 [00:07<00:21, 283.81it/s]
Adding requests:  25%|██▌       | 2060/8192 [00:07<00:21, 285.34it/s]
Adding requests:  26%|██▌       | 2089/8192 [00:07<00:21, 279.19it/s]
Adding requests:  26%|██▌       | 2118/8192 [00:07<00:21, 280.83it/s]
Adding requests:  26%|██▌       | 2147/8192 [00:07<00:21, 279.25it/s]
Adding requests:  27%|██▋       | 2175/8192 [00:07<00:21, 275.89it/s]
Adding requests:  27%|██▋       | 2203/8192 [00:07<00:21, 273.99it/s]
Adding requests:  27%|██▋       | 2233/8192 [00:07<00:21, 281.54it/s]
Adding requests:  28%|██▊       | 2264/8192 [00:07<00:20, 289.44it/s]
Adding requests:  28%|██▊       | 2294/8192 [00:07<00:20, 290.65it/s]
Adding requests:  28%|██▊       | 2324/8192 [00:08<00:20, 285.63it/s]
Adding requests:  29%|██▊       | 2355/8192 [00:08<00:19, 291.98it/s]
Adding requests:  29%|██▉       | 2386/8192 [00:08<00:19, 295.06it/s]
Adding requests:  30%|██▉       | 2417/8192 [00:08<00:19, 296.71it/s]
Adding requests:  30%|██▉       | 2447/8192 [00:08<00:19, 296.40it/s]
Adding requests:  30%|███       | 2478/8192 [00:08<00:19, 298.29it/s]
Adding requests:  31%|███       | 2508/8192 [00:08<00:19, 288.85it/s]
Adding requests:  31%|███       | 2537/8192 [00:08<00:19, 288.86it/s]
Adding requests:  31%|███▏      | 2566/8192 [00:08<00:19, 289.18it/s]
Adding requests:  32%|███▏      | 2595/8192 [00:09<00:19, 287.27it/s]
Adding requests:  32%|███▏      | 2624/8192 [00:09<00:19, 284.73it/s]
Adding requests:  32%|███▏      | 2654/8192 [00:09<00:19, 286.71it/s]
Adding requests:  33%|███▎      | 2683/8192 [00:09<00:19, 279.95it/s]
Adding requests:  33%|███▎      | 2712/8192 [00:09<00:19, 276.91it/s]
Adding requests:  33%|███▎      | 2742/8192 [00:09<00:19, 282.02it/s]
Adding requests:  34%|███▍      | 2771/8192 [00:09<00:19, 280.86it/s]
Adding requests:  34%|███▍      | 2800/8192 [00:09<00:19, 273.76it/s]
Adding requests:  35%|███▍      | 2830/8192 [00:09<00:19, 279.70it/s]
Adding requests:  35%|███▍      | 2859/8192 [00:09<00:18, 282.62it/s]
Adding requests:  35%|███▌      | 2889/8192 [00:10<00:18, 285.65it/s]
Adding requests:  36%|███▌      | 2918/8192 [00:10<00:18, 279.28it/s]
Adding requests:  36%|███▌      | 2946/8192 [00:10<00:19, 275.25it/s]
Adding requests:  36%|███▋      | 2975/8192 [00:10<00:18, 278.12it/s]
Adding requests:  37%|███▋      | 3003/8192 [00:10<00:18, 277.34it/s]
Adding requests:  37%|███▋      | 3031/8192 [00:10<00:18, 273.24it/s]
Adding requests:  37%|███▋      | 3062/8192 [00:10<00:18, 282.72it/s]
Adding requests:  38%|███▊      | 3092/8192 [00:10<00:17, 284.97it/s]
Adding requests:  38%|███▊      | 3126/8192 [00:10<00:16, 299.46it/s]
Adding requests:  39%|███▊      | 3157/8192 [00:10<00:16, 302.06it/s]
Adding requests:  39%|███▉      | 3188/8192 [00:11<00:16, 303.47it/s]
Adding requests:  39%|███▉      | 3219/8192 [00:11<00:16, 304.57it/s]
Adding requests:  40%|███▉      | 3251/8192 [00:11<00:16, 306.06it/s]
Adding requests:  40%|████      | 3282/8192 [00:11<00:16, 306.27it/s]
Adding requests:  40%|████      | 3314/8192 [00:11<00:15, 309.69it/s]
Adding requests:  41%|████      | 3348/8192 [00:11<00:15, 317.35it/s]
Adding requests:  41%|████▏     | 3380/8192 [00:11<00:15, 314.59it/s]
Adding requests:  42%|████▏     | 3412/8192 [00:11<00:15, 306.75it/s]
Adding requests:  42%|████▏     | 3444/8192 [00:11<00:15, 309.75it/s]
Adding requests:  42%|████▏     | 3476/8192 [00:12<00:15, 303.51it/s]
Adding requests:  43%|████▎     | 3507/8192 [00:12<00:15, 304.05it/s]
Adding requests:  43%|████▎     | 3538/8192 [00:12<00:15, 301.35it/s]
Adding requests:  44%|████▎     | 3570/8192 [00:12<00:15, 304.49it/s]
Adding requests:  44%|████▍     | 3601/8192 [00:12<00:15, 302.40it/s]
Adding requests:  44%|████▍     | 3632/8192 [00:12<00:15, 299.19it/s]
Adding requests:  45%|████▍     | 3663/8192 [00:12<00:15, 300.84it/s]
Adding requests:  45%|████▌     | 3694/8192 [00:12<00:14, 300.03it/s]
Adding requests:  45%|████▌     | 3725/8192 [00:12<00:14, 299.95it/s]
Adding requests:  46%|████▌     | 3758/8192 [00:12<00:14, 307.28it/s]
Adding requests:  46%|████▋     | 3789/8192 [00:13<00:14, 306.82it/s]
Adding requests:  47%|████▋     | 3820/8192 [00:13<00:14, 306.23it/s]
Adding requests:  47%|████▋     | 3851/8192 [00:13<00:14, 305.25it/s]
Adding requests:  47%|████▋     | 3882/8192 [00:13<00:14, 298.51it/s]
Adding requests:  48%|████▊     | 3912/8192 [00:13<00:14, 285.56it/s]
Adding requests:  48%|████▊     | 3941/8192 [00:13<00:15, 281.20it/s]
Adding requests:  48%|████▊     | 3970/8192 [00:13<00:14, 282.32it/s]
Adding requests:  49%|████▉     | 3999/8192 [00:13<00:15, 274.16it/s]
Adding requests:  49%|████▉     | 4028/8192 [00:13<00:14, 278.21it/s]
Adding requests:  50%|████▉     | 4056/8192 [00:14<00:15, 275.32it/s]
Adding requests:  50%|████▉     | 4086/8192 [00:14<00:14, 281.44it/s]
Adding requests:  50%|█████     | 4116/8192 [00:14<00:14, 285.00it/s]
Adding requests:  51%|█████     | 4145/8192 [00:14<00:14, 285.99it/s]
Adding requests:  51%|█████     | 4176/8192 [00:14<00:13, 290.89it/s]
Adding requests:  51%|█████▏    | 4207/8192 [00:14<00:13, 294.13it/s]
Adding requests:  52%|█████▏    | 4237/8192 [00:14<00:13, 292.97it/s]
Adding requests:  52%|█████▏    | 4267/8192 [00:14<00:13, 294.11it/s]
Adding requests:  52%|█████▏    | 4297/8192 [00:14<00:13, 294.33it/s]
Adding requests:  53%|█████▎    | 4327/8192 [00:14<00:13, 290.94it/s]
Adding requests:  53%|█████▎    | 4357/8192 [00:15<00:13, 288.64it/s]
Adding requests:  54%|█████▎    | 4386/8192 [00:15<00:13, 276.44it/s]
Adding requests:  54%|█████▍    | 4415/8192 [00:15<00:13, 277.95it/s]
Adding requests:  54%|█████▍    | 4444/8192 [00:15<00:13, 279.36it/s]
Adding requests:  55%|█████▍    | 4473/8192 [00:15<00:13, 281.54it/s]
Adding requests:  55%|█████▍    | 4502/8192 [00:15<00:13, 273.51it/s]
Adding requests:  55%|█████▌    | 4530/8192 [00:15<00:13, 271.79it/s]
Adding requests:  56%|█████▌    | 4559/8192 [00:15<00:13, 273.92it/s]
Adding requests:  56%|█████▌    | 4587/8192 [00:15<00:13, 273.67it/s]
Adding requests:  56%|█████▋    | 4616/8192 [00:15<00:12, 277.20it/s]
Adding requests:  57%|█████▋    | 4646/8192 [00:16<00:12, 282.54it/s]
Adding requests:  57%|█████▋    | 4675/8192 [00:16<00:12, 281.94it/s]
Adding requests:  57%|█████▋    | 4704/8192 [00:16<00:12, 278.30it/s]
Adding requests:  58%|█████▊    | 4734/8192 [00:16<00:12, 283.53it/s]
Adding requests:  58%|█████▊    | 4763/8192 [00:16<00:12, 283.45it/s]
Adding requests:  58%|█████▊    | 4792/8192 [00:16<00:12, 281.21it/s]
Adding requests:  59%|█████▉    | 4821/8192 [00:16<00:12, 279.78it/s]
Adding requests:  59%|█████▉    | 4849/8192 [00:16<00:11, 278.73it/s]
Adding requests:  60%|█████▉    | 4878/8192 [00:16<00:11, 280.17it/s]
Adding requests:  60%|█████▉    | 4907/8192 [00:17<00:11, 277.25it/s]
Adding requests:  60%|██████    | 4936/8192 [00:17<00:11, 279.81it/s]
Adding requests:  61%|██████    | 4964/8192 [00:17<00:11, 278.68it/s]
Adding requests:  67%|██████▋   | 5451/8192 [00:17<00:01, 1623.14it/s]
Adding requests:  69%|██████▊   | 5615/8192 [00:17<00:03, 668.05it/s] 
Adding requests:  70%|███████   | 5738/8192 [00:18<00:04, 494.43it/s]
Adding requests:  71%|███████   | 5833/8192 [00:18<00:05, 421.82it/s]
Adding requests:  72%|███████▏  | 5908/8192 [00:18<00:05, 400.58it/s]
Adding requests:  73%|███████▎  | 5970/8192 [00:19<00:05, 378.99it/s]
Adding requests:  74%|███████▎  | 6023/8192 [00:19<00:05, 369.86it/s]
Adding requests:  74%|███████▍  | 6070/8192 [00:19<00:05, 365.44it/s]
Adding requests:  75%|███████▍  | 6114/8192 [00:19<00:05, 356.08it/s]
Adding requests:  75%|███████▌  | 6154/8192 [00:19<00:05, 352.00it/s]
Adding requests:  76%|███████▌  | 6193/8192 [00:19<00:05, 342.28it/s]
Adding requests:  76%|███████▌  | 6230/8192 [00:19<00:05, 333.84it/s]
Adding requests:  76%|███████▋  | 6265/8192 [00:20<00:05, 330.03it/s]
Adding requests:  77%|███████▋  | 6299/8192 [00:20<00:05, 316.02it/s]
Adding requests:  77%|███████▋  | 6331/8192 [00:20<00:05, 314.52it/s]
Adding requests:  78%|███████▊  | 6364/8192 [00:20<00:05, 318.26it/s]
Adding requests:  78%|███████▊  | 6397/8192 [00:20<00:05, 314.57it/s]
Adding requests:  78%|███████▊  | 6429/8192 [00:20<00:05, 313.38it/s]
Adding requests:  79%|███████▉  | 6461/8192 [00:20<00:05, 315.16it/s]
Adding requests:  79%|███████▉  | 6494/8192 [00:20<00:05, 318.94it/s]
Adding requests:  80%|███████▉  | 6526/8192 [00:20<00:05, 317.29it/s]
Adding requests:  80%|████████  | 6558/8192 [00:20<00:05, 316.93it/s]
Adding requests:  80%|████████  | 6590/8192 [00:21<00:05, 308.42it/s]
Adding requests:  81%|████████  | 6621/8192 [00:21<00:05, 270.26it/s]
Adding requests:  81%|████████  | 6649/8192 [00:21<00:06, 252.04it/s]
Adding requests:  81%|████████▏ | 6675/8192 [00:21<00:06, 232.90it/s]
Adding requests:  82%|████████▏ | 6699/8192 [00:21<00:06, 223.83it/s]
Adding requests:  82%|████████▏ | 6722/8192 [00:21<00:06, 216.74it/s]
Adding requests:  82%|████████▏ | 6744/8192 [00:21<00:06, 211.43it/s]
Adding requests:  83%|████████▎ | 6766/8192 [00:21<00:06, 207.19it/s]
Adding requests:  83%|████████▎ | 6788/8192 [00:22<00:06, 209.38it/s]
Adding requests:  83%|████████▎ | 6810/8192 [00:22<00:06, 206.58it/s]
Adding requests:  83%|████████▎ | 6831/8192 [00:22<00:06, 203.08it/s]
Adding requests:  84%|████████▎ | 6852/8192 [00:22<00:06, 204.90it/s]
Adding requests:  84%|████████▍ | 6874/8192 [00:22<00:06, 207.54it/s]
Adding requests:  84%|████████▍ | 6895/8192 [00:22<00:06, 206.49it/s]
Adding requests:  84%|████████▍ | 6916/8192 [00:22<00:06, 205.77it/s]
Adding requests:  85%|████████▍ | 6939/8192 [00:22<00:05, 212.24it/s]
Adding requests:  85%|████████▍ | 6961/8192 [00:22<00:05, 207.37it/s]
Adding requests:  85%|████████▌ | 6982/8192 [00:23<00:06, 200.91it/s]
Adding requests:  85%|████████▌ | 7003/8192 [00:23<00:05, 200.08it/s]
Adding requests:  86%|████████▌ | 7024/8192 [00:23<00:05, 201.41it/s]
Adding requests:  86%|████████▌ | 7045/8192 [00:23<00:05, 199.30it/s]
Adding requests:  86%|████████▋ | 7066/8192 [00:23<00:05, 201.87it/s]
Adding requests:  87%|████████▋ | 7087/8192 [00:23<00:05, 202.96it/s]
Adding requests:  87%|████████▋ | 7108/8192 [00:23<00:05, 203.30it/s]
Adding requests:  87%|████████▋ | 7129/8192 [00:23<00:05, 190.71it/s]
Adding requests:  87%|████████▋ | 7152/8192 [00:23<00:05, 199.71it/s]
Adding requests:  88%|████████▊ | 7179/8192 [00:23<00:04, 219.49it/s]
Adding requests:  88%|████████▊ | 7207/8192 [00:24<00:04, 236.72it/s]
Adding requests:  88%|████████▊ | 7235/8192 [00:24<00:03, 248.42it/s]
Adding requests:  89%|████████▊ | 7264/8192 [00:24<00:03, 259.88it/s]
Adding requests:  89%|████████▉ | 7292/8192 [00:24<00:03, 264.09it/s]
Adding requests:  89%|████████▉ | 7322/8192 [00:24<00:03, 273.14it/s]
Adding requests:  90%|████████▉ | 7350/8192 [00:24<00:03, 267.53it/s]
Adding requests:  90%|█████████ | 7379/8192 [00:24<00:02, 273.70it/s]
Adding requests:  90%|█████████ | 7409/8192 [00:24<00:02, 279.86it/s]
Adding requests:  91%|█████████ | 7438/8192 [00:24<00:02, 282.23it/s]
Adding requests:  91%|█████████ | 7467/8192 [00:25<00:02, 279.93it/s]
Adding requests:  92%|█████████▏| 7496/8192 [00:25<00:02, 275.61it/s]
Adding requests:  92%|█████████▏| 7525/8192 [00:25<00:02, 278.86it/s]
Adding requests:  92%|█████████▏| 7557/8192 [00:25<00:02, 290.24it/s]
Adding requests:  93%|█████████▎| 7588/8192 [00:25<00:02, 293.14it/s]
Adding requests:  93%|█████████▎| 7618/8192 [00:25<00:01, 288.72it/s]
Adding requests:  93%|█████████▎| 7651/8192 [00:25<00:01, 298.06it/s]
Adding requests:  94%|█████████▍| 7683/8192 [00:25<00:01, 302.56it/s]
Adding requests:  94%|█████████▍| 7714/8192 [00:25<00:01, 299.23it/s]
Adding requests:  95%|█████████▍| 7745/8192 [00:25<00:01, 301.19it/s]
Adding requests:  95%|█████████▍| 7776/8192 [00:26<00:01, 301.85it/s]
Adding requests:  95%|█████████▌| 7807/8192 [00:26<00:01, 283.21it/s]
Adding requests:  96%|█████████▌| 7837/8192 [00:26<00:01, 287.24it/s]
Adding requests:  96%|█████████▌| 7866/8192 [00:26<00:01, 284.08it/s]
Adding requests:  96%|█████████▋| 7896/8192 [00:26<00:01, 287.02it/s]
Adding requests:  97%|█████████▋| 7925/8192 [00:26<00:00, 277.90it/s]
Adding requests:  97%|█████████▋| 7953/8192 [00:26<00:00, 275.75it/s]
Adding requests:  97%|█████████▋| 7983/8192 [00:26<00:00, 281.10it/s]
Adding requests:  98%|█████████▊| 8012/8192 [00:26<00:00, 282.35it/s]
Adding requests:  98%|█████████▊| 8041/8192 [00:26<00:00, 280.95it/s]
Adding requests:  99%|█████████▊| 8070/8192 [00:27<00:00, 282.83it/s]
Adding requests:  99%|█████████▉| 8100/8192 [00:27<00:00, 286.96it/s]
Adding requests:  99%|█████████▉| 8132/8192 [00:27<00:00, 294.41it/s]
Adding requests: 100%|█████████▉| 8163/8192 [00:27<00:00, 297.43it/s]
Adding requests: 100%|██████████| 8192/8192 [00:27<00:00, 297.94it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 322/8192 [00:01<00:29, 270.05it/s, est. speed input: 276533.84 toks/s, output: 270.05 toks/s]
Processed prompts:   5%|▍         | 386/8192 [00:06<02:34, 50.41it/s, est. speed input: 64808.99 toks/s, output: 63.29 toks/s]   
Processed prompts:   5%|▌         | 450/8192 [00:10<04:14, 30.45it/s, est. speed input: 42528.52 toks/s, output: 41.53 toks/s]
Processed prompts:   6%|▋         | 514/8192 [00:15<05:37, 22.73it/s, est. speed input: 33460.43 toks/s, output: 32.68 toks/s]
Processed prompts:   7%|▋         | 578/8192 [00:20<06:36, 19.18it/s, est. speed input: 28876.38 toks/s, output: 28.20 toks/s]
Processed prompts:   8%|▊         | 642/8192 [00:23<06:31, 19.30it/s, est. speed input: 27669.30 toks/s, output: 27.02 toks/s]
Processed prompts:   9%|▊         | 706/8192 [00:28<07:15, 17.19it/s, est. speed input: 25352.10 toks/s, output: 24.76 toks/s]
Processed prompts:   9%|▉         | 770/8192 [00:33<07:50, 15.79it/s, est. speed input: 23617.59 toks/s, output: 23.06 toks/s]
Processed prompts:  10%|█         | 834/8192 [00:38<08:14, 14.89it/s, est. speed input: 22311.95 toks/s, output: 21.79 toks/s]
Processed prompts:  11%|█         | 898/8192 [00:43<08:24, 14.46it/s, est. speed input: 21378.42 toks/s, output: 20.88 toks/s]
Processed prompts:  12%|█▏        | 962/8192 [00:47<08:34, 14.05it/s, est. speed input: 20575.78 toks/s, output: 20.09 toks/s]
Processed prompts:  13%|█▎        | 1026/8192 [00:50<07:41, 15.51it/s, est. speed input: 20607.89 toks/s, output: 20.12 toks/s]
Processed prompts:  13%|█▎        | 1090/8192 [00:55<08:05, 14.64it/s, est. speed input: 19953.96 toks/s, output: 19.49 toks/s]
Processed prompts:  14%|█▍        | 1154/8192 [01:00<08:13, 14.27it/s, est. speed input: 19473.33 toks/s, output: 19.02 toks/s]
Processed prompts:  15%|█▍        | 1218/8192 [01:05<08:22, 13.88it/s, est. speed input: 19013.97 toks/s, output: 18.57 toks/s]
Processed prompts:  16%|█▌        | 1282/8192 [01:10<08:25, 13.66it/s, est. speed input: 18633.88 toks/s, output: 18.20 toks/s]
Processed prompts:  16%|█▋        | 1346/8192 [01:15<08:22, 13.63it/s, est. speed input: 18336.32 toks/s, output: 17.91 toks/s]
Processed prompts:  17%|█▋        | 1410/8192 [01:20<08:25, 13.43it/s, est. speed input: 18024.51 toks/s, output: 17.60 toks/s]
Processed prompts:  18%|█▊        | 1474/8192 [01:22<07:21, 15.22it/s, est. speed input: 18186.09 toks/s, output: 17.76 toks/s]
Processed prompts:  19%|█▉        | 1538/8192 [01:27<07:40, 14.44it/s, est. speed input: 17905.43 toks/s, output: 17.49 toks/s]
Processed prompts:  20%|█▉        | 1602/8192 [01:32<07:46, 14.13it/s, est. speed input: 17694.37 toks/s, output: 17.28 toks/s]
Processed prompts:  20%|██        | 1666/8192 [01:37<07:52, 13.81it/s, est. speed input: 17480.28 toks/s, output: 17.07 toks/s]
Processed prompts:  21%|██        | 1730/8192 [01:42<07:53, 13.65it/s, est. speed input: 17297.83 toks/s, output: 16.89 toks/s]
Processed prompts:  22%|██▏       | 1794/8192 [01:47<07:51, 13.57it/s, est. speed input: 17138.20 toks/s, output: 16.74 toks/s]
Processed prompts:  23%|██▎       | 1858/8192 [01:52<07:50, 13.47it/s, est. speed input: 16983.59 toks/s, output: 16.59 toks/s]
Processed prompts:  23%|██▎       | 1922/8192 [01:54<06:53, 15.17it/s, est. speed input: 17114.40 toks/s, output: 16.71 toks/s]
Processed prompts:  24%|██▍       | 1986/8192 [01:59<07:09, 14.44it/s, est. speed input: 16957.75 toks/s, output: 16.56 toks/s]
Processed prompts:  25%|██▌       | 2050/8192 [02:04<07:15, 14.11it/s, est. speed input: 16833.48 toks/s, output: 16.44 toks/s]
Processed prompts:  26%|██▌       | 2114/8192 [02:09<07:22, 13.74it/s, est. speed input: 16697.30 toks/s, output: 16.31 toks/s]
Processed prompts:  27%|██▋       | 2178/8192 [02:14<07:21, 13.63it/s, est. speed input: 16590.69 toks/s, output: 16.20 toks/s]
Processed prompts:  27%|██▋       | 2242/8192 [02:19<07:19, 13.54it/s, est. speed input: 16489.38 toks/s, output: 16.10 toks/s]
Processed prompts:  28%|██▊       | 2306/8192 [02:22<06:35, 14.89it/s, est. speed input: 16567.05 toks/s, output: 16.18 toks/s]
Processed prompts:  29%|██▉       | 2370/8192 [02:27<06:45, 14.38it/s, est. speed input: 16471.23 toks/s, output: 16.09 toks/s]
Processed prompts:  30%|██▉       | 2434/8192 [02:32<06:53, 13.94it/s, est. speed input: 16369.98 toks/s, output: 15.99 toks/s]
Processed prompts:  30%|███       | 2498/8192 [02:36<06:50, 13.87it/s, est. speed input: 16300.09 toks/s, output: 15.92 toks/s]
Processed prompts:  31%|███▏      | 2562/8192 [02:41<06:55, 13.55it/s, est. speed input: 16204.57 toks/s, output: 15.82 toks/s]
Processed prompts:  32%|███▏      | 2626/8192 [02:46<06:51, 13.52it/s, est. speed input: 16134.67 toks/s, output: 15.76 toks/s]
Processed prompts:  33%|███▎      | 2690/8192 [02:51<06:48, 13.46it/s, est. speed input: 16064.41 toks/s, output: 15.69 toks/s]
Processed prompts:  34%|███▎      | 2754/8192 [02:54<06:03, 14.95it/s, est. speed input: 16148.00 toks/s, output: 15.77 toks/s]
Processed prompts:  34%|███▍      | 2818/8192 [02:59<06:13, 14.40it/s, est. speed input: 16079.37 toks/s, output: 15.70 toks/s]
Processed prompts:  35%|███▌      | 2882/8192 [03:04<06:19, 14.00it/s, est. speed input: 16009.97 toks/s, output: 15.63 toks/s]
Processed prompts:  36%|███▌      | 2946/8192 [03:09<06:20, 13.77it/s, est. speed input: 15948.30 toks/s, output: 15.57 toks/s]
Processed prompts:  37%|███▋      | 3010/8192 [03:14<06:24, 13.49it/s, est. speed input: 15877.30 toks/s, output: 15.51 toks/s]
Processed prompts:  38%|███▊      | 3074/8192 [03:18<06:19, 13.48it/s, est. speed input: 15827.42 toks/s, output: 15.46 toks/s]
Processed prompts:  38%|███▊      | 3138/8192 [03:23<06:16, 13.43it/s, est. speed input: 15775.36 toks/s, output: 15.41 toks/s]
Processed prompts:  39%|███▉      | 3202/8192 [03:26<05:34, 14.91it/s, est. speed input: 15849.20 toks/s, output: 15.48 toks/s]
Processed prompts:  40%|███▉      | 3266/8192 [03:31<05:41, 14.43it/s, est. speed input: 15801.71 toks/s, output: 15.43 toks/s]
Processed prompts:  41%|████      | 3330/8192 [03:36<05:46, 14.02it/s, est. speed input: 15748.86 toks/s, output: 15.38 toks/s]
Processed prompts:  41%|████▏     | 3394/8192 [03:41<05:48, 13.77it/s, est. speed input: 15700.93 toks/s, output: 15.33 toks/s]
Processed prompts:  42%|████▏     | 3458/8192 [03:46<05:50, 13.50it/s, est. speed input: 15646.42 toks/s, output: 15.28 toks/s]
Processed prompts:  43%|████▎     | 3522/8192 [03:51<05:45, 13.50it/s, est. speed input: 15608.97 toks/s, output: 15.24 toks/s]
Processed prompts:  44%|████▍     | 3586/8192 [03:55<05:42, 13.45it/s, est. speed input: 15569.38 toks/s, output: 15.20 toks/s]
Processed prompts:  45%|████▍     | 3650/8192 [03:58<05:00, 15.10it/s, est. speed input: 15646.65 toks/s, output: 15.28 toks/s]
Processed prompts:  45%|████▌     | 3714/8192 [04:03<05:09, 14.45it/s, est. speed input: 15602.24 toks/s, output: 15.24 toks/s]
Processed prompts:  46%|████▌     | 3778/8192 [04:08<05:14, 14.04it/s, est. speed input: 15561.19 toks/s, output: 15.20 toks/s]
Processed prompts:  47%|████▋     | 3842/8192 [04:13<05:13, 13.86it/s, est. speed input: 15527.36 toks/s, output: 15.16 toks/s]
Processed prompts:  48%|████▊     | 3906/8192 [04:18<05:16, 13.55it/s, est. speed input: 15482.27 toks/s, output: 15.12 toks/s]
Processed prompts:  48%|████▊     | 3970/8192 [04:23<05:11, 13.53it/s, est. speed input: 15452.45 toks/s, output: 15.09 toks/s]
Processed prompts:  49%|████▉     | 4034/8192 [04:26<04:34, 15.13it/s, est. speed input: 15520.49 toks/s, output: 15.16 toks/s]
Processed prompts:  50%|█████     | 4098/8192 [04:30<04:39, 14.63it/s, est. speed input: 15492.76 toks/s, output: 15.13 toks/s]
Processed prompts:  51%|█████     | 4162/8192 [04:35<04:45, 14.12it/s, est. speed input: 15454.72 toks/s, output: 15.09 toks/s]
Processed prompts:  52%|█████▏    | 4226/8192 [04:40<04:44, 13.94it/s, est. speed input: 15428.11 toks/s, output: 15.07 toks/s]
Processed prompts:  52%|█████▏    | 4290/8192 [04:45<04:43, 13.77it/s, est. speed input: 15399.41 toks/s, output: 15.04 toks/s]
Processed prompts:  53%|█████▎    | 4354/8192 [04:50<04:44, 13.50it/s, est. speed input: 15362.04 toks/s, output: 15.00 toks/s]
Processed prompts:  54%|█████▍    | 4418/8192 [04:54<04:39, 13.50it/s, est. speed input: 15337.49 toks/s, output: 14.98 toks/s]
Processed prompts:  55%|█████▍    | 4482/8192 [04:58<04:10, 14.78it/s, est. speed input: 15383.87 toks/s, output: 15.02 toks/s]
Processed prompts:  55%|█████▌    | 4546/8192 [05:03<04:13, 14.36it/s, est. speed input: 15358.71 toks/s, output: 15.00 toks/s]
Processed prompts:  56%|█████▋    | 4610/8192 [05:08<04:17, 13.93it/s, est. speed input: 15326.46 toks/s, output: 14.97 toks/s]
Processed prompts:  57%|█████▋    | 4674/8192 [05:12<04:16, 13.70it/s, est. speed input: 15297.97 toks/s, output: 14.94 toks/s]
Processed prompts:  58%|█████▊    | 4738/8192 [05:17<04:13, 13.61it/s, est. speed input: 15274.48 toks/s, output: 14.92 toks/s]
Processed prompts:  59%|█████▊    | 4802/8192 [05:22<04:13, 13.39it/s, est. speed input: 15242.65 toks/s, output: 14.89 toks/s]
Processed prompts:  59%|█████▉    | 4866/8192 [05:27<04:07, 13.43it/s, est. speed input: 15222.54 toks/s, output: 14.87 toks/s]
Processed prompts:  60%|██████    | 4930/8192 [05:30<03:40, 14.82it/s, est. speed input: 15270.15 toks/s, output: 14.91 toks/s]
Processed prompts:  61%|██████    | 4994/8192 [05:35<03:40, 14.50it/s, est. speed input: 15254.26 toks/s, output: 14.90 toks/s]
Processed prompts:  62%|██████▏   | 5058/8192 [05:40<03:43, 14.02it/s, est. speed input: 15226.54 toks/s, output: 14.87 toks/s]
Processed prompts:  63%|██████▎   | 5122/8192 [05:45<03:43, 13.77it/s, est. speed input: 15202.57 toks/s, output: 14.85 toks/s]
Processed prompts:  63%|██████▎   | 5186/8192 [05:49<03:40, 13.66it/s, est. speed input: 15182.57 toks/s, output: 14.83 toks/s]
Processed prompts:  64%|██████▍   | 5250/8192 [05:54<03:39, 13.43it/s, est. speed input: 15155.32 toks/s, output: 14.80 toks/s]
Processed prompts:  65%|██████▍   | 5314/8192 [05:59<03:33, 13.46it/s, est. speed input: 15138.14 toks/s, output: 14.78 toks/s]
Processed prompts:  66%|██████▌   | 5378/8192 [06:02<03:09, 14.85it/s, est. speed input: 15182.35 toks/s, output: 14.83 toks/s]
Processed prompts:  66%|██████▋   | 5442/8192 [06:07<03:10, 14.42it/s, est. speed input: 15165.03 toks/s, output: 14.81 toks/s]
Processed prompts:  67%|██████▋   | 5506/8192 [06:12<03:12, 13.96it/s, est. speed input: 15140.20 toks/s, output: 14.79 toks/s]
Processed prompts:  68%|██████▊   | 5570/8192 [06:17<03:11, 13.72it/s, est. speed input: 15119.12 toks/s, output: 14.76 toks/s]
Processed prompts:  69%|██████▉   | 5634/8192 [06:22<03:07, 13.63it/s, est. speed input: 15102.19 toks/s, output: 14.75 toks/s]
Processed prompts:  70%|██████▉   | 5698/8192 [06:26<03:05, 13.42it/s, est. speed input: 15078.82 toks/s, output: 14.73 toks/s]
Processed prompts:  70%|███████   | 5762/8192 [06:29<02:41, 15.07it/s, est. speed input: 15129.77 toks/s, output: 14.78 toks/s]
Processed prompts:  71%|███████   | 5826/8192 [06:34<02:45, 14.33it/s, est. speed input: 15105.07 toks/s, output: 14.75 toks/s]
Processed prompts:  72%|███████▏  | 5890/8192 [06:39<02:43, 14.10it/s, est. speed input: 15091.18 toks/s, output: 14.74 toks/s]
Processed prompts:  73%|███████▎  | 5954/8192 [06:44<02:42, 13.73it/s, est. speed input: 15068.58 toks/s, output: 14.72 toks/s]
Processed prompts:  73%|███████▎  | 6018/8192 [06:49<02:40, 13.58it/s, est. speed input: 15050.62 toks/s, output: 14.70 toks/s]
Processed prompts:  74%|███████▍  | 6082/8192 [06:54<02:35, 13.55it/s, est. speed input: 15036.52 toks/s, output: 14.68 toks/s]
Processed prompts:  75%|███████▌  | 6146/8192 [06:59<02:32, 13.38it/s, est. speed input: 15016.17 toks/s, output: 14.66 toks/s]
Processed prompts:  76%|███████▌  | 6210/8192 [07:02<02:10, 15.17it/s, est. speed input: 15068.21 toks/s, output: 14.72 toks/s]
Processed prompts:  77%|███████▋  | 6274/8192 [07:06<02:13, 14.42it/s, est. speed input: 15047.03 toks/s, output: 14.69 toks/s]
Processed prompts:  77%|███████▋  | 6338/8192 [07:11<02:11, 14.09it/s, est. speed input: 15031.89 toks/s, output: 14.68 toks/s]
Processed prompts:  78%|███████▊  | 6402/8192 [07:16<02:10, 13.74it/s, est. speed input: 15012.56 toks/s, output: 14.66 toks/s]
Processed prompts:  79%|███████▉  | 6466/8192 [07:21<02:06, 13.60it/s, est. speed input: 14997.18 toks/s, output: 14.65 toks/s]
Processed prompts:  80%|███████▉  | 6530/8192 [07:26<02:02, 13.58it/s, est. speed input: 14985.14 toks/s, output: 14.63 toks/s]
Processed prompts:  80%|████████  | 6594/8192 [07:31<01:58, 13.48it/s, est. speed input: 14970.08 toks/s, output: 14.62 toks/s]
Processed prompts:  81%|████████▏ | 6658/8192 [07:33<01:40, 15.29it/s, est. speed input: 15019.63 toks/s, output: 14.67 toks/s]
Processed prompts:  82%|████████▏ | 6722/8192 [07:38<01:41, 14.54it/s, est. speed input: 15001.69 toks/s, output: 14.65 toks/s]
Processed prompts:  83%|████████▎ | 6786/8192 [07:43<01:39, 14.15it/s, est. speed input: 14987.50 toks/s, output: 14.64 toks/s]
Processed prompts:  84%|████████▎ | 6850/8192 [07:48<01:37, 13.75it/s, est. speed input: 14968.85 toks/s, output: 14.62 toks/s]
Processed prompts:  84%|████████▍ | 6914/8192 [07:53<01:33, 13.64it/s, est. speed input: 14955.96 toks/s, output: 14.61 toks/s]
Processed prompts:  85%|████████▌ | 6978/8192 [07:58<01:29, 13.55it/s, est. speed input: 14943.08 toks/s, output: 14.59 toks/s]
Processed prompts:  86%|████████▌ | 7042/8192 [08:03<01:25, 13.40it/s, est. speed input: 14927.07 toks/s, output: 14.58 toks/s]
Processed prompts:  87%|████████▋ | 7106/8192 [08:06<01:12, 14.89it/s, est. speed input: 14964.33 toks/s, output: 14.61 toks/s]
Processed prompts:  88%|████████▊ | 7170/8192 [08:11<01:11, 14.28it/s, est. speed input: 14947.96 toks/s, output: 14.60 toks/s]
Processed prompts:  88%|████████▊ | 7234/8192 [08:15<01:08, 13.97it/s, est. speed input: 14935.12 toks/s, output: 14.59 toks/s]
Processed prompts:  89%|████████▉ | 7298/8192 [08:20<01:05, 13.66it/s, est. speed input: 14918.86 toks/s, output: 14.57 toks/s]
Processed prompts:  90%|████████▉ | 7362/8192 [08:25<01:01, 13.56it/s, est. speed input: 14907.03 toks/s, output: 14.56 toks/s]
Processed prompts:  91%|█████████ | 7426/8192 [08:30<00:56, 13.52it/s, est. speed input: 14896.20 toks/s, output: 14.55 toks/s]
Processed prompts:  91%|█████████▏| 7490/8192 [08:33<00:46, 14.98it/s, est. speed input: 14931.06 toks/s, output: 14.58 toks/s]
Processed prompts:  92%|█████████▏| 7554/8192 [08:38<00:44, 14.49it/s, est. speed input: 14920.45 toks/s, output: 14.57 toks/s]
Processed prompts:  93%|█████████▎| 7618/8192 [08:43<00:40, 14.00it/s, est. speed input: 14905.20 toks/s, output: 14.56 toks/s]
Processed prompts:  94%|█████████▍| 7682/8192 [08:48<00:37, 13.77it/s, est. speed input: 14892.98 toks/s, output: 14.54 toks/s]
Processed prompts:  95%|█████████▍| 7746/8192 [08:53<00:32, 13.52it/s, est. speed input: 14878.16 toks/s, output: 14.53 toks/s]
Processed prompts:  95%|█████████▌| 7810/8192 [08:57<00:28, 13.46it/s, est. speed input: 14867.16 toks/s, output: 14.52 toks/s]
Processed prompts:  96%|█████████▌| 7874/8192 [09:02<00:23, 13.46it/s, est. speed input: 14857.77 toks/s, output: 14.51 toks/s]
Processed prompts:  97%|█████████▋| 7938/8192 [09:05<00:17, 14.90it/s, est. speed input: 14890.10 toks/s, output: 14.54 toks/s]
Processed prompts:  98%|█████████▊| 8002/8192 [09:10<00:13, 14.44it/s, est. speed input: 14880.48 toks/s, output: 14.53 toks/s]
Processed prompts:  98%|█████████▊| 8066/8192 [09:15<00:08, 14.08it/s, est. speed input: 14869.79 toks/s, output: 14.52 toks/s]
Processed prompts:  99%|█████████▉| 8130/8192 [09:20<00:04, 13.89it/s, est. speed input: 14860.59 toks/s, output: 14.51 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [09:20<00:00, 13.89it/s, est. speed input: 14973.89 toks/s, output: 14.62 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [09:20<00:00, 14.62it/s, est. speed input: 14973.89 toks/s, output: 14.62 toks/s]
[rank0]:[W125 23:14:09.242389466 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 04:46:01
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:46:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:46:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=191876) WARNING 01-26 04:46:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=191876) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=191876) WARNING 01-26 04:46:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.15 requests/s, 8285.32 total tokens/s, 16.15 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 04:46:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:46:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:46:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:46:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:46:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:46:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:46:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:46:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:46:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:46:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:46:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:46:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:46:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:46:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:46:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:46:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:46:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=191876) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=191876) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.96s/it]
(EngineCore_DP0 pid=191876) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.79s/it]
(EngineCore_DP0 pid=191876) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.66s/it]
(EngineCore_DP0 pid=191876) 
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 163676160 bytes
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=191876) [2026-01-26 04:46:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 81543168 bytes
(EngineCore_DP0 pid=191876) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=191876) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  34%|███▍      | 44/128 [00:00<00:00, 436.61it/s]
Adding requests:  71%|███████   | 91/128 [00:00<00:00, 453.27it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 453.29it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:17,  7.29it/s, est. speed input: 3731.92 toks/s, output: 7.29 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:15,  8.11it/s, est. speed input: 4085.75 toks/s, output: 7.98 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:11, 10.77it/s, est. speed input: 5117.75 toks/s, output: 10.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:09, 13.05it/s, est. speed input: 5940.02 toks/s, output: 11.60 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:08, 14.45it/s, est. speed input: 6463.57 toks/s, output: 12.62 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:07, 15.30it/s, est. speed input: 6818.23 toks/s, output: 13.32 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:07, 15.85it/s, est. speed input: 7074.31 toks/s, output: 13.82 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:07, 16.26it/s, est. speed input: 7276.78 toks/s, output: 14.21 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:01<00:06, 16.61it/s, est. speed input: 7447.55 toks/s, output: 14.55 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:06, 16.87it/s, est. speed input: 7588.37 toks/s, output: 14.82 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 16.91it/s, est. speed input: 7687.55 toks/s, output: 15.01 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 16.99it/s, est. speed input: 7775.90 toks/s, output: 15.19 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 16.91it/s, est. speed input: 7836.88 toks/s, output: 15.31 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 17.00it/s, est. speed input: 7904.31 toks/s, output: 15.44 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 17.00it/s, est. speed input: 7955.82 toks/s, output: 15.54 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 17.08it/s, est. speed input: 8009.31 toks/s, output: 15.64 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:02<00:05, 17.18it/s, est. speed input: 8060.35 toks/s, output: 15.74 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:02<00:05, 17.24it/s, est. speed input: 8105.68 toks/s, output: 15.83 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:05, 17.26it/s, est. speed input: 8144.50 toks/s, output: 15.91 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 17.22it/s, est. speed input: 8174.59 toks/s, output: 15.97 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 17.02it/s, est. speed input: 8190.11 toks/s, output: 16.00 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 16.99it/s, est. speed input: 8211.25 toks/s, output: 16.04 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:04, 17.00it/s, est. speed input: 8232.95 toks/s, output: 16.08 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 17.00it/s, est. speed input: 8252.50 toks/s, output: 16.12 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 17.01it/s, est. speed input: 8271.12 toks/s, output: 16.15 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:03<00:04, 17.05it/s, est. speed input: 8290.16 toks/s, output: 16.19 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:04, 17.07it/s, est. speed input: 8307.16 toks/s, output: 16.22 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 17.06it/s, est. speed input: 8321.81 toks/s, output: 16.25 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 17.08it/s, est. speed input: 8336.89 toks/s, output: 16.28 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 17.12it/s, est. speed input: 8352.43 toks/s, output: 16.31 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:03, 17.17it/s, est. speed input: 8368.10 toks/s, output: 16.34 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 17.19it/s, est. speed input: 8382.43 toks/s, output: 16.37 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 17.18it/s, est. speed input: 8394.24 toks/s, output: 16.39 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:04<00:03, 17.26it/s, est. speed input: 8409.89 toks/s, output: 16.43 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:03, 17.29it/s, est. speed input: 8423.25 toks/s, output: 16.45 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 17.38it/s, est. speed input: 8438.99 toks/s, output: 16.48 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 17.44it/s, est. speed input: 8453.69 toks/s, output: 16.51 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 17.31it/s, est. speed input: 8460.39 toks/s, output: 16.52 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 17.24it/s, est. speed input: 8467.41 toks/s, output: 16.54 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:02, 17.31it/s, est. speed input: 8479.19 toks/s, output: 16.56 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 17.34it/s, est. speed input: 8489.70 toks/s, output: 16.58 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 17.25it/s, est. speed input: 8495.34 toks/s, output: 16.59 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:05<00:02, 17.27it/s, est. speed input: 8503.70 toks/s, output: 16.61 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 17.33it/s, est. speed input: 8513.53 toks/s, output: 16.63 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 17.39it/s, est. speed input: 8523.36 toks/s, output: 16.65 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 17.41it/s, est. speed input: 8532.24 toks/s, output: 16.66 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 17.33it/s, est. speed input: 8537.55 toks/s, output: 16.67 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:01, 17.02it/s, est. speed input: 8533.69 toks/s, output: 16.67 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 17.06it/s, est. speed input: 8538.71 toks/s, output: 16.68 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 17.05it/s, est. speed input: 8542.33 toks/s, output: 16.68 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:05<00:01, 17.04it/s, est. speed input: 8545.57 toks/s, output: 16.69 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:06<00:01, 17.11it/s, est. speed input: 8551.21 toks/s, output: 16.70 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 17.01it/s, est. speed input: 8551.95 toks/s, output: 16.70 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 16.92it/s, est. speed input: 8552.00 toks/s, output: 16.70 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 16.88it/s, est. speed input: 8552.97 toks/s, output: 16.70 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 16.85it/s, est. speed input: 8553.71 toks/s, output: 16.71 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:00, 16.79it/s, est. speed input: 8553.27 toks/s, output: 16.71 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 16.98it/s, est. speed input: 8559.53 toks/s, output: 16.72 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:06<00:00, 16.90it/s, est. speed input: 8559.48 toks/s, output: 16.72 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:07<00:00, 16.96it/s, est. speed input: 8562.83 toks/s, output: 16.72 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:07<00:00, 17.10it/s, est. speed input: 8568.65 toks/s, output: 16.74 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 17.09it/s, est. speed input: 8571.44 toks/s, output: 16.74 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 16.90it/s, est. speed input: 8569.03 toks/s, output: 16.74 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 17.00it/s, est. speed input: 8572.93 toks/s, output: 16.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.06it/s, est. speed input: 8576.57 toks/s, output: 16.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.06it/s, est. speed input: 8576.57 toks/s, output: 16.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.75it/s, est. speed input: 8576.57 toks/s, output: 16.75 toks/s]
[rank0]:[W126 04:47:02.699007377 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 04:47:04
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:47:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:47:13 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=193051) WARNING 01-26 04:47:21 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=193051) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=193051) WARNING 01-26 04:47:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.80 requests/s, 16194.24 total tokens/s, 15.80 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 04:47:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:47:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:47:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:47:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:47:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:47:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:47:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:47:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:47:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:47:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:47:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:47:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:47:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:47:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:47:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:47:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:47:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:21] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=193051) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=193051) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.06it/s]
(EngineCore_DP0 pid=193051) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.34s/it]
(EngineCore_DP0 pid=193051) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.28s/it]
(EngineCore_DP0 pid=193051) 
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 163676160 bytes
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=193051) [2026-01-26 04:47:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 81543168 bytes
(EngineCore_DP0 pid=193051) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.23it/s]
(EngineCore_DP0 pid=193051) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.36it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  19%|█▉        | 24/128 [00:00<00:00, 235.40it/s]
Adding requests:  38%|███▊      | 49/128 [00:00<00:00, 237.36it/s]
Adding requests:  58%|█████▊    | 74/128 [00:00<00:00, 242.45it/s]
Adding requests:  77%|███████▋  | 99/128 [00:00<00:00, 244.91it/s]
Adding requests:  97%|█████████▋| 124/128 [00:00<00:00, 245.85it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 243.26it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 32.98it/s, est. speed input: 33775.18 toks/s, output: 32.98 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 21.68it/s, est. speed input: 23544.50 toks/s, output: 22.99 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:05, 19.44it/s, est. speed input: 21419.06 toks/s, output: 20.92 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:06, 18.41it/s, est. speed input: 20393.39 toks/s, output: 19.91 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:06, 17.95it/s, est. speed input: 19934.04 toks/s, output: 19.47 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:06, 17.60it/s, est. speed input: 19589.30 toks/s, output: 19.13 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 17.33it/s, est. speed input: 19314.30 toks/s, output: 18.86 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 17.15it/s, est. speed input: 19099.28 toks/s, output: 18.65 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 17.03it/s, est. speed input: 18924.61 toks/s, output: 18.48 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 16.95it/s, est. speed input: 18782.01 toks/s, output: 18.34 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 16.83it/s, est. speed input: 18644.05 toks/s, output: 18.21 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.80it/s, est. speed input: 18536.41 toks/s, output: 18.10 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 16.76it/s, est. speed input: 18440.30 toks/s, output: 18.01 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 16.65it/s, est. speed input: 18336.67 toks/s, output: 17.91 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.56it/s, est. speed input: 18244.43 toks/s, output: 17.82 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.46it/s, est. speed input: 18151.99 toks/s, output: 17.73 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.47it/s, est. speed input: 18087.35 toks/s, output: 17.66 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.51it/s, est. speed input: 18032.71 toks/s, output: 17.61 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 16.54it/s, est. speed input: 17985.82 toks/s, output: 17.56 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 16.54it/s, est. speed input: 17938.49 toks/s, output: 17.52 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 16.53it/s, est. speed input: 17892.93 toks/s, output: 17.47 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 16.48it/s, est. speed input: 17846.21 toks/s, output: 17.43 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 16.47it/s, est. speed input: 17805.92 toks/s, output: 17.39 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.47it/s, est. speed input: 17769.45 toks/s, output: 17.35 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.54it/s, est. speed input: 17745.81 toks/s, output: 17.33 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.59it/s, est. speed input: 17723.51 toks/s, output: 17.31 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.60it/s, est. speed input: 17699.96 toks/s, output: 17.28 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.63it/s, est. speed input: 17679.46 toks/s, output: 17.26 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 16.63it/s, est. speed input: 17659.01 toks/s, output: 17.25 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 16.65it/s, est. speed input: 17641.43 toks/s, output: 17.23 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.62it/s, est. speed input: 17620.92 toks/s, output: 17.21 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.57it/s, est. speed input: 17598.41 toks/s, output: 17.19 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.60it/s, est. speed input: 17582.97 toks/s, output: 17.17 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.64it/s, est. speed input: 17570.89 toks/s, output: 17.16 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.63it/s, est. speed input: 17556.09 toks/s, output: 17.14 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 16.60it/s, est. speed input: 17539.85 toks/s, output: 17.13 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.61it/s, est. speed input: 17526.75 toks/s, output: 17.12 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 16.61it/s, est. speed input: 17513.76 toks/s, output: 17.10 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 16.64it/s, est. speed input: 17503.71 toks/s, output: 17.09 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.55it/s, est. speed input: 17485.70 toks/s, output: 17.08 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.48it/s, est. speed input: 17467.18 toks/s, output: 17.06 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.50it/s, est. speed input: 17455.42 toks/s, output: 17.05 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.51it/s, est. speed input: 17443.48 toks/s, output: 17.03 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.51it/s, est. speed input: 17432.16 toks/s, output: 17.02 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.51it/s, est. speed input: 17421.03 toks/s, output: 17.01 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 16.46it/s, est. speed input: 17406.53 toks/s, output: 17.00 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 16.48it/s, est. speed input: 17396.49 toks/s, output: 16.99 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.52it/s, est. speed input: 17388.86 toks/s, output: 16.98 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.46it/s, est. speed input: 17375.66 toks/s, output: 16.97 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.49it/s, est. speed input: 17367.62 toks/s, output: 16.96 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.54it/s, est. speed input: 17361.59 toks/s, output: 16.95 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.58it/s, est. speed input: 17356.56 toks/s, output: 16.95 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.60it/s, est. speed input: 17350.98 toks/s, output: 16.94 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 16.59it/s, est. speed input: 17344.09 toks/s, output: 16.94 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 16.59it/s, est. speed input: 17337.62 toks/s, output: 16.93 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 16.60it/s, est. speed input: 17332.61 toks/s, output: 16.93 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 16.62it/s, est. speed input: 17327.89 toks/s, output: 16.92 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 16.54it/s, est. speed input: 17318.25 toks/s, output: 16.91 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.56it/s, est. speed input: 17313.35 toks/s, output: 16.91 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.57it/s, est. speed input: 17308.31 toks/s, output: 16.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.57it/s, est. speed input: 17306.07 toks/s, output: 16.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.90it/s, est. speed input: 17306.07 toks/s, output: 16.90 toks/s]
[rank0]:[W126 04:47:59.610359483 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 04:48:02
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:48:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:48:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=194115) WARNING 01-26 04:48:18 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=194115) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=194115) WARNING 01-26 04:48:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.19 requests/s, 17617.78 total tokens/s, 17.19 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 04:48:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:48:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:48:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:48:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:48:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:48:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:48:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:48:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:48:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:48:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:48:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:48:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:48:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:48:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:48:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:48:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:48:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=194115) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=194115) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.03it/s]
(EngineCore_DP0 pid=194115) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]
(EngineCore_DP0 pid=194115) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]
(EngineCore_DP0 pid=194115) 
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 163676160 bytes
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=194115) [2026-01-26 04:48:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 81543168 bytes
(EngineCore_DP0 pid=194115) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.84it/s]
(EngineCore_DP0 pid=194115) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.93it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   9%|▉         | 24/256 [00:00<00:00, 234.41it/s]
Adding requests:  20%|█▉        | 50/256 [00:00<00:00, 245.77it/s]
Adding requests:  30%|███       | 78/256 [00:00<00:00, 256.90it/s]
Adding requests:  41%|████      | 104/256 [00:00<00:00, 254.64it/s]
Adding requests:  51%|█████     | 130/256 [00:00<00:00, 251.89it/s]
Adding requests:  61%|██████    | 156/256 [00:00<00:00, 250.69it/s]
Adding requests:  71%|███████▏  | 183/256 [00:00<00:00, 256.06it/s]
Adding requests:  82%|████████▏ | 210/256 [00:00<00:00, 259.34it/s]
Adding requests:  93%|█████████▎| 237/256 [00:00<00:00, 260.76it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 255.78it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:03, 75.52it/s, est. speed input: 77341.51 toks/s, output: 75.52 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:00<00:07, 31.74it/s, est. speed input: 36769.23 toks/s, output: 35.91 toks/s]
Processed prompts:  11%|█▏        | 29/256 [00:00<00:08, 28.24it/s, est. speed input: 33065.57 toks/s, output: 32.29 toks/s]
Processed prompts:  13%|█▎        | 33/256 [00:01<00:09, 24.64it/s, est. speed input: 29981.15 toks/s, output: 29.28 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:01<00:10, 20.92it/s, est. speed input: 27189.54 toks/s, output: 26.55 toks/s]
Processed prompts:  15%|█▌        | 39/256 [00:01<00:09, 21.91it/s, est. speed input: 27161.06 toks/s, output: 26.52 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:01<00:11, 18.82it/s, est. speed input: 25298.31 toks/s, output: 24.70 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:01<00:11, 18.54it/s, est. speed input: 24826.84 toks/s, output: 24.24 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:01<00:11, 18.30it/s, est. speed input: 24413.46 toks/s, output: 23.84 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:02<00:11, 18.11it/s, est. speed input: 24051.17 toks/s, output: 23.49 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:02<00:11, 17.98it/s, est. speed input: 23733.26 toks/s, output: 23.18 toks/s]
Processed prompts:  20%|██        | 52/256 [00:02<00:11, 17.87it/s, est. speed input: 23445.98 toks/s, output: 22.90 toks/s]
Processed prompts:  21%|██        | 54/256 [00:02<00:11, 17.79it/s, est. speed input: 23185.02 toks/s, output: 22.64 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:02<00:11, 17.72it/s, est. speed input: 22946.71 toks/s, output: 22.41 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:02<00:11, 17.67it/s, est. speed input: 22729.86 toks/s, output: 22.20 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:02<00:11, 17.63it/s, est. speed input: 22530.69 toks/s, output: 22.00 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:02<00:11, 17.58it/s, est. speed input: 22342.95 toks/s, output: 21.82 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:02<00:10, 17.56it/s, est. speed input: 22173.00 toks/s, output: 21.65 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:03<00:10, 17.55it/s, est. speed input: 22015.82 toks/s, output: 21.50 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:03<00:10, 17.54it/s, est. speed input: 21869.90 toks/s, output: 21.36 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:03<00:10, 17.56it/s, est. speed input: 21737.69 toks/s, output: 21.23 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:03<00:10, 17.55it/s, est. speed input: 21609.95 toks/s, output: 21.10 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:03<00:10, 17.56it/s, est. speed input: 21494.32 toks/s, output: 20.99 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:03<00:10, 17.57it/s, est. speed input: 21385.01 toks/s, output: 20.88 toks/s]
Processed prompts:  30%|███       | 78/256 [00:03<00:10, 17.56it/s, est. speed input: 21281.48 toks/s, output: 20.78 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:03<00:10, 17.56it/s, est. speed input: 21183.76 toks/s, output: 20.69 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:03<00:09, 17.52it/s, est. speed input: 21088.26 toks/s, output: 20.59 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:04<00:09, 17.49it/s, est. speed input: 20996.76 toks/s, output: 20.50 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:04<00:09, 17.48it/s, est. speed input: 20911.43 toks/s, output: 20.42 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:04<00:09, 17.49it/s, est. speed input: 20833.13 toks/s, output: 20.34 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:04<00:09, 17.46it/s, est. speed input: 20755.09 toks/s, output: 20.27 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:04<00:09, 17.46it/s, est. speed input: 20682.49 toks/s, output: 20.20 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:04<00:09, 17.48it/s, est. speed input: 20615.70 toks/s, output: 20.13 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:04<00:09, 17.47it/s, est. speed input: 20549.63 toks/s, output: 20.07 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:04<00:09, 17.44it/s, est. speed input: 20485.09 toks/s, output: 20.00 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:05<00:08, 17.43it/s, est. speed input: 20424.36 toks/s, output: 19.95 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:05<00:08, 17.45it/s, est. speed input: 20368.02 toks/s, output: 19.89 toks/s]
Processed prompts:  41%|████      | 104/256 [00:05<00:08, 17.47it/s, est. speed input: 20314.78 toks/s, output: 19.84 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:05<00:08, 17.46it/s, est. speed input: 20262.37 toks/s, output: 19.79 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:05<00:08, 17.46it/s, est. speed input: 20212.73 toks/s, output: 19.74 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:05<00:08, 17.47it/s, est. speed input: 20165.31 toks/s, output: 19.69 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:05<00:08, 17.49it/s, est. speed input: 20121.43 toks/s, output: 19.65 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:05<00:08, 17.50it/s, est. speed input: 20078.71 toks/s, output: 19.61 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:05<00:07, 17.51it/s, est. speed input: 20037.64 toks/s, output: 19.57 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:06<00:07, 17.53it/s, est. speed input: 19999.07 toks/s, output: 19.53 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:06<00:07, 17.52it/s, est. speed input: 19960.45 toks/s, output: 19.49 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:06<00:07, 17.51it/s, est. speed input: 19923.00 toks/s, output: 19.46 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:06<00:07, 17.50it/s, est. speed input: 19886.73 toks/s, output: 19.42 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:06<00:07, 17.48it/s, est. speed input: 19851.12 toks/s, output: 19.39 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:06<00:07, 17.49it/s, est. speed input: 19817.47 toks/s, output: 19.35 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:06<00:07, 17.49it/s, est. speed input: 19785.58 toks/s, output: 19.32 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:06<00:07, 17.51it/s, est. speed input: 19755.09 toks/s, output: 19.29 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:06<00:06, 17.48it/s, est. speed input: 19723.43 toks/s, output: 19.26 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:07<00:06, 17.50it/s, est. speed input: 19694.96 toks/s, output: 19.23 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:07<00:06, 17.50it/s, est. speed input: 19666.90 toks/s, output: 19.21 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:07<00:06, 17.47it/s, est. speed input: 19637.57 toks/s, output: 19.18 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:07<00:06, 17.45it/s, est. speed input: 19609.74 toks/s, output: 19.15 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:07<00:06, 17.44it/s, est. speed input: 19582.44 toks/s, output: 19.12 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:07<00:06, 17.44it/s, est. speed input: 19556.91 toks/s, output: 19.10 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:07<00:06, 17.46it/s, est. speed input: 19532.52 toks/s, output: 19.07 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:07<00:06, 17.45it/s, est. speed input: 19508.32 toks/s, output: 19.05 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:07<00:05, 17.45it/s, est. speed input: 19484.63 toks/s, output: 19.03 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:08<00:05, 17.45it/s, est. speed input: 19461.51 toks/s, output: 19.01 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:08<00:05, 17.46it/s, est. speed input: 19439.83 toks/s, output: 18.98 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:08<00:05, 17.44it/s, est. speed input: 19417.36 toks/s, output: 18.96 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:08<00:05, 17.44it/s, est. speed input: 19396.10 toks/s, output: 18.94 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:08<00:05, 17.46it/s, est. speed input: 19376.41 toks/s, output: 18.92 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:08<00:05, 17.48it/s, est. speed input: 19357.96 toks/s, output: 18.90 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:08<00:05, 17.48it/s, est. speed input: 19339.05 toks/s, output: 18.89 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:08<00:05, 17.47it/s, est. speed input: 19320.04 toks/s, output: 18.87 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:09<00:04, 17.46it/s, est. speed input: 19301.41 toks/s, output: 18.85 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:09<00:04, 17.48it/s, est. speed input: 19284.57 toks/s, output: 18.83 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:09<00:04, 17.50it/s, est. speed input: 19268.16 toks/s, output: 18.82 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:09<00:04, 17.49it/s, est. speed input: 19251.49 toks/s, output: 18.80 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:09<00:04, 17.48it/s, est. speed input: 19234.67 toks/s, output: 18.78 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:09<00:04, 17.48it/s, est. speed input: 19218.83 toks/s, output: 18.77 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:09<00:04, 17.51it/s, est. speed input: 19204.53 toks/s, output: 18.75 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:09<00:04, 17.51it/s, est. speed input: 19189.52 toks/s, output: 18.74 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:09<00:03, 17.51it/s, est. speed input: 19175.03 toks/s, output: 18.73 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:10<00:03, 17.51it/s, est. speed input: 19160.84 toks/s, output: 18.71 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:10<00:03, 17.51it/s, est. speed input: 19147.30 toks/s, output: 18.70 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:10<00:03, 17.53it/s, est. speed input: 19134.52 toks/s, output: 18.69 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:10<00:03, 17.52it/s, est. speed input: 19120.97 toks/s, output: 18.67 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:10<00:03, 17.48it/s, est. speed input: 19106.53 toks/s, output: 18.66 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:10<00:03, 17.46it/s, est. speed input: 19092.79 toks/s, output: 18.65 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:10<00:03, 17.45it/s, est. speed input: 19079.47 toks/s, output: 18.63 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:10<00:03, 17.45it/s, est. speed input: 19066.75 toks/s, output: 18.62 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:10<00:02, 17.46it/s, est. speed input: 19054.68 toks/s, output: 18.61 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:11<00:02, 17.43it/s, est. speed input: 19041.49 toks/s, output: 18.60 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:11<00:02, 17.43it/s, est. speed input: 19028.96 toks/s, output: 18.58 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:11<00:02, 17.42it/s, est. speed input: 19016.69 toks/s, output: 18.57 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:11<00:02, 17.43it/s, est. speed input: 19005.22 toks/s, output: 18.56 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:11<00:02, 17.41it/s, est. speed input: 18992.96 toks/s, output: 18.55 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:11<00:02, 17.44it/s, est. speed input: 18982.57 toks/s, output: 18.54 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:11<00:02, 17.47it/s, est. speed input: 18972.60 toks/s, output: 18.53 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:11<00:02, 17.48it/s, est. speed input: 18962.53 toks/s, output: 18.52 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:11<00:01, 17.46it/s, est. speed input: 18951.77 toks/s, output: 18.51 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:12<00:01, 17.45it/s, est. speed input: 18941.40 toks/s, output: 18.50 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:12<00:01, 17.45it/s, est. speed input: 18931.33 toks/s, output: 18.49 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:12<00:01, 17.46it/s, est. speed input: 18921.83 toks/s, output: 18.48 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:12<00:01, 17.44it/s, est. speed input: 18911.75 toks/s, output: 18.47 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:12<00:01, 17.45it/s, est. speed input: 18902.16 toks/s, output: 18.46 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:12<00:01, 17.45it/s, est. speed input: 18892.78 toks/s, output: 18.45 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:12<00:01, 17.45it/s, est. speed input: 18883.78 toks/s, output: 18.44 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:12<00:01, 17.46it/s, est. speed input: 18874.99 toks/s, output: 18.43 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:13<00:00, 17.47it/s, est. speed input: 18866.69 toks/s, output: 18.42 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:13<00:00, 17.45it/s, est. speed input: 18857.37 toks/s, output: 18.42 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:13<00:00, 17.47it/s, est. speed input: 18849.39 toks/s, output: 18.41 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:13<00:00, 17.47it/s, est. speed input: 18841.18 toks/s, output: 18.40 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:13<00:00, 17.43it/s, est. speed input: 18832.06 toks/s, output: 18.39 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:13<00:00, 17.41it/s, est. speed input: 18823.15 toks/s, output: 18.38 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:13<00:00, 17.40it/s, est. speed input: 18814.57 toks/s, output: 18.37 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:13<00:00, 17.40it/s, est. speed input: 18806.05 toks/s, output: 18.37 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 17.40it/s, est. speed input: 18870.97 toks/s, output: 18.43 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.43it/s, est. speed input: 18870.97 toks/s, output: 18.43 toks/s]
[rank0]:[W126 04:49:03.523857207 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 04:49:06
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:49:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:49:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=195278) WARNING 01-26 04:49:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=195278) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=195278) WARNING 01-26 04:49:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.94 requests/s, 18388.07 total tokens/s, 17.94 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 04:49:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:49:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:49:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:49:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:49:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:49:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:49:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:49:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:49:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:49:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:49:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:49:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:49:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:49:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:49:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:49:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:49:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=195278) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=195278) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.01it/s]
(EngineCore_DP0 pid=195278) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.27s/it]
(EngineCore_DP0 pid=195278) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]
(EngineCore_DP0 pid=195278) 
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 163676160 bytes
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=195278) [2026-01-26 04:49:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 81543168 bytes
(EngineCore_DP0 pid=195278) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  9.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.51it/s]
(EngineCore_DP0 pid=195278) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.64it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 23/512 [00:00<00:02, 228.67it/s]
Adding requests:  10%|▉         | 50/512 [00:00<00:01, 247.57it/s]
Adding requests:  15%|█▌        | 78/512 [00:00<00:01, 259.11it/s]
Adding requests:  20%|██        | 104/512 [00:00<00:01, 258.87it/s]
Adding requests:  26%|██▌       | 131/512 [00:00<00:01, 260.07it/s]
Adding requests:  31%|███       | 159/512 [00:00<00:01, 265.33it/s]
Adding requests:  36%|███▋      | 186/512 [00:00<00:01, 265.98it/s]
Adding requests:  42%|████▏     | 213/512 [00:00<00:01, 262.98it/s]
Adding requests:  47%|████▋     | 240/512 [00:00<00:01, 261.78it/s]
Adding requests:  52%|█████▏    | 267/512 [00:01<00:00, 263.67it/s]
Adding requests:  57%|█████▋    | 294/512 [00:01<00:00, 261.87it/s]
Adding requests:  63%|██████▎   | 322/512 [00:01<00:00, 266.05it/s]
Adding requests:  68%|██████▊   | 350/512 [00:01<00:00, 268.62it/s]
Adding requests:  74%|███████▍  | 378/512 [00:01<00:00, 270.15it/s]
Adding requests:  79%|███████▉  | 407/512 [00:01<00:00, 275.13it/s]
Adding requests:  85%|████████▍ | 435/512 [00:01<00:00, 272.36it/s]
Adding requests:  90%|█████████ | 463/512 [00:01<00:00, 270.07it/s]
Adding requests:  96%|█████████▌| 491/512 [00:01<00:00, 271.21it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 265.35it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:00<00:01, 254.38it/s, est. speed input: 260549.49 toks/s, output: 254.40 toks/s]
Processed prompts:  11%|█         | 56/512 [00:01<00:13, 33.61it/s, est. speed input: 39998.76 toks/s, output: 39.06 toks/s]   
Processed prompts:  13%|█▎        | 68/512 [00:02<00:16, 27.43it/s, est. speed input: 33288.23 toks/s, output: 32.51 toks/s]
Processed prompts:  15%|█▍        | 76/512 [00:02<00:17, 24.88it/s, est. speed input: 30753.99 toks/s, output: 30.03 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:02<00:19, 21.72it/s, est. speed input: 28287.55 toks/s, output: 27.62 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:03<00:20, 21.14it/s, est. speed input: 27627.75 toks/s, output: 26.98 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:03<00:20, 20.57it/s, est. speed input: 27050.35 toks/s, output: 26.42 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:03<00:20, 20.06it/s, est. speed input: 26541.81 toks/s, output: 25.92 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:03<00:21, 19.62it/s, est. speed input: 26089.65 toks/s, output: 25.48 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:04<00:21, 19.26it/s, est. speed input: 25687.06 toks/s, output: 25.08 toks/s]
Processed prompts:  21%|██        | 106/512 [00:04<00:21, 18.99it/s, est. speed input: 25327.45 toks/s, output: 24.73 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:04<00:21, 18.77it/s, est. speed input: 25000.93 toks/s, output: 24.41 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:04<00:21, 18.61it/s, est. speed input: 24706.39 toks/s, output: 24.13 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:04<00:21, 18.50it/s, est. speed input: 24437.05 toks/s, output: 23.86 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:05<00:21, 18.42it/s, est. speed input: 24191.52 toks/s, output: 23.62 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:05<00:21, 18.35it/s, est. speed input: 23964.45 toks/s, output: 23.40 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:05<00:20, 18.31it/s, est. speed input: 23755.10 toks/s, output: 23.20 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:05<00:20, 18.28it/s, est. speed input: 23562.82 toks/s, output: 23.01 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:06<00:20, 18.26it/s, est. speed input: 23384.18 toks/s, output: 22.84 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:06<00:20, 18.24it/s, est. speed input: 23217.96 toks/s, output: 22.67 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:06<00:20, 18.22it/s, est. speed input: 23061.30 toks/s, output: 22.52 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:06<00:19, 18.21it/s, est. speed input: 22915.05 toks/s, output: 22.38 toks/s]
Processed prompts:  30%|███       | 154/512 [00:06<00:19, 18.19it/s, est. speed input: 22777.33 toks/s, output: 22.24 toks/s]
Processed prompts:  31%|███       | 158/512 [00:07<00:19, 18.17it/s, est. speed input: 22647.45 toks/s, output: 22.12 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:07<00:19, 18.17it/s, est. speed input: 22526.56 toks/s, output: 22.00 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:07<00:19, 18.16it/s, est. speed input: 22411.82 toks/s, output: 21.89 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:07<00:18, 18.16it/s, est. speed input: 22303.61 toks/s, output: 21.78 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:08<00:18, 18.16it/s, est. speed input: 22201.84 toks/s, output: 21.68 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:08<00:18, 18.16it/s, est. speed input: 22105.48 toks/s, output: 21.59 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:08<00:18, 18.15it/s, est. speed input: 22013.08 toks/s, output: 21.50 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:08<00:17, 18.15it/s, est. speed input: 21925.74 toks/s, output: 21.41 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:08<00:17, 18.15it/s, est. speed input: 21843.82 toks/s, output: 21.33 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:09<00:17, 18.15it/s, est. speed input: 21764.84 toks/s, output: 21.25 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:09<00:17, 18.14it/s, est. speed input: 21689.19 toks/s, output: 21.18 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:09<00:17, 18.13it/s, est. speed input: 21616.50 toks/s, output: 21.11 toks/s]
Processed prompts:  40%|████      | 206/512 [00:09<00:16, 18.13it/s, est. speed input: 21547.46 toks/s, output: 21.04 toks/s]
Processed prompts:  41%|████      | 210/512 [00:10<00:16, 18.12it/s, est. speed input: 21480.70 toks/s, output: 20.98 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:10<00:16, 18.11it/s, est. speed input: 21416.88 toks/s, output: 20.91 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:10<00:16, 18.10it/s, est. speed input: 21355.79 toks/s, output: 20.86 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:10<00:16, 18.10it/s, est. speed input: 21297.14 toks/s, output: 20.80 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:10<00:15, 18.09it/s, est. speed input: 21239.93 toks/s, output: 20.74 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:11<00:15, 18.09it/s, est. speed input: 21186.24 toks/s, output: 20.69 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:11<00:15, 18.10it/s, est. speed input: 21134.73 toks/s, output: 20.64 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:11<00:15, 18.10it/s, est. speed input: 21084.86 toks/s, output: 20.59 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:11<00:14, 18.09it/s, est. speed input: 21036.16 toks/s, output: 20.54 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:12<00:14, 18.07it/s, est. speed input: 20989.05 toks/s, output: 20.50 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:12<00:14, 18.07it/s, est. speed input: 20944.08 toks/s, output: 20.45 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:12<00:14, 18.07it/s, est. speed input: 20900.59 toks/s, output: 20.41 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:12<00:14, 18.07it/s, est. speed input: 20858.76 toks/s, output: 20.37 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:12<00:13, 18.07it/s, est. speed input: 20817.98 toks/s, output: 20.33 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:13<00:13, 18.06it/s, est. speed input: 20778.64 toks/s, output: 20.29 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:13<00:13, 18.06it/s, est. speed input: 20740.67 toks/s, output: 20.25 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:13<00:13, 18.06it/s, est. speed input: 20704.08 toks/s, output: 20.22 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:13<00:12, 18.06it/s, est. speed input: 20668.45 toks/s, output: 20.18 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:13<00:12, 18.05it/s, est. speed input: 20633.60 toks/s, output: 20.15 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:14<00:12, 18.04it/s, est. speed input: 20599.51 toks/s, output: 20.12 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:14<00:12, 18.06it/s, est. speed input: 20567.72 toks/s, output: 20.09 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:14<00:12, 18.06it/s, est. speed input: 20536.53 toks/s, output: 20.06 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:14<00:11, 18.05it/s, est. speed input: 20505.43 toks/s, output: 20.02 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:15<00:11, 18.04it/s, est. speed input: 20475.39 toks/s, output: 20.00 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:15<00:11, 18.05it/s, est. speed input: 20446.90 toks/s, output: 19.97 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:15<00:11, 18.05it/s, est. speed input: 20418.89 toks/s, output: 19.94 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:15<00:10, 18.05it/s, est. speed input: 20391.44 toks/s, output: 19.91 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:15<00:10, 18.04it/s, est. speed input: 20364.80 toks/s, output: 19.89 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:16<00:10, 18.04it/s, est. speed input: 20338.67 toks/s, output: 19.86 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:16<00:10, 18.05it/s, est. speed input: 20314.03 toks/s, output: 19.84 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:16<00:10, 18.05it/s, est. speed input: 20289.70 toks/s, output: 19.81 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:16<00:09, 18.04it/s, est. speed input: 20265.54 toks/s, output: 19.79 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:17<00:09, 18.04it/s, est. speed input: 20242.30 toks/s, output: 19.77 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:17<00:09, 18.03it/s, est. speed input: 20219.14 toks/s, output: 19.75 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:17<00:09, 18.02it/s, est. speed input: 20196.63 toks/s, output: 19.72 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:17<00:08, 18.02it/s, est. speed input: 20174.64 toks/s, output: 19.70 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:17<00:08, 18.02it/s, est. speed input: 20153.49 toks/s, output: 19.68 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:18<00:08, 18.02it/s, est. speed input: 20132.76 toks/s, output: 19.66 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:18<00:08, 18.02it/s, est. speed input: 20112.61 toks/s, output: 19.64 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:18<00:08, 18.01it/s, est. speed input: 20092.36 toks/s, output: 19.62 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:18<00:07, 18.00it/s, est. speed input: 20072.69 toks/s, output: 19.60 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:19<00:07, 18.00it/s, est. speed input: 20053.41 toks/s, output: 19.58 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:19<00:07, 18.00it/s, est. speed input: 20034.66 toks/s, output: 19.57 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:19<00:07, 18.00it/s, est. speed input: 20016.61 toks/s, output: 19.55 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:19<00:07, 18.00it/s, est. speed input: 19998.69 toks/s, output: 19.53 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:19<00:06, 18.00it/s, est. speed input: 19981.10 toks/s, output: 19.51 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:20<00:06, 17.99it/s, est. speed input: 19963.94 toks/s, output: 19.50 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:20<00:06, 18.00it/s, est. speed input: 19947.27 toks/s, output: 19.48 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:20<00:06, 17.99it/s, est. speed input: 19930.81 toks/s, output: 19.46 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:20<00:05, 17.99it/s, est. speed input: 19914.65 toks/s, output: 19.45 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:21<00:05, 17.99it/s, est. speed input: 19898.76 toks/s, output: 19.43 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:21<00:05, 17.99it/s, est. speed input: 19883.56 toks/s, output: 19.42 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:21<00:05, 17.99it/s, est. speed input: 19868.31 toks/s, output: 19.40 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:21<00:05, 17.99it/s, est. speed input: 19853.48 toks/s, output: 19.39 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:21<00:04, 17.98it/s, est. speed input: 19838.82 toks/s, output: 19.37 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:22<00:04, 17.99it/s, est. speed input: 19824.89 toks/s, output: 19.36 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:22<00:00, 54.22it/s, est. speed input: 20955.65 toks/s, output: 20.46 toks/s]
Processed prompts:  91%|█████████ | 465/512 [00:22<00:00, 47.04it/s, est. speed input: 21066.54 toks/s, output: 20.57 toks/s]
Processed prompts:  92%|█████████▏| 471/512 [00:23<00:01, 31.14it/s, est. speed input: 20927.02 toks/s, output: 20.44 toks/s]
Processed prompts:  93%|█████████▎| 476/512 [00:23<00:01, 28.96it/s, est. speed input: 20947.32 toks/s, output: 20.46 toks/s]
Processed prompts:  94%|█████████▍| 480/512 [00:23<00:01, 26.06it/s, est. speed input: 20923.71 toks/s, output: 20.43 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [00:23<00:01, 23.84it/s, est. speed input: 20900.28 toks/s, output: 20.41 toks/s]
Processed prompts:  95%|█████████▌| 487/512 [00:23<00:01, 20.92it/s, est. speed input: 20834.49 toks/s, output: 20.35 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:24<00:01, 18.79it/s, est. speed input: 20770.00 toks/s, output: 20.28 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:24<00:00, 18.56it/s, est. speed input: 20748.59 toks/s, output: 20.26 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:24<00:00, 18.39it/s, est. speed input: 20727.70 toks/s, output: 20.24 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:24<00:00, 18.28it/s, est. speed input: 20707.14 toks/s, output: 20.22 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:25<00:00, 18.21it/s, est. speed input: 20687.58 toks/s, output: 20.20 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:25<00:00, 19.32it/s, est. speed input: 20704.79 toks/s, output: 20.22 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:25<00:00, 19.32it/s, est. speed input: 20785.79 toks/s, output: 20.30 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:25<00:00, 20.30it/s, est. speed input: 20785.79 toks/s, output: 20.30 toks/s]
[rank0]:[W126 04:50:22.219354466 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 04:50:24
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:50:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:50:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=196692) WARNING 01-26 04:50:48 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=196692) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=196692) WARNING 01-26 04:51:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 18.14 requests/s, 18591.02 total tokens/s, 18.14 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 04:50:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:50:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:50:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:50:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:50:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:50:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:50:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:50:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:50:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:50:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:50:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:50:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:50:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:50:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:50:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:50:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:50:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:49] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:49] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:49] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:49] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:49] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=196692) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=196692) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.95it/s]
(EngineCore_DP0 pid=196692) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.94it/s]
(EngineCore_DP0 pid=196692) 
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 163676160 bytes
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=196692) [2026-01-26 04:50:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 81543168 bytes
(EngineCore_DP0 pid=196692) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  4.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  5.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  6.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  6.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  5.62it/s]
(EngineCore_DP0 pid=196692) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.45it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.87it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 23/1024 [00:00<00:04, 228.61it/s]
Adding requests:   5%|▍         | 47/1024 [00:00<00:04, 233.69it/s]
Adding requests:   7%|▋         | 73/1024 [00:00<00:03, 245.22it/s]
Adding requests:  10%|▉         | 98/1024 [00:00<00:03, 246.58it/s]
Adding requests:  12%|█▏        | 125/1024 [00:00<00:03, 253.02it/s]
Adding requests:  15%|█▍        | 151/1024 [00:00<00:03, 253.00it/s]
Adding requests:  17%|█▋        | 179/1024 [00:00<00:03, 258.71it/s]
Adding requests:  20%|██        | 208/1024 [00:00<00:03, 265.38it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:02, 269.61it/s]
Adding requests:  26%|██▌       | 263/1024 [00:01<00:02, 266.52it/s]
Adding requests:  28%|██▊       | 290/1024 [00:01<00:02, 264.47it/s]
Adding requests:  31%|███       | 319/1024 [00:01<00:02, 270.87it/s]
Adding requests:  34%|███▍      | 347/1024 [00:01<00:02, 270.76it/s]
Adding requests:  37%|███▋      | 376/1024 [00:01<00:02, 276.44it/s]
Adding requests:  40%|███▉      | 406/1024 [00:01<00:02, 282.47it/s]
Adding requests:  42%|████▏     | 435/1024 [00:01<00:02, 277.10it/s]
Adding requests:  45%|████▌     | 463/1024 [00:01<00:02, 275.10it/s]
Adding requests:  48%|████▊     | 493/1024 [00:01<00:01, 282.34it/s]
Adding requests:  51%|█████     | 524/1024 [00:01<00:01, 289.51it/s]
Adding requests:  54%|█████▍    | 553/1024 [00:02<00:01, 286.07it/s]
Adding requests:  93%|█████████▎| 955/1024 [00:02<00:00, 1382.13it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 423.48it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:00<00:05, 160.34it/s, est. speed input: 164195.92 toks/s, output: 160.34 toks/s]
Processed prompts:   8%|▊         | 83/1024 [00:01<00:17, 54.62it/s, est. speed input: 66375.95 toks/s, output: 64.82 toks/s]   
Processed prompts:   9%|▉         | 91/1024 [00:01<00:22, 41.98it/s, est. speed input: 54332.02 toks/s, output: 53.06 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:02<00:27, 33.37it/s, est. speed input: 46684.41 toks/s, output: 45.59 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:02<00:32, 28.54it/s, est. speed input: 41994.73 toks/s, output: 41.01 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:03<00:35, 25.33it/s, est. speed input: 38653.96 toks/s, output: 37.75 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:03<00:38, 23.17it/s, est. speed input: 36152.26 toks/s, output: 35.30 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:03<00:41, 21.69it/s, est. speed input: 34210.45 toks/s, output: 33.41 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:04<00:42, 20.67it/s, est. speed input: 32655.87 toks/s, output: 31.89 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:04<00:43, 19.95it/s, est. speed input: 31382.59 toks/s, output: 30.65 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:05<00:44, 19.46it/s, est. speed input: 30324.50 toks/s, output: 29.61 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:05<00:45, 19.11it/s, est. speed input: 29426.92 toks/s, output: 28.74 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:06<00:45, 18.87it/s, est. speed input: 28658.77 toks/s, output: 27.99 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:06<00:45, 18.70it/s, est. speed input: 27991.93 toks/s, output: 27.34 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:06<00:45, 18.58it/s, est. speed input: 27409.72 toks/s, output: 26.77 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:07<00:44, 18.49it/s, est. speed input: 26894.98 toks/s, output: 26.26 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:07<00:44, 18.42it/s, est. speed input: 26437.37 toks/s, output: 25.82 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:08<00:44, 18.38it/s, est. speed input: 26028.13 toks/s, output: 25.42 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:08<00:43, 18.35it/s, est. speed input: 25659.44 toks/s, output: 25.06 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:09<00:43, 18.32it/s, est. speed input: 25325.16 toks/s, output: 24.73 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:09<00:43, 18.30it/s, est. speed input: 25021.58 toks/s, output: 24.44 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:10<00:42, 18.28it/s, est. speed input: 24744.27 toks/s, output: 24.16 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:10<00:42, 18.27it/s, est. speed input: 24489.48 toks/s, output: 23.92 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:10<00:41, 18.26it/s, est. speed input: 24255.26 toks/s, output: 23.69 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:11<00:41, 18.25it/s, est. speed input: 24038.95 toks/s, output: 23.48 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:11<00:41, 18.24it/s, est. speed input: 23838.63 toks/s, output: 23.28 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:12<00:40, 18.24it/s, est. speed input: 23652.28 toks/s, output: 23.10 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:12<00:40, 18.23it/s, est. speed input: 23478.62 toks/s, output: 22.93 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:13<00:39, 18.22it/s, est. speed input: 23316.34 toks/s, output: 22.77 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:13<00:39, 18.22it/s, est. speed input: 23164.26 toks/s, output: 22.62 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:13<00:38, 18.21it/s, est. speed input: 23022.01 toks/s, output: 22.48 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:14<00:38, 18.21it/s, est. speed input: 22888.21 toks/s, output: 22.35 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:14<00:38, 18.20it/s, est. speed input: 22761.34 toks/s, output: 22.23 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:15<00:37, 18.20it/s, est. speed input: 22642.77 toks/s, output: 22.11 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:15<00:37, 18.19it/s, est. speed input: 22530.25 toks/s, output: 22.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:16<00:36, 18.20it/s, est. speed input: 22424.29 toks/s, output: 21.90 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:16<00:36, 18.19it/s, est. speed input: 22323.50 toks/s, output: 21.80 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:17<00:35, 18.19it/s, est. speed input: 22228.35 toks/s, output: 21.71 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:17<00:35, 18.19it/s, est. speed input: 22137.82 toks/s, output: 21.62 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:17<00:35, 18.20it/s, est. speed input: 22052.01 toks/s, output: 21.54 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:18<00:34, 18.20it/s, est. speed input: 21970.09 toks/s, output: 21.46 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:18<00:34, 18.19it/s, est. speed input: 21891.93 toks/s, output: 21.38 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:19<00:33, 18.19it/s, est. speed input: 21817.47 toks/s, output: 21.31 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:19<00:33, 18.20it/s, est. speed input: 21746.42 toks/s, output: 21.24 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:20<00:32, 18.20it/s, est. speed input: 21678.38 toks/s, output: 21.17 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:20<00:32, 18.20it/s, est. speed input: 21613.37 toks/s, output: 21.11 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:21<00:31, 18.19it/s, est. speed input: 21550.72 toks/s, output: 21.05 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:21<00:31, 18.20it/s, est. speed input: 21490.93 toks/s, output: 20.99 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:21<00:31, 18.19it/s, est. speed input: 21433.44 toks/s, output: 20.93 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:22<00:30, 18.19it/s, est. speed input: 21378.26 toks/s, output: 20.88 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:22<00:30, 18.20it/s, est. speed input: 21325.51 toks/s, output: 20.83 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:23<00:29, 18.20it/s, est. speed input: 21274.59 toks/s, output: 20.78 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:23<00:29, 18.20it/s, est. speed input: 21225.31 toks/s, output: 20.73 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:24<00:28, 18.20it/s, est. speed input: 21177.99 toks/s, output: 20.68 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:24<00:28, 18.19it/s, est. speed input: 21131.91 toks/s, output: 20.64 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:24<00:28, 18.19it/s, est. speed input: 21087.86 toks/s, output: 20.59 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:25<00:27, 18.20it/s, est. speed input: 21045.59 toks/s, output: 20.55 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:25<00:27, 18.19it/s, est. speed input: 21004.08 toks/s, output: 20.51 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:26<00:26, 18.18it/s, est. speed input: 20963.69 toks/s, output: 20.47 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:26<00:26, 18.18it/s, est. speed input: 20924.77 toks/s, output: 20.43 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:27<00:25, 18.18it/s, est. speed input: 20887.31 toks/s, output: 20.40 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:27<00:25, 18.17it/s, est. speed input: 20850.77 toks/s, output: 20.36 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:28<00:24, 18.17it/s, est. speed input: 20815.35 toks/s, output: 20.33 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:28<00:24, 18.17it/s, est. speed input: 20781.09 toks/s, output: 20.29 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:28<00:24, 18.17it/s, est. speed input: 20748.03 toks/s, output: 20.26 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:29<00:23, 18.17it/s, est. speed input: 20716.06 toks/s, output: 20.23 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:29<00:23, 18.17it/s, est. speed input: 20684.72 toks/s, output: 20.20 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:30<00:22, 18.17it/s, est. speed input: 20654.28 toks/s, output: 20.17 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:30<00:22, 18.16it/s, est. speed input: 20624.67 toks/s, output: 20.14 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:31<00:10, 35.66it/s, est. speed input: 21418.15 toks/s, output: 20.92 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:31<00:12, 30.30it/s, est. speed input: 21379.05 toks/s, output: 20.88 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:31<00:13, 26.60it/s, est. speed input: 21340.81 toks/s, output: 20.84 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:32<00:14, 24.04it/s, est. speed input: 21303.63 toks/s, output: 20.80 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:32<00:15, 22.27it/s, est. speed input: 21267.44 toks/s, output: 20.77 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:33<00:15, 21.03it/s, est. speed input: 21232.31 toks/s, output: 20.73 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:33<00:16, 20.17it/s, est. speed input: 21197.83 toks/s, output: 20.70 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:34<00:16, 19.56it/s, est. speed input: 21164.07 toks/s, output: 20.67 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:34<00:16, 19.14it/s, est. speed input: 21131.51 toks/s, output: 20.64 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:35<00:16, 18.84it/s, est. speed input: 21099.56 toks/s, output: 20.61 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:35<00:15, 18.64it/s, est. speed input: 21068.62 toks/s, output: 20.57 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:35<00:15, 18.50it/s, est. speed input: 21038.32 toks/s, output: 20.55 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:36<00:15, 18.40it/s, est. speed input: 21009.04 toks/s, output: 20.52 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:36<00:14, 18.33it/s, est. speed input: 20980.09 toks/s, output: 20.49 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:37<00:14, 18.28it/s, est. speed input: 20952.00 toks/s, output: 20.46 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:37<00:13, 18.25it/s, est. speed input: 20924.74 toks/s, output: 20.43 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:38<00:13, 18.22it/s, est. speed input: 20897.85 toks/s, output: 20.41 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:38<00:12, 18.68it/s, est. speed input: 20891.90 toks/s, output: 20.40 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:38<00:12, 18.53it/s, est. speed input: 20866.12 toks/s, output: 20.38 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:39<00:12, 18.42it/s, est. speed input: 20840.72 toks/s, output: 20.35 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:39<00:11, 18.34it/s, est. speed input: 20815.79 toks/s, output: 20.33 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:40<00:11, 18.28it/s, est. speed input: 20791.36 toks/s, output: 20.30 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:40<00:10, 18.25it/s, est. speed input: 20767.75 toks/s, output: 20.28 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:41<00:10, 18.22it/s, est. speed input: 20744.49 toks/s, output: 20.26 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:41<00:09, 18.20it/s, est. speed input: 20721.75 toks/s, output: 20.24 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:42<00:09, 18.19it/s, est. speed input: 20699.64 toks/s, output: 20.21 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:42<00:09, 18.19it/s, est. speed input: 20678.07 toks/s, output: 20.19 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:42<00:08, 18.19it/s, est. speed input: 20657.03 toks/s, output: 20.17 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:43<00:08, 18.18it/s, est. speed input: 20636.25 toks/s, output: 20.15 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:43<00:07, 18.18it/s, est. speed input: 20615.99 toks/s, output: 20.13 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:44<00:07, 18.18it/s, est. speed input: 20596.03 toks/s, output: 20.11 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:44<00:06, 18.18it/s, est. speed input: 20576.52 toks/s, output: 20.09 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:45<00:06, 18.18it/s, est. speed input: 20557.37 toks/s, output: 20.08 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:45<00:06, 18.17it/s, est. speed input: 20538.29 toks/s, output: 20.06 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:46<00:05, 18.17it/s, est. speed input: 20519.86 toks/s, output: 20.04 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:46<00:05, 18.17it/s, est. speed input: 20501.79 toks/s, output: 20.02 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:46<00:04, 18.17it/s, est. speed input: 20483.92 toks/s, output: 20.00 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:47<00:04, 18.17it/s, est. speed input: 20466.60 toks/s, output: 19.99 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:47<00:03, 18.17it/s, est. speed input: 20449.43 toks/s, output: 19.97 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:48<00:03, 18.17it/s, est. speed input: 20432.60 toks/s, output: 19.95 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:48<00:02, 18.17it/s, est. speed input: 20416.17 toks/s, output: 19.94 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:49<00:02, 18.17it/s, est. speed input: 20399.89 toks/s, output: 19.92 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:49<00:02, 18.18it/s, est. speed input: 20384.09 toks/s, output: 19.91 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:49<00:01, 18.17it/s, est. speed input: 20368.38 toks/s, output: 19.89 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:50<00:01, 18.17it/s, est. speed input: 20352.82 toks/s, output: 19.88 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:50<00:00, 18.17it/s, est. speed input: 20337.74 toks/s, output: 19.86 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:51<00:00, 18.73it/s, est. speed input: 20340.42 toks/s, output: 19.86 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:51<00:00, 18.73it/s, est. speed input: 20460.20 toks/s, output: 19.98 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:51<00:00, 19.98it/s, est. speed input: 20460.20 toks/s, output: 19.98 toks/s]
[rank0]:[W126 04:52:13.311014740 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 04:52:15
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:52:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:52:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=198580) WARNING 01-26 04:52:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=198580) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=198580) WARNING 01-26 04:52:57 [backends.py:609] Failed to read file <frozen os>
Throughput: 9.57 requests/s, 9806.55 total tokens/s, 9.57 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 04:52:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:52:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:52:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:52:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:52:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:52:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:52:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:52:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:52:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:52:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:52:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:52:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:52:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:52:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:52:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:52:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:52:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=198580) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=198580) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.02it/s]
(EngineCore_DP0 pid=198580) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.26s/it]
(EngineCore_DP0 pid=198580) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]
(EngineCore_DP0 pid=198580) 
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 163676160 bytes
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=198580) [2026-01-26 04:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 81543168 bytes
(EngineCore_DP0 pid=198580) [rank0]:W0126 04:53:06.232000 198580 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=198580) [rank0]:W0126 04:53:06.342000 198580 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=198580) [rank0]:W0126 04:53:08.084000 198580 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=198580) [rank0]:W0126 04:53:08.259000 198580 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=198580) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.49it/s]
(EngineCore_DP0 pid=198580) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.42it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.49it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 23/2048 [00:00<00:08, 227.66it/s]
Adding requests:   2%|▏         | 49/2048 [00:00<00:08, 241.95it/s]
Adding requests:   4%|▎         | 75/2048 [00:00<00:07, 248.00it/s]
Adding requests:   5%|▍         | 100/2048 [00:00<00:07, 246.10it/s]
Adding requests:   6%|▌         | 125/2048 [00:00<00:07, 243.38it/s]
Adding requests:   7%|▋         | 150/2048 [00:00<00:07, 241.26it/s]
Adding requests:   9%|▊         | 176/2048 [00:00<00:07, 245.14it/s]
Adding requests:  10%|▉         | 203/2048 [00:00<00:07, 251.42it/s]
Adding requests:  11%|█         | 229/2048 [00:00<00:07, 253.17it/s]
Adding requests:  12%|█▏        | 255/2048 [00:01<00:07, 248.33it/s]
Adding requests:  14%|█▎        | 280/2048 [00:01<00:07, 248.13it/s]
Adding requests:  15%|█▍        | 305/2048 [00:01<00:07, 246.16it/s]
Adding requests:  16%|█▌        | 330/2048 [00:01<00:07, 245.02it/s]
Adding requests:  17%|█▋        | 355/2048 [00:01<00:06, 246.10it/s]
Adding requests:  19%|█▊        | 381/2048 [00:01<00:06, 247.80it/s]
Adding requests:  20%|█▉        | 408/2048 [00:01<00:06, 254.11it/s]
Adding requests:  21%|██        | 434/2048 [00:01<00:06, 251.31it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:06, 246.44it/s]
Adding requests:  24%|██▍       | 487/2048 [00:01<00:06, 251.56it/s]
Adding requests:  25%|██▌       | 513/2048 [00:02<00:06, 249.72it/s]
Adding requests:  26%|██▋       | 538/2048 [00:02<00:06, 248.76it/s]
Adding requests:  28%|██▊       | 564/2048 [00:02<00:05, 251.62it/s]
Adding requests:  29%|██▉       | 590/2048 [00:02<00:06, 240.43it/s]
Adding requests:  30%|███       | 616/2048 [00:02<00:05, 245.72it/s]
Adding requests:  31%|███▏      | 642/2048 [00:02<00:05, 246.63it/s]
Adding requests:  33%|███▎      | 667/2048 [00:02<00:05, 242.26it/s]
Adding requests:  34%|███▍      | 694/2048 [00:02<00:05, 248.30it/s]
Adding requests:  35%|███▌      | 719/2048 [00:02<00:05, 247.16it/s]
Adding requests:  36%|███▋      | 744/2048 [00:03<00:05, 243.78it/s]
Adding requests:  38%|███▊      | 769/2048 [00:03<00:05, 241.09it/s]
Adding requests:  39%|███▉      | 794/2048 [00:03<00:05, 242.40it/s]
Adding requests:  40%|███▉      | 819/2048 [00:03<00:05, 244.39it/s]
Adding requests:  41%|████▏     | 846/2048 [00:03<00:04, 250.72it/s]
Adding requests:  43%|████▎     | 872/2048 [00:03<00:04, 245.93it/s]
Adding requests:  44%|████▍     | 898/2048 [00:03<00:04, 247.89it/s]
Adding requests:  45%|████▌     | 923/2048 [00:03<00:04, 243.12it/s]
Adding requests:  46%|████▋     | 948/2048 [00:03<00:04, 239.53it/s]
Adding requests:  48%|████▊     | 974/2048 [00:03<00:04, 244.32it/s]
Adding requests:  49%|████▉     | 999/2048 [00:04<00:04, 242.36it/s]
Adding requests:  50%|█████     | 1025/2048 [00:04<00:04, 245.44it/s]
Adding requests:  51%|█████▏    | 1050/2048 [00:04<00:04, 246.61it/s]
Adding requests:  52%|█████▏    | 1075/2048 [00:04<00:04, 241.04it/s]
Adding requests:  54%|█████▎    | 1100/2048 [00:04<00:04, 235.45it/s]
Adding requests:  55%|█████▍    | 1125/2048 [00:04<00:03, 239.40it/s]
Adding requests:  56%|█████▌    | 1149/2048 [00:04<00:03, 238.14it/s]
Adding requests:  57%|█████▋    | 1173/2048 [00:04<00:03, 238.66it/s]
Adding requests:  58%|█████▊    | 1197/2048 [00:04<00:03, 237.34it/s]
Adding requests:  60%|█████▉    | 1225/2048 [00:04<00:03, 247.30it/s]
Adding requests:  61%|██████    | 1250/2048 [00:05<00:03, 244.79it/s]
Adding requests:  62%|██████▏   | 1275/2048 [00:05<00:03, 241.01it/s]
Adding requests:  63%|██████▎   | 1300/2048 [00:05<00:03, 236.61it/s]
Adding requests:  65%|██████▍   | 1324/2048 [00:05<00:03, 236.50it/s]
Adding requests:  66%|██████▌   | 1349/2048 [00:05<00:02, 238.56it/s]
Adding requests:  67%|██████▋   | 1375/2048 [00:05<00:02, 243.02it/s]
Adding requests:  68%|██████▊   | 1400/2048 [00:05<00:02, 244.64it/s]
Adding requests:  70%|██████▉   | 1426/2048 [00:05<00:02, 246.87it/s]
Adding requests:  71%|███████   | 1451/2048 [00:05<00:02, 246.85it/s]
Adding requests:  72%|███████▏  | 1477/2048 [00:06<00:02, 250.47it/s]
Adding requests:  73%|███████▎  | 1504/2048 [00:06<00:02, 255.17it/s]
Adding requests:  75%|███████▍  | 1530/2048 [00:06<00:02, 253.47it/s]
Adding requests:  76%|███████▌  | 1556/2048 [00:06<00:01, 251.20it/s]
Adding requests:  77%|███████▋  | 1582/2048 [00:06<00:01, 244.90it/s]
Adding requests:  78%|███████▊  | 1607/2048 [00:06<00:01, 243.51it/s]
Adding requests:  80%|███████▉  | 1632/2048 [00:06<00:01, 239.93it/s]
Adding requests:  81%|████████  | 1657/2048 [00:06<00:01, 237.93it/s]
Adding requests:  82%|████████▏ | 1683/2048 [00:06<00:01, 241.58it/s]
Adding requests:  83%|████████▎ | 1709/2048 [00:06<00:01, 246.39it/s]
Adding requests:  85%|████████▍ | 1734/2048 [00:07<00:01, 245.60it/s]
Adding requests:  86%|████████▌ | 1760/2048 [00:07<00:01, 248.25it/s]
Adding requests:  87%|████████▋ | 1787/2048 [00:07<00:01, 253.52it/s]
Adding requests:  89%|████████▊ | 1813/2048 [00:07<00:00, 253.49it/s]
Adding requests:  90%|████████▉ | 1840/2048 [00:07<00:00, 255.74it/s]
Adding requests:  91%|█████████ | 1866/2048 [00:07<00:00, 249.92it/s]
Adding requests:  92%|█████████▏| 1892/2048 [00:07<00:00, 249.17it/s]
Adding requests:  94%|█████████▎| 1917/2048 [00:07<00:00, 248.29it/s]
Adding requests:  95%|█████████▍| 1942/2048 [00:07<00:00, 248.67it/s]
Adding requests:  96%|█████████▌| 1967/2048 [00:08<00:00, 245.26it/s]
Adding requests:  97%|█████████▋| 1992/2048 [00:08<00:00, 244.18it/s]
Adding requests:  98%|█████████▊| 2017/2048 [00:08<00:00, 240.39it/s]
Adding requests: 100%|█████████▉| 2042/2048 [00:08<00:00, 232.45it/s]
Adding requests: 100%|██████████| 2048/2048 [00:08<00:00, 244.60it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 82/2048 [00:00<00:15, 127.12it/s, est. speed input: 130176.50 toks/s, output: 127.12 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:02<00:56, 34.66it/s, est. speed input: 43424.71 toks/s, output: 42.41 toks/s]   
Processed prompts:   6%|▌         | 114/2048 [00:03<01:30, 21.41it/s, est. speed input: 29356.21 toks/s, output: 28.67 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:05<01:57, 16.33it/s, est. speed input: 23594.04 toks/s, output: 23.04 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:07<02:18, 13.76it/s, est. speed input: 20454.58 toks/s, output: 19.98 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:08<02:33, 12.30it/s, est. speed input: 18482.26 toks/s, output: 18.05 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:10<02:44, 11.39it/s, est. speed input: 17126.59 toks/s, output: 16.73 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:12<02:51, 10.81it/s, est. speed input: 16138.03 toks/s, output: 15.76 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:13<02:56, 10.43it/s, est. speed input: 15384.42 toks/s, output: 15.02 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:15<02:59, 10.17it/s, est. speed input: 14791.41 toks/s, output: 14.44 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:17<03:00,  9.99it/s, est. speed input: 14312.41 toks/s, output: 13.98 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:18<03:01,  9.87it/s, est. speed input: 13917.47 toks/s, output: 13.59 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:20<03:01,  9.77it/s, est. speed input: 13581.41 toks/s, output: 13.26 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:22<03:00,  9.71it/s, est. speed input: 13299.98 toks/s, output: 12.99 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:23<03:00,  9.67it/s, est. speed input: 13057.31 toks/s, output: 12.75 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:25<02:58,  9.65it/s, est. speed input: 12846.21 toks/s, output: 12.55 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:27<02:57,  9.63it/s, est. speed input: 12660.79 toks/s, output: 12.36 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:29<02:56,  9.61it/s, est. speed input: 12496.85 toks/s, output: 12.20 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:30<02:54,  9.60it/s, est. speed input: 12350.30 toks/s, output: 12.06 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:30<02:10, 12.78it/s, est. speed input: 12765.42 toks/s, output: 12.47 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:32<02:21, 11.61it/s, est. speed input: 12613.81 toks/s, output: 12.32 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:34<02:29, 10.92it/s, est. speed input: 12477.16 toks/s, output: 12.18 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:35<02:34, 10.48it/s, est. speed input: 12352.92 toks/s, output: 12.06 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:37<02:36, 10.19it/s, est. speed input: 12239.70 toks/s, output: 11.95 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:39<02:38,  9.99it/s, est. speed input: 12135.85 toks/s, output: 11.85 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:40<02:38,  9.86it/s, est. speed input: 12040.30 toks/s, output: 11.76 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:42<02:38,  9.77it/s, est. speed input: 11952.41 toks/s, output: 11.67 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:44<02:38,  9.71it/s, est. speed input: 11870.94 toks/s, output: 11.59 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:46<02:37,  9.66it/s, est. speed input: 11795.36 toks/s, output: 11.52 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:47<02:35,  9.63it/s, est. speed input: 11725.04 toks/s, output: 11.45 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:49<02:34,  9.61it/s, est. speed input: 11659.38 toks/s, output: 11.39 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:51<02:33,  9.59it/s, est. speed input: 11597.91 toks/s, output: 11.33 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:52<02:31,  9.58it/s, est. speed input: 11540.38 toks/s, output: 11.27 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:54<02:30,  9.58it/s, est. speed input: 11486.44 toks/s, output: 11.22 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:56<02:28,  9.57it/s, est. speed input: 11435.64 toks/s, output: 11.17 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:57<02:26,  9.57it/s, est. speed input: 11387.83 toks/s, output: 11.12 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:59<02:25,  9.56it/s, est. speed input: 11342.84 toks/s, output: 11.08 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [01:01<02:23,  9.56it/s, est. speed input: 11300.02 toks/s, output: 11.04 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [01:01<01:46, 12.72it/s, est. speed input: 11514.01 toks/s, output: 11.24 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [01:03<01:55, 11.57it/s, est. speed input: 11468.25 toks/s, output: 11.20 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [01:04<02:01, 10.89it/s, est. speed input: 11424.85 toks/s, output: 11.16 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [01:06<02:05, 10.45it/s, est. speed input: 11383.68 toks/s, output: 11.12 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [01:08<02:07, 10.17it/s, est. speed input: 11344.55 toks/s, output: 11.08 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [01:09<02:08,  9.98it/s, est. speed input: 11307.29 toks/s, output: 11.04 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [01:11<02:03, 10.24it/s, est. speed input: 11304.39 toks/s, output: 11.04 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [01:12<02:04, 10.02it/s, est. speed input: 11269.19 toks/s, output: 11.01 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [01:14<02:04,  9.87it/s, est. speed input: 11235.50 toks/s, output: 10.97 toks/s]
Processed prompts:  41%|████      | 834/2048 [01:16<02:04,  9.77it/s, est. speed input: 11203.23 toks/s, output: 10.94 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [01:17<02:03,  9.70it/s, est. speed input: 11172.32 toks/s, output: 10.91 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [01:19<02:02,  9.65it/s, est. speed input: 11142.69 toks/s, output: 10.88 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [01:21<02:01,  9.62it/s, est. speed input: 11114.21 toks/s, output: 10.85 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [01:22<01:59,  9.59it/s, est. speed input: 11086.83 toks/s, output: 10.83 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [01:24<01:58,  9.57it/s, est. speed input: 11060.50 toks/s, output: 10.80 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [01:26<01:56,  9.56it/s, est. speed input: 11035.27 toks/s, output: 10.78 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:27<01:55,  9.56it/s, est. speed input: 11011.18 toks/s, output: 10.75 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:29<01:53,  9.55it/s, est. speed input: 10987.98 toks/s, output: 10.73 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:31<01:52,  9.55it/s, est. speed input: 10965.67 toks/s, output: 10.71 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:33<01:50,  9.55it/s, est. speed input: 10944.14 toks/s, output: 10.69 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:33<01:21, 12.74it/s, est. speed input: 11087.37 toks/s, output: 10.83 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:34<01:28, 11.58it/s, est. speed input: 11064.24 toks/s, output: 10.80 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:36<01:32, 10.90it/s, est. speed input: 11042.53 toks/s, output: 10.78 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:38<01:34, 10.46it/s, est. speed input: 11021.54 toks/s, output: 10.76 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:39<01:35, 10.18it/s, est. speed input: 11001.25 toks/s, output: 10.74 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:41<01:35,  9.99it/s, est. speed input: 10981.62 toks/s, output: 10.72 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:43<01:35,  9.85it/s, est. speed input: 10961.95 toks/s, output: 10.71 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:45<01:35,  9.74it/s, est. speed input: 10942.08 toks/s, output: 10.69 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:46<01:34,  9.67it/s, est. speed input: 10922.88 toks/s, output: 10.67 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:48<01:32,  9.62it/s, est. speed input: 10904.24 toks/s, output: 10.65 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:50<01:31,  9.58it/s, est. speed input: 10886.20 toks/s, output: 10.63 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:51<01:30,  9.56it/s, est. speed input: 10869.11 toks/s, output: 10.61 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:53<01:28,  9.56it/s, est. speed input: 10852.81 toks/s, output: 10.60 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:55<01:26,  9.55it/s, est. speed input: 10836.89 toks/s, output: 10.58 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:56<01:25,  9.54it/s, est. speed input: 10821.49 toks/s, output: 10.57 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:58<01:23,  9.54it/s, est. speed input: 10806.51 toks/s, output: 10.55 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [02:00<01:21,  9.54it/s, est. speed input: 10792.14 toks/s, output: 10.54 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [02:01<01:20,  9.55it/s, est. speed input: 10778.23 toks/s, output: 10.53 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [02:03<01:18,  9.55it/s, est. speed input: 10764.64 toks/s, output: 10.51 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [02:03<00:57, 12.79it/s, est. speed input: 10874.54 toks/s, output: 10.62 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [02:05<01:01, 11.61it/s, est. speed input: 10859.88 toks/s, output: 10.61 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [02:07<01:04, 10.90it/s, est. speed input: 10845.59 toks/s, output: 10.59 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [02:08<01:05, 10.46it/s, est. speed input: 10831.62 toks/s, output: 10.58 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [02:10<01:05, 10.16it/s, est. speed input: 10818.02 toks/s, output: 10.56 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [02:12<01:05,  9.97it/s, est. speed input: 10804.81 toks/s, output: 10.55 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [02:13<01:04,  9.84it/s, est. speed input: 10791.89 toks/s, output: 10.54 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [02:15<01:03,  9.75it/s, est. speed input: 10779.33 toks/s, output: 10.53 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [02:17<01:02,  9.69it/s, est. speed input: 10767.13 toks/s, output: 10.51 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [02:18<01:01,  9.65it/s, est. speed input: 10755.21 toks/s, output: 10.50 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [02:20<00:59,  9.62it/s, est. speed input: 10743.54 toks/s, output: 10.49 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [02:22<00:58,  9.60it/s, est. speed input: 10732.13 toks/s, output: 10.48 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [02:23<00:56,  9.58it/s, est. speed input: 10721.01 toks/s, output: 10.47 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [02:25<00:54,  9.57it/s, est. speed input: 10710.14 toks/s, output: 10.46 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [02:27<00:53,  9.56it/s, est. speed input: 10699.53 toks/s, output: 10.45 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [02:28<00:51,  9.56it/s, est. speed input: 10689.16 toks/s, output: 10.44 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [02:30<00:50,  9.56it/s, est. speed input: 10679.03 toks/s, output: 10.43 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [02:32<00:48,  9.55it/s, est. speed input: 10669.08 toks/s, output: 10.42 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [02:33<00:46,  9.55it/s, est. speed input: 10659.36 toks/s, output: 10.41 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [02:35<00:43,  9.92it/s, est. speed input: 10664.20 toks/s, output: 10.41 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [02:35<00:31, 13.18it/s, est. speed input: 10749.96 toks/s, output: 10.50 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [02:37<00:33, 11.83it/s, est. speed input: 10739.60 toks/s, output: 10.49 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [02:38<00:34, 11.04it/s, est. speed input: 10729.47 toks/s, output: 10.48 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [02:40<00:34, 10.55it/s, est. speed input: 10719.56 toks/s, output: 10.47 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [02:42<00:34, 10.22it/s, est. speed input: 10709.84 toks/s, output: 10.46 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [02:44<00:33, 10.01it/s, est. speed input: 10700.33 toks/s, output: 10.45 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [02:45<00:32,  9.87it/s, est. speed input: 10691.01 toks/s, output: 10.44 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [02:47<00:30,  9.77it/s, est. speed input: 10681.87 toks/s, output: 10.43 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [02:49<00:29,  9.70it/s, est. speed input: 10672.93 toks/s, output: 10.42 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [02:50<00:27,  9.66it/s, est. speed input: 10664.12 toks/s, output: 10.41 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [02:52<00:26,  9.62it/s, est. speed input: 10655.53 toks/s, output: 10.41 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [02:54<00:24,  9.60it/s, est. speed input: 10647.05 toks/s, output: 10.40 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [02:55<00:23,  9.58it/s, est. speed input: 10638.77 toks/s, output: 10.39 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [02:57<00:21,  9.57it/s, est. speed input: 10630.62 toks/s, output: 10.38 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [02:59<00:19,  9.57it/s, est. speed input: 10622.63 toks/s, output: 10.37 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [03:00<00:18,  9.56it/s, est. speed input: 10614.80 toks/s, output: 10.37 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [03:02<00:16,  9.56it/s, est. speed input: 10607.10 toks/s, output: 10.36 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [03:04<00:14,  9.55it/s, est. speed input: 10599.57 toks/s, output: 10.35 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [03:05<00:13,  9.55it/s, est. speed input: 10592.17 toks/s, output: 10.34 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [03:06<00:08, 12.73it/s, est. speed input: 10664.09 toks/s, output: 10.41 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [03:07<00:08, 11.57it/s, est. speed input: 10656.19 toks/s, output: 10.41 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [03:09<00:07, 10.88it/s, est. speed input: 10648.42 toks/s, output: 10.40 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [03:11<00:05, 10.44it/s, est. speed input: 10640.77 toks/s, output: 10.39 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [03:12<00:04, 10.16it/s, est. speed input: 10633.29 toks/s, output: 10.38 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [03:14<00:03,  9.97it/s, est. speed input: 10625.90 toks/s, output: 10.38 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [03:15<00:01, 10.25it/s, est. speed input: 10630.54 toks/s, output: 10.38 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [03:15<00:00, 10.25it/s, est. speed input: 10703.69 toks/s, output: 10.45 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [03:15<00:00, 10.45it/s, est. speed input: 10703.69 toks/s, output: 10.45 toks/s]
[rank0]:[W126 04:56:41.385413242 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 04:56:44
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:57:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 04:57:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=202856) WARNING 01-26 04:57:29 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=202856) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=202856) WARNING 01-26 04:57:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 5.01 requests/s, 5135.02 total tokens/s, 5.01 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 04:57:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:57:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:57:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:57:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:57:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:57:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:57:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:57:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:57:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:57:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:57:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 04:57:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 04:57:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 04:57:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:57:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:57:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:57:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:30] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:30] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:30] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:30] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=202856) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=202856) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.06it/s]
(EngineCore_DP0 pid=202856) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.21s/it]
(EngineCore_DP0 pid=202856) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.17s/it]
(EngineCore_DP0 pid=202856) 
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 163676160 bytes
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=202856) [2026-01-26 04:57:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 81543168 bytes
(EngineCore_DP0 pid=202856) [rank0]:W0126 04:57:50.639000 202856 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=202856) [rank0]:W0126 04:57:50.753000 202856 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=202856) [rank0]:W0126 04:57:52.278000 202856 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=202856) [rank0]:W0126 04:57:52.458000 202856 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=202856) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  8.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.58it/s]
(EngineCore_DP0 pid=202856) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.28it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.99it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  9.06it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.93it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 21/4096 [00:00<00:19, 208.45it/s]
Adding requests:   1%|          | 43/4096 [00:00<00:18, 214.14it/s]
Adding requests:   2%|▏         | 67/4096 [00:00<00:18, 223.13it/s]
Adding requests:   2%|▏         | 90/4096 [00:00<00:17, 224.32it/s]
Adding requests:   3%|▎         | 114/4096 [00:00<00:17, 229.44it/s]
Adding requests:   3%|▎         | 138/4096 [00:00<00:17, 230.35it/s]
Adding requests:   4%|▍         | 162/4096 [00:00<00:17, 231.07it/s]
Adding requests:   5%|▍         | 186/4096 [00:00<00:17, 228.08it/s]
Adding requests:   5%|▌         | 212/4096 [00:00<00:16, 236.32it/s]
Adding requests:   6%|▌         | 238/4096 [00:01<00:15, 242.64it/s]
Adding requests:   6%|▋         | 263/4096 [00:01<00:15, 241.03it/s]
Adding requests:   7%|▋         | 288/4096 [00:01<00:16, 227.83it/s]
Adding requests:   8%|▊         | 311/4096 [00:01<00:16, 225.61it/s]
Adding requests:   8%|▊         | 335/4096 [00:01<00:16, 229.51it/s]
Adding requests:   9%|▉         | 360/4096 [00:01<00:15, 235.24it/s]
Adding requests:   9%|▉         | 384/4096 [00:01<00:16, 226.17it/s]
Adding requests:  10%|▉         | 408/4096 [00:01<00:16, 227.63it/s]
Adding requests:  11%|█         | 434/4096 [00:01<00:15, 234.05it/s]
Adding requests:  11%|█         | 460/4096 [00:01<00:15, 240.80it/s]
Adding requests:  12%|█▏        | 485/4096 [00:02<00:14, 243.38it/s]
Adding requests:  12%|█▏        | 510/4096 [00:02<00:15, 236.91it/s]
Adding requests:  13%|█▎        | 538/4096 [00:02<00:14, 247.08it/s]
Adding requests:  14%|█▍        | 564/4096 [00:02<00:14, 250.21it/s]
Adding requests:  14%|█▍        | 590/4096 [00:02<00:14, 239.41it/s]
Adding requests:  15%|█▌        | 615/4096 [00:02<00:15, 229.19it/s]
Adding requests:  16%|█▌        | 640/4096 [00:02<00:14, 234.68it/s]
Adding requests:  16%|█▌        | 665/4096 [00:02<00:14, 236.73it/s]
Adding requests:  17%|█▋        | 694/4096 [00:02<00:13, 248.69it/s]
Adding requests:  18%|█▊        | 719/4096 [00:03<00:14, 239.31it/s]
Adding requests:  18%|█▊        | 744/4096 [00:03<00:13, 239.60it/s]
Adding requests:  19%|█▉        | 770/4096 [00:03<00:13, 245.37it/s]
Adding requests:  19%|█▉        | 798/4096 [00:03<00:13, 253.38it/s]
Adding requests:  20%|██        | 824/4096 [00:03<00:13, 242.68it/s]
Adding requests:  21%|██        | 849/4096 [00:03<00:13, 241.59it/s]
Adding requests:  30%|██▉       | 1216/4096 [00:03<00:02, 1218.18it/s]
Adding requests:  33%|███▎      | 1343/4096 [00:04<00:05, 488.46it/s] 
Adding requests:  35%|███▌      | 1438/4096 [00:04<00:06, 399.51it/s]
Adding requests:  37%|███▋      | 1512/4096 [00:04<00:07, 354.59it/s]
Adding requests:  38%|███▊      | 1572/4096 [00:05<00:07, 327.88it/s]
Adding requests:  40%|███▉      | 1621/4096 [00:05<00:08, 302.08it/s]
Adding requests:  41%|████      | 1663/4096 [00:05<00:08, 290.39it/s]
Adding requests:  42%|████▏     | 1700/4096 [00:05<00:08, 277.81it/s]
Adding requests:  42%|████▏     | 1733/4096 [00:05<00:08, 271.93it/s]
Adding requests:  43%|████▎     | 1764/4096 [00:06<00:08, 273.65it/s]
Adding requests:  44%|████▍     | 1794/4096 [00:06<00:08, 267.40it/s]
Adding requests:  45%|████▍     | 1823/4096 [00:06<00:09, 249.47it/s]
Adding requests:  45%|████▌     | 1849/4096 [00:06<00:08, 249.69it/s]
Adding requests:  46%|████▌     | 1875/4096 [00:06<00:09, 246.30it/s]
Adding requests:  46%|████▋     | 1901/4096 [00:06<00:09, 242.68it/s]
Adding requests:  47%|████▋     | 1926/4096 [00:06<00:09, 236.05it/s]
Adding requests:  48%|████▊     | 1952/4096 [00:06<00:08, 242.11it/s]
Adding requests:  48%|████▊     | 1979/4096 [00:06<00:08, 247.75it/s]
Adding requests:  49%|████▉     | 2004/4096 [00:07<00:08, 246.49it/s]
Adding requests:  50%|████▉     | 2029/4096 [00:07<00:08, 229.89it/s]
Adding requests:  50%|█████     | 2053/4096 [00:07<00:08, 229.68it/s]
Adding requests:  51%|█████     | 2077/4096 [00:07<00:08, 230.15it/s]
Adding requests:  51%|█████▏    | 2105/4096 [00:07<00:08, 242.68it/s]
Adding requests:  52%|█████▏    | 2130/4096 [00:07<00:08, 233.00it/s]
Adding requests:  53%|█████▎    | 2154/4096 [00:07<00:08, 231.93it/s]
Adding requests:  53%|█████▎    | 2179/4096 [00:07<00:08, 236.13it/s]
Adding requests:  54%|█████▍    | 2205/4096 [00:07<00:07, 242.77it/s]
Adding requests:  54%|█████▍    | 2230/4096 [00:07<00:07, 242.70it/s]
Adding requests:  55%|█████▌    | 2255/4096 [00:08<00:07, 240.20it/s]
Adding requests:  56%|█████▌    | 2281/4096 [00:08<00:07, 245.02it/s]
Adding requests:  56%|█████▋    | 2308/4096 [00:08<00:07, 249.97it/s]
Adding requests:  57%|█████▋    | 2337/4096 [00:08<00:06, 258.70it/s]
Adding requests:  58%|█████▊    | 2363/4096 [00:08<00:07, 237.44it/s]
Adding requests:  58%|█████▊    | 2391/4096 [00:08<00:06, 248.02it/s]
Adding requests:  59%|█████▉    | 2419/4096 [00:08<00:06, 254.92it/s]
Adding requests:  60%|█████▉    | 2447/4096 [00:08<00:06, 259.49it/s]
Adding requests:  60%|██████    | 2474/4096 [00:08<00:06, 247.31it/s]
Adding requests:  61%|██████    | 2500/4096 [00:09<00:06, 249.27it/s]
Adding requests:  62%|██████▏   | 2527/4096 [00:09<00:06, 254.91it/s]
Adding requests:  62%|██████▏   | 2558/4096 [00:09<00:05, 269.16it/s]
Adding requests:  63%|██████▎   | 2586/4096 [00:09<00:05, 261.05it/s]
Adding requests:  64%|██████▍   | 2613/4096 [00:09<00:05, 249.82it/s]
Adding requests:  64%|██████▍   | 2639/4096 [00:09<00:05, 246.62it/s]
Adding requests:  65%|██████▌   | 2665/4096 [00:09<00:05, 249.82it/s]
Adding requests:  66%|██████▌   | 2691/4096 [00:09<00:05, 242.32it/s]
Adding requests:  66%|██████▋   | 2716/4096 [00:09<00:05, 231.68it/s]
Adding requests:  67%|██████▋   | 2743/4096 [00:10<00:05, 240.36it/s]
Adding requests:  68%|██████▊   | 2771/4096 [00:10<00:05, 249.97it/s]
Adding requests:  68%|██████▊   | 2798/4096 [00:10<00:05, 253.28it/s]
Adding requests:  69%|██████▉   | 2824/4096 [00:10<00:05, 239.19it/s]
Adding requests:  70%|██████▉   | 2851/4096 [00:10<00:05, 245.54it/s]
Adding requests:  70%|███████   | 2878/4096 [00:10<00:04, 251.22it/s]
Adding requests:  71%|███████   | 2905/4096 [00:10<00:04, 255.16it/s]
Adding requests:  72%|███████▏  | 2931/4096 [00:10<00:04, 243.84it/s]
Adding requests:  72%|███████▏  | 2959/4096 [00:10<00:04, 252.14it/s]
Adding requests:  73%|███████▎  | 2986/4096 [00:11<00:04, 254.68it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:11<00:04, 263.43it/s]
Adding requests:  74%|███████▍  | 3042/4096 [00:11<00:04, 254.36it/s]
Adding requests:  75%|███████▍  | 3068/4096 [00:11<00:04, 252.68it/s]
Adding requests:  76%|███████▌  | 3095/4096 [00:11<00:03, 257.39it/s]
Adding requests:  76%|███████▋  | 3124/4096 [00:11<00:03, 265.16it/s]
Adding requests:  77%|███████▋  | 3151/4096 [00:11<00:03, 258.14it/s]
Adding requests:  78%|███████▊  | 3177/4096 [00:11<00:03, 248.11it/s]
Adding requests:  78%|███████▊  | 3204/4096 [00:11<00:03, 253.37it/s]
Adding requests:  79%|███████▉  | 3231/4096 [00:11<00:03, 258.12it/s]
Adding requests:  80%|███████▉  | 3257/4096 [00:12<00:03, 247.38it/s]
Adding requests:  80%|████████  | 3282/4096 [00:12<00:03, 226.02it/s]
Adding requests:  81%|████████  | 3305/4096 [00:12<00:03, 218.15it/s]
Adding requests:  81%|████████▏ | 3328/4096 [00:12<00:03, 220.64it/s]
Adding requests:  82%|████████▏ | 3354/4096 [00:12<00:03, 229.37it/s]
Adding requests:  82%|████████▏ | 3378/4096 [00:12<00:03, 209.68it/s]
Adding requests:  83%|████████▎ | 3400/4096 [00:12<00:03, 198.27it/s]
Adding requests:  84%|████████▎ | 3423/4096 [00:12<00:03, 204.51it/s]
Adding requests:  84%|████████▍ | 3448/4096 [00:13<00:03, 213.61it/s]
Adding requests:  85%|████████▍ | 3470/4096 [00:13<00:02, 211.84it/s]
Adding requests:  85%|████████▌ | 3494/4096 [00:13<00:02, 217.50it/s]
Adding requests:  86%|████████▌ | 3522/4096 [00:13<00:02, 233.73it/s]
Adding requests:  87%|████████▋ | 3550/4096 [00:13<00:02, 246.66it/s]
Adding requests:  87%|████████▋ | 3575/4096 [00:13<00:02, 236.50it/s]
Adding requests:  88%|████████▊ | 3599/4096 [00:13<00:02, 235.22it/s]
Adding requests:  89%|████████▊ | 3627/4096 [00:13<00:01, 245.99it/s]
Adding requests:  89%|████████▉ | 3655/4096 [00:13<00:01, 253.12it/s]
Adding requests:  90%|████████▉ | 3681/4096 [00:13<00:01, 243.95it/s]
Adding requests:  90%|█████████ | 3706/4096 [00:14<00:01, 242.92it/s]
Adding requests:  91%|█████████ | 3731/4096 [00:14<00:01, 239.59it/s]
Adding requests:  92%|█████████▏| 3756/4096 [00:14<00:01, 238.22it/s]
Adding requests:  92%|█████████▏| 3780/4096 [00:14<00:01, 229.34it/s]
Adding requests:  93%|█████████▎| 3804/4096 [00:14<00:01, 219.13it/s]
Adding requests:  93%|█████████▎| 3829/4096 [00:14<00:01, 225.55it/s]
Adding requests:  94%|█████████▍| 3855/4096 [00:14<00:01, 233.65it/s]
Adding requests:  95%|█████████▍| 3880/4096 [00:14<00:00, 237.56it/s]
Adding requests:  95%|█████████▌| 3904/4096 [00:14<00:00, 227.51it/s]
Adding requests:  96%|█████████▌| 3928/4096 [00:15<00:00, 229.33it/s]
Adding requests:  97%|█████████▋| 3953/4096 [00:15<00:00, 234.71it/s]
Adding requests:  97%|█████████▋| 3978/4096 [00:15<00:00, 235.82it/s]
Adding requests:  98%|█████████▊| 4002/4096 [00:15<00:00, 231.80it/s]
Adding requests:  98%|█████████▊| 4027/4096 [00:15<00:00, 234.30it/s]
Adding requests:  99%|█████████▉| 4052/4096 [00:15<00:00, 238.59it/s]
Adding requests: 100%|█████████▉| 4078/4096 [00:15<00:00, 243.88it/s]
Adding requests: 100%|██████████| 4096/4096 [00:15<00:00, 260.17it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 66/4096 [00:02<02:37, 25.54it/s, est. speed input: 26150.27 toks/s, output: 25.54 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:08<06:59,  9.54it/s, est. speed input: 11185.97 toks/s, output: 10.92 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:15<09:14,  7.16it/s, est. speed input: 8669.24 toks/s, output: 8.47 toks/s] 
Processed prompts:   4%|▍         | 162/4096 [00:20<09:31,  6.88it/s, est. speed input: 8152.72 toks/s, output: 7.96 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:26<10:36,  6.13it/s, est. speed input: 7432.42 toks/s, output: 7.26 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:33<11:16,  5.72it/s, est. speed input: 6987.60 toks/s, output: 6.82 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:39<11:41,  5.47it/s, est. speed input: 6686.27 toks/s, output: 6.53 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:45<11:55,  5.32it/s, est. speed input: 6468.55 toks/s, output: 6.32 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:50<11:12,  5.61it/s, est. speed input: 6478.26 toks/s, output: 6.33 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:57<11:30,  5.42it/s, est. speed input: 6330.82 toks/s, output: 6.18 toks/s]
Processed prompts:   9%|▉         | 386/4096 [01:03<11:42,  5.28it/s, est. speed input: 6208.17 toks/s, output: 6.06 toks/s]
Processed prompts:  10%|█         | 418/4096 [01:10<11:48,  5.19it/s, est. speed input: 6107.28 toks/s, output: 5.96 toks/s]
Processed prompts:  11%|█         | 450/4096 [01:16<11:50,  5.13it/s, est. speed input: 6024.90 toks/s, output: 5.88 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [01:21<11:01,  5.46it/s, est. speed input: 6059.18 toks/s, output: 5.92 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [01:27<11:13,  5.32it/s, est. speed input: 5991.28 toks/s, output: 5.85 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [01:34<11:20,  5.22it/s, est. speed input: 5932.60 toks/s, output: 5.79 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [01:40<11:22,  5.15it/s, est. speed input: 5881.47 toks/s, output: 5.74 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [01:47<11:22,  5.11it/s, est. speed input: 5836.48 toks/s, output: 5.70 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [01:52<10:35,  5.44it/s, est. speed input: 5868.20 toks/s, output: 5.73 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [01:58<10:45,  5.30it/s, est. speed input: 5828.16 toks/s, output: 5.69 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [02:04<10:50,  5.21it/s, est. speed input: 5792.20 toks/s, output: 5.66 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [02:11<10:52,  5.15it/s, est. speed input: 5759.75 toks/s, output: 5.62 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [02:17<10:45,  5.16it/s, est. speed input: 5739.11 toks/s, output: 5.60 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [02:22<10:01,  5.48it/s, est. speed input: 5767.83 toks/s, output: 5.63 toks/s]
Processed prompts:  20%|██        | 834/4096 [02:28<10:12,  5.33it/s, est. speed input: 5740.23 toks/s, output: 5.61 toks/s]
Processed prompts:  21%|██        | 866/4096 [02:35<10:18,  5.23it/s, est. speed input: 5714.93 toks/s, output: 5.58 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [02:41<10:20,  5.16it/s, est. speed input: 5691.66 toks/s, output: 5.56 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [02:47<10:19,  5.11it/s, est. speed input: 5670.12 toks/s, output: 5.54 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [02:54<10:17,  5.08it/s, est. speed input: 5650.20 toks/s, output: 5.52 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [02:59<09:32,  5.42it/s, est. speed input: 5675.45 toks/s, output: 5.54 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [03:05<09:40,  5.29it/s, est. speed input: 5656.56 toks/s, output: 5.52 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [03:12<09:44,  5.20it/s, est. speed input: 5638.90 toks/s, output: 5.51 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [03:18<09:44,  5.14it/s, est. speed input: 5622.40 toks/s, output: 5.49 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [03:24<09:43,  5.10it/s, est. speed input: 5606.93 toks/s, output: 5.48 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [03:29<09:01,  5.43it/s, est. speed input: 5629.50 toks/s, output: 5.50 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [03:36<09:09,  5.30it/s, est. speed input: 5614.64 toks/s, output: 5.48 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [03:42<09:12,  5.21it/s, est. speed input: 5600.60 toks/s, output: 5.47 toks/s]
Processed prompts:  31%|███       | 1250/4096 [03:49<09:13,  5.14it/s, est. speed input: 5587.36 toks/s, output: 5.46 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [03:55<09:11,  5.10it/s, est. speed input: 5574.84 toks/s, output: 5.44 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [04:00<08:32,  5.43it/s, est. speed input: 5595.30 toks/s, output: 5.46 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [04:06<08:38,  5.30it/s, est. speed input: 5583.26 toks/s, output: 5.45 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [04:13<08:42,  5.21it/s, est. speed input: 5571.66 toks/s, output: 5.44 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [04:19<08:42,  5.14it/s, est. speed input: 5560.57 toks/s, output: 5.43 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [04:26<08:40,  5.10it/s, est. speed input: 5550.09 toks/s, output: 5.42 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [04:31<08:02,  5.43it/s, est. speed input: 5568.73 toks/s, output: 5.44 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [04:37<08:08,  5.30it/s, est. speed input: 5558.53 toks/s, output: 5.43 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [04:43<08:11,  5.21it/s, est. speed input: 5548.78 toks/s, output: 5.42 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [04:50<08:10,  5.14it/s, est. speed input: 5539.47 toks/s, output: 5.41 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [04:56<08:03,  5.15it/s, est. speed input: 5534.49 toks/s, output: 5.40 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [05:01<07:29,  5.48it/s, est. speed input: 5551.47 toks/s, output: 5.42 toks/s]
Processed prompts:  41%|████      | 1666/4096 [05:07<07:36,  5.33it/s, est. speed input: 5542.63 toks/s, output: 5.41 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [05:14<07:38,  5.22it/s, est. speed input: 5534.13 toks/s, output: 5.40 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [05:20<07:38,  5.16it/s, est. speed input: 5526.07 toks/s, output: 5.40 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [05:26<07:36,  5.11it/s, est. speed input: 5518.34 toks/s, output: 5.39 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [05:31<07:03,  5.44it/s, est. speed input: 5533.84 toks/s, output: 5.40 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [05:38<07:08,  5.29it/s, est. speed input: 5525.71 toks/s, output: 5.40 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [05:44<07:10,  5.20it/s, est. speed input: 5518.05 toks/s, output: 5.39 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [05:51<07:09,  5.14it/s, est. speed input: 5510.96 toks/s, output: 5.38 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [05:57<07:06,  5.10it/s, est. speed input: 5504.13 toks/s, output: 5.38 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [06:02<06:33,  5.44it/s, est. speed input: 5519.30 toks/s, output: 5.39 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [06:08<06:37,  5.31it/s, est. speed input: 5512.53 toks/s, output: 5.38 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [06:15<06:38,  5.21it/s, est. speed input: 5505.96 toks/s, output: 5.38 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [06:21<06:37,  5.15it/s, est. speed input: 5499.60 toks/s, output: 5.37 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [06:28<06:34,  5.11it/s, est. speed input: 5493.46 toks/s, output: 5.36 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [06:33<06:04,  5.43it/s, est. speed input: 5506.92 toks/s, output: 5.38 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [06:39<06:07,  5.30it/s, est. speed input: 5500.87 toks/s, output: 5.37 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [06:45<06:04,  5.26it/s, est. speed input: 5497.83 toks/s, output: 5.37 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [06:52<06:03,  5.18it/s, est. speed input: 5492.07 toks/s, output: 5.36 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [06:58<06:01,  5.13it/s, est. speed input: 5486.49 toks/s, output: 5.36 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [07:03<05:34,  5.45it/s, est. speed input: 5499.08 toks/s, output: 5.37 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [07:09<05:36,  5.31it/s, est. speed input: 5493.57 toks/s, output: 5.36 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [07:16<05:37,  5.22it/s, est. speed input: 5488.16 toks/s, output: 5.36 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [07:22<05:35,  5.15it/s, est. speed input: 5482.92 toks/s, output: 5.35 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [07:29<05:31,  5.11it/s, est. speed input: 5477.81 toks/s, output: 5.35 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [07:34<05:05,  5.44it/s, est. speed input: 5489.74 toks/s, output: 5.36 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [07:40<05:07,  5.30it/s, est. speed input: 5484.81 toks/s, output: 5.36 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [07:46<05:06,  5.21it/s, est. speed input: 5479.81 toks/s, output: 5.35 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [07:53<05:04,  5.14it/s, est. speed input: 5474.82 toks/s, output: 5.35 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [07:59<05:01,  5.10it/s, est. speed input: 5469.96 toks/s, output: 5.34 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [08:04<04:36,  5.44it/s, est. speed input: 5481.56 toks/s, output: 5.35 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [08:10<04:36,  5.31it/s, est. speed input: 5477.46 toks/s, output: 5.35 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [08:17<04:35,  5.22it/s, est. speed input: 5473.01 toks/s, output: 5.34 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [08:23<04:33,  5.13it/s, est. speed input: 5467.73 toks/s, output: 5.34 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [08:30<04:29,  5.09it/s, est. speed input: 5463.35 toks/s, output: 5.34 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [08:36<04:24,  5.07it/s, est. speed input: 5459.33 toks/s, output: 5.33 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [08:41<04:01,  5.42it/s, est. speed input: 5470.19 toks/s, output: 5.34 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [08:47<04:01,  5.29it/s, est. speed input: 5466.03 toks/s, output: 5.34 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [08:54<03:59,  5.20it/s, est. speed input: 5461.96 toks/s, output: 5.33 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [09:00<03:56,  5.14it/s, est. speed input: 5458.01 toks/s, output: 5.33 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [09:07<03:51,  5.10it/s, est. speed input: 5454.15 toks/s, output: 5.33 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [09:12<03:31,  5.43it/s, est. speed input: 5463.98 toks/s, output: 5.34 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [09:18<03:31,  5.29it/s, est. speed input: 5460.10 toks/s, output: 5.33 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [09:24<03:28,  5.20it/s, est. speed input: 5456.31 toks/s, output: 5.33 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [09:31<03:24,  5.14it/s, est. speed input: 5452.64 toks/s, output: 5.32 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [09:37<03:20,  5.10it/s, est. speed input: 5449.04 toks/s, output: 5.32 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [09:42<03:02,  5.43it/s, est. speed input: 5458.51 toks/s, output: 5.33 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [09:49<03:00,  5.29it/s, est. speed input: 5454.82 toks/s, output: 5.33 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [09:55<02:57,  5.20it/s, est. speed input: 5451.24 toks/s, output: 5.32 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [10:01<02:53,  5.14it/s, est. speed input: 5447.78 toks/s, output: 5.32 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [10:08<02:49,  5.10it/s, est. speed input: 5444.37 toks/s, output: 5.32 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [10:13<02:32,  5.44it/s, est. speed input: 5453.59 toks/s, output: 5.33 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [10:19<02:30,  5.30it/s, est. speed input: 5450.21 toks/s, output: 5.32 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [10:26<02:27,  5.21it/s, est. speed input: 5446.89 toks/s, output: 5.32 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [10:32<02:22,  5.15it/s, est. speed input: 5443.65 toks/s, output: 5.32 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [10:38<02:17,  5.10it/s, est. speed input: 5440.47 toks/s, output: 5.31 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [10:43<02:03,  5.43it/s, est. speed input: 5449.16 toks/s, output: 5.32 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [10:50<02:00,  5.30it/s, est. speed input: 5445.97 toks/s, output: 5.32 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [10:56<01:56,  5.21it/s, est. speed input: 5442.83 toks/s, output: 5.32 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [11:02<01:51,  5.14it/s, est. speed input: 5439.75 toks/s, output: 5.31 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [11:09<01:46,  5.10it/s, est. speed input: 5436.74 toks/s, output: 5.31 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [11:14<01:33,  5.44it/s, est. speed input: 5445.12 toks/s, output: 5.32 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [11:20<01:30,  5.30it/s, est. speed input: 5442.16 toks/s, output: 5.31 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [11:27<01:25,  5.21it/s, est. speed input: 5439.17 toks/s, output: 5.31 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [11:33<01:19,  5.20it/s, est. speed input: 5437.87 toks/s, output: 5.31 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [11:39<01:14,  5.14it/s, est. speed input: 5434.97 toks/s, output: 5.31 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [11:44<01:04,  5.46it/s, est. speed input: 5443.01 toks/s, output: 5.32 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [11:51<00:59,  5.32it/s, est. speed input: 5440.14 toks/s, output: 5.31 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [11:57<00:54,  5.22it/s, est. speed input: 5437.33 toks/s, output: 5.31 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [12:03<00:49,  5.15it/s, est. speed input: 5434.55 toks/s, output: 5.31 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [12:10<00:43,  5.11it/s, est. speed input: 5431.86 toks/s, output: 5.30 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [12:15<00:34,  5.50it/s, est. speed input: 5441.09 toks/s, output: 5.31 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [12:21<00:29,  5.34it/s, est. speed input: 5438.36 toks/s, output: 5.31 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [12:27<00:24,  5.23it/s, est. speed input: 5435.67 toks/s, output: 5.31 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [12:34<00:18,  5.16it/s, est. speed input: 5433.03 toks/s, output: 5.31 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [12:40<00:12,  5.12it/s, est. speed input: 5430.46 toks/s, output: 5.30 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [12:45<00:05,  5.51it/s, est. speed input: 5439.44 toks/s, output: 5.31 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [12:45<00:00,  5.51it/s, est. speed input: 5479.57 toks/s, output: 5.35 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [12:45<00:00,  5.35it/s, est. speed input: 5479.57 toks/s, output: 5.35 toks/s]
[rank0]:[W126 05:11:03.600812937 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 05:11:06
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 05:12:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 05:12:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=215897) WARNING 01-26 05:12:21 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=215897) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=215897) WARNING 01-26 05:12:35 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     def forward(
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     raise e
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/tmp/torchinductor_root/ar/cargdknnyxc27i6nfnlzvi5zgcsm4wlzok6c55nlais5diyrot6v.py", line 1093, in call
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 10)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/RTX4090_cc89_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 263, in quant_slide_fp8_triton
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) ERROR 01-26 05:12:44 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered

STDERR:
[2026-01-26 05:12:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 05:12:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 05:12:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 05:12:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 05:12:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 05:12:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 05:12:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 05:12:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 05:12:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 05:12:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 05:12:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 05:12:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 05:12:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 05:12:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 05:12:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 05:12:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 05:12:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=215897) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=215897) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.02it/s]
(EngineCore_DP0 pid=215897) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]
(EngineCore_DP0 pid=215897) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]
(EngineCore_DP0 pid=215897) 
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 163676160 bytes
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=215897) [2026-01-26 05:12:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 81543168 bytes
(EngineCore_DP0 pid=215897) [rank0]:W0126 05:12:41.775000 215897 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=215897) [rank0]:W0126 05:12:41.887000 215897 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=215897) [rank0]:W0126 05:12:43.541000 215897 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=215897) [rank0]:W0126 05:12:43.706000 215897 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=215897) Process EngineCore_DP0:
(EngineCore_DP0 pid=215897) Traceback (most recent call last):
(EngineCore_DP0 pid=215897)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=215897)     self.run()
(EngineCore_DP0 pid=215897)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=215897)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=215897)     raise e
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=215897)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=215897)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=215897)     super().__init__(
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=215897)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=215897)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=215897)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=215897)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=215897)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=215897)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=215897)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=215897)     return func(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=215897)     return func(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=215897)     self.model_runner.profile_run()
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=215897)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=215897)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=215897)     return func(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=215897)     outputs = self.model(
(EngineCore_DP0 pid=215897)               ^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=215897)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=215897)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=215897)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=215897)     hidden_states = self.model(
(EngineCore_DP0 pid=215897)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=215897)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=215897)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=215897)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=215897)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=215897)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=215897)     def forward(
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=215897)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=215897)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=215897)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=215897)     raise e
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=215897)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=215897)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=215897)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=215897)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=215897)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=215897)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=215897)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=215897)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=215897)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=215897)     return compiled_fn(full_args)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=215897)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=215897)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=215897)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=215897)                             ^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=215897)     outs = compiled_fn(args)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=215897)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=215897)     return self.current_callable(inputs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=215897)     out = model(new_inputs)
(EngineCore_DP0 pid=215897)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/tmp/torchinductor_root/ar/cargdknnyxc27i6nfnlzvi5zgcsm4wlzok6c55nlais5diyrot6v.py", line 1093, in call
(EngineCore_DP0 pid=215897)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 10)
(EngineCore_DP0 pid=215897)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=215897)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=215897)     return fn(input, L)
(EngineCore_DP0 pid=215897)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/RTX4090_cc89_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 263, in quant_slide_fp8_triton
(EngineCore_DP0 pid=215897)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=215897)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=215897)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=215897)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=215897)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=215897)     self._init_handles()
(EngineCore_DP0 pid=215897)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=215897)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=215897)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=215897) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 05:12:45.798436163 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 08:18:28
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:18:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:18:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=393227) WARNING 01-26 08:18:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=393227) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=393227) WARNING 01-26 08:19:19 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=393227) ERROR 01-26 08:19:39 [core.py:866] ValueError: To serve at least one request with the models's max seq len (513), (0.10 GiB KV cache is needed, which is larger than the available KV cache memory (0.01 GiB). Based on the available memory, the estimated maximum model length is 48. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:18:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:18:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:18:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:18:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:18:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:18:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:18:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:18:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:18:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:18:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:18:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:18:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:18:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:18:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:18:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:18:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:18:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=393227) [2026-01-26 08:18:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=393227) [2026-01-26 08:18:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=393227) [2026-01-26 08:18:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=393227) [2026-01-26 08:18:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=393227) [2026-01-26 08:18:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=393227) [2026-01-26 08:18:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=393227) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=393227) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:17,  5.92s/it]
(EngineCore_DP0 pid=393227) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:11<00:11,  5.54s/it]
(EngineCore_DP0 pid=393227) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:12<00:03,  3.60s/it]
(EngineCore_DP0 pid=393227) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:18<00:00,  4.45s/it]
(EngineCore_DP0 pid=393227) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:18<00:00,  4.56s/it]
(EngineCore_DP0 pid=393227) 
(EngineCore_DP0 pid=393227) [2026-01-26 08:19:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=393227) [2026-01-26 08:19:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44040192 bytes
(EngineCore_DP0 pid=393227) [2026-01-26 08:19:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=393227) [2026-01-26 08:19:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=393227) [2026-01-26 08:19:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=393227) [2026-01-26 08:19:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 169869312 bytes
(EngineCore_DP0 pid=393227) [2026-01-26 08:19:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=393227) [2026-01-26 08:19:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 85032960 bytes
(EngineCore_DP0 pid=393227) Process EngineCore_DP0:
(EngineCore_DP0 pid=393227) Traceback (most recent call last):
(EngineCore_DP0 pid=393227)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=393227)     self.run()
(EngineCore_DP0 pid=393227)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=393227)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=393227)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=393227)     raise e
(EngineCore_DP0 pid=393227)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=393227)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=393227)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=393227)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=393227)     super().__init__(
(EngineCore_DP0 pid=393227)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=393227)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=393227)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=393227)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=393227)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=393227)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=393227)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=393227)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=393227)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=393227)     raise ValueError(
(EngineCore_DP0 pid=393227) ValueError: To serve at least one request with the models's max seq len (513), (0.10 GiB KV cache is needed, which is larger than the available KV cache memory (0.01 GiB). Based on the available memory, the estimated maximum model length is 48. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:19:40.958264215 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=1024 ==========
Time: 2026-01-26 08:19:45
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:19:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:19:53 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=394516) WARNING 01-26 08:20:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=394516) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=394516) WARNING 01-26 08:20:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=394516) ERROR 01-26 08:20:55 [core.py:866] ValueError: To serve at least one request with the models's max seq len (1025), (0.19 GiB KV cache is needed, which is larger than the available KV cache memory (0.00 GiB).  Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:19:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:19:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:19:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:19:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:19:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:19:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:19:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:19:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:19:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:19:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:19:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:19:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:19:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:19:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:20:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:20:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:20:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:20:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:20:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:20:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:20:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:20:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:20:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:20:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:20:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:20:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:20:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:20:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=394516) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=394516) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:17,  5.80s/it]
(EngineCore_DP0 pid=394516) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:11<00:11,  5.60s/it]
(EngineCore_DP0 pid=394516) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:12<00:03,  3.66s/it]
(EngineCore_DP0 pid=394516) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:17<00:00,  4.29s/it]
(EngineCore_DP0 pid=394516) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:17<00:00,  4.47s/it]
(EngineCore_DP0 pid=394516) 
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44040192 bytes
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 169869312 bytes
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=394516) [2026-01-26 08:20:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 85032960 bytes
(EngineCore_DP0 pid=394516) Process EngineCore_DP0:
(EngineCore_DP0 pid=394516) Traceback (most recent call last):
(EngineCore_DP0 pid=394516)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=394516)     self.run()
(EngineCore_DP0 pid=394516)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=394516)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=394516)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=394516)     raise e
(EngineCore_DP0 pid=394516)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=394516)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=394516)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394516)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=394516)     super().__init__(
(EngineCore_DP0 pid=394516)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=394516)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=394516)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394516)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=394516)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=394516)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394516)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=394516)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=394516)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=394516)     raise ValueError(
(EngineCore_DP0 pid=394516) ValueError: To serve at least one request with the models's max seq len (1025), (0.19 GiB KV cache is needed, which is larger than the available KV cache memory (0.00 GiB).  Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:20:56.087011796 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=1024

========== M=2048 ==========
Time: 2026-01-26 08:21:01
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:21:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:21:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=395779) WARNING 01-26 08:21:19 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=395779) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=395779) WARNING 01-26 08:21:41 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=395779) ERROR 01-26 08:22:00 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:21:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:21:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:21:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:21:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:21:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:21:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:21:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:21:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:21:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:21:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:21:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:21:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:21:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:21:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:21:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:21:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:21:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=395779) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=395779) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.55s/it]
(EngineCore_DP0 pid=395779) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.54s/it]
(EngineCore_DP0 pid=395779) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]
(EngineCore_DP0 pid=395779) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.22s/it]
(EngineCore_DP0 pid=395779) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.26s/it]
(EngineCore_DP0 pid=395779) 
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44040192 bytes
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 169869312 bytes
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=395779) [2026-01-26 08:21:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 85032960 bytes
(EngineCore_DP0 pid=395779) Process EngineCore_DP0:
(EngineCore_DP0 pid=395779) Traceback (most recent call last):
(EngineCore_DP0 pid=395779)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=395779)     self.run()
(EngineCore_DP0 pid=395779)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=395779)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=395779)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=395779)     raise e
(EngineCore_DP0 pid=395779)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=395779)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=395779)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=395779)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=395779)     super().__init__(
(EngineCore_DP0 pid=395779)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=395779)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=395779)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=395779)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=395779)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=395779)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=395779)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=395779)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=395779)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=395779)     raise ValueError(
(EngineCore_DP0 pid=395779) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:22:01.302881978 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=2048

========== M=4096 ==========
Time: 2026-01-26 08:22:06
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:22:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:22:19 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=396943) WARNING 01-26 08:22:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=396943) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=396943) WARNING 01-26 08:22:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=396943) ERROR 01-26 08:23:15 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:22:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:22:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:22:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:22:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:22:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:22:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:22:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:22:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:22:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:22:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:22:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:22:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:22:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:22:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:22:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:22:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:22:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=396943) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=396943) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.55s/it]
(EngineCore_DP0 pid=396943) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.54s/it]
(EngineCore_DP0 pid=396943) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
(EngineCore_DP0 pid=396943) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.20s/it]
(EngineCore_DP0 pid=396943) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.24s/it]
(EngineCore_DP0 pid=396943) 
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44040192 bytes
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 169869312 bytes
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=396943) [2026-01-26 08:22:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 85032960 bytes
(EngineCore_DP0 pid=396943) Process EngineCore_DP0:
(EngineCore_DP0 pid=396943) Traceback (most recent call last):
(EngineCore_DP0 pid=396943)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=396943)     self.run()
(EngineCore_DP0 pid=396943)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=396943)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=396943)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=396943)     raise e
(EngineCore_DP0 pid=396943)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=396943)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=396943)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=396943)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=396943)     super().__init__(
(EngineCore_DP0 pid=396943)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=396943)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=396943)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=396943)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=396943)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=396943)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=396943)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=396943)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=396943)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=396943)     raise ValueError(
(EngineCore_DP0 pid=396943) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:23:17.342968887 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=4096

========== M=8192 ==========
Time: 2026-01-26 08:23:21
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:23:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:23:36 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=398237) WARNING 01-26 08:23:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=398237) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=398237) WARNING 01-26 08:24:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=398237) ERROR 01-26 08:24:47 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:23:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:23:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:23:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:23:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:23:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:23:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:23:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:23:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:23:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:23:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:23:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:23:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:23:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:23:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:23:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:23:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:23:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=398237) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=398237) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.55s/it]
(EngineCore_DP0 pid=398237) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.54s/it]
(EngineCore_DP0 pid=398237) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]
(EngineCore_DP0 pid=398237) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.22s/it]
(EngineCore_DP0 pid=398237) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.26s/it]
(EngineCore_DP0 pid=398237) 
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44040192 bytes
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 169869312 bytes
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=398237) [2026-01-26 08:23:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 85032960 bytes
(EngineCore_DP0 pid=398237) Process EngineCore_DP0:
(EngineCore_DP0 pid=398237) Traceback (most recent call last):
(EngineCore_DP0 pid=398237)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=398237)     self.run()
(EngineCore_DP0 pid=398237)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=398237)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=398237)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=398237)     raise e
(EngineCore_DP0 pid=398237)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=398237)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=398237)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=398237)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=398237)     super().__init__(
(EngineCore_DP0 pid=398237)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=398237)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=398237)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=398237)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=398237)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=398237)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=398237)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=398237)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=398237)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=398237)     raise ValueError(
(EngineCore_DP0 pid=398237) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:24:48.091208966 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=8192

========== M=16384 ==========
Time: 2026-01-26 08:24:52
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:25:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:25:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=399795) WARNING 01-26 08:25:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=399795) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=399795) WARNING 01-26 08:25:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=399795) ERROR 01-26 08:28:18 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:25:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:25:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:25:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:25:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:25:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:25:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:25:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:25:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:25:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:25:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:25:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:25:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:25:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:25:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:25:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:25:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:25:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:24] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=399795) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=399795) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.47s/it]
(EngineCore_DP0 pid=399795) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.82it/s]
(EngineCore_DP0 pid=399795) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.14it/s]
(EngineCore_DP0 pid=399795) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.17it/s]
(EngineCore_DP0 pid=399795) 
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44040192 bytes
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 169869312 bytes
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=399795) [2026-01-26 08:25:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 85032960 bytes
(EngineCore_DP0 pid=399795) [rank0]:W0126 08:25:59.320000 399795 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=399795) [rank0]:W0126 08:25:59.399000 399795 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=399795) [rank0]:W0126 08:26:01.010000 399795 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=399795) [rank0]:W0126 08:26:01.131000 399795 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=399795) Process EngineCore_DP0:
(EngineCore_DP0 pid=399795) Traceback (most recent call last):
(EngineCore_DP0 pid=399795)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=399795)     self.run()
(EngineCore_DP0 pid=399795)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=399795)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=399795)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=399795)     raise e
(EngineCore_DP0 pid=399795)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=399795)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=399795)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=399795)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=399795)     super().__init__(
(EngineCore_DP0 pid=399795)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=399795)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=399795)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=399795)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=399795)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=399795)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=399795)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=399795)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=399795)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=399795)     raise ValueError(
(EngineCore_DP0 pid=399795) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:28:20.876909750 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=32768 ==========
Time: 2026-01-26 08:28:24
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:29:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:29:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=403198) WARNING 01-26 08:29:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=403198) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=403198) WARNING 01-26 08:29:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=403198) ERROR 01-26 08:34:06 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:29:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:29:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:29:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:29:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:29:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:29:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:29:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:29:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:29:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:29:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:29:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:29:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:29:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:29:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:29:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:29:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:29:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:11] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=403198) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=403198) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.52s/it]
(EngineCore_DP0 pid=403198) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.50s/it]
(EngineCore_DP0 pid=403198) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
(EngineCore_DP0 pid=403198) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.18s/it]
(EngineCore_DP0 pid=403198) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.22s/it]
(EngineCore_DP0 pid=403198) 
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44040192 bytes
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 169869312 bytes
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=403198) [2026-01-26 08:29:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 85032960 bytes
(EngineCore_DP0 pid=403198) [rank0]:W0126 08:29:48.745000 403198 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=403198) [rank0]:W0126 08:29:48.825000 403198 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=403198) [rank0]:W0126 08:29:50.673000 403198 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=403198) [rank0]:W0126 08:29:50.798000 403198 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=403198) Process EngineCore_DP0:
(EngineCore_DP0 pid=403198) Traceback (most recent call last):
(EngineCore_DP0 pid=403198)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=403198)     self.run()
(EngineCore_DP0 pid=403198)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=403198)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=403198)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=403198)     raise e
(EngineCore_DP0 pid=403198)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=403198)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=403198)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=403198)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=403198)     super().__init__(
(EngineCore_DP0 pid=403198)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=403198)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=403198)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=403198)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=403198)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=403198)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=403198)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=403198)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=403198)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=403198)     raise ValueError(
(EngineCore_DP0 pid=403198) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:34:07.054175155 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=65536 ==========
Time: 2026-01-26 08:34:11
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:35:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:35:19 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=408658) WARNING 01-26 08:35:28 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=408658) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=408658) WARNING 01-26 08:35:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=408658) ERROR 01-26 08:45:09 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 08:35:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:35:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:35:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:35:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:35:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:35:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:35:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:35:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:35:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:35:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:35:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 08:35:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 08:35:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:35:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:35:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:35:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:35:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:28] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:28] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:28] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:28] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=408658) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=408658) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.53s/it]
(EngineCore_DP0 pid=408658) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.53s/it]
(EngineCore_DP0 pid=408658) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
(EngineCore_DP0 pid=408658) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.22s/it]
(EngineCore_DP0 pid=408658) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.25s/it]
(EngineCore_DP0 pid=408658) 
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44040192 bytes
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 169869312 bytes
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=408658) [2026-01-26 08:35:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 85032960 bytes
(EngineCore_DP0 pid=408658) [rank0]:W0126 08:36:06.011000 408658 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=408658) [rank0]:W0126 08:36:06.094000 408658 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=408658) [rank0]:W0126 08:36:10.462000 408658 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=408658) [rank0]:W0126 08:36:10.584000 408658 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=408658) Process EngineCore_DP0:
(EngineCore_DP0 pid=408658) Traceback (most recent call last):
(EngineCore_DP0 pid=408658)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=408658)     self.run()
(EngineCore_DP0 pid=408658)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=408658)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=408658)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=408658)     raise e
(EngineCore_DP0 pid=408658)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=408658)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=408658)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=408658)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=408658)     super().__init__(
(EngineCore_DP0 pid=408658)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=408658)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=408658)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=408658)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=408658)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=408658)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=408658)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=408658)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=408658)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=408658)     raise ValueError(
(EngineCore_DP0 pid=408658) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 08:45:10.391616071 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 21:29:39
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:29:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:29:49 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1095226) ERROR 01-26 21:30:04 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:29:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:29:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:29:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:29:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:29:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:29:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:29:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:29:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:29:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:29:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:29:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:29:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:29:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:29:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:29:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:29:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:29:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1095226) Process EngineCore_DP0:
(EngineCore_DP0 pid=1095226) Traceback (most recent call last):
(EngineCore_DP0 pid=1095226)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1095226)     self.run()
(EngineCore_DP0 pid=1095226)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1095226)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1095226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1095226)     raise e
(EngineCore_DP0 pid=1095226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1095226)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1095226)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1095226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1095226)     super().__init__(
(EngineCore_DP0 pid=1095226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1095226)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1095226)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1095226)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1095226)     self._init_executor()
(EngineCore_DP0 pid=1095226)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1095226)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1095226)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1095226)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1095226)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1095226)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1095226)     raise ValueError(
(EngineCore_DP0 pid=1095226) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:30:05.366185195 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=512 ==========
Time: 2026-01-26 21:30:39
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:30:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:30:49 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1096218) ERROR 01-26 21:31:02 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:30:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:30:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:30:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:30:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:30:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:30:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:30:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:30:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:30:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:30:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:30:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:30:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:30:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:30:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:30:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:30:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:30:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1096218) Process EngineCore_DP0:
(EngineCore_DP0 pid=1096218) Traceback (most recent call last):
(EngineCore_DP0 pid=1096218)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1096218)     self.run()
(EngineCore_DP0 pid=1096218)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1096218)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1096218)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1096218)     raise e
(EngineCore_DP0 pid=1096218)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1096218)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1096218)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1096218)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1096218)     super().__init__(
(EngineCore_DP0 pid=1096218)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1096218)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1096218)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1096218)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1096218)     self._init_executor()
(EngineCore_DP0 pid=1096218)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1096218)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1096218)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1096218)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1096218)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1096218)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1096218)     raise ValueError(
(EngineCore_DP0 pid=1096218) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:31:03.421252656 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=1024 ==========
Time: 2026-01-26 21:31:29
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:31:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:31:40 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1097113) ERROR 01-26 21:31:51 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:31:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:31:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:31:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:31:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:31:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:31:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:31:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:31:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:31:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:31:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:31:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:31:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:31:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:31:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:31:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:31:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:31:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1097113) Process EngineCore_DP0:
(EngineCore_DP0 pid=1097113) Traceback (most recent call last):
(EngineCore_DP0 pid=1097113)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1097113)     self.run()
(EngineCore_DP0 pid=1097113)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1097113)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1097113)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1097113)     raise e
(EngineCore_DP0 pid=1097113)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1097113)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1097113)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097113)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1097113)     super().__init__(
(EngineCore_DP0 pid=1097113)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1097113)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1097113)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097113)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1097113)     self._init_executor()
(EngineCore_DP0 pid=1097113)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1097113)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1097113)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1097113)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1097113)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097113)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1097113)     raise ValueError(
(EngineCore_DP0 pid=1097113) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:31:52.472358306 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=1024

========== M=1024 ==========
Time: 2026-01-26 21:32:16
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:32:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:32:27 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1097950) ERROR 01-26 21:32:37 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:32:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:32:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:32:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:32:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:32:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:32:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:32:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:32:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:32:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:32:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:32:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:32:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:32:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:32:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:32:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:32:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:32:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1097950) Process EngineCore_DP0:
(EngineCore_DP0 pid=1097950) Traceback (most recent call last):
(EngineCore_DP0 pid=1097950)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1097950)     self.run()
(EngineCore_DP0 pid=1097950)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1097950)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1097950)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1097950)     raise e
(EngineCore_DP0 pid=1097950)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1097950)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1097950)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097950)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1097950)     super().__init__(
(EngineCore_DP0 pid=1097950)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1097950)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1097950)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097950)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1097950)     self._init_executor()
(EngineCore_DP0 pid=1097950)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1097950)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1097950)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1097950)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1097950)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1097950)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1097950)     raise ValueError(
(EngineCore_DP0 pid=1097950) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:32:38.466059295 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=1024

========== M=2048 ==========
Time: 2026-01-26 21:33:06
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:33:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:33:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1098822) ERROR 01-26 21:33:27 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:33:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:33:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:33:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:33:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:33:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:33:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:33:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:33:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:33:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:33:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:33:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:33:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:33:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:33:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:33:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:33:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:33:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1098822) Process EngineCore_DP0:
(EngineCore_DP0 pid=1098822) Traceback (most recent call last):
(EngineCore_DP0 pid=1098822)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1098822)     self.run()
(EngineCore_DP0 pid=1098822)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1098822)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1098822)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1098822)     raise e
(EngineCore_DP0 pid=1098822)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1098822)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1098822)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1098822)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1098822)     super().__init__(
(EngineCore_DP0 pid=1098822)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1098822)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1098822)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1098822)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1098822)     self._init_executor()
(EngineCore_DP0 pid=1098822)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1098822)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1098822)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1098822)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1098822)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1098822)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1098822)     raise ValueError(
(EngineCore_DP0 pid=1098822) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:33:28.138312963 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=2048

========== M=2048 ==========
Time: 2026-01-26 21:33:53
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:34:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:34:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1099707) ERROR 01-26 21:34:14 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:34:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:34:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:34:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:34:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:34:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:34:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:34:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:34:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:34:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:34:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:34:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:34:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:34:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:34:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:34:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:34:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1099707) Process EngineCore_DP0:
(EngineCore_DP0 pid=1099707) Traceback (most recent call last):
(EngineCore_DP0 pid=1099707)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1099707)     self.run()
(EngineCore_DP0 pid=1099707)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1099707)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1099707)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1099707)     raise e
(EngineCore_DP0 pid=1099707)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1099707)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1099707)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1099707)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1099707)     super().__init__(
(EngineCore_DP0 pid=1099707)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1099707)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1099707)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1099707)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1099707)     self._init_executor()
(EngineCore_DP0 pid=1099707)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1099707)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1099707)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1099707)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1099707)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1099707)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1099707)     raise ValueError(
(EngineCore_DP0 pid=1099707) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:34:15.471944529 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=2048

========== M=4096 ==========
Time: 2026-01-26 21:34:40
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:34:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:34:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1100601) ERROR 01-26 21:35:05 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:34:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:34:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:34:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:34:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:34:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:34:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:34:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:34:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:34:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:35:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:35:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:35:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:35:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:35:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:35:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:35:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:35:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1100601) Process EngineCore_DP0:
(EngineCore_DP0 pid=1100601) Traceback (most recent call last):
(EngineCore_DP0 pid=1100601)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1100601)     self.run()
(EngineCore_DP0 pid=1100601)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1100601)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1100601)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1100601)     raise e
(EngineCore_DP0 pid=1100601)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1100601)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1100601)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1100601)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1100601)     super().__init__(
(EngineCore_DP0 pid=1100601)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1100601)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1100601)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1100601)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1100601)     self._init_executor()
(EngineCore_DP0 pid=1100601)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1100601)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1100601)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1100601)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1100601)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1100601)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1100601)     raise ValueError(
(EngineCore_DP0 pid=1100601) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:35:06.598946993 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=4096

========== M=4096 ==========
Time: 2026-01-26 21:35:29
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:35:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:35:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1101481) ERROR 01-26 21:35:52 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:35:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:35:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:35:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:35:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:35:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:35:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:35:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:35:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:35:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:35:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:35:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:35:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:35:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:35:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:35:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:35:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:35:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1101481) Process EngineCore_DP0:
(EngineCore_DP0 pid=1101481) Traceback (most recent call last):
(EngineCore_DP0 pid=1101481)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1101481)     self.run()
(EngineCore_DP0 pid=1101481)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1101481)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1101481)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1101481)     raise e
(EngineCore_DP0 pid=1101481)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1101481)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1101481)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1101481)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1101481)     super().__init__(
(EngineCore_DP0 pid=1101481)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1101481)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1101481)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1101481)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1101481)     self._init_executor()
(EngineCore_DP0 pid=1101481)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1101481)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1101481)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1101481)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1101481)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1101481)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1101481)     raise ValueError(
(EngineCore_DP0 pid=1101481) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:35:53.755184522 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=4096

========== M=8192 ==========
Time: 2026-01-26 21:36:20
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:36:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:36:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1102411) ERROR 01-26 21:36:47 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:36:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:36:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:36:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:36:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:36:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:36:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:36:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:36:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:36:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:36:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:36:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:36:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:36:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:36:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:36:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:36:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:36:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1102411) Process EngineCore_DP0:
(EngineCore_DP0 pid=1102411) Traceback (most recent call last):
(EngineCore_DP0 pid=1102411)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1102411)     self.run()
(EngineCore_DP0 pid=1102411)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1102411)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1102411)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1102411)     raise e
(EngineCore_DP0 pid=1102411)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1102411)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1102411)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1102411)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1102411)     super().__init__(
(EngineCore_DP0 pid=1102411)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1102411)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1102411)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1102411)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1102411)     self._init_executor()
(EngineCore_DP0 pid=1102411)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1102411)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1102411)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1102411)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1102411)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1102411)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1102411)     raise ValueError(
(EngineCore_DP0 pid=1102411) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:36:48.877624146 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=8192

========== M=8192 ==========
Time: 2026-01-26 21:37:14
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:37:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:37:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1103362) ERROR 01-26 21:37:41 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:37:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:37:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:37:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:37:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:37:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:37:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:37:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:37:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:37:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:37:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:37:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:37:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:37:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:37:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:37:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:37:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:37:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1103362) Process EngineCore_DP0:
(EngineCore_DP0 pid=1103362) Traceback (most recent call last):
(EngineCore_DP0 pid=1103362)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1103362)     self.run()
(EngineCore_DP0 pid=1103362)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1103362)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1103362)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1103362)     raise e
(EngineCore_DP0 pid=1103362)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1103362)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1103362)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1103362)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1103362)     super().__init__(
(EngineCore_DP0 pid=1103362)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1103362)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1103362)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1103362)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1103362)     self._init_executor()
(EngineCore_DP0 pid=1103362)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1103362)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1103362)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1103362)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1103362)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1103362)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1103362)     raise ValueError(
(EngineCore_DP0 pid=1103362) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:37:42.354868397 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=8192

========== M=16384 ==========
Time: 2026-01-26 21:38:07
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:38:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:38:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1104382) ERROR 01-26 21:38:42 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:38:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:38:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:38:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:38:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:38:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:38:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:38:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:38:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:38:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:38:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:38:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:38:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:38:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:38:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:38:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:38:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:38:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1104382) Process EngineCore_DP0:
(EngineCore_DP0 pid=1104382) Traceback (most recent call last):
(EngineCore_DP0 pid=1104382)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1104382)     self.run()
(EngineCore_DP0 pid=1104382)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1104382)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1104382)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1104382)     raise e
(EngineCore_DP0 pid=1104382)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1104382)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1104382)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1104382)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1104382)     super().__init__(
(EngineCore_DP0 pid=1104382)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1104382)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1104382)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1104382)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1104382)     self._init_executor()
(EngineCore_DP0 pid=1104382)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1104382)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1104382)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1104382)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1104382)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1104382)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1104382)     raise ValueError(
(EngineCore_DP0 pid=1104382) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:38:43.440916313 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=16384 ==========
Time: 2026-01-26 21:39:08
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:39:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:39:31 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1105453) ERROR 01-26 21:39:42 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:39:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:39:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:39:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:39:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:39:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:39:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:39:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:39:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:39:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:39:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:39:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:39:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:39:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:39:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:39:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:39:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:39:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1105453) Process EngineCore_DP0:
(EngineCore_DP0 pid=1105453) Traceback (most recent call last):
(EngineCore_DP0 pid=1105453)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1105453)     self.run()
(EngineCore_DP0 pid=1105453)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1105453)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1105453)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1105453)     raise e
(EngineCore_DP0 pid=1105453)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1105453)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1105453)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1105453)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1105453)     super().__init__(
(EngineCore_DP0 pid=1105453)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1105453)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1105453)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1105453)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1105453)     self._init_executor()
(EngineCore_DP0 pid=1105453)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1105453)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1105453)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1105453)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1105453)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1105453)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1105453)     raise ValueError(
(EngineCore_DP0 pid=1105453) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:39:43.524377118 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=32768 ==========
Time: 2026-01-26 21:40:08
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:40:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:40:47 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1106744) ERROR 01-26 21:40:56 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:40:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:40:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:40:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:40:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:40:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:40:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:40:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:40:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:40:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:40:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:40:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:40:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:40:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:40:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:40:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:40:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:40:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1106744) Process EngineCore_DP0:
(EngineCore_DP0 pid=1106744) Traceback (most recent call last):
(EngineCore_DP0 pid=1106744)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1106744)     self.run()
(EngineCore_DP0 pid=1106744)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1106744)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1106744)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1106744)     raise e
(EngineCore_DP0 pid=1106744)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1106744)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1106744)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1106744)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1106744)     super().__init__(
(EngineCore_DP0 pid=1106744)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1106744)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1106744)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1106744)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1106744)     self._init_executor()
(EngineCore_DP0 pid=1106744)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1106744)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1106744)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1106744)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1106744)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1106744)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1106744)     raise ValueError(
(EngineCore_DP0 pid=1106744) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:40:57.323765612 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=32768 ==========
Time: 2026-01-26 21:41:22
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:42:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:42:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1108244) ERROR 01-26 21:42:12 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:42:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:42:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:42:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:42:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:42:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:42:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:42:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:42:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:42:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:42:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:42:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:42:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:42:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:42:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:42:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:42:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:42:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1108244) Process EngineCore_DP0:
(EngineCore_DP0 pid=1108244) Traceback (most recent call last):
(EngineCore_DP0 pid=1108244)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1108244)     self.run()
(EngineCore_DP0 pid=1108244)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1108244)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1108244)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1108244)     raise e
(EngineCore_DP0 pid=1108244)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1108244)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1108244)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1108244)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1108244)     super().__init__(
(EngineCore_DP0 pid=1108244)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1108244)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1108244)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1108244)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1108244)     self._init_executor()
(EngineCore_DP0 pid=1108244)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1108244)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1108244)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1108244)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1108244)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1108244)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1108244)     raise ValueError(
(EngineCore_DP0 pid=1108244) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:42:13.343396554 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=65536 ==========
Time: 2026-01-26 21:42:38
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:43:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:43:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1109933) ERROR 01-26 21:43:56 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:43:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:43:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:43:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:43:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:43:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:43:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:43:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:43:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:43:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:43:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:43:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:43:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:43:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:43:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:43:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:43:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:43:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1109933) Process EngineCore_DP0:
(EngineCore_DP0 pid=1109933) Traceback (most recent call last):
(EngineCore_DP0 pid=1109933)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1109933)     self.run()
(EngineCore_DP0 pid=1109933)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1109933)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1109933)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1109933)     raise e
(EngineCore_DP0 pid=1109933)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1109933)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1109933)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1109933)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1109933)     super().__init__(
(EngineCore_DP0 pid=1109933)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1109933)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1109933)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1109933)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1109933)     self._init_executor()
(EngineCore_DP0 pid=1109933)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1109933)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1109933)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1109933)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1109933)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1109933)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1109933)     raise ValueError(
(EngineCore_DP0 pid=1109933) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:43:57.501150236 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=65536 ==========
Time: 2026-01-26 21:44:20
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:45:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:45:27 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1111742) ERROR 01-26 21:45:38 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:45:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:45:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:45:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:45:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:45:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:45:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:45:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:45:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:45:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:45:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:45:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:45:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:45:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:45:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:45:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:45:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:45:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1111742) Process EngineCore_DP0:
(EngineCore_DP0 pid=1111742) Traceback (most recent call last):
(EngineCore_DP0 pid=1111742)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1111742)     self.run()
(EngineCore_DP0 pid=1111742)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1111742)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1111742)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1111742)     raise e
(EngineCore_DP0 pid=1111742)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1111742)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1111742)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1111742)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1111742)     super().__init__(
(EngineCore_DP0 pid=1111742)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1111742)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1111742)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1111742)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1111742)     self._init_executor()
(EngineCore_DP0 pid=1111742)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1111742)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1111742)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1111742)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1111742)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1111742)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1111742)     raise ValueError(
(EngineCore_DP0 pid=1111742) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:45:39.817478997 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-28 12:19:03
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:19:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:19:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=48749) WARNING 01-28 12:19:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=48749) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=48749) WARNING 01-28 12:19:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.04 requests/s, 8227.81 total tokens/s, 16.04 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-28 12:19:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:19:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:19:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:19:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:19:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:19:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:19:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:19:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:19:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:19:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:19:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:19:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:19:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:19:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:19:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:19:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=48749) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=48749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.13s/it]
(EngineCore_DP0 pid=48749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.13s/it]
(EngineCore_DP0 pid=48749) 
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=48749) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.73it/s]
(EngineCore_DP0 pid=48749) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  51%|█████     | 65/128 [00:00<00:00, 645.60it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 655.78it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:20,  6.24it/s, est. speed input: 3195.93 toks/s, output: 6.24 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:10, 11.55it/s, est. speed input: 5449.48 toks/s, output: 10.64 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:08, 13.70it/s, est. speed input: 6365.92 toks/s, output: 12.43 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 14.83it/s, est. speed input: 6867.68 toks/s, output: 13.41 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 15.46it/s, est. speed input: 7176.32 toks/s, output: 14.02 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 15.77it/s, est. speed input: 7368.82 toks/s, output: 14.39 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:07, 16.01it/s, est. speed input: 7517.34 toks/s, output: 14.68 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:06, 16.16it/s, est. speed input: 7628.70 toks/s, output: 14.90 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 16.34it/s, est. speed input: 7729.29 toks/s, output: 15.10 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 16.43it/s, est. speed input: 7805.50 toks/s, output: 15.24 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 16.44it/s, est. speed input: 7860.59 toks/s, output: 15.35 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 16.46it/s, est. speed input: 7908.32 toks/s, output: 15.45 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 16.39it/s, est. speed input: 7939.04 toks/s, output: 15.51 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 16.44it/s, est. speed input: 7976.64 toks/s, output: 15.58 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 16.50it/s, est. speed input: 8011.61 toks/s, output: 15.65 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.49it/s, est. speed input: 8037.27 toks/s, output: 15.70 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 16.54it/s, est. speed input: 8065.82 toks/s, output: 15.75 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.64it/s, est. speed input: 8096.14 toks/s, output: 15.81 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.57it/s, est. speed input: 8111.90 toks/s, output: 15.84 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.62it/s, est. speed input: 8134.57 toks/s, output: 15.89 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.66it/s, est. speed input: 8155.12 toks/s, output: 15.93 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.72it/s, est. speed input: 8175.85 toks/s, output: 15.97 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 16.66it/s, est. speed input: 8188.10 toks/s, output: 15.99 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 16.61it/s, est. speed input: 8198.98 toks/s, output: 16.01 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:04, 16.55it/s, est. speed input: 8207.17 toks/s, output: 16.03 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 16.55it/s, est. speed input: 8217.33 toks/s, output: 16.05 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 16.60it/s, est. speed input: 8229.92 toks/s, output: 16.07 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.67it/s, est. speed input: 8243.14 toks/s, output: 16.10 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.71it/s, est. speed input: 8255.17 toks/s, output: 16.12 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.73it/s, est. speed input: 8266.31 toks/s, output: 16.15 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.75it/s, est. speed input: 8276.72 toks/s, output: 16.17 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.74it/s, est. speed input: 8285.29 toks/s, output: 16.18 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:03, 16.73it/s, est. speed input: 8293.49 toks/s, output: 16.20 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.65it/s, est. speed input: 8297.41 toks/s, output: 16.21 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.63it/s, est. speed input: 8302.97 toks/s, output: 16.22 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.60it/s, est. speed input: 8307.41 toks/s, output: 16.23 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.57it/s, est. speed input: 8311.00 toks/s, output: 16.23 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.61it/s, est. speed input: 8317.34 toks/s, output: 16.24 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.66it/s, est. speed input: 8324.07 toks/s, output: 16.26 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 16.59it/s, est. speed input: 8326.57 toks/s, output: 16.26 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.60it/s, est. speed input: 8331.10 toks/s, output: 16.27 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 16.56it/s, est. speed input: 8333.24 toks/s, output: 16.28 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.60it/s, est. speed input: 8338.26 toks/s, output: 16.29 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.68it/s, est. speed input: 8344.93 toks/s, output: 16.30 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.74it/s, est. speed input: 8351.45 toks/s, output: 16.31 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.77it/s, est. speed input: 8357.34 toks/s, output: 16.32 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.80it/s, est. speed input: 8363.12 toks/s, output: 16.33 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.83it/s, est. speed input: 8368.98 toks/s, output: 16.35 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.85it/s, est. speed input: 8374.57 toks/s, output: 16.36 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.82it/s, est. speed input: 8378.66 toks/s, output: 16.36 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.81it/s, est. speed input: 8382.68 toks/s, output: 16.37 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.82it/s, est. speed input: 8387.45 toks/s, output: 16.38 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.85it/s, est. speed input: 8392.32 toks/s, output: 16.39 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.87it/s, est. speed input: 8397.38 toks/s, output: 16.40 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.86it/s, est. speed input: 8401.34 toks/s, output: 16.41 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.88it/s, est. speed input: 8405.90 toks/s, output: 16.42 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.77it/s, est. speed input: 8406.94 toks/s, output: 16.42 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 16.67it/s, est. speed input: 8406.97 toks/s, output: 16.42 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 16.59it/s, est. speed input: 8407.04 toks/s, output: 16.42 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 16.56it/s, est. speed input: 8407.47 toks/s, output: 16.42 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 16.63it/s, est. speed input: 8410.52 toks/s, output: 16.43 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 16.71it/s, est. speed input: 8414.39 toks/s, output: 16.43 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.61it/s, est. speed input: 8414.04 toks/s, output: 16.43 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.69it/s, est. speed input: 8417.58 toks/s, output: 16.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.69it/s, est. speed input: 8419.34 toks/s, output: 16.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.44it/s, est. speed input: 8419.34 toks/s, output: 16.44 toks/s]
[rank0]:[W128 12:19:57.629382068 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-28 12:19:59
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:20:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:20:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=49397) WARNING 01-28 12:20:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=49397) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=49397) WARNING 01-28 12:20:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.39 requests/s, 16796.54 total tokens/s, 16.39 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-28 12:20:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:20:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:20:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:20:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:20:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:20:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:20:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:20:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:20:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:20:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:20:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:20:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:20:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:20:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:20:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:20:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=49397) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=49397) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=49397) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=49397) 
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=49397) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.04it/s]
(EngineCore_DP0 pid=49397) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  27%|██▋       | 35/128 [00:00<00:00, 347.79it/s]
Adding requests:  55%|█████▌    | 71/128 [00:00<00:00, 351.89it/s]
Adding requests:  84%|████████▎ | 107/128 [00:00<00:00, 342.42it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 346.34it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 36.18it/s, est. speed input: 37055.67 toks/s, output: 36.18 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 22.37it/s, est. speed input: 24462.29 toks/s, output: 23.89 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:05, 20.06it/s, est. speed input: 22209.81 toks/s, output: 21.69 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:05, 18.99it/s, est. speed input: 21110.79 toks/s, output: 20.62 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:06, 18.48it/s, est. speed input: 20608.43 toks/s, output: 20.12 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:06, 18.09it/s, est. speed input: 20229.14 toks/s, output: 19.75 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 17.82it/s, est. speed input: 19938.23 toks/s, output: 19.47 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 17.61it/s, est. speed input: 19699.01 toks/s, output: 19.24 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.42it/s, est. speed input: 19489.01 toks/s, output: 19.03 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.29it/s, est. speed input: 19317.78 toks/s, output: 18.86 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.14it/s, est. speed input: 19153.05 toks/s, output: 18.70 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.08it/s, est. speed input: 19026.20 toks/s, output: 18.58 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 17.09it/s, est. speed input: 18926.57 toks/s, output: 18.48 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 17.02it/s, est. speed input: 18824.34 toks/s, output: 18.38 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.98it/s, est. speed input: 18733.86 toks/s, output: 18.29 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.03it/s, est. speed input: 18668.90 toks/s, output: 18.23 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 17.06it/s, est. speed input: 18611.59 toks/s, output: 18.18 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.03it/s, est. speed input: 18550.14 toks/s, output: 18.12 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.02it/s, est. speed input: 18495.84 toks/s, output: 18.06 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 17.00it/s, est. speed input: 18445.07 toks/s, output: 18.01 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 17.00it/s, est. speed input: 18399.88 toks/s, output: 17.97 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 16.94it/s, est. speed input: 18349.64 toks/s, output: 17.92 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:04, 16.93it/s, est. speed input: 18308.30 toks/s, output: 17.88 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.94it/s, est. speed input: 18272.57 toks/s, output: 17.84 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.94it/s, est. speed input: 18238.15 toks/s, output: 17.81 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.96it/s, est. speed input: 18209.38 toks/s, output: 17.78 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 16.96it/s, est. speed input: 18180.84 toks/s, output: 17.75 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.84it/s, est. speed input: 18139.59 toks/s, output: 17.71 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 16.81it/s, est. speed input: 18106.93 toks/s, output: 17.68 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 16.81it/s, est. speed input: 18078.81 toks/s, output: 17.65 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:03, 16.84it/s, est. speed input: 18055.43 toks/s, output: 17.63 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.79it/s, est. speed input: 18026.60 toks/s, output: 17.60 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.86it/s, est. speed input: 18009.46 toks/s, output: 17.59 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.88it/s, est. speed input: 17991.21 toks/s, output: 17.57 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.72it/s, est. speed input: 17956.74 toks/s, output: 17.54 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 16.67it/s, est. speed input: 17929.33 toks/s, output: 17.51 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.64it/s, est. speed input: 17904.56 toks/s, output: 17.48 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 16.55it/s, est. speed input: 17874.81 toks/s, output: 17.46 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 16.62it/s, est. speed input: 17857.70 toks/s, output: 17.44 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:02, 16.57it/s, est. speed input: 17833.17 toks/s, output: 17.42 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.56it/s, est. speed input: 17812.21 toks/s, output: 17.39 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.59it/s, est. speed input: 17795.18 toks/s, output: 17.38 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.63it/s, est. speed input: 17780.10 toks/s, output: 17.36 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.59it/s, est. speed input: 17760.68 toks/s, output: 17.34 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.53it/s, est. speed input: 17739.38 toks/s, output: 17.32 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 16.45it/s, est. speed input: 17716.18 toks/s, output: 17.30 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 16.52it/s, est. speed input: 17702.88 toks/s, output: 17.29 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:05<00:01, 16.50it/s, est. speed input: 17685.76 toks/s, output: 17.27 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.61it/s, est. speed input: 17677.76 toks/s, output: 17.26 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.76it/s, est. speed input: 17674.99 toks/s, output: 17.26 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.83it/s, est. speed input: 17669.63 toks/s, output: 17.26 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.91it/s, est. speed input: 17667.10 toks/s, output: 17.25 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.97it/s, est. speed input: 17664.42 toks/s, output: 17.25 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 16.99it/s, est. speed input: 17660.45 toks/s, output: 17.25 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 16.99it/s, est. speed input: 17656.01 toks/s, output: 17.24 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 17.05it/s, est. speed input: 17655.00 toks/s, output: 17.24 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 17.03it/s, est. speed input: 17650.64 toks/s, output: 17.24 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 16.89it/s, est. speed input: 17639.16 toks/s, output: 17.23 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.86it/s, est. speed input: 17631.75 toks/s, output: 17.22 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.79it/s, est. speed input: 17621.71 toks/s, output: 17.21 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.79it/s, est. speed input: 17617.15 toks/s, output: 17.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.20it/s, est. speed input: 17617.15 toks/s, output: 17.20 toks/s]
[rank0]:[W128 12:20:53.173345890 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-28 12:20:55
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:21:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:21:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=50008) WARNING 01-28 12:21:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=50008) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=50008) WARNING 01-28 12:21:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.95 requests/s, 32745.15 total tokens/s, 31.95 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-28 12:21:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:21:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:21:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:21:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:21:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:21:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:21:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:21:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:21:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:21:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:21:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:21:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:21:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:21:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:21:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:21:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=50008) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=50008) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.46s/it]
(EngineCore_DP0 pid=50008) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.46s/it]
(EngineCore_DP0 pid=50008) 
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=50008) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.86it/s]
(EngineCore_DP0 pid=50008) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.95it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 34/256 [00:00<00:00, 333.82it/s]
Adding requests:  28%|██▊       | 71/256 [00:00<00:00, 351.46it/s]
Adding requests:  42%|████▏     | 107/256 [00:00<00:00, 351.11it/s]
Adding requests:  56%|█████▌    | 143/256 [00:00<00:00, 349.70it/s]
Adding requests:  70%|██████▉   | 178/256 [00:00<00:00, 341.03it/s]
Adding requests:  83%|████████▎ | 213/256 [00:00<00:00, 340.97it/s]
Adding requests:  98%|█████████▊| 252/256 [00:00<00:00, 355.00it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 349.50it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 157.70it/s, est. speed input: 161510.78 toks/s, output: 157.71 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:00<00:04, 52.36it/s, est. speed input: 60336.02 toks/s, output: 58.92 toks/s]   
Processed prompts:  18%|█▊        | 45/256 [00:00<00:04, 46.92it/s, est. speed input: 54252.17 toks/s, output: 52.98 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:05, 40.75it/s, est. speed input: 48900.06 toks/s, output: 47.75 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:01<00:04, 40.99it/s, est. speed input: 48317.55 toks/s, output: 47.18 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:05, 36.99it/s, est. speed input: 45721.38 toks/s, output: 44.65 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:01<00:04, 38.09it/s, est. speed input: 45503.95 toks/s, output: 44.44 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:05, 34.89it/s, est. speed input: 43737.88 toks/s, output: 42.71 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:05, 34.66it/s, est. speed input: 43148.37 toks/s, output: 42.14 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:05, 34.46it/s, est. speed input: 42626.93 toks/s, output: 41.63 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:02<00:05, 34.34it/s, est. speed input: 42176.07 toks/s, output: 41.19 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:02<00:04, 34.13it/s, est. speed input: 41746.07 toks/s, output: 40.77 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:02<00:04, 33.98it/s, est. speed input: 41361.47 toks/s, output: 40.39 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:04, 33.92it/s, est. speed input: 41026.46 toks/s, output: 40.06 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:04, 33.87it/s, est. speed input: 40722.54 toks/s, output: 39.77 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:04, 33.82it/s, est. speed input: 40442.67 toks/s, output: 39.49 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:04, 33.83it/s, est. speed input: 40194.58 toks/s, output: 39.25 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:04, 33.81it/s, est. speed input: 39961.88 toks/s, output: 39.02 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:04, 33.66it/s, est. speed input: 39727.10 toks/s, output: 38.80 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:03<00:04, 33.39it/s, est. speed input: 39484.47 toks/s, output: 38.56 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:03<00:03, 33.03it/s, est. speed input: 39235.30 toks/s, output: 38.32 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:03<00:03, 32.53it/s, est. speed input: 38968.59 toks/s, output: 38.05 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:03<00:03, 32.49it/s, est. speed input: 38762.98 toks/s, output: 37.85 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 32.59it/s, est. speed input: 38589.41 toks/s, output: 37.68 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:03<00:03, 32.53it/s, est. speed input: 38410.16 toks/s, output: 37.51 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:03<00:03, 32.45it/s, est. speed input: 38236.96 toks/s, output: 37.34 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:03, 32.56it/s, est. speed input: 38095.89 toks/s, output: 37.20 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:04<00:03, 32.66it/s, est. speed input: 37964.95 toks/s, output: 37.07 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:04<00:03, 32.69it/s, est. speed input: 37837.01 toks/s, output: 36.95 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:04<00:02, 32.76it/s, est. speed input: 37721.44 toks/s, output: 36.84 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:04<00:02, 32.77it/s, est. speed input: 37609.00 toks/s, output: 36.73 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:04<00:02, 32.80it/s, est. speed input: 37503.94 toks/s, output: 36.62 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:02, 32.91it/s, est. speed input: 37412.78 toks/s, output: 36.54 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:04<00:02, 32.87it/s, est. speed input: 37316.08 toks/s, output: 36.44 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:04<00:02, 32.96it/s, est. speed input: 37234.11 toks/s, output: 36.36 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:05<00:02, 32.80it/s, est. speed input: 37136.34 toks/s, output: 36.27 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:05<00:02, 32.78it/s, est. speed input: 37050.89 toks/s, output: 36.18 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:05<00:01, 32.81it/s, est. speed input: 36973.67 toks/s, output: 36.11 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:05<00:01, 32.63it/s, est. speed input: 36883.09 toks/s, output: 36.02 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:05<00:01, 32.58it/s, est. speed input: 36802.62 toks/s, output: 35.94 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:05<00:01, 32.61it/s, est. speed input: 36731.15 toks/s, output: 35.87 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 32.55it/s, est. speed input: 36655.04 toks/s, output: 35.80 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 32.57it/s, est. speed input: 36588.18 toks/s, output: 35.73 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:06<00:01, 32.58it/s, est. speed input: 36522.91 toks/s, output: 35.67 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:06<00:01, 32.58it/s, est. speed input: 36460.39 toks/s, output: 35.61 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:06<00:00, 32.81it/s, est. speed input: 36416.65 toks/s, output: 35.56 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:06<00:00, 32.71it/s, est. speed input: 36355.68 toks/s, output: 35.50 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:08<00:05,  4.66it/s, est. speed input: 26408.11 toks/s, output: 25.79 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:09<00:03,  6.28it/s, est. speed input: 26502.16 toks/s, output: 25.88 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:09<00:01,  8.29it/s, est. speed input: 26595.00 toks/s, output: 25.97 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:09<00:01, 10.68it/s, est. speed input: 26687.30 toks/s, output: 26.06 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:09<00:00, 13.39it/s, est. speed input: 26774.86 toks/s, output: 26.15 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:09<00:00, 16.23it/s, est. speed input: 26854.83 toks/s, output: 26.23 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 19.13it/s, est. speed input: 26939.02 toks/s, output: 26.31 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 19.13it/s, est. speed input: 26939.02 toks/s, output: 26.31 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 26.31it/s, est. speed input: 26939.02 toks/s, output: 26.31 toks/s]
[rank0]:[W128 12:21:51.444071275 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-28 12:21:53
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:22:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:22:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=50659) WARNING 01-28 12:22:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=50659) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=50659) WARNING 01-28 12:22:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.00 requests/s, 52274.16 total tokens/s, 51.00 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-28 12:22:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:22:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:22:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:22:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:22:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:22:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:22:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:22:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:22:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:22:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:22:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:22:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:22:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:22:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:22:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:22:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=50659) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=50659) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=50659) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=50659) 
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=50659) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.18it/s]
(EngineCore_DP0 pid=50659) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.54it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 34/512 [00:00<00:01, 339.11it/s]
Adding requests:  14%|█▍        | 71/512 [00:00<00:01, 355.75it/s]
Adding requests:  21%|██▏       | 110/512 [00:00<00:01, 368.70it/s]
Adding requests:  29%|██▊       | 147/512 [00:00<00:00, 366.67it/s]
Adding requests:  36%|███▌      | 185/512 [00:00<00:00, 368.07it/s]
Adding requests:  44%|████▍     | 224/512 [00:00<00:00, 373.84it/s]
Adding requests:  51%|█████▏    | 263/512 [00:00<00:00, 378.00it/s]
Adding requests:  59%|█████▉    | 301/512 [00:00<00:00, 374.66it/s]
Adding requests:  66%|██████▋   | 340/512 [00:00<00:00, 377.72it/s]
Adding requests:  74%|███████▍  | 379/512 [00:01<00:00, 381.03it/s]
Adding requests:  82%|████████▏ | 418/512 [00:01<00:00, 382.83it/s]
Adding requests:  89%|████████▉ | 457/512 [00:01<00:00, 380.45it/s]
Adding requests:  97%|█████████▋| 496/512 [00:01<00:00, 380.24it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 375.11it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:00<00:00, 506.38it/s, est. speed input: 518611.80 toks/s, output: 506.40 toks/s]
Processed prompts:  22%|██▏       | 113/512 [00:01<00:04, 94.81it/s, est. speed input: 112077.05 toks/s, output: 109.45 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:01<00:05, 74.22it/s, est. speed input: 90432.81 toks/s, output: 88.31 toks/s]  
Processed prompts:  30%|███       | 154/512 [00:01<00:05, 68.57it/s, est. speed input: 84476.71 toks/s, output: 82.50 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:02<00:05, 65.10it/s, est. speed input: 81161.10 toks/s, output: 79.26 toks/s]
Processed prompts:  34%|███▍      | 176/512 [00:02<00:05, 65.23it/s, est. speed input: 80229.70 toks/s, output: 78.35 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:02<00:05, 64.07it/s, est. speed input: 79000.94 toks/s, output: 77.15 toks/s]
Processed prompts:  38%|███▊      | 193/512 [00:02<00:05, 61.46it/s, est. speed input: 77473.53 toks/s, output: 75.66 toks/s]
Processed prompts:  39%|███▉      | 200/512 [00:02<00:05, 57.70it/s, est. speed input: 75744.78 toks/s, output: 73.97 toks/s]
Processed prompts:  40%|████      | 207/512 [00:02<00:05, 54.67it/s, est. speed input: 74203.67 toks/s, output: 72.46 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:03<00:05, 52.27it/s, est. speed input: 72810.81 toks/s, output: 71.10 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:03<00:05, 52.25it/s, est. speed input: 71872.93 toks/s, output: 70.19 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:03<00:05, 52.23it/s, est. speed input: 71020.86 toks/s, output: 69.36 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:03<00:05, 52.22it/s, est. speed input: 70244.48 toks/s, output: 68.60 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:03<00:05, 52.22it/s, est. speed input: 69535.76 toks/s, output: 67.91 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:03<00:04, 52.24it/s, est. speed input: 68887.70 toks/s, output: 67.27 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:03<00:04, 52.28it/s, est. speed input: 68294.28 toks/s, output: 66.69 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:04<00:04, 52.27it/s, est. speed input: 67739.22 toks/s, output: 66.15 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:04<00:04, 52.27it/s, est. speed input: 67225.92 toks/s, output: 65.65 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:04<00:04, 52.17it/s, est. speed input: 66732.15 toks/s, output: 65.17 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:04<00:04, 52.11it/s, est. speed input: 66274.84 toks/s, output: 64.72 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:04<00:04, 52.16it/s, est. speed input: 65859.02 toks/s, output: 64.32 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:04<00:03, 52.17it/s, est. speed input: 65466.60 toks/s, output: 63.93 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:05<00:03, 52.12it/s, est. speed input: 65091.59 toks/s, output: 63.57 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:05<00:03, 52.11it/s, est. speed input: 64741.16 toks/s, output: 63.22 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:05<00:03, 52.14it/s, est. speed input: 64416.24 toks/s, output: 62.91 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:05<00:03, 52.15it/s, est. speed input: 64106.83 toks/s, output: 62.60 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:05<00:03, 52.18it/s, est. speed input: 63817.78 toks/s, output: 62.32 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:05<00:02, 52.19it/s, est. speed input: 63543.43 toks/s, output: 62.05 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:05<00:02, 52.21it/s, est. speed input: 63283.65 toks/s, output: 61.80 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:06<00:02, 52.19it/s, est. speed input: 63033.75 toks/s, output: 61.56 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:06<00:02, 52.19it/s, est. speed input: 62797.80 toks/s, output: 61.33 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:06<00:02, 52.16it/s, est. speed input: 62570.71 toks/s, output: 61.10 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:06<00:02, 52.07it/s, est. speed input: 62347.62 toks/s, output: 60.89 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:06<00:02, 52.11it/s, est. speed input: 62143.85 toks/s, output: 60.69 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:06<00:01, 52.14it/s, est. speed input: 61948.92 toks/s, output: 60.50 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:06<00:01, 52.12it/s, est. speed input: 61759.95 toks/s, output: 60.31 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:07<00:01, 52.03it/s, est. speed input: 61571.83 toks/s, output: 60.13 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:07<00:01, 51.91it/s, est. speed input: 61387.52 toks/s, output: 59.95 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:07<00:01, 51.96it/s, est. speed input: 61221.91 toks/s, output: 59.79 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:07<00:01, 51.97it/s, est. speed input: 61060.02 toks/s, output: 59.63 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:07<00:00, 52.03it/s, est. speed input: 60909.32 toks/s, output: 59.48 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:07<00:00, 52.09it/s, est. speed input: 60765.65 toks/s, output: 59.34 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:08<00:00, 52.08it/s, est. speed input: 60623.51 toks/s, output: 59.20 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:08<00:00, 52.11it/s, est. speed input: 60490.06 toks/s, output: 59.07 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:08<00:00, 52.13it/s, est. speed input: 60360.70 toks/s, output: 58.95 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:08<00:00, 52.13it/s, est. speed input: 60234.93 toks/s, output: 58.82 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:08<00:00, 53.69it/s, est. speed input: 60217.24 toks/s, output: 58.81 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 53.69it/s, est. speed input: 60452.06 toks/s, output: 59.04 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 59.03it/s, est. speed input: 60452.06 toks/s, output: 59.04 toks/s]
[rank0]:[W128 12:22:49.208734631 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-28 12:22:51
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:23:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:23:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=51275) WARNING 01-28 12:23:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=51275) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=51275) WARNING 01-28 12:23:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.52 requests/s, 52807.92 total tokens/s, 51.52 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-28 12:23:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:23:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:23:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:23:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:23:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:23:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:23:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:23:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:23:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:23:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:23:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:23:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:23:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:23:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:23:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:23:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=51275) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=51275) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=51275) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=51275) 
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=51275) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  5.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  6.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  6.78it/s]
(EngineCore_DP0 pid=51275) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.32it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.25it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.56it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 35/1024 [00:00<00:02, 349.42it/s]
Adding requests:   7%|▋         | 73/1024 [00:00<00:02, 365.60it/s]
Adding requests:  11%|█         | 112/1024 [00:00<00:02, 375.41it/s]
Adding requests:  15%|█▍        | 150/1024 [00:00<00:02, 376.15it/s]
Adding requests:  18%|█▊        | 188/1024 [00:00<00:02, 374.09it/s]
Adding requests:  22%|██▏       | 227/1024 [00:00<00:02, 379.24it/s]
Adding requests:  26%|██▌       | 266/1024 [00:00<00:01, 382.02it/s]
Adding requests:  30%|██▉       | 305/1024 [00:00<00:01, 375.08it/s]
Adding requests:  33%|███▎      | 343/1024 [00:00<00:01, 372.41it/s]
Adding requests:  37%|███▋      | 383/1024 [00:01<00:01, 376.92it/s]
Adding requests:  41%|████▏     | 423/1024 [00:01<00:01, 381.69it/s]
Adding requests:  45%|████▌     | 463/1024 [00:01<00:01, 385.23it/s]
Adding requests:  49%|████▉     | 503/1024 [00:01<00:01, 388.91it/s]
Adding requests:  53%|█████▎    | 542/1024 [00:01<00:01, 384.08it/s]
Adding requests:  57%|█████▋    | 584/1024 [00:01<00:01, 392.80it/s]
Adding requests:  61%|██████    | 624/1024 [00:01<00:01, 389.33it/s]
Adding requests:  65%|██████▍   | 665/1024 [00:01<00:00, 394.49it/s]
Adding requests:  69%|██████▉   | 708/1024 [00:01<00:00, 403.73it/s]
Adding requests:  73%|███████▎  | 749/1024 [00:01<00:00, 401.87it/s]
Adding requests:  77%|███████▋  | 790/1024 [00:02<00:00, 402.47it/s]
Adding requests:  81%|████████  | 831/1024 [00:02<00:00, 397.47it/s]
Adding requests:  85%|████████▌ | 873/1024 [00:02<00:00, 402.02it/s]
Adding requests:  89%|████████▉ | 915/1024 [00:02<00:00, 406.11it/s]
Adding requests:  93%|█████████▎| 956/1024 [00:02<00:00, 404.28it/s]
Adding requests:  97%|█████████▋| 997/1024 [00:02<00:00, 404.60it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 390.78it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:00<00:00, 1189.54it/s, est. speed input: 1218424.77 toks/s, output: 1189.64 toks/s]
Processed prompts:  24%|██▎       | 241/1024 [00:02<00:08, 93.10it/s, est. speed input: 110849.45 toks/s, output: 108.25 toks/s]    
Processed prompts:  29%|██▊       | 294/1024 [00:03<00:09, 75.36it/s, est. speed input: 91472.13 toks/s, output: 89.33 toks/s]  
Processed prompts:  32%|███▏      | 325/1024 [00:03<00:10, 69.27it/s, est. speed input: 85352.79 toks/s, output: 83.35 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:04<00:10, 64.34it/s, est. speed input: 81345.91 toks/s, output: 79.44 toks/s]
Processed prompts:  35%|███▌      | 361/1024 [00:04<00:09, 67.47it/s, est. speed input: 82004.76 toks/s, output: 80.08 toks/s]
Processed prompts:  37%|███▋      | 375/1024 [00:04<00:10, 63.04it/s, est. speed input: 79806.10 toks/s, output: 77.94 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:05<00:11, 56.89it/s, est. speed input: 77260.96 toks/s, output: 75.45 toks/s]
Processed prompts:  39%|███▊      | 395/1024 [00:05<00:10, 57.20it/s, est. speed input: 76775.77 toks/s, output: 74.98 toks/s]
Processed prompts:  39%|███▉      | 403/1024 [00:05<00:11, 56.41it/s, est. speed input: 76128.40 toks/s, output: 74.34 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:05<00:11, 54.41it/s, est. speed input: 75336.28 toks/s, output: 73.57 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:05<00:11, 54.04it/s, est. speed input: 74769.39 toks/s, output: 73.02 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:05<00:11, 53.70it/s, est. speed input: 74228.90 toks/s, output: 72.49 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:06<00:11, 53.44it/s, est. speed input: 73716.99 toks/s, output: 71.99 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:06<00:10, 53.23it/s, est. speed input: 73230.01 toks/s, output: 71.51 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:06<00:10, 53.05it/s, est. speed input: 72764.46 toks/s, output: 71.06 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:06<00:10, 52.91it/s, est. speed input: 72319.87 toks/s, output: 70.62 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:06<00:10, 52.76it/s, est. speed input: 71890.06 toks/s, output: 70.20 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:06<00:10, 52.67it/s, est. speed input: 71481.14 toks/s, output: 69.81 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:06<00:10, 52.61it/s, est. speed input: 71091.63 toks/s, output: 69.43 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:07<00:10, 52.58it/s, est. speed input: 70719.60 toks/s, output: 69.06 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:07<00:10, 52.55it/s, est. speed input: 70362.58 toks/s, output: 68.71 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:07<00:09, 52.50it/s, est. speed input: 70017.14 toks/s, output: 68.38 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:07<00:09, 52.48it/s, est. speed input: 69687.69 toks/s, output: 68.05 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:07<00:09, 52.46it/s, est. speed input: 69370.57 toks/s, output: 67.74 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:09, 52.46it/s, est. speed input: 69066.67 toks/s, output: 67.45 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:08<00:09, 52.46it/s, est. speed input: 68774.11 toks/s, output: 67.16 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:08<00:09, 52.43it/s, est. speed input: 68490.23 toks/s, output: 66.88 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:08<00:08, 52.42it/s, est. speed input: 68218.27 toks/s, output: 66.62 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:08<00:08, 52.43it/s, est. speed input: 67957.27 toks/s, output: 66.36 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:08<00:08, 52.39it/s, est. speed input: 67701.33 toks/s, output: 66.11 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:08<00:08, 52.40it/s, est. speed input: 67457.70 toks/s, output: 65.88 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:08<00:08, 52.43it/s, est. speed input: 67224.05 toks/s, output: 65.65 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:09<00:08, 52.45it/s, est. speed input: 66997.63 toks/s, output: 65.43 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:09<00:08, 52.45it/s, est. speed input: 66778.26 toks/s, output: 65.21 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:09<00:07, 52.41it/s, est. speed input: 66562.71 toks/s, output: 65.00 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:09<00:07, 52.38it/s, est. speed input: 66354.23 toks/s, output: 64.80 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:09<00:07, 52.42it/s, est. speed input: 66156.72 toks/s, output: 64.61 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:09<00:07, 52.43it/s, est. speed input: 65963.54 toks/s, output: 64.42 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:09<00:07, 52.44it/s, est. speed input: 65776.89 toks/s, output: 64.24 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:10<00:07, 52.48it/s, est. speed input: 65597.53 toks/s, output: 64.06 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:10<00:06, 52.49it/s, est. speed input: 65422.52 toks/s, output: 63.89 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:10<00:06, 52.49it/s, est. speed input: 65252.74 toks/s, output: 63.72 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:10<00:06, 52.47it/s, est. speed input: 65086.23 toks/s, output: 63.56 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:10<00:06, 52.44it/s, est. speed input: 64923.41 toks/s, output: 63.40 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:10<00:06, 52.40it/s, est. speed input: 64764.08 toks/s, output: 63.25 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:11<00:06, 52.41it/s, est. speed input: 64611.17 toks/s, output: 63.10 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:11<00:06, 52.41it/s, est. speed input: 64462.44 toks/s, output: 62.95 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:11<00:05, 52.39it/s, est. speed input: 64316.36 toks/s, output: 62.81 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:11<00:05, 52.38it/s, est. speed input: 64174.22 toks/s, output: 62.67 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:11<00:05, 52.38it/s, est. speed input: 64036.67 toks/s, output: 62.54 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:11<00:05, 52.39it/s, est. speed input: 63902.83 toks/s, output: 62.41 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:11<00:05, 52.38it/s, est. speed input: 63771.59 toks/s, output: 62.28 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:12<00:05, 52.39it/s, est. speed input: 63644.26 toks/s, output: 62.15 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:12<00:05, 52.35it/s, est. speed input: 63518.33 toks/s, output: 62.03 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:12<00:04, 52.36it/s, est. speed input: 63396.68 toks/s, output: 61.91 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:12<00:04, 52.35it/s, est. speed input: 63277.50 toks/s, output: 61.79 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:12<00:04, 52.34it/s, est. speed input: 63161.23 toks/s, output: 61.68 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:12<00:04, 52.38it/s, est. speed input: 63049.62 toks/s, output: 61.57 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:13<00:04, 52.38it/s, est. speed input: 62939.71 toks/s, output: 61.46 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:04, 52.39it/s, est. speed input: 62832.28 toks/s, output: 61.36 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:13<00:03, 52.36it/s, est. speed input: 62726.09 toks/s, output: 61.26 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:13<00:03, 52.35it/s, est. speed input: 62622.56 toks/s, output: 61.15 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:13<00:03, 52.04it/s, est. speed input: 62507.74 toks/s, output: 61.04 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:13<00:03, 51.64it/s, est. speed input: 62387.25 toks/s, output: 60.92 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:13<00:03, 51.37it/s, est. speed input: 62269.83 toks/s, output: 60.81 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:14<00:03, 51.19it/s, est. speed input: 62155.03 toks/s, output: 60.70 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:14<00:03, 51.04it/s, est. speed input: 62041.96 toks/s, output: 60.59 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:14<00:02, 50.94it/s, est. speed input: 61931.71 toks/s, output: 60.48 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:14<00:02, 50.91it/s, est. speed input: 61825.48 toks/s, output: 60.38 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:14<00:02, 50.88it/s, est. speed input: 61720.85 toks/s, output: 60.27 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:14<00:02, 50.87it/s, est. speed input: 61618.89 toks/s, output: 60.17 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:15<00:02, 50.88it/s, est. speed input: 61520.04 toks/s, output: 60.08 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:15<00:02, 50.88it/s, est. speed input: 61422.96 toks/s, output: 59.98 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:15<00:02, 50.87it/s, est. speed input: 61327.28 toks/s, output: 59.89 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:15<00:01, 50.84it/s, est. speed input: 61232.61 toks/s, output: 59.80 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:15<00:01, 50.85it/s, est. speed input: 61141.03 toks/s, output: 59.71 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:15<00:01, 50.85it/s, est. speed input: 61051.01 toks/s, output: 59.62 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:16<00:01, 50.85it/s, est. speed input: 60963.09 toks/s, output: 59.53 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:16<00:01, 50.83it/s, est. speed input: 60875.94 toks/s, output: 59.45 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:16<00:01, 50.80it/s, est. speed input: 60789.63 toks/s, output: 59.36 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:16<00:00, 50.77it/s, est. speed input: 60704.60 toks/s, output: 59.28 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:16<00:00, 52.17it/s, est. speed input: 60673.63 toks/s, output: 59.25 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:16<00:00, 51.76it/s, est. speed input: 60592.64 toks/s, output: 59.17 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:16<00:00, 51.49it/s, est. speed input: 60513.95 toks/s, output: 59.10 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:17<00:00, 51.32it/s, est. speed input: 60437.02 toks/s, output: 59.02 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:17<00:00, 52.86it/s, est. speed input: 60418.93 toks/s, output: 59.00 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 52.86it/s, est. speed input: 60774.18 toks/s, output: 59.35 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 59.35it/s, est. speed input: 60774.18 toks/s, output: 59.35 toks/s]
[rank0]:[W128 12:24:03.754809807 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-28 12:24:05
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:24:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:24:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=52018) WARNING 01-28 12:24:33 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=52018) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=52018) WARNING 01-28 12:24:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.82 requests/s, 52091.29 total tokens/s, 50.82 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-28 12:24:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:24:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:24:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:24:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:24:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:24:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:24:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:24:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:24:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:24:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:24:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:24:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:24:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:24:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:24:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:24:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=52018) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=52018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=52018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=52018) 
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=52018) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.27it/s]
(EngineCore_DP0 pid=52018) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.15it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.24it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.80it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 37/2048 [00:00<00:05, 365.72it/s]
Adding requests:   4%|▎         | 76/2048 [00:00<00:05, 378.07it/s]
Adding requests:   6%|▌         | 115/2048 [00:00<00:05, 382.57it/s]
Adding requests:   8%|▊         | 154/2048 [00:00<00:04, 382.83it/s]
Adding requests:   9%|▉         | 193/2048 [00:00<00:04, 380.39it/s]
Adding requests:  11%|█▏        | 232/2048 [00:00<00:04, 381.80it/s]
Adding requests:  13%|█▎        | 271/2048 [00:00<00:04, 381.93it/s]
Adding requests:  15%|█▌        | 310/2048 [00:00<00:04, 380.74it/s]
Adding requests:  17%|█▋        | 349/2048 [00:00<00:04, 381.60it/s]
Adding requests:  19%|█▉        | 389/2048 [00:01<00:04, 386.03it/s]
Adding requests:  21%|██        | 429/2048 [00:01<00:04, 390.04it/s]
Adding requests:  23%|██▎       | 469/2048 [00:01<00:04, 388.04it/s]
Adding requests:  25%|██▍       | 508/2048 [00:01<00:03, 386.25it/s]
Adding requests:  27%|██▋       | 547/2048 [00:01<00:03, 380.30it/s]
Adding requests:  29%|██▊       | 586/2048 [00:01<00:03, 373.31it/s]
Adding requests:  31%|███       | 626/2048 [00:01<00:03, 378.22it/s]
Adding requests:  33%|███▎      | 666/2048 [00:01<00:03, 382.04it/s]
Adding requests:  35%|███▍      | 708/2048 [00:01<00:03, 390.90it/s]
Adding requests:  37%|███▋      | 748/2048 [00:01<00:03, 381.48it/s]
Adding requests:  39%|███▊      | 789/2048 [00:02<00:03, 387.07it/s]
Adding requests:  40%|████      | 828/2048 [00:02<00:03, 384.14it/s]
Adding requests:  42%|████▏     | 869/2048 [00:02<00:03, 389.85it/s]
Adding requests:  44%|████▍     | 911/2048 [00:02<00:02, 398.27it/s]
Adding requests:  46%|████▋     | 951/2048 [00:02<00:02, 397.16it/s]
Adding requests:  48%|████▊     | 991/2048 [00:02<00:02, 395.42it/s]
Adding requests:  50%|█████     | 1032/2048 [00:02<00:02, 397.20it/s]
Adding requests:  52%|█████▏    | 1072/2048 [00:02<00:02, 385.36it/s]
Adding requests:  54%|█████▍    | 1112/2048 [00:02<00:02, 388.91it/s]
Adding requests:  56%|█████▌    | 1151/2048 [00:02<00:02, 388.22it/s]
Adding requests:  58%|█████▊    | 1191/2048 [00:03<00:02, 389.67it/s]
Adding requests:  60%|██████    | 1230/2048 [00:03<00:02, 387.86it/s]
Adding requests:  62%|██████▏   | 1269/2048 [00:03<00:02, 382.82it/s]
Adding requests:  64%|██████▍   | 1309/2048 [00:03<00:01, 386.87it/s]
Adding requests:  66%|██████▌   | 1351/2048 [00:03<00:01, 395.15it/s]
Adding requests:  68%|██████▊   | 1393/2048 [00:03<00:01, 399.90it/s]
Adding requests:  70%|███████   | 1434/2048 [00:03<00:01, 396.25it/s]
Adding requests:  72%|███████▏  | 1475/2048 [00:03<00:01, 397.89it/s]
Adding requests:  74%|███████▍  | 1517/2048 [00:03<00:01, 403.90it/s]
Adding requests:  76%|███████▌  | 1558/2048 [00:04<00:01, 403.97it/s]
Adding requests:  78%|███████▊  | 1602/2048 [00:04<00:01, 412.71it/s]
Adding requests:  80%|████████  | 1644/2048 [00:04<00:00, 405.56it/s]
Adding requests:  82%|████████▏ | 1685/2048 [00:04<00:00, 405.43it/s]
Adding requests:  84%|████████▍ | 1727/2048 [00:04<00:00, 408.52it/s]
Adding requests:  86%|████████▋ | 1768/2048 [00:04<00:00, 406.47it/s]
Adding requests:  88%|████████▊ | 1809/2048 [00:04<00:00, 406.80it/s]
Adding requests:  90%|█████████ | 1851/2048 [00:04<00:00, 410.12it/s]
Adding requests:  92%|█████████▏| 1893/2048 [00:04<00:00, 406.17it/s]
Adding requests:  94%|█████████▍| 1934/2048 [00:04<00:00, 394.37it/s]
Adding requests:  96%|█████████▋| 1976/2048 [00:05<00:00, 399.89it/s]
Adding requests:  99%|█████████▊| 2018/2048 [00:05<00:00, 404.79it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 392.87it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:00<00:01, 1178.01it/s, est. speed input: 1206429.16 toks/s, output: 1178.06 toks/s]
Processed prompts:  18%|█▊        | 376/2048 [00:02<00:12, 129.16it/s, est. speed input: 161942.96 toks/s, output: 158.15 toks/s]   
Processed prompts:  21%|██        | 428/2048 [00:03<00:15, 102.34it/s, est. speed input: 132415.46 toks/s, output: 129.31 toks/s]
Processed prompts:  22%|██▏       | 459/2048 [00:03<00:17, 88.56it/s, est. speed input: 119368.17 toks/s, output: 116.57 toks/s] 
Processed prompts:  23%|██▎       | 480/2048 [00:04<00:18, 85.31it/s, est. speed input: 115704.28 toks/s, output: 112.99 toks/s]
Processed prompts:  24%|██▍       | 496/2048 [00:04<00:19, 79.17it/s, est. speed input: 111413.71 toks/s, output: 108.80 toks/s]
Processed prompts:  25%|██▍       | 509/2048 [00:04<00:21, 71.51it/s, est. speed input: 107053.10 toks/s, output: 104.54 toks/s]
Processed prompts:  25%|██▌       | 519/2048 [00:05<00:24, 62.54it/s, est. speed input: 102597.73 toks/s, output: 100.19 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:05<00:27, 55.89it/s, est. speed input: 98846.81 toks/s, output: 96.53 toks/s]  
Processed prompts:  27%|██▋       | 546/2048 [00:05<00:27, 54.76it/s, est. speed input: 96379.71 toks/s, output: 94.12 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:06<00:27, 53.88it/s, est. speed input: 94162.69 toks/s, output: 91.96 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:06<00:27, 53.22it/s, est. speed input: 92159.21 toks/s, output: 90.00 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:06<00:27, 52.74it/s, est. speed input: 90341.81 toks/s, output: 88.22 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:07<00:27, 52.38it/s, est. speed input: 88683.62 toks/s, output: 86.60 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:07<00:27, 52.12it/s, est. speed input: 87163.18 toks/s, output: 85.12 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:07<00:27, 51.95it/s, est. speed input: 85771.52 toks/s, output: 83.76 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:07<00:26, 51.81it/s, est. speed input: 84483.66 toks/s, output: 82.50 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:08<00:26, 51.71it/s, est. speed input: 83291.03 toks/s, output: 81.34 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:08<00:26, 51.65it/s, est. speed input: 82186.85 toks/s, output: 80.26 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:08<00:26, 51.60it/s, est. speed input: 81158.99 toks/s, output: 79.26 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:09<00:25, 51.56it/s, est. speed input: 80199.03 toks/s, output: 78.32 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:09<00:25, 51.48it/s, est. speed input: 79293.62 toks/s, output: 77.44 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:09<00:25, 50.61it/s, est. speed input: 78311.83 toks/s, output: 76.48 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:12<01:29, 14.25it/s, est. speed input: 61277.21 toks/s, output: 59.84 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:13<01:09, 18.09it/s, est. speed input: 60992.45 toks/s, output: 59.56 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:13<00:55, 22.28it/s, est. speed input: 60719.02 toks/s, output: 59.30 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:13<00:46, 26.61it/s, est. speed input: 60461.89 toks/s, output: 59.04 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:14<00:39, 30.79it/s, est. speed input: 60214.86 toks/s, output: 58.80 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:14<00:34, 34.60it/s, est. speed input: 59979.60 toks/s, output: 58.57 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:14<00:31, 37.88it/s, est. speed input: 59754.15 toks/s, output: 58.35 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:15<00:28, 40.57it/s, est. speed input: 59537.65 toks/s, output: 58.14 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:15<00:26, 42.69it/s, est. speed input: 59331.31 toks/s, output: 57.94 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:15<00:25, 44.31it/s, est. speed input: 59132.51 toks/s, output: 57.75 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:16<00:24, 45.52it/s, est. speed input: 58941.81 toks/s, output: 57.56 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:16<00:23, 46.41it/s, est. speed input: 58759.10 toks/s, output: 57.38 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:16<00:23, 47.05it/s, est. speed input: 58583.45 toks/s, output: 57.21 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:17<00:22, 48.13it/s, est. speed input: 58463.76 toks/s, output: 57.09 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:17<00:21, 48.27it/s, est. speed input: 58299.97 toks/s, output: 56.93 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:17<00:21, 48.37it/s, est. speed input: 58141.91 toks/s, output: 56.78 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:18<00:21, 48.44it/s, est. speed input: 57989.97 toks/s, output: 56.63 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:18<00:20, 48.50it/s, est. speed input: 57843.74 toks/s, output: 56.49 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:18<00:20, 48.61it/s, est. speed input: 57707.70 toks/s, output: 56.36 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:19<00:19, 49.08it/s, est. speed input: 57602.92 toks/s, output: 56.25 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:19<00:19, 49.42it/s, est. speed input: 57501.71 toks/s, output: 56.15 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:19<00:18, 49.65it/s, est. speed input: 57402.91 toks/s, output: 56.06 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:20<00:18, 49.83it/s, est. speed input: 57308.47 toks/s, output: 55.97 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:20<00:18, 50.01it/s, est. speed input: 57220.21 toks/s, output: 55.88 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:20<00:17, 50.91it/s, est. speed input: 57179.29 toks/s, output: 55.84 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:20<00:17, 50.84it/s, est. speed input: 57099.85 toks/s, output: 55.76 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:21<00:16, 50.80it/s, est. speed input: 57023.31 toks/s, output: 55.69 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:21<00:16, 50.77it/s, est. speed input: 56948.75 toks/s, output: 55.61 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:21<00:16, 50.75it/s, est. speed input: 56876.58 toks/s, output: 55.54 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:22<00:16, 50.73it/s, est. speed input: 56805.85 toks/s, output: 55.47 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:22<00:15, 50.72it/s, est. speed input: 56737.22 toks/s, output: 55.41 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:22<00:15, 50.69it/s, est. speed input: 56669.70 toks/s, output: 55.34 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:23<00:15, 50.69it/s, est. speed input: 56604.57 toks/s, output: 55.28 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:23<00:14, 50.68it/s, est. speed input: 56541.34 toks/s, output: 55.22 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:23<00:14, 50.68it/s, est. speed input: 56479.88 toks/s, output: 55.16 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:24<00:14, 50.69it/s, est. speed input: 56420.23 toks/s, output: 55.10 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:24<00:13, 50.69it/s, est. speed input: 56361.96 toks/s, output: 55.04 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:24<00:13, 50.69it/s, est. speed input: 56305.16 toks/s, output: 54.99 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:25<00:13, 50.68it/s, est. speed input: 56249.48 toks/s, output: 54.93 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:25<00:12, 50.67it/s, est. speed input: 56194.98 toks/s, output: 54.88 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:25<00:12, 50.67it/s, est. speed input: 56142.10 toks/s, output: 54.83 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:26<00:12, 50.67it/s, est. speed input: 56090.52 toks/s, output: 54.78 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:26<00:11, 50.67it/s, est. speed input: 56039.84 toks/s, output: 54.73 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:26<00:11, 50.67it/s, est. speed input: 55990.89 toks/s, output: 54.68 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:26<00:11, 50.69it/s, est. speed input: 55943.67 toks/s, output: 54.63 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:27<00:11, 50.68it/s, est. speed input: 55896.66 toks/s, output: 54.59 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:27<00:10, 50.69it/s, est. speed input: 55851.05 toks/s, output: 54.54 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:27<00:10, 50.68it/s, est. speed input: 55806.17 toks/s, output: 54.50 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:28<00:10, 50.73it/s, est. speed input: 55764.64 toks/s, output: 54.46 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:28<00:09, 51.12it/s, est. speed input: 55737.97 toks/s, output: 54.43 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:28<00:09, 51.42it/s, est. speed input: 55712.89 toks/s, output: 54.41 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:29<00:08, 51.58it/s, est. speed input: 55686.42 toks/s, output: 54.38 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:29<00:08, 51.73it/s, est. speed input: 55661.83 toks/s, output: 54.36 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:29<00:08, 51.82it/s, est. speed input: 55637.61 toks/s, output: 54.33 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:30<00:07, 51.89it/s, est. speed input: 55613.54 toks/s, output: 54.31 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:30<00:07, 51.94it/s, est. speed input: 55590.08 toks/s, output: 54.29 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:30<00:07, 51.96it/s, est. speed input: 55566.78 toks/s, output: 54.26 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:31<00:07, 51.98it/s, est. speed input: 55543.93 toks/s, output: 54.24 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:31<00:06, 51.98it/s, est. speed input: 55521.27 toks/s, output: 54.22 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:31<00:06, 51.98it/s, est. speed input: 55499.03 toks/s, output: 54.20 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:31<00:06, 52.00it/s, est. speed input: 55477.69 toks/s, output: 54.18 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:32<00:05, 52.01it/s, est. speed input: 55456.74 toks/s, output: 54.16 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:32<00:05, 52.00it/s, est. speed input: 55435.79 toks/s, output: 54.14 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:32<00:05, 52.02it/s, est. speed input: 55415.75 toks/s, output: 54.12 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:33<00:04, 52.02it/s, est. speed input: 55395.86 toks/s, output: 54.10 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:33<00:04, 52.05it/s, est. speed input: 55377.41 toks/s, output: 54.08 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:33<00:04, 52.01it/s, est. speed input: 55357.14 toks/s, output: 54.06 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:34<00:03, 52.01it/s, est. speed input: 55338.17 toks/s, output: 54.04 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:34<00:03, 52.02it/s, est. speed input: 55319.81 toks/s, output: 54.02 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:34<00:03, 52.75it/s, est. speed input: 55324.37 toks/s, output: 54.03 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:34<00:03, 52.53it/s, est. speed input: 55306.22 toks/s, output: 54.01 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:35<00:02, 52.38it/s, est. speed input: 55288.62 toks/s, output: 53.99 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:35<00:02, 52.28it/s, est. speed input: 55271.37 toks/s, output: 53.98 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:35<00:02, 52.20it/s, est. speed input: 55254.33 toks/s, output: 53.96 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:36<00:01, 52.14it/s, est. speed input: 55237.12 toks/s, output: 53.94 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:36<00:01, 51.92it/s, est. speed input: 55215.01 toks/s, output: 53.92 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:36<00:01, 51.77it/s, est. speed input: 55193.62 toks/s, output: 53.90 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:37<00:00, 51.67it/s, est. speed input: 55172.38 toks/s, output: 53.88 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:37<00:00, 51.60it/s, est. speed input: 55151.67 toks/s, output: 53.86 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:37<00:00, 52.40it/s, est. speed input: 55155.69 toks/s, output: 53.86 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:37<00:00, 52.40it/s, est. speed input: 55534.82 toks/s, output: 54.23 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:37<00:00, 54.23it/s, est. speed input: 55534.82 toks/s, output: 54.23 toks/s]
[rank0]:[W128 12:25:46.695582609 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-28 12:25:49
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:26:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:26:20 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=53210) WARNING 01-28 12:26:27 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=53210) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=53210) WARNING 01-28 12:26:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.73 requests/s, 51995.61 total tokens/s, 50.73 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-28 12:26:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:26:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:26:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:26:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:26:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:26:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:26:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:26:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:26:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:26:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:26:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:26:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:26:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:26:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:26:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:26:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=53210) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=53210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]
(EngineCore_DP0 pid=53210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]
(EngineCore_DP0 pid=53210) 
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=53210) [rank0]:W0128 12:26:48.286000 53210 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=53210) [rank0]:W0128 12:26:48.385000 53210 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=53210) [rank0]:W0128 12:26:49.723000 53210 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=53210) [rank0]:W0128 12:26:49.877000 53210 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=53210) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.35it/s]
(EngineCore_DP0 pid=53210) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.19it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  8.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.68it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 36/4096 [00:00<00:11, 355.40it/s]
Adding requests:   2%|▏         | 75/4096 [00:00<00:10, 370.76it/s]
Adding requests:   3%|▎         | 114/4096 [00:00<00:10, 374.99it/s]
Adding requests:   4%|▎         | 152/4096 [00:00<00:10, 375.52it/s]
Adding requests:   5%|▍         | 190/4096 [00:00<00:10, 375.96it/s]
Adding requests:   6%|▌         | 230/4096 [00:00<00:10, 382.59it/s]
Adding requests:   7%|▋         | 269/4096 [00:00<00:10, 378.34it/s]
Adding requests:   7%|▋         | 307/4096 [00:00<00:10, 375.47it/s]
Adding requests:   8%|▊         | 345/4096 [00:00<00:09, 376.29it/s]
Adding requests:   9%|▉         | 383/4096 [00:01<00:09, 375.88it/s]
Adding requests:  10%|█         | 422/4096 [00:01<00:09, 377.94it/s]
Adding requests:  11%|█▏        | 461/4096 [00:01<00:09, 379.76it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:09, 381.53it/s]
Adding requests:  13%|█▎        | 539/4096 [00:01<00:09, 376.92it/s]
Adding requests:  14%|█▍        | 579/4096 [00:01<00:09, 383.65it/s]
Adding requests:  15%|█▌        | 618/4096 [00:01<00:09, 384.30it/s]
Adding requests:  16%|█▌        | 658/4096 [00:01<00:08, 387.92it/s]
Adding requests:  17%|█▋        | 700/4096 [00:01<00:08, 396.37it/s]
Adding requests:  18%|█▊        | 740/4096 [00:01<00:08, 395.53it/s]
Adding requests:  19%|█▉        | 780/4096 [00:02<00:08, 389.89it/s]
Adding requests:  20%|██        | 820/4096 [00:02<00:08, 385.09it/s]
Adding requests:  21%|██        | 859/4096 [00:02<00:08, 385.58it/s]
Adding requests:  22%|██▏       | 900/4096 [00:02<00:08, 391.87it/s]
Adding requests:  23%|██▎       | 940/4096 [00:02<00:08, 393.91it/s]
Adding requests:  24%|██▍       | 980/4096 [00:02<00:07, 394.13it/s]
Adding requests:  25%|██▍       | 1021/4096 [00:02<00:07, 398.30it/s]
Adding requests:  26%|██▌       | 1062/4096 [00:02<00:07, 401.00it/s]
Adding requests:  27%|██▋       | 1103/4096 [00:02<00:07, 397.68it/s]
Adding requests:  28%|██▊       | 1143/4096 [00:02<00:07, 393.63it/s]
Adding requests:  29%|██▉       | 1186/4096 [00:03<00:07, 390.23it/s]
Adding requests:  30%|██▉       | 1227/4096 [00:03<00:07, 393.35it/s]
Adding requests:  31%|███       | 1267/4096 [00:03<00:07, 393.12it/s]
Adding requests:  32%|███▏      | 1307/4096 [00:03<00:07, 394.71it/s]
Adding requests:  33%|███▎      | 1347/4096 [00:03<00:07, 392.32it/s]
Adding requests:  34%|███▍      | 1387/4096 [00:03<00:06, 391.65it/s]
Adding requests:  35%|███▍      | 1427/4096 [00:03<00:06, 391.23it/s]
Adding requests:  36%|███▌      | 1467/4096 [00:03<00:06, 392.52it/s]
Adding requests:  37%|███▋      | 1508/4096 [00:03<00:06, 396.60it/s]
Adding requests:  38%|███▊      | 1549/4096 [00:03<00:06, 399.60it/s]
Adding requests:  39%|███▉      | 1589/4096 [00:04<00:06, 396.95it/s]
Adding requests:  40%|███▉      | 1629/4096 [00:04<00:06, 396.31it/s]
Adding requests:  41%|████      | 1669/4096 [00:04<00:06, 389.14it/s]
Adding requests:  42%|████▏     | 1708/4096 [00:04<00:06, 389.24it/s]
Adding requests:  43%|████▎     | 1747/4096 [00:04<00:06, 387.43it/s]
Adding requests:  44%|████▎     | 1786/4096 [00:04<00:05, 386.75it/s]
Adding requests:  45%|████▍     | 1825/4096 [00:04<00:05, 385.89it/s]
Adding requests:  46%|████▌     | 1864/4096 [00:04<00:05, 379.50it/s]
Adding requests:  46%|████▋     | 1904/4096 [00:04<00:05, 384.20it/s]
Adding requests:  47%|████▋     | 1943/4096 [00:05<00:05, 384.49it/s]
Adding requests:  48%|████▊     | 1982/4096 [00:05<00:05, 383.24it/s]
Adding requests:  49%|████▉     | 2021/4096 [00:05<00:05, 381.17it/s]
Adding requests:  50%|█████     | 2061/4096 [00:05<00:05, 385.40it/s]
Adding requests:  51%|█████▏    | 2100/4096 [00:05<00:05, 382.20it/s]
Adding requests:  52%|█████▏    | 2139/4096 [00:05<00:05, 376.33it/s]
Adding requests:  53%|█████▎    | 2177/4096 [00:05<00:05, 374.70it/s]
Adding requests:  54%|█████▍    | 2215/4096 [00:05<00:05, 373.75it/s]
Adding requests:  55%|█████▌    | 2253/4096 [00:05<00:04, 375.33it/s]
Adding requests:  56%|█████▌    | 2291/4096 [00:05<00:04, 373.25it/s]
Adding requests:  57%|█████▋    | 2329/4096 [00:08<00:42, 41.36it/s] 
Adding requests:  58%|█████▊    | 2365/4096 [00:08<00:31, 55.46it/s]
Adding requests:  59%|█████▊    | 2402/4096 [00:08<00:22, 74.18it/s]
Adding requests:  59%|█████▉    | 2436/4096 [00:09<00:17, 94.81it/s]
Adding requests:  60%|██████    | 2474/4096 [00:09<00:13, 123.37it/s]
Adding requests:  61%|██████▏   | 2512/4096 [00:09<00:10, 155.37it/s]
Adding requests:  62%|██████▏   | 2549/4096 [00:09<00:08, 187.70it/s]
Adding requests:  63%|██████▎   | 2588/4096 [00:09<00:06, 223.05it/s]
Adding requests:  64%|██████▍   | 2626/4096 [00:09<00:05, 253.85it/s]
Adding requests:  65%|██████▌   | 2666/4096 [00:09<00:05, 285.10it/s]
Adding requests:  66%|██████▌   | 2704/4096 [00:09<00:04, 306.11it/s]
Adding requests:  67%|██████▋   | 2743/4096 [00:09<00:04, 326.64it/s]
Adding requests:  68%|██████▊   | 2781/4096 [00:10<00:03, 337.92it/s]
Adding requests:  69%|██████▉   | 2819/4096 [00:10<00:03, 346.64it/s]
Adding requests:  70%|██████▉   | 2857/4096 [00:10<00:03, 352.22it/s]
Adding requests:  71%|███████   | 2896/4096 [00:10<00:03, 362.31it/s]
Adding requests:  72%|███████▏  | 2934/4096 [00:10<00:03, 365.27it/s]
Adding requests:  73%|███████▎  | 2973/4096 [00:10<00:03, 370.18it/s]
Adding requests:  74%|███████▎  | 3012/4096 [00:10<00:02, 374.52it/s]
Adding requests:  74%|███████▍  | 3051/4096 [00:10<00:02, 376.04it/s]
Adding requests:  75%|███████▌  | 3089/4096 [00:10<00:02, 374.80it/s]
Adding requests:  76%|███████▋  | 3127/4096 [00:10<00:02, 373.80it/s]
Adding requests:  77%|███████▋  | 3165/4096 [00:11<00:02, 374.20it/s]
Adding requests:  78%|███████▊  | 3203/4096 [00:11<00:02, 373.92it/s]
Adding requests:  79%|███████▉  | 3242/4096 [00:11<00:02, 378.44it/s]
Adding requests:  80%|████████  | 3281/4096 [00:11<00:02, 381.01it/s]
Adding requests:  81%|████████  | 3320/4096 [00:11<00:02, 380.24it/s]
Adding requests:  82%|████████▏ | 3359/4096 [00:11<00:01, 380.36it/s]
Adding requests:  83%|████████▎ | 3398/4096 [00:11<00:01, 378.21it/s]
Adding requests:  84%|████████▍ | 3438/4096 [00:11<00:01, 380.45it/s]
Adding requests:  85%|████████▍ | 3477/4096 [00:11<00:01, 374.58it/s]
Adding requests:  86%|████████▌ | 3515/4096 [00:11<00:01, 375.20it/s]
Adding requests:  87%|████████▋ | 3553/4096 [00:12<00:01, 366.90it/s]
Adding requests:  88%|████████▊ | 3590/4096 [00:12<00:01, 365.62it/s]
Adding requests:  89%|████████▊ | 3627/4096 [00:12<00:01, 362.76it/s]
Adding requests:  90%|████████▉ | 3666/4096 [00:12<00:01, 369.88it/s]
Adding requests:  91%|█████████ | 3707/4096 [00:12<00:01, 379.28it/s]
Adding requests:  91%|█████████▏| 3746/4096 [00:12<00:00, 381.98it/s]
Adding requests:  92%|█████████▏| 3785/4096 [00:12<00:00, 377.04it/s]
Adding requests:  93%|█████████▎| 3825/4096 [00:12<00:00, 381.48it/s]
Adding requests:  94%|█████████▍| 3865/4096 [00:12<00:00, 385.25it/s]
Adding requests:  95%|█████████▌| 3904/4096 [00:12<00:00, 384.20it/s]
Adding requests:  96%|█████████▋| 3944/4096 [00:13<00:00, 388.11it/s]
Adding requests:  97%|█████████▋| 3984/4096 [00:13<00:00, 390.47it/s]
Adding requests:  98%|█████████▊| 4024/4096 [00:13<00:00, 392.97it/s]
Adding requests:  99%|█████████▉| 4064/4096 [00:13<00:00, 389.75it/s]
Adding requests: 100%|██████████| 4096/4096 [00:13<00:00, 304.13it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:00<00:03, 970.80it/s, est. speed input: 994155.25 toks/s, output: 970.81 toks/s]
Processed prompts:  15%|█▍        | 612/4096 [00:02<00:17, 204.09it/s, est. speed input: 260909.29 toks/s, output: 254.79 toks/s]
Processed prompts:  16%|█▌        | 656/4096 [00:03<00:20, 166.63it/s, est. speed input: 222506.99 toks/s, output: 217.29 toks/s]
Processed prompts:  17%|█▋        | 683/4096 [00:03<00:25, 131.37it/s, est. speed input: 192380.75 toks/s, output: 187.87 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:04<00:32, 104.00it/s, est. speed input: 170015.55 toks/s, output: 166.03 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:04<00:37, 88.70it/s, est. speed input: 155211.87 toks/s, output: 151.57 toks/s] 
Processed prompts:  19%|█▉        | 770/4096 [00:05<00:42, 77.83it/s, est. speed input: 143737.48 toks/s, output: 140.37 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:06<00:46, 70.12it/s, est. speed input: 134576.07 toks/s, output: 131.42 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:06<00:50, 64.68it/s, est. speed input: 127096.13 toks/s, output: 124.12 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:07<00:53, 60.85it/s, est. speed input: 120872.35 toks/s, output: 118.04 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:07<00:54, 58.17it/s, est. speed input: 115617.51 toks/s, output: 112.91 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:08<00:56, 56.28it/s, est. speed input: 111116.44 toks/s, output: 108.51 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:09<00:57, 54.96it/s, est. speed input: 107220.09 toks/s, output: 104.71 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:09<00:57, 54.10it/s, est. speed input: 103842.27 toks/s, output: 101.41 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:10<00:57, 53.51it/s, est. speed input: 100867.57 toks/s, output: 98.50 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:11<00:57, 53.09it/s, est. speed input: 98224.40 toks/s, output: 95.92 toks/s] 
Processed prompts:  27%|██▋       | 1090/4096 [00:11<00:56, 52.80it/s, est. speed input: 95857.78 toks/s, output: 93.61 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:12<00:56, 52.59it/s, est. speed input: 93729.19 toks/s, output: 91.53 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:12<00:55, 52.83it/s, est. speed input: 91909.68 toks/s, output: 89.76 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:13<00:55, 52.62it/s, est. speed input: 90155.08 toks/s, output: 88.04 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:14<00:54, 52.47it/s, est. speed input: 88551.53 toks/s, output: 86.48 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:14<00:54, 52.37it/s, est. speed input: 87082.23 toks/s, output: 85.04 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:15<00:53, 52.29it/s, est. speed input: 85730.46 toks/s, output: 83.72 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:15<00:53, 52.23it/s, est. speed input: 84481.19 toks/s, output: 82.50 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:16<00:52, 52.19it/s, est. speed input: 83324.62 toks/s, output: 81.37 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:17<00:52, 52.10it/s, est. speed input: 82239.88 toks/s, output: 80.31 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:17<00:51, 51.70it/s, est. speed input: 81169.07 toks/s, output: 79.27 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:18<00:51, 51.38it/s, est. speed input: 80163.75 toks/s, output: 78.28 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:19<00:51, 51.15it/s, est. speed input: 79224.51 toks/s, output: 77.37 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:19<00:50, 51.00it/s, est. speed input: 78345.78 toks/s, output: 76.51 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:20<00:50, 50.89it/s, est. speed input: 77522.00 toks/s, output: 75.70 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:20<00:49, 50.81it/s, est. speed input: 76747.42 toks/s, output: 74.95 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:21<00:49, 50.76it/s, est. speed input: 76018.00 toks/s, output: 74.24 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:22<00:48, 50.72it/s, est. speed input: 75330.55 toks/s, output: 73.56 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:22<00:47, 50.69it/s, est. speed input: 74680.83 toks/s, output: 72.93 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:23<00:47, 50.67it/s, est. speed input: 74065.56 toks/s, output: 72.33 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:24<00:46, 50.67it/s, est. speed input: 73483.80 toks/s, output: 71.76 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:24<00:46, 50.65it/s, est. speed input: 72930.17 toks/s, output: 71.22 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:25<00:45, 50.61it/s, est. speed input: 72401.42 toks/s, output: 70.70 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:26<00:45, 49.94it/s, est. speed input: 71822.53 toks/s, output: 70.14 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:26<00:44, 49.87it/s, est. speed input: 71317.74 toks/s, output: 69.65 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:27<00:44, 49.40it/s, est. speed input: 70789.51 toks/s, output: 69.13 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:30<01:40, 21.68it/s, est. speed input: 64007.78 toks/s, output: 62.51 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:31<01:22, 25.98it/s, est. speed input: 63704.09 toks/s, output: 62.21 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:32<01:09, 30.18it/s, est. speed input: 63413.79 toks/s, output: 61.93 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:32<01:01, 34.03it/s, est. speed input: 63134.39 toks/s, output: 61.65 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:33<00:54, 37.64it/s, est. speed input: 62906.86 toks/s, output: 61.43 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:34<00:49, 40.71it/s, est. speed input: 62693.32 toks/s, output: 61.22 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:34<00:45, 43.18it/s, est. speed input: 62487.66 toks/s, output: 61.02 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:35<00:43, 45.10it/s, est. speed input: 62289.62 toks/s, output: 60.83 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:35<00:41, 46.54it/s, est. speed input: 62098.51 toks/s, output: 60.64 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:36<00:39, 47.95it/s, est. speed input: 61941.21 toks/s, output: 60.49 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:37<00:37, 49.13it/s, est. speed input: 61799.69 toks/s, output: 60.35 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:37<00:36, 50.34it/s, est. speed input: 61686.98 toks/s, output: 60.24 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:38<00:35, 50.86it/s, est. speed input: 61553.68 toks/s, output: 60.11 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:38<00:34, 51.27it/s, est. speed input: 61427.45 toks/s, output: 59.99 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:39<00:33, 51.49it/s, est. speed input: 61300.26 toks/s, output: 59.86 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:40<00:32, 51.72it/s, est. speed input: 61181.76 toks/s, output: 59.75 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:40<00:32, 51.78it/s, est. speed input: 61060.95 toks/s, output: 59.63 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:41<00:31, 51.89it/s, est. speed input: 60947.13 toks/s, output: 59.52 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:42<00:30, 52.33it/s, est. speed input: 60857.96 toks/s, output: 59.43 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:42<00:29, 52.28it/s, est. speed input: 60750.49 toks/s, output: 59.33 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:43<00:29, 52.22it/s, est. speed input: 60645.53 toks/s, output: 59.22 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:43<00:28, 52.19it/s, est. speed input: 60543.44 toks/s, output: 59.12 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:44<00:28, 52.17it/s, est. speed input: 60444.63 toks/s, output: 59.03 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:45<00:27, 52.16it/s, est. speed input: 60348.49 toks/s, output: 58.93 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:45<00:26, 52.14it/s, est. speed input: 60254.48 toks/s, output: 58.84 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:46<00:26, 52.17it/s, est. speed input: 60165.26 toks/s, output: 58.76 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:46<00:25, 52.10it/s, est. speed input: 60073.61 toks/s, output: 58.67 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:47<00:25, 52.12it/s, est. speed input: 59987.47 toks/s, output: 58.58 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:48<00:24, 52.11it/s, est. speed input: 59902.99 toks/s, output: 58.50 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:48<00:23, 52.11it/s, est. speed input: 59820.66 toks/s, output: 58.42 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:49<00:23, 52.11it/s, est. speed input: 59740.49 toks/s, output: 58.34 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:50<00:22, 52.11it/s, est. speed input: 59662.02 toks/s, output: 58.26 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:50<00:22, 52.11it/s, est. speed input: 59585.52 toks/s, output: 58.19 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:51<00:21, 52.11it/s, est. speed input: 59510.96 toks/s, output: 58.12 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:51<00:20, 52.11it/s, est. speed input: 59438.07 toks/s, output: 58.04 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:52<00:20, 51.82it/s, est. speed input: 59354.05 toks/s, output: 57.96 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:53<00:19, 51.42it/s, est. speed input: 59263.19 toks/s, output: 57.87 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:53<00:19, 51.16it/s, est. speed input: 59174.94 toks/s, output: 57.79 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:54<00:18, 50.97it/s, est. speed input: 59088.42 toks/s, output: 57.70 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:55<00:18, 50.84it/s, est. speed input: 59004.06 toks/s, output: 57.62 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:55<00:17, 50.75it/s, est. speed input: 58921.67 toks/s, output: 57.54 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:56<00:17, 50.69it/s, est. speed input: 58841.04 toks/s, output: 57.46 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:56<00:16, 50.64it/s, est. speed input: 58762.08 toks/s, output: 57.38 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:57<00:15, 50.62it/s, est. speed input: 58685.19 toks/s, output: 57.31 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:58<00:15, 50.60it/s, est. speed input: 58609.91 toks/s, output: 57.24 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:58<00:14, 50.58it/s, est. speed input: 58536.19 toks/s, output: 57.16 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:59<00:13, 50.57it/s, est. speed input: 58464.09 toks/s, output: 57.09 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:00<00:13, 50.56it/s, est. speed input: 58393.24 toks/s, output: 57.02 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:00<00:12, 49.84it/s, est. speed input: 58294.87 toks/s, output: 56.93 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:01<00:12, 49.31it/s, est. speed input: 58197.06 toks/s, output: 56.83 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:02<00:11, 48.94it/s, est. speed input: 58101.12 toks/s, output: 56.74 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:02<00:11, 48.68it/s, est. speed input: 58007.27 toks/s, output: 56.65 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:06<00:23, 21.40it/s, est. speed input: 55479.16 toks/s, output: 54.18 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:06<00:18, 25.69it/s, est. speed input: 55418.04 toks/s, output: 54.12 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:07<00:14, 30.10it/s, est. speed input: 55380.63 toks/s, output: 54.08 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:08<00:12, 34.22it/s, est. speed input: 55344.27 toks/s, output: 54.05 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:08<00:10, 38.06it/s, est. speed input: 55320.96 toks/s, output: 54.02 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:09<00:08, 41.04it/s, est. speed input: 55285.53 toks/s, output: 53.99 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:10<00:07, 43.44it/s, est. speed input: 55251.29 toks/s, output: 53.96 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:10<00:06, 45.29it/s, est. speed input: 55217.25 toks/s, output: 53.92 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:11<00:05, 46.96it/s, est. speed input: 55194.54 toks/s, output: 53.90 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:11<00:04, 48.40it/s, est. speed input: 55178.95 toks/s, output: 53.89 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:12<00:03, 49.45it/s, est. speed input: 55163.54 toks/s, output: 53.87 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:13<00:03, 50.22it/s, est. speed input: 55148.42 toks/s, output: 53.86 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:13<00:02, 50.82it/s, est. speed input: 55134.81 toks/s, output: 53.84 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:14<00:01, 51.17it/s, est. speed input: 55119.29 toks/s, output: 53.83 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:14<00:01, 51.82it/s, est. speed input: 55115.94 toks/s, output: 53.82 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:15<00:00, 52.41it/s, est. speed input: 55116.08 toks/s, output: 53.82 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:15<00:00, 52.41it/s, est. speed input: 55522.40 toks/s, output: 54.22 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:15<00:00, 54.22it/s, est. speed input: 55522.40 toks/s, output: 54.22 toks/s]
[rank0]:[W128 12:28:27.964629058 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

