
========== M=16 ==========
Time: 2026-01-21 23:40:37
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-FP8_M16.json


========== M=128 ==========
Time: 2026-01-21 23:40:37
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-FP8_M128.json


========== M=512 ==========
Time: 2026-01-21 23:40:37
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 641 --max-num-batched-tokens 641 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-FP8_M512.json


========== M=16 ==========
Time: 2026-01-21 23:42:28
Backend: cuBLASLt [inner32]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-FP8_M16.json


========== M=16 ==========
Time: 2026-01-21 23:42:28
Backend: cuBLASLt [inner32]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M16.json


========== M=16 ==========
Time: 2026-01-21 23:42:28
Backend: cuBLASLt [inner32]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-1.5B-FP8_M16.json


========== M=16 ==========
Time: 2026-01-21 23:42:28
Backend: cuBLASLt [inner32]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-3B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-3B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-3B-FP8_M16.json


========== M=16 ==========
Time: 2026-01-21 23:42:28
Backend: cuBLASLt [inner32]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-FP8_M16.json


========== M=16 ==========
Time: 2026-01-21 23:42:28
Backend: cuBLASLt [inner32]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-7B-FP8_M16.json


========== M=16 ==========
Time: 2026-01-21 23:42:28
Backend: cuBLASLt [inner32]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-14B-FP8_M16.json


========== M=16 ==========
Time: 2026-01-21 23:43:25
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-FP8_M16.json


========== M=16 ==========
Time: 2026-01-21 23:45:51
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 217.16 requests/s, 3691.70 total tokens/s, 217.16 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3235061) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3235061) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.96it/s]
(EngineCore_DP0 pid=3235061) 
(EngineCore_DP0 pid=3235061) 2026-01-21 23:46:07,466 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3235061) 2026-01-21 23:46:07,471 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3235061) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 91.98it/s]
(EngineCore_DP0 pid=3235061) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4378.58it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:00, 163.91it/s, est. speed input: 2622.71 toks/s, output: 163.91 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:00, 210.76it/s, est. speed input: 3259.21 toks/s, output: 203.69 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:00<00:00, 227.05it/s, est. speed input: 3486.18 toks/s, output: 217.88 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:00<00:00, 231.81it/s, est. speed input: 3570.21 toks/s, output: 223.14 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:00<00:00, 236.69it/s, est. speed input: 3641.16 toks/s, output: 227.57 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 236.69it/s, est. speed input: 3667.44 toks/s, output: 229.21 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 229.16it/s, est. speed input: 3667.44 toks/s, output: 229.21 toks/s]
[rank0]:[W121 23:46:09.039602290 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-21 23:46:10
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 207.63 requests/s, 26784.34 total tokens/s, 207.63 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3235435) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3235435) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.16it/s]
(EngineCore_DP0 pid=3235435) 
(EngineCore_DP0 pid=3235435) 2026-01-21 23:46:26,516 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3235435) 2026-01-21 23:46:26,520 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3235435) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 89.29it/s]
(EngineCore_DP0 pid=3235435) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.93it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2067.46it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:00, 216.56it/s, est. speed input: 27720.91 toks/s, output: 216.56 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:00, 232.35it/s, est. speed input: 29439.86 toks/s, output: 230.00 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:00<00:00, 232.21it/s, est. speed input: 29529.41 toks/s, output: 230.69 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:00<00:00, 226.77it/s, est. speed input: 29134.00 toks/s, output: 227.60 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:00<00:00, 233.05it/s, est. speed input: 29559.80 toks/s, output: 230.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 233.05it/s, est. speed input: 29640.11 toks/s, output: 231.56 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 231.54it/s, est. speed input: 29640.11 toks/s, output: 231.56 toks/s]
[rank0]:[W121 23:46:28.102264488 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-21 23:46:29
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 641 --max-num-batched-tokens 641 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 168.73 requests/s, 86560.45 total tokens/s, 168.73 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3235813) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3235813) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.20it/s]
(EngineCore_DP0 pid=3235813) 
(EngineCore_DP0 pid=3235813) 2026-01-21 23:46:47,892 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3235813) 2026-01-21 23:46:47,896 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3235813) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 85.92it/s]
(EngineCore_DP0 pid=3235813) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  66%|██████▌   | 84/128 [00:00<00:00, 836.66it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 894.43it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:00, 332.58it/s, est. speed input: 170290.92 toks/s, output: 332.59 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:00<00:00, 224.26it/s, est. speed input: 120720.85 toks/s, output: 235.78 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:00<00:00, 207.07it/s, est. speed input: 112274.99 toks/s, output: 219.28 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:00<00:00, 199.13it/s, est. speed input: 108352.89 toks/s, output: 211.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 199.13it/s, est. speed input: 106793.62 toks/s, output: 208.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 208.55it/s, est. speed input: 106793.62 toks/s, output: 208.58 toks/s]
[rank0]:[W121 23:46:49.764128739 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-21 23:51:09
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 187.41 requests/s, 3185.89 total tokens/s, 187.41 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3242306) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3242306) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.89it/s]
(EngineCore_DP0 pid=3242306) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.89it/s]
(EngineCore_DP0 pid=3242306) 
(EngineCore_DP0 pid=3242306) 2026-01-21 23:51:23,797 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3242306) 2026-01-21 23:51:23,800 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3242306) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 119.86it/s]
(EngineCore_DP0 pid=3242306) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4601.70it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:00, 155.14it/s, est. speed input: 2482.71 toks/s, output: 155.15 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:00, 184.04it/s, est. speed input: 2875.39 toks/s, output: 179.70 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:00, 193.11it/s, est. speed input: 3004.37 toks/s, output: 187.77 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:00<00:00, 197.37it/s, est. speed input: 3068.76 toks/s, output: 191.80 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:00<00:00, 199.50it/s, est. speed input: 3105.57 toks/s, output: 194.10 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:00<00:00, 200.45it/s, est. speed input: 3127.55 toks/s, output: 195.47 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 200.45it/s, est. speed input: 3134.22 toks/s, output: 195.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 195.87it/s, est. speed input: 3134.22 toks/s, output: 195.89 toks/s]
[rank0]:[W121 23:51:25.469426133 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-21 23:51:26
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 184.06 requests/s, 23744.28 total tokens/s, 184.06 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3242695) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3242695) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.25it/s]
(EngineCore_DP0 pid=3242695) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.24it/s]
(EngineCore_DP0 pid=3242695) 
(EngineCore_DP0 pid=3242695) 2026-01-21 23:51:43,176 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3242695) 2026-01-21 23:51:43,179 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3242695) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 120.65it/s]
(EngineCore_DP0 pid=3242695) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2779.34it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:00, 189.56it/s, est. speed input: 24265.64 toks/s, output: 189.57 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:00, 194.93it/s, est. speed input: 24849.12 toks/s, output: 194.13 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:00<00:00, 195.93it/s, est. speed input: 24976.69 toks/s, output: 195.13 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:00<00:00, 197.91it/s, est. speed input: 25164.96 toks/s, output: 196.60 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:00<00:00, 199.02it/s, est. speed input: 25278.66 toks/s, output: 197.49 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:00<00:00, 198.70it/s, est. speed input: 25290.25 toks/s, output: 197.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 198.70it/s, est. speed input: 25301.23 toks/s, output: 197.66 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 197.64it/s, est. speed input: 25301.23 toks/s, output: 197.66 toks/s]
[rank0]:[W121 23:51:45.937342359 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-21 23:51:46
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 641 --max-num-batched-tokens 641 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 118.08 requests/s, 60575.32 total tokens/s, 118.08 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3243089) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3243089) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.29it/s]
(EngineCore_DP0 pid=3243089) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.29it/s]
(EngineCore_DP0 pid=3243089) 
(EngineCore_DP0 pid=3243089) 2026-01-21 23:52:03,310 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3243089) 2026-01-21 23:52:03,313 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3243089) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 53.12it/s]
(EngineCore_DP0 pid=3243089) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  86%|████████▌ | 110/128 [00:00<00:00, 1093.01it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1120.29it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:00, 196.28it/s, est. speed input: 100502.45 toks/s, output: 196.28 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:00, 147.22it/s, est. speed input: 78388.71 toks/s, output: 153.10 toks/s] 
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 137.39it/s, est. speed input: 73689.13 toks/s, output: 143.92 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:00<00:00, 132.60it/s, est. speed input: 71361.47 toks/s, output: 139.38 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:00<00:00, 129.85it/s, est. speed input: 69977.19 toks/s, output: 136.67 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:00<00:00, 128.01it/s, est. speed input: 69001.53 toks/s, output: 134.77 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:00<00:00, 126.84it/s, est. speed input: 68324.92 toks/s, output: 133.45 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:00<00:00, 125.91it/s, est. speed input: 67776.53 toks/s, output: 132.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 125.91it/s, est. speed input: 67707.04 toks/s, output: 132.24 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 132.23it/s, est. speed input: 67707.04 toks/s, output: 132.24 toks/s]
[rank0]:[W121 23:52:05.480503055 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

