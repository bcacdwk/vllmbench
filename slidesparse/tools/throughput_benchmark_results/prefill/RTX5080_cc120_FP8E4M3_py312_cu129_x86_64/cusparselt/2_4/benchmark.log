
========== M=16 ==========
Time: 2026-01-21 23:48:09
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 220.26 requests/s, 3744.40 total tokens/s, 220.26 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3237900) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3237900) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.05it/s]
(EngineCore_DP0 pid=3237900) 
(EngineCore_DP0 pid=3237900) 2026-01-21 23:48:25,419 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3237900) 2026-01-21 23:48:25,423 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3237900) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 90.62it/s]
(EngineCore_DP0 pid=3237900) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.01it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 7349.06it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:00, 165.02it/s, est. speed input: 2640.48 toks/s, output: 165.02 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:00, 207.84it/s, est. speed input: 3221.68 toks/s, output: 201.35 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:00<00:00, 223.73it/s, est. speed input: 3442.45 toks/s, output: 215.13 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:00<00:00, 231.18it/s, est. speed input: 3552.44 toks/s, output: 222.02 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:00<00:00, 235.04it/s, est. speed input: 3616.21 toks/s, output: 226.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 235.04it/s, est. speed input: 3641.45 toks/s, output: 227.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 227.56it/s, est. speed input: 3641.45 toks/s, output: 227.59 toks/s]
[rank0]:[W121 23:48:27.987270389 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-21 23:48:28
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_4
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_4 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-0.5B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 217.40 requests/s, 28044.74 total tokens/s, 217.40 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3238289) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3238289) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.41it/s]
(EngineCore_DP0 pid=3238289) 
(EngineCore_DP0 pid=3238289) 2026-01-21 23:48:44,895 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3238289) 2026-01-21 23:48:44,899 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3238289) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 50.02it/s]
(EngineCore_DP0 pid=3238289) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2163.76it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:00, 240.23it/s, est. speed input: 30752.71 toks/s, output: 240.24 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:00, 242.00it/s, est. speed input: 30943.93 toks/s, output: 241.74 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:00<00:00, 242.37it/s, est. speed input: 30988.55 toks/s, output: 242.09 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:00<00:00, 242.84it/s, est. speed input: 31034.99 toks/s, output: 242.46 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:00<00:00, 242.84it/s, est. speed input: 31044.74 toks/s, output: 242.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 242.84it/s, est. speed input: 31042.86 toks/s, output: 242.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 242.48it/s, est. speed input: 31042.86 toks/s, output: 242.52 toks/s]
[rank0]:[W121 23:48:46.535980150 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-21 23:48:47
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 641 --max-num-batched-tokens 641 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-0.5B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
Throughput: 171.00 requests/s, 87725.32 total tokens/s, 171.00 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3238673) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3238673) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.35it/s]
(EngineCore_DP0 pid=3238673) 
(EngineCore_DP0 pid=3238673) 2026-01-21 23:49:04,053 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3238673) 2026-01-21 23:49:04,058 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3238673) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 85.57it/s]
(EngineCore_DP0 pid=3238673) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  73%|███████▎  | 93/128 [00:00<00:00, 929.08it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 952.82it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:00, 332.04it/s, est. speed input: 170020.48 toks/s, output: 332.05 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:00<00:00, 225.57it/s, est. speed input: 121332.51 toks/s, output: 236.97 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:00<00:00, 207.82it/s, est. speed input: 112682.18 toks/s, output: 220.08 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:00<00:00, 199.00it/s, est. speed input: 108455.15 toks/s, output: 211.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 199.00it/s, est. speed input: 106840.15 toks/s, output: 208.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 208.64it/s, est. speed input: 106840.15 toks/s, output: 208.67 toks/s]
[rank0]:[W121 23:49:05.788464062 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

