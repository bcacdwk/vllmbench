
========== M=16 ==========
Time: 2026-01-21 23:50:05
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     return loader.load_weights(weights)
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     yield from self._load_module(
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     param.load_merged_column_weight(
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314) ERROR 01-21 23:50:14 [core.py:866] AssertionError

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3240314) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3240314) Process EngineCore_DP0:
(EngineCore_DP0 pid=3240314) Traceback (most recent call last):
(EngineCore_DP0 pid=3240314)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3240314)     self.run()
(EngineCore_DP0 pid=3240314)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3240314)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3240314)     raise e
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3240314)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3240314)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3240314)     super().__init__(
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3240314)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3240314)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3240314)     self._init_executor()
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3240314)     self.driver_worker.load_model()
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3240314)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3240314)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3240314)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3240314)     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3240314)     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3240314)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3240314)     return loader.load_weights(weights)
(EngineCore_DP0 pid=3240314)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3240314)     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3240314)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3240314)     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3240314)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3240314)     yield from self._load_module(
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3240314)     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3240314)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3240314)     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3240314)     param.load_merged_column_weight(
(EngineCore_DP0 pid=3240314)   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3240314)     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3240314)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240314) AssertionError
(EngineCore_DP0 pid=3240314) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3240314) 
[rank0]:[W121 23:50:14.667757087 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=128 ==========
Time: 2026-01-21 23:50:16
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     return loader.load_weights(weights)
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     yield from self._load_module(
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     param.load_merged_column_weight(
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693) ERROR 01-21 23:50:25 [core.py:866] AssertionError

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3240693) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3240693) Process EngineCore_DP0:
(EngineCore_DP0 pid=3240693) Traceback (most recent call last):
(EngineCore_DP0 pid=3240693)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3240693)     self.run()
(EngineCore_DP0 pid=3240693)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3240693)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3240693)     raise e
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3240693)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3240693)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3240693)     super().__init__(
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3240693)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3240693)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3240693)     self._init_executor()
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3240693)     self.driver_worker.load_model()
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3240693)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3240693)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3240693)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3240693)     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3240693)     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3240693)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3240693)     return loader.load_weights(weights)
(EngineCore_DP0 pid=3240693)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3240693)     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3240693)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3240693)     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3240693)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3240693)     yield from self._load_module(
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3240693)     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3240693)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3240693)     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3240693)     param.load_merged_column_weight(
(EngineCore_DP0 pid=3240693)   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3240693)     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3240693)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3240693) AssertionError
(EngineCore_DP0 pid=3240693) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3240693) 
[rank0]:[W121 23:50:26.801065727 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=512 ==========
Time: 2026-01-21 23:50:27
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 641 --max-num-batched-tokens 641 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     return loader.load_weights(weights)
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     yield from self._load_module(
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     param.load_merged_column_weight(
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022) ERROR 01-21 23:50:36 [core.py:866] AssertionError

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3241022) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3241022) Process EngineCore_DP0:
(EngineCore_DP0 pid=3241022) Traceback (most recent call last):
(EngineCore_DP0 pid=3241022)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3241022)     self.run()
(EngineCore_DP0 pid=3241022)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3241022)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3241022)     raise e
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3241022)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3241022)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3241022)     super().__init__(
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3241022)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3241022)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3241022)     self._init_executor()
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3241022)     self.driver_worker.load_model()
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3241022)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3241022)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3241022)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3241022)     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3241022)     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3241022)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3241022)     return loader.load_weights(weights)
(EngineCore_DP0 pid=3241022)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3241022)     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3241022)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3241022)     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3241022)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3241022)     yield from self._load_module(
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3241022)     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3241022)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3241022)     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3241022)     param.load_merged_column_weight(
(EngineCore_DP0 pid=3241022)   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3241022)     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3241022)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3241022) AssertionError
(EngineCore_DP0 pid=3241022) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3241022) 
[rank0]:[W121 23:50:36.353030652 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=16 ==========
Time: 2026-01-21 23:53:11
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 640, in load_weights
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     return loader.load_weights(
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     yield from self._load_module(
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 497, in load_weights
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     param.load_merged_column_weight(
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137) ERROR 01-21 23:53:20 [core.py:866] AssertionError

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3245137) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3245137) Process EngineCore_DP0:
(EngineCore_DP0 pid=3245137) Traceback (most recent call last):
(EngineCore_DP0 pid=3245137)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3245137)     self.run()
(EngineCore_DP0 pid=3245137)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3245137)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3245137)     raise e
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3245137)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3245137)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3245137)     super().__init__(
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3245137)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3245137)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3245137)     self._init_executor()
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3245137)     self.driver_worker.load_model()
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3245137)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3245137)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3245137)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3245137)     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3245137)     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3245137)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 640, in load_weights
(EngineCore_DP0 pid=3245137)     return loader.load_weights(
(EngineCore_DP0 pid=3245137)            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3245137)     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3245137)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3245137)     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3245137)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3245137)     yield from self._load_module(
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3245137)     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3245137)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 497, in load_weights
(EngineCore_DP0 pid=3245137)     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3245137)     param.load_merged_column_weight(
(EngineCore_DP0 pid=3245137)   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3245137)     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3245137)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245137) AssertionError
(EngineCore_DP0 pid=3245137) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3245137) 
[rank0]:[W121 23:53:20.685338588 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=128 ==========
Time: 2026-01-21 23:53:22
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 640, in load_weights
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     return loader.load_weights(
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     yield from self._load_module(
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 497, in load_weights
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     param.load_merged_column_weight(
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486) ERROR 01-21 23:53:31 [core.py:866] AssertionError

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3245486) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3245486) Process EngineCore_DP0:
(EngineCore_DP0 pid=3245486) Traceback (most recent call last):
(EngineCore_DP0 pid=3245486)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3245486)     self.run()
(EngineCore_DP0 pid=3245486)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3245486)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3245486)     raise e
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3245486)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3245486)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3245486)     super().__init__(
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3245486)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3245486)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3245486)     self._init_executor()
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3245486)     self.driver_worker.load_model()
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3245486)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3245486)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3245486)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3245486)     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3245486)     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3245486)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 640, in load_weights
(EngineCore_DP0 pid=3245486)     return loader.load_weights(
(EngineCore_DP0 pid=3245486)            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3245486)     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3245486)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3245486)     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3245486)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3245486)     yield from self._load_module(
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3245486)     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3245486)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 497, in load_weights
(EngineCore_DP0 pid=3245486)     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3245486)     param.load_merged_column_weight(
(EngineCore_DP0 pid=3245486)   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3245486)     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3245486)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245486) AssertionError
(EngineCore_DP0 pid=3245486) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3245486) 
[rank0]:[W121 23:53:31.787183426 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=512 ==========
Time: 2026-01-21 23:53:33
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 641 --max-num-batched-tokens 641 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 640, in load_weights
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     return loader.load_weights(
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     yield from self._load_module(
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 497, in load_weights
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     param.load_merged_column_weight(
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847) ERROR 01-21 23:53:42 [core.py:866] AssertionError

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3245847) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3245847) Process EngineCore_DP0:
(EngineCore_DP0 pid=3245847) Traceback (most recent call last):
(EngineCore_DP0 pid=3245847)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3245847)     self.run()
(EngineCore_DP0 pid=3245847)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3245847)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3245847)     raise e
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3245847)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3245847)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3245847)     super().__init__(
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3245847)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3245847)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3245847)     self._init_executor()
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3245847)     self.driver_worker.load_model()
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3245847)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3245847)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3245847)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3245847)     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3245847)     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3245847)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 640, in load_weights
(EngineCore_DP0 pid=3245847)     return loader.load_weights(
(EngineCore_DP0 pid=3245847)            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3245847)     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3245847)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3245847)     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3245847)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3245847)     yield from self._load_module(
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3245847)     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3245847)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 497, in load_weights
(EngineCore_DP0 pid=3245847)     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3245847)     param.load_merged_column_weight(
(EngineCore_DP0 pid=3245847)   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3245847)     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3245847)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3245847) AssertionError
(EngineCore_DP0 pid=3245847) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3245847) 
[rank0]:[W121 23:53:42.537886392 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=16 ==========
Time: 2026-01-22 00:07:28
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     return loader.load_weights(weights)
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     yield from self._load_module(
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     param.load_merged_column_weight(
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711) ERROR 01-22 00:07:36 [core.py:866] AssertionError

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3251711) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3251711) Process EngineCore_DP0:
(EngineCore_DP0 pid=3251711) Traceback (most recent call last):
(EngineCore_DP0 pid=3251711)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3251711)     self.run()
(EngineCore_DP0 pid=3251711)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3251711)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3251711)     raise e
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3251711)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3251711)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3251711)     super().__init__(
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3251711)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3251711)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3251711)     self._init_executor()
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3251711)     self.driver_worker.load_model()
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3251711)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3251711)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3251711)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3251711)     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3251711)     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3251711)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3251711)     return loader.load_weights(weights)
(EngineCore_DP0 pid=3251711)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3251711)     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3251711)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3251711)     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3251711)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3251711)     yield from self._load_module(
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3251711)     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3251711)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3251711)     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3251711)     param.load_merged_column_weight(
(EngineCore_DP0 pid=3251711)   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3251711)     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3251711)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3251711) AssertionError
(EngineCore_DP0 pid=3251711) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3251711) 
[rank0]:[W122 00:07:37.068694131 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:10:16
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:10:19 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:10:19 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:10:19 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:10:19 [model.py:1661] Using max model len 145
INFO 01-22 00:10:20 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3253052) INFO 01-22 00:10:23 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3253052) INFO 01-22 00:10:23 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:55507 backend=nccl
(EngineCore_DP0 pid=3253052) INFO 01-22 00:10:23 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3253052) INFO 01-22 00:10:24 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3253052) INFO 01-22 00:10:24 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     return loader.load_weights(weights)
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     yield from self._load_module(
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     param.load_merged_column_weight(
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052) ERROR 01-22 00:10:24 [core.py:866] AssertionError

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3253052) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3253052) Process EngineCore_DP0:
(EngineCore_DP0 pid=3253052) Traceback (most recent call last):
(EngineCore_DP0 pid=3253052)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3253052)     self.run()
(EngineCore_DP0 pid=3253052)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3253052)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3253052)     raise e
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3253052)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3253052)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3253052)     super().__init__(
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3253052)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3253052)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3253052)     self._init_executor()
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3253052)     self.driver_worker.load_model()
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3253052)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3253052)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3253052)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3253052)     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3253052)     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3253052)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3253052)     return loader.load_weights(weights)
(EngineCore_DP0 pid=3253052)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3253052)     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3253052)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3253052)     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3253052)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3253052)     yield from self._load_module(
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3253052)     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3253052)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3253052)     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3253052)     param.load_merged_column_weight(
(EngineCore_DP0 pid=3253052)   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3253052)     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3253052)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3253052) AssertionError
(EngineCore_DP0 pid=3253052) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3253052) 
[rank0]:[W122 00:10:25.026398073 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:13:52
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:13:55 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-22 00:13:55 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6'}
INFO 01-22 00:13:55 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-22 00:13:55 [model.py:1661] Using max model len 145
INFO 01-22 00:13:56 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3255830) INFO 01-22 00:13:59 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3255830) INFO 01-22 00:14:00 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:52619 backend=nccl
(EngineCore_DP0 pid=3255830) INFO 01-22 00:14:00 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3255830) INFO 01-22 00:14:00 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3255830) INFO 01-22 00:14:00 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 640, in load_weights
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     return loader.load_weights(
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     yield from self._load_module(
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 497, in load_weights
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     param.load_merged_column_weight(
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830) ERROR 01-22 00:14:01 [core.py:866] AssertionError

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3255830) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3255830) Process EngineCore_DP0:
(EngineCore_DP0 pid=3255830) Traceback (most recent call last):
(EngineCore_DP0 pid=3255830)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3255830)     self.run()
(EngineCore_DP0 pid=3255830)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3255830)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3255830)     raise e
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3255830)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3255830)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3255830)     super().__init__(
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3255830)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3255830)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3255830)     self._init_executor()
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3255830)     self.driver_worker.load_model()
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3255830)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3255830)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3255830)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3255830)     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3255830)     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3255830)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 640, in load_weights
(EngineCore_DP0 pid=3255830)     return loader.load_weights(
(EngineCore_DP0 pid=3255830)            ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3255830)     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3255830)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3255830)     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3255830)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3255830)     yield from self._load_module(
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3255830)     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3255830)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 497, in load_weights
(EngineCore_DP0 pid=3255830)     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3255830)     param.load_merged_column_weight(
(EngineCore_DP0 pid=3255830)   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3255830)     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3255830)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3255830) AssertionError
(EngineCore_DP0 pid=3255830) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3255830) 
[rank0]:[W122 00:14:01.345922288 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:15:38
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:15:41 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:15:42 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:15:42 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:15:42 [model.py:1661] Using max model len 145
INFO 01-22 00:15:42 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3257462) INFO 01-22 00:15:45 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3257462) INFO 01-22 00:15:46 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:37811 backend=nccl
(EngineCore_DP0 pid=3257462) INFO 01-22 00:15:46 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3257462) INFO 01-22 00:15:46 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3257462) INFO 01-22 00:15:46 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     return loader.load_weights(weights)
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     yield from self._load_module(
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     param.load_merged_column_weight(
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462) ERROR 01-22 00:15:46 [core.py:866] AssertionError

STDERR:
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
/root/vllmbench/vllm/model_executor/layers/quantization/slidesparse.py:81: UserWarning: Failed to import slidesparse modules: No module named 'slidesparse'. SlideSparse features will be disabled.
  warnings.warn(
(EngineCore_DP0 pid=3257462) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3257462) Process EngineCore_DP0:
(EngineCore_DP0 pid=3257462) Traceback (most recent call last):
(EngineCore_DP0 pid=3257462)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3257462)     self.run()
(EngineCore_DP0 pid=3257462)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3257462)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3257462)     raise e
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3257462)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3257462)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3257462)     super().__init__(
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3257462)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3257462)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3257462)     self._init_executor()
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=3257462)     self.driver_worker.load_model()
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=3257462)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=3257462)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=3257462)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
(EngineCore_DP0 pid=3257462)     self.load_weights(model, model_config)
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
(EngineCore_DP0 pid=3257462)     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
(EngineCore_DP0 pid=3257462)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 600, in load_weights
(EngineCore_DP0 pid=3257462)     return loader.load_weights(weights)
(EngineCore_DP0 pid=3257462)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
(EngineCore_DP0 pid=3257462)     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
(EngineCore_DP0 pid=3257462)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 335, in load_weights
(EngineCore_DP0 pid=3257462)     autoloaded_weights = set(self._load_module("", self.module, weights))
(EngineCore_DP0 pid=3257462)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 288, in _load_module
(EngineCore_DP0 pid=3257462)     yield from self._load_module(
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 261, in _load_module
(EngineCore_DP0 pid=3257462)     loaded_params = module_load_weights(weights)
(EngineCore_DP0 pid=3257462)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 501, in load_weights
(EngineCore_DP0 pid=3257462)     weight_loader(param, loaded_weight, shard_id)
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 858, in weight_loader_v2
(EngineCore_DP0 pid=3257462)     param.load_merged_column_weight(
(EngineCore_DP0 pid=3257462)   File "/root/vllmbench/vllm/model_executor/parameter.py", line 175, in load_merged_column_weight
(EngineCore_DP0 pid=3257462)     assert param_data.shape == loaded_weight.shape
(EngineCore_DP0 pid=3257462)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3257462) AssertionError
(EngineCore_DP0 pid=3257462) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3257462) 
[rank0]:[W122 00:15:47.057305181 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:25:38
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:25:42 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:25:42 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:25:42 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:25:42 [model.py:1661] Using max model len 145
INFO 01-22 00:25:42 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3266241) INFO 01-22 00:25:46 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3266241) INFO 01-22 00:25:46 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:44575 backend=nccl
(EngineCore_DP0 pid=3266241) INFO 01-22 00:25:46 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3266241) INFO 01-22 00:25:46 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3266241) INFO 01-22 00:25:47 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3266241) INFO 01-22 00:25:47 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3266241) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3266241) INFO 01-22 00:25:47 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.322614 seconds
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] 
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] 
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] 
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] 
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] torch._dynamo.exc.Unsupported: Graph break under GenericContextWrappingVariable
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   Explanation: Attempted to graph break in an active context manager(s) that doesn't support graph breaking.
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   Hint: Move the offending context manager(s) to outside the compiled region.
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   Hint: This graph break may have been caused by an earlier graph break. Resolving the earlier graph break may resolve this one.
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] 
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   Developer debug context: Active generic context managers: [GenericContextWrappingVariable(ProfileTimer)]
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] 
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0066.html
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] 
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] from user code:
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1144, in apply_weights
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 947, in apply
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 738, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866]     ext = _get_gemm_extension("cusparselt")
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] 
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3266241) ERROR 01-22 00:25:47 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1191: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:881: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1017: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1022: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3266241) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3266241) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.62it/s]
(EngineCore_DP0 pid=3266241) 
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3266241) [2026-01-22 00:25:47] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3266241) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3266241) If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3266241) If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3266241)   torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))
(EngineCore_DP0 pid=3266241) Process EngineCore_DP0:
(EngineCore_DP0 pid=3266241) torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3266241)   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3266241)   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3266241)   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3266241) 
(EngineCore_DP0 pid=3266241)   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3266241) 
(EngineCore_DP0 pid=3266241)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3266241) 
(EngineCore_DP0 pid=3266241) The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=3266241) 
(EngineCore_DP0 pid=3266241) Traceback (most recent call last):
(EngineCore_DP0 pid=3266241)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3266241)     self.run()
(EngineCore_DP0 pid=3266241)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3266241)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3266241)     raise e
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3266241)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3266241)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3266241)     super().__init__(
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3266241)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3266241)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3266241)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3266241)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3266241)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3266241)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3266241)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3266241)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3266241)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3266241)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3266241)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3266241)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3266241)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3266241)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3266241)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3266241)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3266241)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3266241)     outputs = self.model(
(EngineCore_DP0 pid=3266241)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3266241)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3266241)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3266241)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3266241)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3266241)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3266241)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3266241)     hidden_states = self.model(
(EngineCore_DP0 pid=3266241)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3266241)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3266241)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3266241)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3266241)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3266241)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3266241)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3266241)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3266241)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3266241) torch._dynamo.exc.Unsupported: Graph break under GenericContextWrappingVariable
(EngineCore_DP0 pid=3266241)   Explanation: Attempted to graph break in an active context manager(s) that doesn't support graph breaking.
(EngineCore_DP0 pid=3266241)   Hint: Move the offending context manager(s) to outside the compiled region.
(EngineCore_DP0 pid=3266241)   Hint: This graph break may have been caused by an earlier graph break. Resolving the earlier graph break may resolve this one.
(EngineCore_DP0 pid=3266241) 
(EngineCore_DP0 pid=3266241)   Developer debug context: Active generic context managers: [GenericContextWrappingVariable(ProfileTimer)]
(EngineCore_DP0 pid=3266241) 
(EngineCore_DP0 pid=3266241)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0066.html
(EngineCore_DP0 pid=3266241) 
(EngineCore_DP0 pid=3266241) from user code:
(EngineCore_DP0 pid=3266241)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3266241)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3266241)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3266241)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3266241)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3266241)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1144, in apply_weights
(EngineCore_DP0 pid=3266241)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 947, in apply
(EngineCore_DP0 pid=3266241)     return self._linear_fn(
(EngineCore_DP0 pid=3266241)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 738, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3266241)     ext = _get_gemm_extension("cusparselt")
(EngineCore_DP0 pid=3266241) 
(EngineCore_DP0 pid=3266241) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3266241) 
[rank0]:[W122 00:25:48.045191283 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:28:10
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:28:13 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:28:13 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enforce_eager': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:28:13 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:28:13 [model.py:1661] Using max model len 145
INFO 01-22 00:28:14 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
WARNING 01-22 00:28:14 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
INFO 01-22 00:28:14 [vllm.py:722] Cudagraph is disabled under eager mode
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:17 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:17 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:55399 backend=nccl
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:17 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:18 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:18 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:18 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3268538) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:19 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.360505 seconds
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:20 [gpu_worker.py:375] Available KV cache memory: 13.27 GiB
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:20 [kv_cache_utils.py:1291] GPU KV cache size: 1,159,328 tokens
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:20 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 7245.80x
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:21 [core.py:259] init engine (profile, create kv cache, warmup model) took 2.15 seconds
(EngineCore_DP0 pid=3268538) WARNING 01-22 00:28:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
(EngineCore_DP0 pid=3268538) INFO 01-22 00:28:21 [vllm.py:722] Cudagraph is disabled under eager mode
INFO 01-22 00:28:21 [llm.py:360] Supported tasks: ['generate']
Throughput: 89.04 requests/s, 1513.71 total tokens/s, 89.04 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1191: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:881: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1017: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1022: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3268538) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3268538) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.41it/s]
(EngineCore_DP0 pid=3268538) 
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:18] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:19] INFO SlideSparseLinearMethod_FP8.py:446: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:19] INFO SlideSparseLinearMethod_FP8.py:590: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=3268538) [2026-01-22 00:28:19] INFO SlideSparseLinearMethod_FP8.py:478: Dequant+bias kernel loaded
(EngineCore_DP0 pid=3268538) 2026-01-22 00:28:20,591 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3268538) 2026-01-22 00:28:20,599 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 4660.50it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:38,  3.33it/s, est. speed input: 53.21 toks/s, output: 3.33 toks/s]
Processed prompts:  10%|         | 13/128 [00:00<00:02, 39.97it/s, est. speed input: 509.93 toks/s, output: 31.87 toks/s]
Processed prompts:  20%|        | 25/128 [00:00<00:01, 63.58it/s, est. speed input: 778.07 toks/s, output: 48.63 toks/s]
Processed prompts:  29%|       | 37/128 [00:00<00:01, 79.47it/s, est. speed input: 956.23 toks/s, output: 59.76 toks/s]
Processed prompts:  38%|      | 49/128 [00:00<00:00, 90.59it/s, est. speed input: 1084.70 toks/s, output: 67.79 toks/s]
Processed prompts:  48%|     | 61/128 [00:00<00:00, 98.20it/s, est. speed input: 1180.67 toks/s, output: 73.79 toks/s]
Processed prompts:  57%|    | 73/128 [00:00<00:00, 103.56it/s, est. speed input: 1255.67 toks/s, output: 78.48 toks/s]
Processed prompts:  66%|   | 85/128 [00:01<00:00, 107.19it/s, est. speed input: 1315.25 toks/s, output: 82.20 toks/s]
Processed prompts:  76%|  | 97/128 [00:01<00:00, 109.60it/s, est. speed input: 1363.53 toks/s, output: 85.22 toks/s]
Processed prompts:  85%| | 109/128 [00:01<00:00, 111.53it/s, est. speed input: 1404.59 toks/s, output: 87.79 toks/s]
Processed prompts:  95%|| 121/128 [00:01<00:00, 112.16it/s, est. speed input: 1436.96 toks/s, output: 89.81 toks/s]
Processed prompts: 100%|| 128/128 [00:01<00:00, 112.16it/s, est. speed input: 1454.26 toks/s, output: 90.89 toks/s]
Processed prompts: 100%|| 128/128 [00:01<00:00, 90.88it/s, est. speed input: 1454.26 toks/s, output: 90.89 toks/s] 
[rank0]:[W122 00:28:23.107363644 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-22 00:29:47
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:29:51 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:29:51 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:29:51 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:29:51 [model.py:1661] Using max model len 145
INFO 01-22 00:29:51 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3270283) INFO 01-22 00:29:55 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3270283) INFO 01-22 00:29:55 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:56617 backend=nccl
(EngineCore_DP0 pid=3270283) INFO 01-22 00:29:55 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3270283) INFO 01-22 00:29:55 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3270283) INFO 01-22 00:29:56 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3270283) INFO 01-22 00:29:56 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3270283) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3270283) INFO 01-22 00:29:56 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.324170 seconds
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] 
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] 
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] 
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] 
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] torch._dynamo.exc.Unsupported: Graph break under GenericContextWrappingVariable
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   Explanation: Attempted to graph break in an active context manager(s) that doesn't support graph breaking.
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   Hint: Move the offending context manager(s) to outside the compiled region.
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   Hint: This graph break may have been caused by an earlier graph break. Resolving the earlier graph break may resolve this one.
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] 
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   Developer debug context: Active generic context managers: [GenericContextWrappingVariable(ProfileTimer)]
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] 
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0066.html
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] 
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] from user code:
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1144, in apply_weights
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 947, in apply
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 738, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866]     ext = _get_gemm_extension("cusparselt")
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] 
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3270283) ERROR 01-22 00:29:56 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1191: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:881: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1017: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1022: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3270283) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3270283) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.45it/s]
(EngineCore_DP0 pid=3270283) 
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1122: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3270283) [2026-01-22 00:29:56] INFO SlideSparseLinearMethod_FP8.py:1132: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3270283) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3270283) If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3270283) If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3270283)   torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))
(EngineCore_DP0 pid=3270283) Process EngineCore_DP0:
(EngineCore_DP0 pid=3270283) torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3270283)   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3270283)   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3270283)   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3270283) 
(EngineCore_DP0 pid=3270283)   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3270283) 
(EngineCore_DP0 pid=3270283)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3270283) 
(EngineCore_DP0 pid=3270283) The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=3270283) 
(EngineCore_DP0 pid=3270283) Traceback (most recent call last):
(EngineCore_DP0 pid=3270283)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3270283)     self.run()
(EngineCore_DP0 pid=3270283)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3270283)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3270283)     raise e
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3270283)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3270283)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3270283)     super().__init__(
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3270283)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3270283)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3270283)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3270283)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3270283)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3270283)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3270283)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3270283)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3270283)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3270283)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3270283)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3270283)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3270283)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3270283)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3270283)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3270283)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3270283)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3270283)     outputs = self.model(
(EngineCore_DP0 pid=3270283)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3270283)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3270283)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3270283)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3270283)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3270283)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3270283)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3270283)     hidden_states = self.model(
(EngineCore_DP0 pid=3270283)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3270283)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3270283)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3270283)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3270283)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3270283)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3270283)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3270283)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3270283)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3270283) torch._dynamo.exc.Unsupported: Graph break under GenericContextWrappingVariable
(EngineCore_DP0 pid=3270283)   Explanation: Attempted to graph break in an active context manager(s) that doesn't support graph breaking.
(EngineCore_DP0 pid=3270283)   Hint: Move the offending context manager(s) to outside the compiled region.
(EngineCore_DP0 pid=3270283)   Hint: This graph break may have been caused by an earlier graph break. Resolving the earlier graph break may resolve this one.
(EngineCore_DP0 pid=3270283) 
(EngineCore_DP0 pid=3270283)   Developer debug context: Active generic context managers: [GenericContextWrappingVariable(ProfileTimer)]
(EngineCore_DP0 pid=3270283) 
(EngineCore_DP0 pid=3270283)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0066.html
(EngineCore_DP0 pid=3270283) 
(EngineCore_DP0 pid=3270283) from user code:
(EngineCore_DP0 pid=3270283)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3270283)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3270283)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3270283)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3270283)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3270283)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1144, in apply_weights
(EngineCore_DP0 pid=3270283)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 947, in apply
(EngineCore_DP0 pid=3270283)     return self._linear_fn(
(EngineCore_DP0 pid=3270283)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 738, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3270283)     ext = _get_gemm_extension("cusparselt")
(EngineCore_DP0 pid=3270283) 
(EngineCore_DP0 pid=3270283) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3270283) 
[rank0]:[W122 00:29:57.043884735 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:34:37
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:34:40 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:34:40 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:34:40 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:34:40 [model.py:1661] Using max model len 145
INFO 01-22 00:34:41 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3274356) INFO 01-22 00:34:44 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3274356) INFO 01-22 00:34:45 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:35735 backend=nccl
(EngineCore_DP0 pid=3274356) INFO 01-22 00:34:45 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3274356) INFO 01-22 00:34:45 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3274356) INFO 01-22 00:34:45 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3274356) INFO 01-22 00:34:45 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3274356) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3274356) INFO 01-22 00:34:46 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.328957 seconds
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866] torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866] 
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866] 
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866] 
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866] from user code:
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1158, in apply_weights
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 961, in apply
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 752, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     ext = _get_gemm_extension("cusparselt")
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 445, in _get_gemm_extension
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     so_path = find_file(so_prefix, search_dir=build_dir, ext=".so")
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     if not search_dir.exists():
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866]     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866] 
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3274356) ERROR 01-22 00:34:46 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1205: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:895: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1031: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1036: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3274356) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3274356) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.34it/s]
(EngineCore_DP0 pid=3274356) 
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1136: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1146: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1136: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1146: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1136: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1146: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1136: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3274356) [2026-01-22 00:34:45] INFO SlideSparseLinearMethod_FP8.py:1146: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3274356) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3274356) If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3274356) If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3274356)   torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))
(EngineCore_DP0 pid=3274356) Process EngineCore_DP0:
(EngineCore_DP0 pid=3274356) Traceback (most recent call last):
(EngineCore_DP0 pid=3274356)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3274356)     self.run()
(EngineCore_DP0 pid=3274356)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3274356)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3274356)     raise e
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3274356)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3274356)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3274356)     super().__init__(
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3274356)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3274356)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3274356)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3274356)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3274356)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3274356)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3274356)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3274356)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3274356)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3274356)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3274356)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3274356)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3274356)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3274356)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3274356)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3274356)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3274356)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3274356)     outputs = self.model(
(EngineCore_DP0 pid=3274356)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3274356)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3274356)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3274356)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3274356)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3274356)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3274356)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3274356)     hidden_states = self.model(
(EngineCore_DP0 pid=3274356)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3274356)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3274356)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3274356)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3274356)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3274356)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3274356)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3274356)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3274356)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3274356) torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3274356)   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3274356)   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3274356)   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3274356) 
(EngineCore_DP0 pid=3274356)   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3274356) 
(EngineCore_DP0 pid=3274356)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3274356) 
(EngineCore_DP0 pid=3274356) from user code:
(EngineCore_DP0 pid=3274356)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3274356)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3274356)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3274356)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3274356)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3274356)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1158, in apply_weights
(EngineCore_DP0 pid=3274356)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 961, in apply
(EngineCore_DP0 pid=3274356)     return self._linear_fn(
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 752, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3274356)     ext = _get_gemm_extension("cusparselt")
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 445, in _get_gemm_extension
(EngineCore_DP0 pid=3274356)     so_path = find_file(so_prefix, search_dir=build_dir, ext=".so")
(EngineCore_DP0 pid=3274356)   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3274356)     if not search_dir.exists():
(EngineCore_DP0 pid=3274356)   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3274356)     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3274356)   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3274356)     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3274356) 
(EngineCore_DP0 pid=3274356) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3274356) 
[rank0]:[W122 00:34:46.641415377 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:36:13
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:36:17 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:36:17 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:36:17 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:36:17 [model.py:1661] Using max model len 145
INFO 01-22 00:36:17 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3276092) INFO 01-22 00:36:21 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3276092) INFO 01-22 00:36:21 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:43301 backend=nccl
(EngineCore_DP0 pid=3276092) INFO 01-22 00:36:21 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3276092) INFO 01-22 00:36:21 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3276092) INFO 01-22 00:36:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3276092) INFO 01-22 00:36:22 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3276092) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3276092) INFO 01-22 00:36:22 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.326902 seconds
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866] torch._dynamo.exc.Unsupported: Skip calling `torch.compiler.disable()`d function
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   Explanation: Skip calling function `<function _get_gemm_extension at 0x71907219d3a0>` since it was wrapped with `torch.compiler.disable` (reason: None)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   Hint: Remove the `torch.compiler.disable` call
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866] 
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   Developer debug context: <function _get_gemm_extension at 0x71907219d3a0>
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866] 
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0098.html
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866] 
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866] from user code:
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1162, in apply_weights
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 965, in apply
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 756, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866]     ext = _get_gemm_extension("cusparselt")
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866] 
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3276092) ERROR 01-22 00:36:22 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1209: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:899: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1035: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1040: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3276092) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3276092) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.38it/s]
(EngineCore_DP0 pid=3276092) 
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1140: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1150: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1140: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1150: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1140: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1150: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1140: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3276092) [2026-01-22 00:36:22] INFO SlideSparseLinearMethod_FP8.py:1150: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3276092) Process EngineCore_DP0:
(EngineCore_DP0 pid=3276092) Traceback (most recent call last):
(EngineCore_DP0 pid=3276092)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3276092)     self.run()
(EngineCore_DP0 pid=3276092)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3276092)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3276092)     raise e
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3276092)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3276092)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3276092)     super().__init__(
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3276092)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3276092)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3276092)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3276092)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3276092)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3276092)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3276092)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3276092)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3276092)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3276092)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3276092)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3276092)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3276092)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3276092)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3276092)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3276092)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3276092)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3276092)     outputs = self.model(
(EngineCore_DP0 pid=3276092)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3276092)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3276092)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3276092)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3276092)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3276092)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3276092)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3276092)     hidden_states = self.model(
(EngineCore_DP0 pid=3276092)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3276092)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3276092)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3276092)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3276092)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3276092)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3276092)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3276092)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3276092)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3276092) torch._dynamo.exc.Unsupported: Skip calling `torch.compiler.disable()`d function
(EngineCore_DP0 pid=3276092)   Explanation: Skip calling function `<function _get_gemm_extension at 0x71907219d3a0>` since it was wrapped with `torch.compiler.disable` (reason: None)
(EngineCore_DP0 pid=3276092)   Hint: Remove the `torch.compiler.disable` call
(EngineCore_DP0 pid=3276092) 
(EngineCore_DP0 pid=3276092)   Developer debug context: <function _get_gemm_extension at 0x71907219d3a0>
(EngineCore_DP0 pid=3276092) 
(EngineCore_DP0 pid=3276092)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0098.html
(EngineCore_DP0 pid=3276092) 
(EngineCore_DP0 pid=3276092) from user code:
(EngineCore_DP0 pid=3276092)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3276092)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3276092)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3276092)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3276092)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3276092)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1162, in apply_weights
(EngineCore_DP0 pid=3276092)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 965, in apply
(EngineCore_DP0 pid=3276092)     return self._linear_fn(
(EngineCore_DP0 pid=3276092)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 756, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3276092)     ext = _get_gemm_extension("cusparselt")
(EngineCore_DP0 pid=3276092) 
(EngineCore_DP0 pid=3276092) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3276092) 
[rank0]:[W122 00:36:23.027249608 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:42:26
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:42:30 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:42:30 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:42:30 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:42:30 [model.py:1661] Using max model len 145
INFO 01-22 00:42:30 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3281085) INFO 01-22 00:42:34 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3281085) INFO 01-22 00:42:34 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:57841 backend=nccl
(EngineCore_DP0 pid=3281085) INFO 01-22 00:42:34 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3281085) INFO 01-22 00:42:34 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3281085) INFO 01-22 00:42:34 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3281085) INFO 01-22 00:42:34 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3281085) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3281085) INFO 01-22 00:42:35 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.327806 seconds
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866] torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866] 
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866] 
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866] 
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866] from user code:
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1163, in apply_weights
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 966, in apply
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 765, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, L)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 628, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     fn = _load_quant_slide_fp8_kernel()
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 596, in _load_quant_slide_fp8_kernel
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     module = load_module("quant_slide_tuned", search_dir=build_dir, ext=".py")
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1680, in load_module
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     module_path = find_file(
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     if not search_dir.exists():
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866]     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866] 
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3281085) ERROR 01-22 00:42:35 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1210: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:463: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:900: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1036: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1041: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3281085) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3281085) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.51it/s]
(EngineCore_DP0 pid=3281085) 
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1141: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1151: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1141: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1151: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1141: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1151: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1141: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3281085) [2026-01-22 00:42:34] INFO SlideSparseLinearMethod_FP8.py:1151: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3281085) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3281085) If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3281085) If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3281085)   torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))
(EngineCore_DP0 pid=3281085) Process EngineCore_DP0:
(EngineCore_DP0 pid=3281085) Traceback (most recent call last):
(EngineCore_DP0 pid=3281085)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3281085)     self.run()
(EngineCore_DP0 pid=3281085)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3281085)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3281085)     raise e
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3281085)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3281085)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3281085)     super().__init__(
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3281085)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3281085)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3281085)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3281085)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3281085)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3281085)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3281085)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3281085)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3281085)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281085)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3281085)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281085)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3281085)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3281085)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3281085)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3281085)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281085)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3281085)     outputs = self.model(
(EngineCore_DP0 pid=3281085)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3281085)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3281085)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3281085)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3281085)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3281085)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3281085)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3281085)     hidden_states = self.model(
(EngineCore_DP0 pid=3281085)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3281085)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3281085)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3281085)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3281085)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3281085)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3281085)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3281085)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3281085)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281085) torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3281085)   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3281085)   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3281085)   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3281085) 
(EngineCore_DP0 pid=3281085)   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3281085) 
(EngineCore_DP0 pid=3281085)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3281085) 
(EngineCore_DP0 pid=3281085) from user code:
(EngineCore_DP0 pid=3281085)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3281085)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3281085)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3281085)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3281085)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3281085)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1163, in apply_weights
(EngineCore_DP0 pid=3281085)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 966, in apply
(EngineCore_DP0 pid=3281085)     return self._linear_fn(
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 765, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3281085)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, L)
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 628, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=3281085)     fn = _load_quant_slide_fp8_kernel()
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 596, in _load_quant_slide_fp8_kernel
(EngineCore_DP0 pid=3281085)     module = load_module("quant_slide_tuned", search_dir=build_dir, ext=".py")
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/slidesparse/utils.py", line 1680, in load_module
(EngineCore_DP0 pid=3281085)     module_path = find_file(
(EngineCore_DP0 pid=3281085)   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3281085)     if not search_dir.exists():
(EngineCore_DP0 pid=3281085)   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3281085)     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3281085)   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3281085)     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3281085) 
(EngineCore_DP0 pid=3281085) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3281085) 
[rank0]:[W122 00:42:35.645601682 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:42:55
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:42:58 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:42:58 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:42:58 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:42:58 [model.py:1661] Using max model len 145
INFO 01-22 00:42:59 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3281779) INFO 01-22 00:43:02 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3281779) INFO 01-22 00:43:02 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:44771 backend=nccl
(EngineCore_DP0 pid=3281779) INFO 01-22 00:43:02 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3281779) INFO 01-22 00:43:03 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3281779) INFO 01-22 00:43:03 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3281779) INFO 01-22 00:43:03 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3281779) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3281779) INFO 01-22 00:43:04 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.382716 seconds
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866] torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866] 
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866] 
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866] 
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866] from user code:
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1163, in apply_weights
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 966, in apply
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 765, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, L)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 628, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     fn = _load_quant_slide_fp8_kernel()
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 596, in _load_quant_slide_fp8_kernel
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     module = load_module("quant_slide_tuned", search_dir=build_dir, ext=".py")
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1680, in load_module
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     module_path = find_file(
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     if not search_dir.exists():
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866]     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866] 
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3281779) ERROR 01-22 00:43:04 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1210: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:463: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:900: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1036: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1041: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3281779) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3281779) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.33it/s]
(EngineCore_DP0 pid=3281779) 
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1141: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1151: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1141: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1151: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1141: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1151: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1141: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3281779) [2026-01-22 00:43:03] INFO SlideSparseLinearMethod_FP8.py:1151: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3281779) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3281779) If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3281779) If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3281779)   torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))
(EngineCore_DP0 pid=3281779) Process EngineCore_DP0:
(EngineCore_DP0 pid=3281779) Traceback (most recent call last):
(EngineCore_DP0 pid=3281779)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3281779)     self.run()
(EngineCore_DP0 pid=3281779)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3281779)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3281779)     raise e
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3281779)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3281779)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3281779)     super().__init__(
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3281779)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3281779)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3281779)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3281779)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3281779)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3281779)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3281779)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3281779)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3281779)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281779)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3281779)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281779)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3281779)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3281779)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3281779)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3281779)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3281779)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3281779)     outputs = self.model(
(EngineCore_DP0 pid=3281779)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3281779)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3281779)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3281779)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3281779)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3281779)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3281779)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3281779)     hidden_states = self.model(
(EngineCore_DP0 pid=3281779)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3281779)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3281779)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3281779)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3281779)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3281779)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3281779)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3281779)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3281779)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3281779) torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3281779)   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3281779)   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3281779)   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3281779) 
(EngineCore_DP0 pid=3281779)   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3281779) 
(EngineCore_DP0 pid=3281779)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3281779) 
(EngineCore_DP0 pid=3281779) from user code:
(EngineCore_DP0 pid=3281779)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3281779)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3281779)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3281779)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3281779)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3281779)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1163, in apply_weights
(EngineCore_DP0 pid=3281779)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 966, in apply
(EngineCore_DP0 pid=3281779)     return self._linear_fn(
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 765, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3281779)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, L)
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 628, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=3281779)     fn = _load_quant_slide_fp8_kernel()
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 596, in _load_quant_slide_fp8_kernel
(EngineCore_DP0 pid=3281779)     module = load_module("quant_slide_tuned", search_dir=build_dir, ext=".py")
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/slidesparse/utils.py", line 1680, in load_module
(EngineCore_DP0 pid=3281779)     module_path = find_file(
(EngineCore_DP0 pid=3281779)   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3281779)     if not search_dir.exists():
(EngineCore_DP0 pid=3281779)   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3281779)     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3281779)   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3281779)     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3281779) 
(EngineCore_DP0 pid=3281779) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3281779) 
[rank0]:[W122 00:43:04.469175962 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:45:14
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:45:17 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:45:17 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:45:17 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:45:17 [model.py:1661] Using max model len 145
INFO 01-22 00:45:18 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3283913) INFO 01-22 00:45:21 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3283913) INFO 01-22 00:45:21 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:38621 backend=nccl
(EngineCore_DP0 pid=3283913) INFO 01-22 00:45:21 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3283913) INFO 01-22 00:45:22 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3283913) INFO 01-22 00:45:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3283913) INFO 01-22 00:45:22 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3283913) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3283913) INFO 01-22 00:45:22 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.331327 seconds
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866] torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866] 
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866] 
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866] 
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866] from user code:
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1165, in apply_weights
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 968, in apply
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 767, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, L)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 628, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     fn = _load_quant_slide_fp8_kernel()
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 596, in _load_quant_slide_fp8_kernel
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     module = load_module("quant_slide_tuned", search_dir=build_dir, ext=".py")
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1680, in load_module
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     module_path = find_file(
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     if not search_dir.exists():
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866]     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866] 
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3283913) ERROR 01-22 00:45:23 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1212: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:463: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:902: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1038: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1043: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3283913) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3283913) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.11it/s]
(EngineCore_DP0 pid=3283913) 
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3283913) [2026-01-22 00:45:22] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3283913) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3283913) If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3283913) If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3283913)   torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))
(EngineCore_DP0 pid=3283913) Process EngineCore_DP0:
(EngineCore_DP0 pid=3283913) Traceback (most recent call last):
(EngineCore_DP0 pid=3283913)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3283913)     self.run()
(EngineCore_DP0 pid=3283913)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3283913)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3283913)     raise e
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3283913)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3283913)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3283913)     super().__init__(
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3283913)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3283913)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3283913)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3283913)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3283913)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3283913)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3283913)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3283913)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3283913)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3283913)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3283913)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3283913)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3283913)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3283913)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3283913)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3283913)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3283913)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3283913)     outputs = self.model(
(EngineCore_DP0 pid=3283913)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3283913)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3283913)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3283913)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3283913)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3283913)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3283913)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3283913)     hidden_states = self.model(
(EngineCore_DP0 pid=3283913)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3283913)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3283913)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3283913)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3283913)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3283913)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3283913)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3283913)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3283913)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3283913) torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3283913)   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3283913)   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3283913)   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3283913) 
(EngineCore_DP0 pid=3283913)   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3283913) 
(EngineCore_DP0 pid=3283913)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3283913) 
(EngineCore_DP0 pid=3283913) from user code:
(EngineCore_DP0 pid=3283913)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3283913)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3283913)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3283913)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3283913)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3283913)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1165, in apply_weights
(EngineCore_DP0 pid=3283913)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 968, in apply
(EngineCore_DP0 pid=3283913)     return self._linear_fn(
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 767, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3283913)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, L)
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 628, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=3283913)     fn = _load_quant_slide_fp8_kernel()
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 596, in _load_quant_slide_fp8_kernel
(EngineCore_DP0 pid=3283913)     module = load_module("quant_slide_tuned", search_dir=build_dir, ext=".py")
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/slidesparse/utils.py", line 1680, in load_module
(EngineCore_DP0 pid=3283913)     module_path = find_file(
(EngineCore_DP0 pid=3283913)   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3283913)     if not search_dir.exists():
(EngineCore_DP0 pid=3283913)   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3283913)     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3283913)   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3283913)     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3283913) 
(EngineCore_DP0 pid=3283913) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3283913) 
[rank0]:[W122 00:45:23.267176218 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:45:40
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:45:44 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:45:44 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:45:44 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:45:44 [model.py:1661] Using max model len 145
INFO 01-22 00:45:44 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3284588) INFO 01-22 00:45:47 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3284588) INFO 01-22 00:45:48 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:52979 backend=nccl
(EngineCore_DP0 pid=3284588) INFO 01-22 00:45:48 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3284588) INFO 01-22 00:45:49 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3284588) INFO 01-22 00:45:49 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3284588) INFO 01-22 00:45:49 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3284588) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3284588) INFO 01-22 00:45:49 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.335619 seconds
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866] torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866] 
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866] 
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866] 
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866] from user code:
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1165, in apply_weights
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 968, in apply
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 767, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, L)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 628, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     fn = _load_quant_slide_fp8_kernel()
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 596, in _load_quant_slide_fp8_kernel
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     module = load_module("quant_slide_tuned", search_dir=build_dir, ext=".py")
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1680, in load_module
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     module_path = find_file(
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     if not search_dir.exists():
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866]     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866] 
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3284588) ERROR 01-22 00:45:50 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1212: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:463: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:902: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1038: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1043: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3284588) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3284588) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.30it/s]
(EngineCore_DP0 pid=3284588) 
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3284588) [2026-01-22 00:45:49] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3284588) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3284588) If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3284588) If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3284588)   torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))
(EngineCore_DP0 pid=3284588) Process EngineCore_DP0:
(EngineCore_DP0 pid=3284588) Traceback (most recent call last):
(EngineCore_DP0 pid=3284588)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3284588)     self.run()
(EngineCore_DP0 pid=3284588)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3284588)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3284588)     raise e
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3284588)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3284588)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3284588)     super().__init__(
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3284588)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3284588)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3284588)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3284588)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3284588)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3284588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3284588)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3284588)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3284588)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3284588)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3284588)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3284588)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3284588)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3284588)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3284588)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3284588)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3284588)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3284588)     outputs = self.model(
(EngineCore_DP0 pid=3284588)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3284588)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3284588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3284588)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3284588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3284588)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3284588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3284588)     hidden_states = self.model(
(EngineCore_DP0 pid=3284588)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3284588)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3284588)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3284588)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3284588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3284588)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3284588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3284588)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3284588)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3284588) torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3284588)   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3284588)   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3284588)   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3284588) 
(EngineCore_DP0 pid=3284588)   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3284588) 
(EngineCore_DP0 pid=3284588)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3284588) 
(EngineCore_DP0 pid=3284588) from user code:
(EngineCore_DP0 pid=3284588)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3284588)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3284588)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3284588)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3284588)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3284588)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1165, in apply_weights
(EngineCore_DP0 pid=3284588)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 968, in apply
(EngineCore_DP0 pid=3284588)     return self._linear_fn(
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 767, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3284588)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, L)
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 628, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=3284588)     fn = _load_quant_slide_fp8_kernel()
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 596, in _load_quant_slide_fp8_kernel
(EngineCore_DP0 pid=3284588)     module = load_module("quant_slide_tuned", search_dir=build_dir, ext=".py")
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/slidesparse/utils.py", line 1680, in load_module
(EngineCore_DP0 pid=3284588)     module_path = find_file(
(EngineCore_DP0 pid=3284588)   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3284588)     if not search_dir.exists():
(EngineCore_DP0 pid=3284588)   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3284588)     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3284588)   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3284588)     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3284588) 
(EngineCore_DP0 pid=3284588) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3284588) 
[rank0]:[W122 00:45:50.199386927 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:46:10
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:46:14 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:46:14 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:46:14 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:46:14 [model.py:1661] Using max model len 145
INFO 01-22 00:46:14 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3285314) INFO 01-22 00:46:18 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3285314) INFO 01-22 00:46:18 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:54719 backend=nccl
(EngineCore_DP0 pid=3285314) INFO 01-22 00:46:18 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3285314) INFO 01-22 00:46:19 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3285314) INFO 01-22 00:46:19 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3285314) INFO 01-22 00:46:19 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3285314) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3285314) INFO 01-22 00:46:19 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.323520 seconds
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866] torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866] 
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866] 
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866] 
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866] from user code:
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1165, in apply_weights
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 968, in apply
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 767, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, L)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 628, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     fn = _load_quant_slide_fp8_kernel()
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 596, in _load_quant_slide_fp8_kernel
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     module = load_module("quant_slide_tuned", search_dir=build_dir, ext=".py")
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1680, in load_module
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     module_path = find_file(
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     if not search_dir.exists():
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866]     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866] 
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3285314) ERROR 01-22 00:46:20 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1212: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:463: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:902: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1038: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1043: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3285314) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3285314) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.55it/s]
(EngineCore_DP0 pid=3285314) 
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1143: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3285314) [2026-01-22 00:46:19] INFO SlideSparseLinearMethod_FP8.py:1153: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3285314) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3285314) If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3285314) If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3285314)   torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))
(EngineCore_DP0 pid=3285314) Process EngineCore_DP0:
(EngineCore_DP0 pid=3285314) Traceback (most recent call last):
(EngineCore_DP0 pid=3285314)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3285314)     self.run()
(EngineCore_DP0 pid=3285314)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3285314)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3285314)     raise e
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3285314)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3285314)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3285314)     super().__init__(
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3285314)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3285314)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3285314)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3285314)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3285314)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3285314)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3285314)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3285314)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3285314)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3285314)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3285314)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3285314)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3285314)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3285314)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3285314)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3285314)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3285314)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3285314)     outputs = self.model(
(EngineCore_DP0 pid=3285314)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3285314)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3285314)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3285314)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3285314)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3285314)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3285314)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3285314)     hidden_states = self.model(
(EngineCore_DP0 pid=3285314)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3285314)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3285314)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3285314)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3285314)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3285314)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3285314)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3285314)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3285314)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3285314) torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
(EngineCore_DP0 pid=3285314)   Explanation: Dynamo does not know how to trace the builtin `posix.stat.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
(EngineCore_DP0 pid=3285314)   Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
(EngineCore_DP0 pid=3285314)   Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
(EngineCore_DP0 pid=3285314) 
(EngineCore_DP0 pid=3285314)   Developer debug context: module: posix, qualname: stat, skip reason: <missing reason>
(EngineCore_DP0 pid=3285314) 
(EngineCore_DP0 pid=3285314)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html
(EngineCore_DP0 pid=3285314) 
(EngineCore_DP0 pid=3285314) from user code:
(EngineCore_DP0 pid=3285314)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3285314)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3285314)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3285314)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3285314)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3285314)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1165, in apply_weights
(EngineCore_DP0 pid=3285314)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 968, in apply
(EngineCore_DP0 pid=3285314)     return self._linear_fn(
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 767, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3285314)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, L)
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 628, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=3285314)     fn = _load_quant_slide_fp8_kernel()
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 596, in _load_quant_slide_fp8_kernel
(EngineCore_DP0 pid=3285314)     module = load_module("quant_slide_tuned", search_dir=build_dir, ext=".py")
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/slidesparse/utils.py", line 1680, in load_module
(EngineCore_DP0 pid=3285314)     module_path = find_file(
(EngineCore_DP0 pid=3285314)   File "/root/vllmbench/slidesparse/utils.py", line 1489, in find_file
(EngineCore_DP0 pid=3285314)     if not search_dir.exists():
(EngineCore_DP0 pid=3285314)   File "/usr/lib/python3.12/pathlib.py", line 860, in exists
(EngineCore_DP0 pid=3285314)     self.stat(follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3285314)   File "/usr/lib/python3.12/pathlib.py", line 840, in stat
(EngineCore_DP0 pid=3285314)     return os.stat(self, follow_symlinks=follow_symlinks)
(EngineCore_DP0 pid=3285314) 
(EngineCore_DP0 pid=3285314) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3285314) 
[rank0]:[W122 00:46:20.265647368 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:49:05
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:49:09 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:49:09 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:49:09 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:49:09 [model.py:1661] Using max model len 145
INFO 01-22 00:49:09 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3287786) INFO 01-22 00:49:13 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3287786) INFO 01-22 00:49:13 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:44667 backend=nccl
(EngineCore_DP0 pid=3287786) INFO 01-22 00:49:13 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3287786) INFO 01-22 00:49:13 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3287786) INFO 01-22 00:49:14 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3287786) INFO 01-22 00:49:14 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3287786) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3287786) INFO 01-22 00:49:14 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.327193 seconds
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866] torch._dynamo.exc.Unsupported: Unsupported method call
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   Explanation: Dynamo does not know how to trace method `__call__` of class `_FuncPtr`
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   Hint: Avoid calling `_FuncPtr.__call__` in your code.
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   Hint: Please report an issue to PyTorch.
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866] 
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   Developer debug context: call_method UserDefinedObjectVariable(_FuncPtr) __call__ [DataPtrVariable(), DataPtrVariable(), DataPtrVariable(), SymNodeVariable(), ConstantVariable(int: 1152), ConstantVariable(int: 1216), ConstantVariable(bytes: b'bf16'), GetAttrVariable(StreamVariable(), cuda_stream)] {}
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866] 
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0156.html
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866] 
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866] from user code:
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1166, in apply_weights
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 969, in apply
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 782, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     gemm_out_pad = ext.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 404, in cusparselt_fp8_mm
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866]     ret = self._lib.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866] 
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3287786) ERROR 01-22 00:49:14 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:13] INFO SlideSparseLinearMethod_FP8.py:1213: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:13] INFO SlideSparseLinearMethod_FP8.py:463: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:13] INFO SlideSparseLinearMethod_FP8.py:605: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:13] INFO SlideSparseLinearMethod_FP8.py:495: Dequant+bias kernel loaded
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:13] INFO SlideSparseLinearMethod_FP8.py:903: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:13] INFO SlideSparseLinearMethod_FP8.py:1039: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:13] INFO SlideSparseLinearMethod_FP8.py:1044: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3287786) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3287786) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.59it/s]
(EngineCore_DP0 pid=3287786) 
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:14] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:14] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:14] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:14] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:14] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:14] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:14] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3287786) [2026-01-22 00:49:14] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3287786) Process EngineCore_DP0:
(EngineCore_DP0 pid=3287786) Traceback (most recent call last):
(EngineCore_DP0 pid=3287786)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3287786)     self.run()
(EngineCore_DP0 pid=3287786)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3287786)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3287786)     raise e
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3287786)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3287786)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3287786)     super().__init__(
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3287786)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3287786)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3287786)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3287786)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3287786)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3287786)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3287786)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3287786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3287786)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3287786)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3287786)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3287786)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3287786)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3287786)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3287786)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3287786)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3287786)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3287786)     outputs = self.model(
(EngineCore_DP0 pid=3287786)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3287786)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3287786)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3287786)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3287786)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3287786)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3287786)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3287786)     hidden_states = self.model(
(EngineCore_DP0 pid=3287786)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3287786)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3287786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3287786)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3287786)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3287786)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3287786)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3287786)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3287786)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3287786) torch._dynamo.exc.Unsupported: Unsupported method call
(EngineCore_DP0 pid=3287786)   Explanation: Dynamo does not know how to trace method `__call__` of class `_FuncPtr`
(EngineCore_DP0 pid=3287786)   Hint: Avoid calling `_FuncPtr.__call__` in your code.
(EngineCore_DP0 pid=3287786)   Hint: Please report an issue to PyTorch.
(EngineCore_DP0 pid=3287786) 
(EngineCore_DP0 pid=3287786)   Developer debug context: call_method UserDefinedObjectVariable(_FuncPtr) __call__ [DataPtrVariable(), DataPtrVariable(), DataPtrVariable(), SymNodeVariable(), ConstantVariable(int: 1152), ConstantVariable(int: 1216), ConstantVariable(bytes: b'bf16'), GetAttrVariable(StreamVariable(), cuda_stream)] {}
(EngineCore_DP0 pid=3287786) 
(EngineCore_DP0 pid=3287786)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0156.html
(EngineCore_DP0 pid=3287786) 
(EngineCore_DP0 pid=3287786) from user code:
(EngineCore_DP0 pid=3287786)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3287786)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3287786)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3287786)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3287786)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3287786)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1166, in apply_weights
(EngineCore_DP0 pid=3287786)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 969, in apply
(EngineCore_DP0 pid=3287786)     return self._linear_fn(
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 782, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3287786)     gemm_out_pad = ext.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3287786)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 404, in cusparselt_fp8_mm
(EngineCore_DP0 pid=3287786)     ret = self._lib.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3287786) 
(EngineCore_DP0 pid=3287786) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3287786) 
[rank0]:[W122 00:49:15.966482365 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:49:34
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:49:38 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:49:38 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:49:38 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:49:38 [model.py:1661] Using max model len 145
INFO 01-22 00:49:38 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3288469) INFO 01-22 00:49:41 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3288469) INFO 01-22 00:49:42 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:58365 backend=nccl
(EngineCore_DP0 pid=3288469) INFO 01-22 00:49:42 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3288469) INFO 01-22 00:49:42 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3288469) INFO 01-22 00:49:42 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3288469) INFO 01-22 00:49:42 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3288469) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3288469) INFO 01-22 00:49:43 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.331672 seconds
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866] torch._dynamo.exc.Unsupported: Unsupported method call
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   Explanation: Dynamo does not know how to trace method `__call__` of class `_FuncPtr`
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   Hint: Avoid calling `_FuncPtr.__call__` in your code.
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   Hint: Please report an issue to PyTorch.
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866] 
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   Developer debug context: call_method UserDefinedObjectVariable(_FuncPtr) __call__ [DataPtrVariable(), DataPtrVariable(), DataPtrVariable(), SymNodeVariable(), ConstantVariable(int: 1152), ConstantVariable(int: 1216), ConstantVariable(bytes: b'bf16'), GetAttrVariable(StreamVariable(), cuda_stream)] {}
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866] 
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0156.html
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866] 
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866] from user code:
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1166, in apply_weights
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 969, in apply
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 782, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     gemm_out_pad = ext.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 404, in cusparselt_fp8_mm
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866]     ret = self._lib.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866] 
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3288469) ERROR 01-22 00:49:43 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:42] INFO SlideSparseLinearMethod_FP8.py:1213: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:42] INFO SlideSparseLinearMethod_FP8.py:463: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:42] INFO SlideSparseLinearMethod_FP8.py:605: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:42] INFO SlideSparseLinearMethod_FP8.py:495: Dequant+bias kernel loaded
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:42] INFO SlideSparseLinearMethod_FP8.py:903: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:42] INFO SlideSparseLinearMethod_FP8.py:1039: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:42] INFO SlideSparseLinearMethod_FP8.py:1044: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3288469) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3288469) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.20it/s]
(EngineCore_DP0 pid=3288469) 
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:42] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:43] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:43] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:43] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:43] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:43] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:43] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3288469) [2026-01-22 00:49:43] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3288469) Process EngineCore_DP0:
(EngineCore_DP0 pid=3288469) Traceback (most recent call last):
(EngineCore_DP0 pid=3288469)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3288469)     self.run()
(EngineCore_DP0 pid=3288469)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3288469)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3288469)     raise e
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3288469)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3288469)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3288469)     super().__init__(
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3288469)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3288469)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3288469)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3288469)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3288469)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3288469)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3288469)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3288469)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3288469)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3288469)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3288469)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3288469)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3288469)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3288469)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3288469)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3288469)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3288469)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3288469)     outputs = self.model(
(EngineCore_DP0 pid=3288469)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3288469)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3288469)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3288469)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3288469)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3288469)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3288469)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3288469)     hidden_states = self.model(
(EngineCore_DP0 pid=3288469)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3288469)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3288469)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3288469)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3288469)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3288469)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3288469)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3288469)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3288469)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3288469) torch._dynamo.exc.Unsupported: Unsupported method call
(EngineCore_DP0 pid=3288469)   Explanation: Dynamo does not know how to trace method `__call__` of class `_FuncPtr`
(EngineCore_DP0 pid=3288469)   Hint: Avoid calling `_FuncPtr.__call__` in your code.
(EngineCore_DP0 pid=3288469)   Hint: Please report an issue to PyTorch.
(EngineCore_DP0 pid=3288469) 
(EngineCore_DP0 pid=3288469)   Developer debug context: call_method UserDefinedObjectVariable(_FuncPtr) __call__ [DataPtrVariable(), DataPtrVariable(), DataPtrVariable(), SymNodeVariable(), ConstantVariable(int: 1152), ConstantVariable(int: 1216), ConstantVariable(bytes: b'bf16'), GetAttrVariable(StreamVariable(), cuda_stream)] {}
(EngineCore_DP0 pid=3288469) 
(EngineCore_DP0 pid=3288469)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0156.html
(EngineCore_DP0 pid=3288469) 
(EngineCore_DP0 pid=3288469) from user code:
(EngineCore_DP0 pid=3288469)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3288469)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3288469)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3288469)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3288469)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3288469)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1166, in apply_weights
(EngineCore_DP0 pid=3288469)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 969, in apply
(EngineCore_DP0 pid=3288469)     return self._linear_fn(
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 782, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3288469)     gemm_out_pad = ext.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3288469)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 404, in cusparselt_fp8_mm
(EngineCore_DP0 pid=3288469)     ret = self._lib.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3288469) 
(EngineCore_DP0 pid=3288469) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3288469) 
[rank0]:[W122 00:49:43.712450739 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=16 ==========
Time: 2026-01-22 00:50:10
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.9 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-0.5B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-22 00:50:13 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 00:50:14 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', 'max_model_len': 145, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6'}
INFO 01-22 00:50:14 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 00:50:14 [model.py:1661] Using max model len 145
INFO 01-22 00:50:14 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=3289338) INFO 01-22 00:50:18 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=3289338) INFO 01-22 00:50:18 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:51909 backend=nccl
(EngineCore_DP0 pid=3289338) INFO 01-22 00:50:18 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3289338) INFO 01-22 00:50:18 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=3289338) INFO 01-22 00:50:18 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=3289338) INFO 01-22 00:50:19 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=3289338) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3289338) INFO 01-22 00:50:19 [gpu_model_runner.py:3659] Model loading took 0.5444 GiB memory and 0.335823 seconds
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866] torch._dynamo.exc.Unsupported: Unsupported method call
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   Explanation: Dynamo does not know how to trace method `__call__` of class `_FuncPtr`
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   Hint: Avoid calling `_FuncPtr.__call__` in your code.
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   Hint: Please report an issue to PyTorch.
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866] 
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   Developer debug context: call_method UserDefinedObjectVariable(_FuncPtr) __call__ [DataPtrVariable(), DataPtrVariable(), DataPtrVariable(), SymNodeVariable(), ConstantVariable(int: 1152), ConstantVariable(int: 1216), ConstantVariable(bytes: b'bf16'), GetAttrVariable(StreamVariable(), cuda_stream)] {}
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866] 
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0156.html
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866] 
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866] from user code:
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1166, in apply_weights
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 969, in apply
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 782, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     gemm_out_pad = ext.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 404, in cusparselt_fp8_mm
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866]     ret = self._lib.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866] 
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3289338) ERROR 01-22 00:50:19 [core.py:866] 

STDERR:
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:18] INFO SlideSparseLinearMethod_FP8.py:1213: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:18] INFO SlideSparseLinearMethod_FP8.py:463: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:18] INFO SlideSparseLinearMethod_FP8.py:605: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:18] INFO SlideSparseLinearMethod_FP8.py:495: Dequant+bias kernel loaded
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:18] INFO SlideSparseLinearMethod_FP8.py:903: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:18] INFO SlideSparseLinearMethod_FP8.py:1039: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:18] INFO SlideSparseLinearMethod_FP8.py:1044: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3289338) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3289338) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.83it/s]
(EngineCore_DP0 pid=3289338) 
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:19] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:19] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:19] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:19] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:19] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:19] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:19] INFO SlideSparseLinearMethod_FP8.py:1144: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=3289338) [2026-01-22 00:50:19] INFO SlideSparseLinearMethod_FP8.py:1154: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=3289338) Process EngineCore_DP0:
(EngineCore_DP0 pid=3289338) Traceback (most recent call last):
(EngineCore_DP0 pid=3289338)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3289338)     self.run()
(EngineCore_DP0 pid=3289338)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3289338)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=3289338)     raise e
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=3289338)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3289338)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=3289338)     super().__init__(
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3289338)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3289338)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=3289338)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=3289338)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=3289338)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=3289338)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3289338)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3289338)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=3289338)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3289338)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3289338)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3289338)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=3289338)     self.model_runner.profile_run()
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=3289338)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=3289338)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3289338)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3289338)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=3289338)     outputs = self.model(
(EngineCore_DP0 pid=3289338)               ^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=3289338)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3289338)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3289338)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3289338)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3289338)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3289338)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=3289338)     hidden_states = self.model(
(EngineCore_DP0 pid=3289338)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=3289338)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3289338)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=3289338)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=3289338)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=3289338)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=3289338)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 841, in compile_wrapper
(EngineCore_DP0 pid=3289338)     raise e.with_traceback(None) from e.__cause__  # User compiler error
(EngineCore_DP0 pid=3289338)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=3289338) torch._dynamo.exc.Unsupported: Unsupported method call
(EngineCore_DP0 pid=3289338)   Explanation: Dynamo does not know how to trace method `__call__` of class `_FuncPtr`
(EngineCore_DP0 pid=3289338)   Hint: Avoid calling `_FuncPtr.__call__` in your code.
(EngineCore_DP0 pid=3289338)   Hint: Please report an issue to PyTorch.
(EngineCore_DP0 pid=3289338) 
(EngineCore_DP0 pid=3289338)   Developer debug context: call_method UserDefinedObjectVariable(_FuncPtr) __call__ [DataPtrVariable(), DataPtrVariable(), DataPtrVariable(), SymNodeVariable(), ConstantVariable(int: 1152), ConstantVariable(int: 1216), ConstantVariable(bytes: b'bf16'), GetAttrVariable(StreamVariable(), cuda_stream)] {}
(EngineCore_DP0 pid=3289338) 
(EngineCore_DP0 pid=3289338)  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0156.html
(EngineCore_DP0 pid=3289338) 
(EngineCore_DP0 pid=3289338) from user code:
(EngineCore_DP0 pid=3289338)    File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 442, in forward
(EngineCore_DP0 pid=3289338)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 296, in forward
(EngineCore_DP0 pid=3289338)     hidden_states = self.self_attn(
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 206, in forward
(EngineCore_DP0 pid=3289338)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=3289338)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 951, in apply
(EngineCore_DP0 pid=3289338)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 1166, in apply_weights
(EngineCore_DP0 pid=3289338)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 969, in apply
(EngineCore_DP0 pid=3289338)     return self._linear_fn(
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 782, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=3289338)     gemm_out_pad = ext.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3289338)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 404, in cusparselt_fp8_mm
(EngineCore_DP0 pid=3289338)     ret = self._lib.cusparselt_fp8_mm(
(EngineCore_DP0 pid=3289338) 
(EngineCore_DP0 pid=3289338) Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
(EngineCore_DP0 pid=3289338) 
[rank0]:[W122 00:50:20.869988252 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16
