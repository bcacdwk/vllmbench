
========== M=16 ==========
Time: 2026-01-25 18:07:17
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:07:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 187.24 requests/s, 3183.07 total tokens/s, 187.24 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 18:07:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:07:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:07:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:07:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:07:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:07:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:21] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:07:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:07:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:07:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:07:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:07:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:07:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:07:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:07:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:07:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:07:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:07:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:25] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:07:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:07:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:07:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:07:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:07:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=271448) [2026-01-25 18:07:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=271448) [2026-01-25 18:07:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=271448) [2026-01-25 18:07:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=271448) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=271448) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.47it/s]
(EngineCore_DP0 pid=271448) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.47it/s]
(EngineCore_DP0 pid=271448) 
(EngineCore_DP0 pid=271448) 2026-01-25 18:07:34,427 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=271448) 2026-01-25 18:07:34,430 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=271448) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 119.54it/s]
(EngineCore_DP0 pid=271448) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4860.89it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:00, 155.31it/s, est. speed input: 2485.25 toks/s, output: 155.31 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:00, 178.97it/s, est. speed input: 2806.61 toks/s, output: 175.41 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 189.27it/s, est. speed input: 2946.35 toks/s, output: 184.14 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:00<00:00, 195.70it/s, est. speed input: 3031.71 toks/s, output: 189.48 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:00<00:00, 199.10it/s, est. speed input: 3081.65 toks/s, output: 192.60 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:00<00:00, 201.30it/s, est. speed input: 3116.12 toks/s, output: 194.76 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 201.30it/s, est. speed input: 3124.14 toks/s, output: 195.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 195.24it/s, est. speed input: 3124.14 toks/s, output: 195.26 toks/s]
[rank0]:[W125 18:07:36.215883450 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 18:07:37
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:07:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 186.00 requests/s, 23994.39 total tokens/s, 186.00 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 18:07:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:07:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:07:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:07:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:07:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:07:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:41] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:07:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:07:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:07:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:07:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:07:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:07:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:07:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:07:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:07:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:07:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:07:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:07:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:07:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:07:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:07:45] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:07:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:07:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:07:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:07:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:07:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=272018) [2026-01-25 18:07:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=272018) [2026-01-25 18:07:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=272018) [2026-01-25 18:07:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=272018) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=272018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.43it/s]
(EngineCore_DP0 pid=272018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.43it/s]
(EngineCore_DP0 pid=272018) 
(EngineCore_DP0 pid=272018) 2026-01-25 18:07:53,987 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=272018) 2026-01-25 18:07:53,990 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=272018) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 57.39it/s]
(EngineCore_DP0 pid=272018) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.79it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3309.77it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:00, 195.12it/s, est. speed input: 24976.72 toks/s, output: 195.12 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:00, 197.38it/s, est. speed input: 25220.93 toks/s, output: 197.04 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:00<00:00, 197.59it/s, est. speed input: 25255.18 toks/s, output: 197.30 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:00<00:00, 197.75it/s, est. speed input: 25276.74 toks/s, output: 197.47 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:00<00:00, 197.25it/s, est. speed input: 25248.94 toks/s, output: 197.25 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:00<00:00, 197.29it/s, est. speed input: 25250.66 toks/s, output: 197.27 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 197.29it/s, est. speed input: 25263.23 toks/s, output: 197.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 197.35it/s, est. speed input: 25263.23 toks/s, output: 197.37 toks/s]
[rank0]:[W125 18:07:55.773779040 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 18:07:57
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:08:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 156.19 requests/s, 40140.73 total tokens/s, 156.19 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 18:08:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:08:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:08:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:08:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:08:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:08:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:01] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:08:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:08:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:08:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:08:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:08:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:08:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:08:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:08:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:08:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:08:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:08:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:05] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:08:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:08:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:08:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:08:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:08:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=272558) [2026-01-25 18:08:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=272558) [2026-01-25 18:08:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=272558) [2026-01-25 18:08:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=272558) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=272558) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.53it/s]
(EngineCore_DP0 pid=272558) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.52it/s]
(EngineCore_DP0 pid=272558) 
(EngineCore_DP0 pid=272558) 2026-01-25 18:08:13,404 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=272558) 2026-01-25 18:08:13,406 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=272558) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 119.33it/s]
(EngineCore_DP0 pid=272558) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1692.19it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:00, 200.63it/s, est. speed input: 51364.56 toks/s, output: 200.63 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:00, 179.06it/s, est. speed input: 46593.03 toks/s, output: 182.00 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 173.26it/s, est. speed input: 45250.47 toks/s, output: 176.76 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:00<00:00, 171.51it/s, est. speed input: 44755.70 toks/s, output: 174.83 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:00<00:00, 170.75it/s, est. speed input: 44488.17 toks/s, output: 173.78 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:00<00:00, 170.05it/s, est. speed input: 44276.62 toks/s, output: 172.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 170.05it/s, est. speed input: 44157.09 toks/s, output: 172.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 172.46it/s, est. speed input: 44157.09 toks/s, output: 172.49 toks/s]
[rank0]:[W125 18:08:15.278818618 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

