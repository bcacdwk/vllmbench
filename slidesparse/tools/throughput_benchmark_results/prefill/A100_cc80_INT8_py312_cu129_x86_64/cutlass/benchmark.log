
========== M=16 ==========
Time: 2026-01-25 16:24:35
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:24:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 100.73 requests/s, 1712.38 total tokens/s, 100.73 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 16:24:43] INFO font_manager.py:1639: generated new fontManager
[2026-01-25 16:24:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:24:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:24:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:24:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:24:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:24:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:24:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:24:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:24:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:24:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:24:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:24:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:24:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:24:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:24:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:24:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:24:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:24:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:24:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:24:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:24:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:24:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:24:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:24:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:24:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:24:51] INFO kernels.py:719: Preloaded 20 Triton kernels from A100_cc80_py312_cu129_x86_64
[2026-01-25 16:24:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=4 models
[2026-01-25 16:24:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:24:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:24:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:24:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:24:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:24:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:24:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:24:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:24:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:24:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:24:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:24:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:24:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:24:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:24:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:24:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:24:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:24:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:24:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:24:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:24:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:24:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:24:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:24:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:24:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:24:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:24:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:24:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:24:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:24:59] INFO kernels.py:719: Preloaded 20 Triton kernels from A100_cc80_py312_cu129_x86_64
[2026-01-25 16:24:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=4 models
[2026-01-25 16:24:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:24:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:24:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:24:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=178513) [2026-01-25 16:25:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=178513) [2026-01-25 16:25:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=178513) [2026-01-25 16:25:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=178513) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=178513) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.32it/s]
(EngineCore_DP0 pid=178513) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.31it/s]
(EngineCore_DP0 pid=178513) 
(EngineCore_DP0 pid=178513) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 55.09it/s]
(EngineCore_DP0 pid=178513) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4102.67it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:27,  4.67it/s, est. speed input: 74.76 toks/s, output: 4.67 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 53.64it/s, est. speed input: 700.87 toks/s, output: 43.80 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 80.52it/s, est. speed input: 1025.21 toks/s, output: 64.07 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:00, 95.67it/s, est. speed input: 1216.37 toks/s, output: 76.02 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:00<00:00, 104.41it/s, est. speed input: 1339.61 toks/s, output: 83.72 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:00<00:00, 112.12it/s, est. speed input: 1439.35 toks/s, output: 89.96 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:00<00:00, 116.61it/s, est. speed input: 1511.15 toks/s, output: 94.44 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:00<00:00, 118.36it/s, est. speed input: 1561.16 toks/s, output: 97.57 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 118.51it/s, est. speed input: 1596.51 toks/s, output: 99.78 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:01<00:00, 119.05it/s, est. speed input: 1627.04 toks/s, output: 101.69 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 119.05it/s, est. speed input: 1653.67 toks/s, output: 103.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 103.34it/s, est. speed input: 1653.67 toks/s, output: 103.35 toks/s]
[rank0]:[W125 16:25:40.197304296 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 16:25:42
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:25:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 123.38 requests/s, 15915.51 total tokens/s, 123.38 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 16:25:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:25:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:25:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:25:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:25:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:25:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:25:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:25:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:25:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:25:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:25:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:25:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:25:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:25:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:25:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:25:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:25:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:25:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:25:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:25:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:25:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:25:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:25:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:25:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:25:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:25:50] INFO kernels.py:719: Preloaded 20 Triton kernels from A100_cc80_py312_cu129_x86_64
[2026-01-25 16:25:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=4 models
[2026-01-25 16:25:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:25:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:25:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:25:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:25:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:25:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:25:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:25:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:25:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:25:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:25:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:25:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:25:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:25:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:25:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:25:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:25:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:25:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:25:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:25:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:25:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:25:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:25:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:25:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:25:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:25:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:25:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:25:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:25:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:25:57] INFO kernels.py:719: Preloaded 20 Triton kernels from A100_cc80_py312_cu129_x86_64
[2026-01-25 16:25:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=4 models
[2026-01-25 16:25:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:25:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:25:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:25:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=179810) [2026-01-25 16:25:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=179810) [2026-01-25 16:25:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=179810) [2026-01-25 16:25:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=179810) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=179810) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.49it/s]
(EngineCore_DP0 pid=179810) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.48it/s]
(EngineCore_DP0 pid=179810) 
(EngineCore_DP0 pid=179810) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 56.79it/s]
(EngineCore_DP0 pid=179810) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 31.29it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1747.13it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:00, 182.25it/s, est. speed input: 23331.48 toks/s, output: 182.26 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:00, 142.13it/s, est. speed input: 18814.82 toks/s, output: 146.98 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:00<00:00, 135.34it/s, est. speed input: 17978.91 toks/s, output: 140.45 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:00<00:00, 133.42it/s, est. speed input: 17680.58 toks/s, output: 138.12 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:00<00:00, 131.24it/s, est. speed input: 17417.82 toks/s, output: 136.07 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:00<00:00, 130.87it/s, est. speed input: 17300.40 toks/s, output: 135.16 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:00<00:00, 129.21it/s, est. speed input: 17135.68 toks/s, output: 133.87 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:00<00:00, 128.09it/s, est. speed input: 17014.34 toks/s, output: 132.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 128.09it/s, est. speed input: 17012.51 toks/s, output: 132.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 132.89it/s, est. speed input: 17012.51 toks/s, output: 132.91 toks/s]
[rank0]:[W125 16:26:19.802839687 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 16:26:21
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:26:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 121.77 requests/s, 31295.44 total tokens/s, 121.77 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 16:26:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:26:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:26:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:26:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:26:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:26:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:26:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:26:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:26:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:26:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:26:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:26:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:26:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:26:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:26:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:26:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:26:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:26:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:26:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:26:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:26:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:26:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:26:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:26:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:26:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:26:28] INFO kernels.py:719: Preloaded 20 Triton kernels from A100_cc80_py312_cu129_x86_64
[2026-01-25 16:26:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=4 models
[2026-01-25 16:26:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:26:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:26:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:26:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:26:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:26:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:26:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:26:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:26:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:26:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:26:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:26:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:26:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:26:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:26:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:26:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:26:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:26:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:26:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:26:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:26:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:26:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:26:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:26:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:26:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:26:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:26:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:26:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:26:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:26:36] INFO kernels.py:719: Preloaded 20 Triton kernels from A100_cc80_py312_cu129_x86_64
[2026-01-25 16:26:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=4 models
[2026-01-25 16:26:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:26:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:26:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:26:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=180767) [2026-01-25 16:26:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=180767) [2026-01-25 16:26:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=180767) [2026-01-25 16:26:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=180767) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=180767) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.26it/s]
(EngineCore_DP0 pid=180767) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.25it/s]
(EngineCore_DP0 pid=180767) 
(EngineCore_DP0 pid=180767) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 63.23it/s]
(EngineCore_DP0 pid=180767) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 33.91it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 1115.95it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1130.14it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:00, 219.57it/s, est. speed input: 56218.64 toks/s, output: 219.58 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:00, 153.85it/s, est. speed input: 41283.75 toks/s, output: 161.25 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:00<00:00, 144.41it/s, est. speed input: 38893.32 toks/s, output: 151.92 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:00<00:00, 137.86it/s, est. speed input: 37406.99 toks/s, output: 146.12 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:00<00:00, 132.51it/s, est. speed input: 36262.42 toks/s, output: 141.65 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:00<00:00, 130.13it/s, est. speed input: 35629.93 toks/s, output: 139.18 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:00<00:00, 128.31it/s, est. speed input: 35137.42 toks/s, output: 137.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 128.31it/s, est. speed input: 34986.06 toks/s, output: 136.66 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 136.64it/s, est. speed input: 34986.06 toks/s, output: 136.66 toks/s]
[rank0]:[W125 16:26:56.553655324 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

