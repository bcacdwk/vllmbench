
========== M=16 ==========
Time: 2026-01-25 17:11:36
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:11:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:11:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=171524) WARNING 01-25 17:11:51 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=171524) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=171524) WARNING 01-25 17:12:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.08 requests/s, 273.37 total tokens/s, 16.08 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 17:11:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:11:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:11:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:11:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:11:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:11:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:11:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:11:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:11:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:11:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:11:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:11:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:11:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:11:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:11:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:11:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:11:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:11:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:11:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:11:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:11:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:11:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:11:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:11:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:11:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:11:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:11:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:11:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:52] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:52] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:52] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:52] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:52] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=171524) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=171524) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.70s/it]
(EngineCore_DP0 pid=171524) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.70s/it]
(EngineCore_DP0 pid=171524) 
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=171524) [2026-01-25 17:11:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=171524) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.46it/s]
(EngineCore_DP0 pid=171524) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3067.71it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:45,  2.79it/s, est. speed input: 44.72 toks/s, output: 2.79 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:16,  7.43it/s, est. speed input: 102.01 toks/s, output: 6.38 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:11, 10.71it/s, est. speed input: 138.04 toks/s, output: 8.63 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:09, 12.96it/s, est. speed input: 162.37 toks/s, output: 10.15 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 14.48it/s, est. speed input: 179.74 toks/s, output: 11.23 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 15.23it/s, est. speed input: 191.34 toks/s, output: 11.96 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:07, 15.72it/s, est. speed input: 200.22 toks/s, output: 12.51 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:06, 16.45it/s, est. speed input: 208.94 toks/s, output: 13.06 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 16.85it/s, est. speed input: 215.71 toks/s, output: 13.48 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 17.21it/s, est. speed input: 221.63 toks/s, output: 13.85 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 17.34it/s, est. speed input: 226.26 toks/s, output: 14.14 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 17.56it/s, est. speed input: 230.63 toks/s, output: 14.41 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.77it/s, est. speed input: 234.60 toks/s, output: 14.66 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.61it/s, est. speed input: 237.23 toks/s, output: 14.83 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.46it/s, est. speed input: 239.44 toks/s, output: 14.96 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:05, 17.43it/s, est. speed input: 241.60 toks/s, output: 15.10 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 17.41it/s, est. speed input: 243.52 toks/s, output: 15.22 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 17.24it/s, est. speed input: 244.87 toks/s, output: 15.30 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 17.08it/s, est. speed input: 246.00 toks/s, output: 15.38 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.11it/s, est. speed input: 247.34 toks/s, output: 15.46 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.95it/s, est. speed input: 248.15 toks/s, output: 15.51 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.05it/s, est. speed input: 249.36 toks/s, output: 15.58 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 16.96it/s, est. speed input: 250.13 toks/s, output: 15.63 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 16.83it/s, est. speed input: 250.70 toks/s, output: 15.67 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:04, 16.74it/s, est. speed input: 251.25 toks/s, output: 15.70 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 16.76it/s, est. speed input: 251.89 toks/s, output: 15.74 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 16.45it/s, est. speed input: 251.91 toks/s, output: 15.74 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.46it/s, est. speed input: 252.33 toks/s, output: 15.77 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.56it/s, est. speed input: 252.86 toks/s, output: 15.80 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.58it/s, est. speed input: 253.28 toks/s, output: 15.83 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.54it/s, est. speed input: 253.60 toks/s, output: 15.85 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.70it/s, est. speed input: 254.19 toks/s, output: 15.89 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:03, 16.73it/s, est. speed input: 254.60 toks/s, output: 15.91 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.79it/s, est. speed input: 255.07 toks/s, output: 15.94 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.66it/s, est. speed input: 255.26 toks/s, output: 15.95 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.56it/s, est. speed input: 255.42 toks/s, output: 15.96 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.64it/s, est. speed input: 255.78 toks/s, output: 15.99 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.71it/s, est. speed input: 256.14 toks/s, output: 16.01 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.67it/s, est. speed input: 256.37 toks/s, output: 16.02 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 16.58it/s, est. speed input: 256.51 toks/s, output: 16.03 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:02, 16.71it/s, est. speed input: 256.88 toks/s, output: 16.05 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 16.53it/s, est. speed input: 256.90 toks/s, output: 16.06 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.72it/s, est. speed input: 257.30 toks/s, output: 16.08 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.66it/s, est. speed input: 257.46 toks/s, output: 16.09 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.64it/s, est. speed input: 257.63 toks/s, output: 16.10 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.73it/s, est. speed input: 257.91 toks/s, output: 16.12 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.85it/s, est. speed input: 258.24 toks/s, output: 16.14 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.76it/s, est. speed input: 258.38 toks/s, output: 16.15 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:01, 16.59it/s, est. speed input: 258.40 toks/s, output: 16.15 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.65it/s, est. speed input: 258.59 toks/s, output: 16.16 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.70it/s, est. speed input: 258.80 toks/s, output: 16.17 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.72it/s, est. speed input: 258.97 toks/s, output: 16.19 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.80it/s, est. speed input: 259.20 toks/s, output: 16.20 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.64it/s, est. speed input: 259.23 toks/s, output: 16.20 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.42it/s, est. speed input: 259.15 toks/s, output: 16.20 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.54it/s, est. speed input: 259.32 toks/s, output: 16.21 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.73it/s, est. speed input: 259.59 toks/s, output: 16.22 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 16.44it/s, est. speed input: 259.46 toks/s, output: 16.22 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 16.08it/s, est. speed input: 259.20 toks/s, output: 16.20 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 16.04it/s, est. speed input: 259.13 toks/s, output: 16.20 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 16.00it/s, est. speed input: 259.05 toks/s, output: 16.19 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 15.91it/s, est. speed input: 258.92 toks/s, output: 16.18 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 15.92it/s, est. speed input: 258.86 toks/s, output: 16.18 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 15.84it/s, est. speed input: 258.73 toks/s, output: 16.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 15.84it/s, est. speed input: 258.70 toks/s, output: 16.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.17it/s, est. speed input: 258.70 toks/s, output: 16.17 toks/s]
[rank0]:[W125 17:12:22.543171330 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 17:12:23
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:12:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:12:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=172452) WARNING 01-25 17:12:40 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=172452) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=172452) WARNING 01-25 17:12:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.67 requests/s, 2021.34 total tokens/s, 15.67 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 17:12:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:12:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:12:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:12:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:12:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:12:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:12:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:12:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:12:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:12:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:12:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:12:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:12:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:12:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:12:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:12:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:12:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:12:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:12:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:12:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:12:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:12:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:12:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:12:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:12:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:12:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:12:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:12:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=172452) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=172452) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
(EngineCore_DP0 pid=172452) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
(EngineCore_DP0 pid=172452) 
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=172452) [2026-01-25 17:12:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=172452) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.65it/s]
(EngineCore_DP0 pid=172452) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.65it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1288.95it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:45,  2.79it/s, est. speed input: 356.68 toks/s, output: 2.79 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:17,  7.19it/s, est. speed input: 794.77 toks/s, output: 6.21 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:12, 10.07it/s, est. speed input: 1054.96 toks/s, output: 8.24 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:09, 12.11it/s, est. speed input: 1233.55 toks/s, output: 9.64 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 13.50it/s, est. speed input: 1361.17 toks/s, output: 10.63 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:08, 14.50it/s, est. speed input: 1458.67 toks/s, output: 11.40 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:07, 15.19it/s, est. speed input: 1534.50 toks/s, output: 11.99 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 15.50it/s, est. speed input: 1589.56 toks/s, output: 12.42 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 15.74it/s, est. speed input: 1635.25 toks/s, output: 12.78 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 15.85it/s, est. speed input: 1671.62 toks/s, output: 13.06 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 16.02it/s, est. speed input: 1704.76 toks/s, output: 13.32 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 16.13it/s, est. speed input: 1733.07 toks/s, output: 13.54 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 16.20it/s, est. speed input: 1757.19 toks/s, output: 13.73 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 16.14it/s, est. speed input: 1775.96 toks/s, output: 13.87 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:06, 15.97it/s, est. speed input: 1789.63 toks/s, output: 13.98 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 15.94it/s, est. speed input: 1803.52 toks/s, output: 14.09 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 16.07it/s, est. speed input: 1818.91 toks/s, output: 14.21 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.22it/s, est. speed input: 1833.78 toks/s, output: 14.33 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.29it/s, est. speed input: 1846.74 toks/s, output: 14.43 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.27it/s, est. speed input: 1857.29 toks/s, output: 14.51 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.46it/s, est. speed input: 1870.33 toks/s, output: 14.61 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.75it/s, est. speed input: 1884.60 toks/s, output: 14.72 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:03<00:04, 17.00it/s, est. speed input: 1898.43 toks/s, output: 14.83 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:04, 17.11it/s, est. speed input: 1910.34 toks/s, output: 14.92 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:04, 17.24it/s, est. speed input: 1922.14 toks/s, output: 15.02 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 17.08it/s, est. speed input: 1929.86 toks/s, output: 15.08 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 17.29it/s, est. speed input: 1941.00 toks/s, output: 15.16 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 17.16it/s, est. speed input: 1948.14 toks/s, output: 15.22 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 17.27it/s, est. speed input: 1957.25 toks/s, output: 15.29 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 17.32it/s, est. speed input: 1965.44 toks/s, output: 15.35 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 17.45it/s, est. speed input: 1974.14 toks/s, output: 15.42 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:03, 17.49it/s, est. speed input: 1981.88 toks/s, output: 15.48 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:03, 17.53it/s, est. speed input: 1989.29 toks/s, output: 15.54 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 17.52it/s, est. speed input: 1995.93 toks/s, output: 15.59 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 17.41it/s, est. speed input: 2001.28 toks/s, output: 15.63 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 17.41it/s, est. speed input: 2007.07 toks/s, output: 15.68 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 17.46it/s, est. speed input: 2013.03 toks/s, output: 15.73 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 17.48it/s, est. speed input: 2018.50 toks/s, output: 15.77 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.20it/s, est. speed input: 2021.10 toks/s, output: 15.79 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 16.94it/s, est. speed input: 2022.89 toks/s, output: 15.80 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:02, 16.75it/s, est. speed input: 2024.46 toks/s, output: 15.82 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 16.65it/s, est. speed input: 2026.24 toks/s, output: 15.83 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.54it/s, est. speed input: 2027.57 toks/s, output: 15.84 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.29it/s, est. speed input: 2027.28 toks/s, output: 15.84 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.24it/s, est. speed input: 2028.09 toks/s, output: 15.84 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.26it/s, est. speed input: 2029.35 toks/s, output: 15.85 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.20it/s, est. speed input: 2029.93 toks/s, output: 15.86 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:02, 16.14it/s, est. speed input: 2030.33 toks/s, output: 15.86 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:01, 16.07it/s, est. speed input: 2030.43 toks/s, output: 15.86 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.07it/s, est. speed input: 2030.95 toks/s, output: 15.87 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.12it/s, est. speed input: 2031.92 toks/s, output: 15.87 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.08it/s, est. speed input: 2032.16 toks/s, output: 15.88 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.00it/s, est. speed input: 2032.00 toks/s, output: 15.87 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 15.87it/s, est. speed input: 2031.32 toks/s, output: 15.87 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 15.87it/s, est. speed input: 2031.28 toks/s, output: 15.87 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 15.94it/s, est. speed input: 2031.79 toks/s, output: 15.87 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:00, 15.97it/s, est. speed input: 2032.20 toks/s, output: 15.88 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 15.73it/s, est. speed input: 2030.64 toks/s, output: 15.86 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 15.71it/s, est. speed input: 2030.18 toks/s, output: 15.86 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 15.69it/s, est. speed input: 2029.68 toks/s, output: 15.86 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 15.79it/s, est. speed input: 2030.03 toks/s, output: 15.86 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 15.85it/s, est. speed input: 2030.31 toks/s, output: 15.86 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 15.93it/s, est. speed input: 2030.82 toks/s, output: 15.87 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:08<00:00, 15.91it/s, est. speed input: 2030.84 toks/s, output: 15.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.91it/s, est. speed input: 2030.80 toks/s, output: 15.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.87it/s, est. speed input: 2030.80 toks/s, output: 15.87 toks/s]
[rank0]:[W125 17:13:07.078213436 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 17:13:10
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:13:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:13:19 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=173353) WARNING 01-25 17:13:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=173353) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=173353) WARNING 01-25 17:13:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.81 requests/s, 4063.38 total tokens/s, 15.81 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 17:13:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:13:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:13:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:13:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:13:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:13:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:13:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:13:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:13:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:13:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:13:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:13:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:13:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:13:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:13:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:13:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:13:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:13:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:13:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:13:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:13:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:13:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:13:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:13:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:13:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:13:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:13:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:13:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=173353) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=173353) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]
(EngineCore_DP0 pid=173353) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]
(EngineCore_DP0 pid=173353) 
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=173353) [2026-01-25 17:13:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=173353) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.98it/s]
(EngineCore_DP0 pid=173353) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  70%|███████   | 90/128 [00:00<00:00, 891.20it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 906.95it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:37,  3.41it/s, est. speed input: 874.20 toks/s, output: 3.41 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:15,  8.19it/s, est. speed input: 1840.19 toks/s, output: 7.19 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:11, 11.02it/s, est. speed input: 2372.10 toks/s, output: 9.27 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:09, 12.71it/s, est. speed input: 2698.29 toks/s, output: 10.54 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 13.82it/s, est. speed input: 2923.70 toks/s, output: 11.42 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:08, 14.54it/s, est. speed input: 3086.48 toks/s, output: 12.06 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:07, 15.06it/s, est. speed input: 3213.92 toks/s, output: 12.55 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 15.45it/s, est. speed input: 3316.27 toks/s, output: 12.95 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 15.65it/s, est. speed input: 3394.30 toks/s, output: 13.26 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:07, 15.54it/s, est. speed input: 3442.53 toks/s, output: 13.45 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 15.63it/s, est. speed input: 3493.04 toks/s, output: 13.64 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 15.77it/s, est. speed input: 3539.91 toks/s, output: 13.83 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 15.96it/s, est. speed input: 3585.25 toks/s, output: 14.00 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 15.99it/s, est. speed input: 3619.61 toks/s, output: 14.14 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:06, 16.00it/s, est. speed input: 3649.14 toks/s, output: 14.25 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 16.04it/s, est. speed input: 3676.85 toks/s, output: 14.36 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 16.05it/s, est. speed input: 3700.50 toks/s, output: 14.45 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 15.95it/s, est. speed input: 3717.69 toks/s, output: 14.52 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.00it/s, est. speed input: 3737.61 toks/s, output: 14.60 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.02it/s, est. speed input: 3755.07 toks/s, output: 14.67 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 15.96it/s, est. speed input: 3768.55 toks/s, output: 14.72 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.16it/s, est. speed input: 3789.08 toks/s, output: 14.80 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:03<00:05, 16.47it/s, est. speed input: 3812.94 toks/s, output: 14.89 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:04, 16.69it/s, est. speed input: 3835.09 toks/s, output: 14.98 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:04, 16.93it/s, est. speed input: 3857.87 toks/s, output: 15.07 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 17.14it/s, est. speed input: 3879.99 toks/s, output: 15.16 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 17.16it/s, est. speed input: 3897.62 toks/s, output: 15.22 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.80it/s, est. speed input: 3904.58 toks/s, output: 15.25 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.88it/s, est. speed input: 3919.16 toks/s, output: 15.31 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 17.09it/s, est. speed input: 3936.64 toks/s, output: 15.38 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 17.26it/s, est. speed input: 3953.39 toks/s, output: 15.44 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:03, 17.40it/s, est. speed input: 3969.65 toks/s, output: 15.51 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:03, 17.50it/s, est. speed input: 3985.16 toks/s, output: 15.57 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 17.53it/s, est. speed input: 3999.02 toks/s, output: 15.62 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 17.70it/s, est. speed input: 4014.85 toks/s, output: 15.68 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 17.65it/s, est. speed input: 4026.91 toks/s, output: 15.73 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 17.66it/s, est. speed input: 4039.14 toks/s, output: 15.78 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 17.47it/s, est. speed input: 4047.09 toks/s, output: 15.81 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.32it/s, est. speed input: 4054.32 toks/s, output: 15.84 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 17.43it/s, est. speed input: 4065.13 toks/s, output: 15.88 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:02, 17.51it/s, est. speed input: 4075.42 toks/s, output: 15.92 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 17.58it/s, est. speed input: 4085.56 toks/s, output: 15.96 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 17.77it/s, est. speed input: 4097.53 toks/s, output: 16.01 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 17.67it/s, est. speed input: 4105.40 toks/s, output: 16.04 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 17.65it/s, est. speed input: 4113.62 toks/s, output: 16.07 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 17.25it/s, est. speed input: 4115.34 toks/s, output: 16.08 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 17.01it/s, est. speed input: 4117.53 toks/s, output: 16.08 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.81it/s, est. speed input: 4119.02 toks/s, output: 16.09 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:01, 16.59it/s, est. speed input: 4118.98 toks/s, output: 16.09 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.48it/s, est. speed input: 4119.69 toks/s, output: 16.09 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.44it/s, est. speed input: 4120.93 toks/s, output: 16.10 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.51it/s, est. speed input: 4123.78 toks/s, output: 16.11 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.45it/s, est. speed input: 4124.76 toks/s, output: 16.11 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.45it/s, est. speed input: 4126.39 toks/s, output: 16.12 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.31it/s, est. speed input: 4125.74 toks/s, output: 16.12 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.22it/s, est. speed input: 4125.25 toks/s, output: 16.11 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:00, 16.18it/s, est. speed input: 4125.12 toks/s, output: 16.11 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 16.11it/s, est. speed input: 4124.44 toks/s, output: 16.11 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 16.09it/s, est. speed input: 4124.12 toks/s, output: 16.11 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 15.98it/s, est. speed input: 4122.49 toks/s, output: 16.10 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 16.08it/s, est. speed input: 4123.34 toks/s, output: 16.11 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 15.98it/s, est. speed input: 4121.87 toks/s, output: 16.10 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 15.85it/s, est. speed input: 4119.52 toks/s, output: 16.09 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 15.95it/s, est. speed input: 4119.94 toks/s, output: 16.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 15.95it/s, est. speed input: 4120.23 toks/s, output: 16.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.09it/s, est. speed input: 4120.23 toks/s, output: 16.09 toks/s]
[rank0]:[W125 17:13:54.019965747 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-25 18:20:58
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:21:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:21:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=246347) WARNING 01-25 18:21:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=246347) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=246347) WARNING 01-25 18:21:30 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.20 requests/s, 8308.64 total tokens/s, 16.20 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 18:21:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:21:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:21:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:21:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:21:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:21:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:21:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:21:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:21:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:21:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:21:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:21:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:21:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:21:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:21:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:21:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:21:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:21:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:21:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:21:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:21:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:21:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:21:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:21:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:21:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:21:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:21:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:21:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W125 18:21:22.035373706 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=246347) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=246347) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=246347) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=246347) 
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=246347) [2026-01-25 18:21:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=246347) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.57it/s]
(EngineCore_DP0 pid=246347) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  44%|████▍     | 56/128 [00:00<00:00, 557.66it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 558.52it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 557.22it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 33.91it/s, est. speed input: 17365.13 toks/s, output: 33.91 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 21.68it/s, est. speed input: 11811.38 toks/s, output: 23.07 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:06, 19.31it/s, est. speed input: 10680.40 toks/s, output: 20.86 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:06, 18.22it/s, est. speed input: 10136.29 toks/s, output: 19.80 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:06, 17.74it/s, est. speed input: 9894.84 toks/s, output: 19.32 toks/s] 
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 17.17it/s, est. speed input: 9661.21 toks/s, output: 18.87 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 16.87it/s, est. speed input: 9506.71 toks/s, output: 18.57 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 16.73it/s, est. speed input: 9397.73 toks/s, output: 18.35 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 16.67it/s, est. speed input: 9315.60 toks/s, output: 18.19 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 16.62it/s, est. speed input: 9244.65 toks/s, output: 18.06 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 16.46it/s, est. speed input: 9167.17 toks/s, output: 17.90 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.30it/s, est. speed input: 9093.90 toks/s, output: 17.76 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 16.24it/s, est. speed input: 9038.27 toks/s, output: 17.65 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 15.65it/s, est. speed input: 8923.63 toks/s, output: 17.43 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 15.71it/s, est. speed input: 8875.91 toks/s, output: 17.34 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 15.93it/s, est. speed input: 8852.25 toks/s, output: 17.29 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 15.82it/s, est. speed input: 8804.66 toks/s, output: 17.20 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 15.68it/s, est. speed input: 8756.14 toks/s, output: 17.10 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 15.41it/s, est. speed input: 8696.17 toks/s, output: 16.98 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:05, 15.66it/s, est. speed input: 8680.48 toks/s, output: 16.95 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:05, 15.10it/s, est. speed input: 8604.43 toks/s, output: 16.81 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:05, 15.39it/s, est. speed input: 8589.59 toks/s, output: 16.78 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:05, 14.93it/s, est. speed input: 8524.42 toks/s, output: 16.65 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 15.11it/s, est. speed input: 8502.96 toks/s, output: 16.61 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 15.46it/s, est. speed input: 8498.18 toks/s, output: 16.60 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 15.46it/s, est. speed input: 8477.14 toks/s, output: 16.56 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 15.00it/s, est. speed input: 8427.31 toks/s, output: 16.46 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:04, 15.32it/s, est. speed input: 8421.81 toks/s, output: 16.45 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:04, 15.42it/s, est. speed input: 8408.91 toks/s, output: 16.42 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 15.35it/s, est. speed input: 8388.43 toks/s, output: 16.38 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 15.37it/s, est. speed input: 8373.38 toks/s, output: 16.35 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 15.07it/s, est. speed input: 8341.81 toks/s, output: 16.29 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 15.24it/s, est. speed input: 8332.49 toks/s, output: 16.27 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 15.51it/s, est. speed input: 8331.16 toks/s, output: 16.27 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 15.61it/s, est. speed input: 8325.26 toks/s, output: 16.26 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:03, 15.72it/s, est. speed input: 8321.53 toks/s, output: 16.25 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 15.91it/s, est. speed input: 8323.12 toks/s, output: 16.26 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 15.77it/s, est. speed input: 8312.81 toks/s, output: 16.24 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.17it/s, est. speed input: 8323.37 toks/s, output: 16.26 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.57it/s, est. speed input: 8338.10 toks/s, output: 16.29 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.81it/s, est. speed input: 8350.06 toks/s, output: 16.31 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 17.02it/s, est. speed input: 8362.69 toks/s, output: 16.33 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 17.13it/s, est. speed input: 8373.77 toks/s, output: 16.35 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:05<00:00, 88.53it/s, est. speed input: 10914.63 toks/s, output: 21.32 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 88.53it/s, est. speed input: 10845.59 toks/s, output: 21.18 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 21.18it/s, est. speed input: 10845.59 toks/s, output: 21.18 toks/s]
[rank0]:[W125 18:21:50.618094935 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 18:21:53
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:22:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:22:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=247357) WARNING 01-25 18:22:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=247357) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=247357) WARNING 01-25 18:22:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.87 requests/s, 16268.23 total tokens/s, 15.87 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 18:22:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:22:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:22:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:22:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:22:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:22:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:22:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:22:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:22:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:22:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:22:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:22:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:22:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:22:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:22:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:22:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:22:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:22:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=247357) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=247357) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.18it/s]
(EngineCore_DP0 pid=247357) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.18it/s]
(EngineCore_DP0 pid=247357) 
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=247357) [2026-01-25 18:22:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=247357) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.46it/s]
(EngineCore_DP0 pid=247357) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.65it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  24%|██▍       | 31/128 [00:00<00:00, 307.07it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 311.83it/s]
Adding requests:  75%|███████▌  | 96/128 [00:00<00:00, 317.33it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 317.77it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 40.45it/s, est. speed input: 41432.82 toks/s, output: 40.45 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:05, 22.66it/s, est. speed input: 24845.76 toks/s, output: 24.26 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 20.56it/s, est. speed input: 22760.30 toks/s, output: 22.23 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 19.41it/s, est. speed input: 21631.56 toks/s, output: 21.12 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:05, 18.63it/s, est. speed input: 20865.76 toks/s, output: 20.38 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:05, 18.31it/s, est. speed input: 20520.64 toks/s, output: 20.04 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 18.08it/s, est. speed input: 20256.75 toks/s, output: 19.78 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.37it/s, est. speed input: 19836.70 toks/s, output: 19.37 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 16.85it/s, est. speed input: 19486.15 toks/s, output: 19.03 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 16.53it/s, est. speed input: 19211.60 toks/s, output: 18.76 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.32it/s, est. speed input: 18984.64 toks/s, output: 18.54 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 16.24it/s, est. speed input: 18805.79 toks/s, output: 18.36 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 16.14it/s, est. speed input: 18640.91 toks/s, output: 18.20 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 15.98it/s, est. speed input: 18475.52 toks/s, output: 18.04 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 15.91it/s, est. speed input: 18339.00 toks/s, output: 17.91 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 15.91it/s, est. speed input: 18227.88 toks/s, output: 17.80 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 15.85it/s, est. speed input: 18115.67 toks/s, output: 17.69 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 15.86it/s, est. speed input: 18024.75 toks/s, output: 17.60 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:05, 15.88it/s, est. speed input: 17944.64 toks/s, output: 17.52 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 15.93it/s, est. speed input: 17877.54 toks/s, output: 17.46 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 15.77it/s, est. speed input: 17785.07 toks/s, output: 17.37 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 15.82it/s, est. speed input: 17724.58 toks/s, output: 17.31 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 15.89it/s, est. speed input: 17674.52 toks/s, output: 17.26 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 15.92it/s, est. speed input: 17625.13 toks/s, output: 17.21 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 15.81it/s, est. speed input: 17561.77 toks/s, output: 17.15 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 15.71it/s, est. speed input: 17499.94 toks/s, output: 17.09 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:04, 15.73it/s, est. speed input: 17453.71 toks/s, output: 17.04 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:04, 15.74it/s, est. speed input: 17410.55 toks/s, output: 17.00 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 15.89it/s, est. speed input: 17386.94 toks/s, output: 16.98 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 15.90it/s, est. speed input: 17353.27 toks/s, output: 16.95 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 15.95it/s, est. speed input: 17326.70 toks/s, output: 16.92 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 15.99it/s, est. speed input: 17301.52 toks/s, output: 16.90 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.10it/s, est. speed input: 17287.24 toks/s, output: 16.88 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 15.97it/s, est. speed input: 17252.35 toks/s, output: 16.85 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:03, 16.03it/s, est. speed input: 17233.88 toks/s, output: 16.83 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.02it/s, est. speed input: 17212.37 toks/s, output: 16.81 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 15.97it/s, est. speed input: 17187.22 toks/s, output: 16.78 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 15.91it/s, est. speed input: 17160.81 toks/s, output: 16.76 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 15.91it/s, est. speed input: 17140.03 toks/s, output: 16.74 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 15.89it/s, est. speed input: 17118.30 toks/s, output: 16.72 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 15.78it/s, est. speed input: 17089.61 toks/s, output: 16.69 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 15.78it/s, est. speed input: 17068.29 toks/s, output: 16.67 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:02, 15.72it/s, est. speed input: 17043.37 toks/s, output: 16.64 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 15.92it/s, est. speed input: 17038.33 toks/s, output: 16.64 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 16.12it/s, est. speed input: 17037.42 toks/s, output: 16.64 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.22it/s, est. speed input: 17033.77 toks/s, output: 16.63 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.24it/s, est. speed input: 17026.58 toks/s, output: 16.63 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.31it/s, est. speed input: 17023.51 toks/s, output: 16.62 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.25it/s, est. speed input: 17013.39 toks/s, output: 16.61 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.56it/s, est. speed input: 17026.24 toks/s, output: 16.63 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.76it/s, est. speed input: 17037.24 toks/s, output: 16.64 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.93it/s, est. speed input: 17049.83 toks/s, output: 16.65 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 17.19it/s, est. speed input: 17069.29 toks/s, output: 16.67 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 17.18it/s, est. speed input: 17077.68 toks/s, output: 16.68 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 17.00it/s, est. speed input: 17076.31 toks/s, output: 16.68 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 17.18it/s, est. speed input: 17091.15 toks/s, output: 16.69 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 17.02it/s, est. speed input: 17090.75 toks/s, output: 16.69 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 17.19it/s, est. speed input: 17104.80 toks/s, output: 16.70 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 17.18it/s, est. speed input: 17111.72 toks/s, output: 16.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.18it/s, est. speed input: 17110.29 toks/s, output: 16.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.71it/s, est. speed input: 17110.29 toks/s, output: 16.71 toks/s]
[rank0]:[W125 18:22:38.716540622 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 18:22:41
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:22:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:22:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=248272) WARNING 01-25 18:22:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=248272) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=248272) WARNING 01-25 18:23:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.84 requests/s, 32639.01 total tokens/s, 31.84 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 18:22:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:22:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:22:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:22:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:22:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:22:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:22:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:22:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:22:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:22:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:22:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:22:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:22:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:22:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:22:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:22:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:22:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:22:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:22:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=248272) [2026-01-25 18:22:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=248272) [2026-01-25 18:22:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=248272) [2026-01-25 18:22:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=248272) [2026-01-25 18:22:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=248272) [2026-01-25 18:22:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=248272) [2026-01-25 18:22:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=248272) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=248272) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=248272) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=248272) 
(EngineCore_DP0 pid=248272) [2026-01-25 18:23:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=248272) [2026-01-25 18:23:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=248272) [2026-01-25 18:23:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=248272) [2026-01-25 18:23:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=248272) [2026-01-25 18:23:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=248272) [2026-01-25 18:23:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=248272) [2026-01-25 18:23:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=248272) [2026-01-25 18:23:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=248272) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.25it/s]
(EngineCore_DP0 pid=248272) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  8.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.43it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  12%|█▏        | 31/256 [00:00<00:00, 308.71it/s]
Adding requests:  25%|██▌       | 65/256 [00:00<00:00, 324.50it/s]
Adding requests:  39%|███▊      | 99/256 [00:00<00:00, 327.37it/s]
Adding requests:  52%|█████▏    | 132/256 [00:00<00:00, 324.50it/s]
Adding requests:  64%|██████▍   | 165/256 [00:00<00:00, 322.96it/s]
Adding requests:  77%|███████▋  | 198/256 [00:00<00:00, 314.15it/s]
Adding requests:  91%|█████████ | 233/256 [00:00<00:00, 324.51it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 322.08it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:00<00:01, 194.02it/s, est. speed input: 198723.01 toks/s, output: 194.04 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:04, 52.98it/s, est. speed input: 61577.03 toks/s, output: 60.13 toks/s]   
Processed prompts:  21%|██        | 54/256 [00:01<00:04, 45.09it/s, est. speed input: 53247.08 toks/s, output: 52.00 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:01<00:04, 43.37it/s, est. speed input: 51120.24 toks/s, output: 49.92 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:01<00:04, 40.51it/s, est. speed input: 48779.62 toks/s, output: 47.64 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:05, 36.78it/s, est. speed input: 46301.48 toks/s, output: 45.22 toks/s]
Processed prompts:  30%|███       | 77/256 [00:01<00:04, 37.35it/s, est. speed input: 45868.43 toks/s, output: 44.79 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:05, 34.10it/s, est. speed input: 44062.90 toks/s, output: 43.03 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:02<00:05, 33.77it/s, est. speed input: 43419.48 toks/s, output: 42.40 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:02<00:00, 141.95it/s, est. speed input: 68245.24 toks/s, output: 66.65 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 71.59it/s, est. speed input: 60452.18 toks/s, output: 59.03 toks/s] 
Processed prompts:  69%|██████▉   | 177/256 [00:03<00:01, 57.41it/s, est. speed input: 57170.10 toks/s, output: 55.83 toks/s]
Processed prompts:  74%|███████▍  | 189/256 [00:03<00:01, 48.73it/s, est. speed input: 54473.83 toks/s, output: 53.20 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:01, 43.13it/s, est. speed input: 52447.69 toks/s, output: 51.22 toks/s]
Processed prompts:  80%|████████  | 205/256 [00:04<00:01, 42.26it/s, est. speed input: 51862.53 toks/s, output: 50.65 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:04<00:01, 40.14it/s, est. speed input: 51045.35 toks/s, output: 49.85 toks/s]
Processed prompts:  85%|████████▍ | 217/256 [00:04<00:01, 38.04it/s, est. speed input: 50230.46 toks/s, output: 49.05 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:04<00:00, 35.26it/s, est. speed input: 49319.34 toks/s, output: 48.16 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:04<00:00, 34.75it/s, est. speed input: 48905.87 toks/s, output: 47.76 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:04<00:00, 34.25it/s, est. speed input: 48507.19 toks/s, output: 47.37 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:04<00:00, 33.88it/s, est. speed input: 48137.78 toks/s, output: 47.01 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:05<00:00, 33.63it/s, est. speed input: 47792.03 toks/s, output: 46.67 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:05<00:00, 33.46it/s, est. speed input: 47466.75 toks/s, output: 46.35 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:05<00:00, 33.83it/s, est. speed input: 47214.10 toks/s, output: 46.11 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:05<00:00, 34.16it/s, est. speed input: 46976.68 toks/s, output: 45.88 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:05<00:00, 34.29it/s, est. speed input: 46737.23 toks/s, output: 45.64 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 34.29it/s, est. speed input: 46627.33 toks/s, output: 45.53 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 45.53it/s, est. speed input: 46627.33 toks/s, output: 45.53 toks/s]
[rank0]:[W125 18:23:26.382167902 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 18:23:29
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:23:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:23:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=249206) WARNING 01-25 18:23:49 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=249206) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=249206) WARNING 01-25 18:23:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.55 requests/s, 64110.36 total tokens/s, 62.55 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 18:23:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:23:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:23:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:23:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:23:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:23:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:23:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:23:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:23:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:23:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:23:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:23:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:23:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:23:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:23:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:23:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:23:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:23:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:23:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:23:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:23:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:23:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:23:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:23:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:23:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:23:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:23:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:23:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=249206) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=249206) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.38it/s]
(EngineCore_DP0 pid=249206) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.38it/s]
(EngineCore_DP0 pid=249206) 
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=249206) [2026-01-25 18:23:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=249206) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.24it/s]
(EngineCore_DP0 pid=249206) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  8.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.62it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 272.71it/s]
Adding requests:  12%|█▏        | 59/512 [00:00<00:01, 292.59it/s]
Adding requests:  17%|█▋        | 89/512 [00:00<00:01, 289.37it/s]
Adding requests:  23%|██▎       | 119/512 [00:00<00:01, 293.23it/s]
Adding requests:  29%|██▉       | 149/512 [00:00<00:01, 290.09it/s]
Adding requests:  35%|███▍      | 179/512 [00:00<00:01, 290.91it/s]
Adding requests:  41%|████      | 210/512 [00:00<00:01, 294.54it/s]
Adding requests:  47%|████▋     | 241/512 [00:00<00:00, 296.85it/s]
Adding requests:  53%|█████▎    | 271/512 [00:00<00:00, 294.24it/s]
Adding requests:  59%|█████▉    | 301/512 [00:01<00:00, 289.62it/s]
Adding requests:  65%|██████▌   | 333/512 [00:01<00:00, 297.13it/s]
Adding requests:  71%|███████▏  | 365/512 [00:01<00:00, 302.63it/s]
Adding requests:  77%|███████▋  | 396/512 [00:01<00:00, 304.01it/s]
Adding requests:  83%|████████▎ | 427/512 [00:01<00:00, 304.35it/s]
Adding requests:  89%|████████▉ | 458/512 [00:01<00:00, 303.59it/s]
Adding requests:  96%|█████████▌| 489/512 [00:01<00:00, 302.86it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 297.62it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:00<00:00, 747.67it/s, est. speed input: 765819.39 toks/s, output: 747.72 toks/s]
Processed prompts:  31%|███       | 157/512 [00:01<00:03, 110.62it/s, est. speed input: 130730.67 toks/s, output: 127.67 toks/s]
Processed prompts:  38%|███▊      | 192/512 [00:01<00:03, 91.83it/s, est. speed input: 110312.72 toks/s, output: 107.73 toks/s] 
Processed prompts:  42%|████▏     | 214/512 [00:02<00:03, 83.06it/s, est. speed input: 101924.78 toks/s, output: 99.53 toks/s] 
Processed prompts:  45%|████▍     | 230/512 [00:02<00:03, 78.94it/s, est. speed input: 98069.02 toks/s, output: 95.77 toks/s] 
Processed prompts:  47%|████▋     | 243/512 [00:02<00:03, 77.28it/s, est. speed input: 96157.53 toks/s, output: 93.90 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:02<00:03, 73.90it/s, est. speed input: 93923.12 toks/s, output: 91.72 toks/s]
Processed prompts:  52%|█████▏    | 264/512 [00:02<00:03, 75.22it/s, est. speed input: 93501.78 toks/s, output: 91.31 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:03<00:03, 75.97it/s, est. speed input: 93044.92 toks/s, output: 90.86 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:03<00:03, 69.88it/s, est. speed input: 91032.93 toks/s, output: 88.90 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:03<00:03, 70.53it/s, est. speed input: 90492.52 toks/s, output: 88.37 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:03<00:03, 70.69it/s, est. speed input: 89912.41 toks/s, output: 87.80 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:03<00:02, 70.46it/s, est. speed input: 89308.66 toks/s, output: 87.21 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:03<00:02, 70.53it/s, est. speed input: 88780.29 toks/s, output: 86.70 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:03<00:02, 70.29it/s, est. speed input: 88244.86 toks/s, output: 86.18 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:03<00:02, 70.15it/s, est. speed input: 87745.58 toks/s, output: 85.69 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:03<00:02, 70.47it/s, est. speed input: 87328.11 toks/s, output: 85.28 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:04<00:02, 69.62it/s, est. speed input: 86803.87 toks/s, output: 84.77 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:04<00:02, 70.21it/s, est. speed input: 86447.18 toks/s, output: 84.42 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:04<00:02, 69.99it/s, est. speed input: 86037.21 toks/s, output: 84.02 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:04<00:02, 70.04it/s, est. speed input: 85671.60 toks/s, output: 83.66 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:04<00:01, 69.97it/s, est. speed input: 85313.02 toks/s, output: 83.31 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:04<00:01, 70.20it/s, est. speed input: 84999.95 toks/s, output: 83.01 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:04<00:01, 70.19it/s, est. speed input: 84685.07 toks/s, output: 82.70 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:04<00:01, 70.07it/s, est. speed input: 84374.62 toks/s, output: 82.40 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:04<00:01, 69.65it/s, est. speed input: 84047.55 toks/s, output: 82.08 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:05<00:01, 70.39it/s, est. speed input: 83827.48 toks/s, output: 81.86 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:05<00:01, 70.33it/s, est. speed input: 83566.59 toks/s, output: 81.61 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:05<00:01, 70.48it/s, est. speed input: 83332.93 toks/s, output: 81.38 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:05<00:00, 70.38it/s, est. speed input: 83092.34 toks/s, output: 81.14 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:05<00:00, 70.07it/s, est. speed input: 82842.28 toks/s, output: 80.90 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:05<00:00, 70.12it/s, est. speed input: 82623.13 toks/s, output: 80.69 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:05<00:00, 69.74it/s, est. speed input: 82380.12 toks/s, output: 80.45 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:05<00:00, 68.45it/s, est. speed input: 82066.92 toks/s, output: 80.14 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:06<00:00, 67.68it/s, est. speed input: 81774.67 toks/s, output: 79.86 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:06<00:00, 67.20it/s, est. speed input: 81497.78 toks/s, output: 79.59 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:06<00:00, 66.71it/s, est. speed input: 81219.40 toks/s, output: 79.32 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:06<00:00, 66.21it/s, est. speed input: 80939.06 toks/s, output: 79.04 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 66.21it/s, est. speed input: 81113.67 toks/s, output: 79.21 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 79.21it/s, est. speed input: 81113.67 toks/s, output: 79.21 toks/s]
[rank0]:[W125 18:24:17.379752109 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 18:24:20
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:24:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:24:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=250175) WARNING 01-25 18:24:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=250175) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=250175) WARNING 01-25 18:24:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 93.92 requests/s, 96269.42 total tokens/s, 93.92 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-25 18:24:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:24:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:24:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:24:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:24:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:24:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:24:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:24:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:24:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:24:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:24:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:24:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:24:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:24:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:24:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:24:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:24:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:24:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:24:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:24:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:24:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:24:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:24:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:24:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:24:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:24:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:24:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:24:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=250175) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=250175) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.20it/s]
(EngineCore_DP0 pid=250175) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.20it/s]
(EngineCore_DP0 pid=250175) 
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=250175) [2026-01-25 18:24:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=250175) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  7.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  6.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  5.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  6.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  5.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  5.97it/s]
(EngineCore_DP0 pid=250175) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  6.12it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  7.67it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.31it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 27/1024 [00:00<00:03, 268.76it/s]
Adding requests:   5%|▌         | 56/1024 [00:00<00:03, 275.50it/s]
Adding requests:   8%|▊         | 87/1024 [00:00<00:03, 288.05it/s]
Adding requests:  11%|█▏        | 116/1024 [00:00<00:03, 288.33it/s]
Adding requests:  14%|█▍        | 147/1024 [00:00<00:02, 294.77it/s]
Adding requests:  17%|█▋        | 178/1024 [00:00<00:02, 299.55it/s]
Adding requests:  20%|██        | 208/1024 [00:00<00:02, 299.18it/s]
Adding requests:  24%|██▎       | 241/1024 [00:00<00:02, 307.41it/s]
Adding requests:  27%|██▋       | 273/1024 [00:00<00:02, 309.58it/s]
Adding requests:  30%|██▉       | 304/1024 [00:01<00:02, 306.82it/s]
Adding requests:  33%|███▎      | 335/1024 [00:01<00:02, 304.24it/s]
Adding requests:  36%|███▌      | 368/1024 [00:01<00:02, 309.58it/s]
Adding requests:  39%|███▉      | 402/1024 [00:01<00:01, 315.85it/s]
Adding requests:  42%|████▏     | 435/1024 [00:01<00:01, 317.35it/s]
Adding requests:  46%|████▌     | 467/1024 [00:01<00:01, 317.98it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 317.54it/s]
Adding requests:  52%|█████▏    | 532/1024 [00:01<00:01, 306.40it/s]
Adding requests:  55%|█████▌    | 566/1024 [00:01<00:01, 315.77it/s]
Adding requests:  59%|█████▊    | 600/1024 [00:01<00:01, 321.22it/s]
Adding requests:  62%|██████▏   | 636/1024 [00:02<00:01, 330.24it/s]
Adding requests:  66%|██████▌   | 671/1024 [00:02<00:01, 334.18it/s]
Adding requests:  69%|██████▉   | 705/1024 [00:02<00:00, 335.21it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:02<00:00, 337.23it/s]
Adding requests:  76%|███████▌  | 774/1024 [00:02<00:00, 335.98it/s]
Adding requests:  79%|███████▉  | 808/1024 [00:02<00:00, 333.38it/s]
Adding requests:  82%|████████▏ | 842/1024 [00:02<00:00, 332.98it/s]
Adding requests:  86%|████████▌ | 878/1024 [00:02<00:00, 339.91it/s]
Adding requests:  89%|████████▉ | 913/1024 [00:02<00:00, 338.35it/s]
Adding requests:  93%|█████████▎| 948/1024 [00:02<00:00, 340.94it/s]
Adding requests:  96%|█████████▌| 983/1024 [00:03<00:00, 341.70it/s]
Adding requests:  99%|█████████▉| 1018/1024 [00:03<00:00, 343.34it/s]
Adding requests: 100%|██████████| 1024/1024 [00:03<00:00, 320.80it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:00<00:00, 1892.76it/s, est. speed input: 1938537.89 toks/s, output: 1892.87 toks/s]
Processed prompts:  48%|████▊     | 488/1024 [00:02<00:02, 190.99it/s, est. speed input: 234134.25 toks/s, output: 228.65 toks/s]   
Processed prompts:  56%|█████▌    | 572/1024 [00:03<00:03, 150.66it/s, est. speed input: 189990.22 toks/s, output: 185.54 toks/s]
Processed prompts:  61%|██████    | 622/1024 [00:03<00:02, 137.75it/s, est. speed input: 176730.52 toks/s, output: 172.59 toks/s]
Processed prompts:  64%|██████▍   | 656/1024 [00:03<00:02, 130.49it/s, est. speed input: 170037.68 toks/s, output: 166.05 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:04<00:02, 118.86it/s, est. speed input: 162454.13 toks/s, output: 158.65 toks/s]
Processed prompts:  69%|██████▊   | 702/1024 [00:04<00:02, 118.46it/s, est. speed input: 160758.31 toks/s, output: 156.99 toks/s]
Processed prompts:  70%|███████   | 719/1024 [00:04<00:02, 115.25it/s, est. speed input: 158494.40 toks/s, output: 154.78 toks/s]
Processed prompts:  72%|███████▏  | 734/1024 [00:04<00:02, 109.94it/s, est. speed input: 155955.72 toks/s, output: 152.30 toks/s]
Processed prompts:  73%|███████▎  | 747/1024 [00:04<00:02, 102.71it/s, est. speed input: 153200.04 toks/s, output: 149.61 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:05<00:02, 99.15it/s, est. speed input: 151060.72 toks/s, output: 147.52 toks/s] 
Processed prompts:  76%|███████▌  | 778/1024 [00:05<00:02, 98.20it/s, est. speed input: 149371.98 toks/s, output: 145.87 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:05<00:02, 98.66it/s, est. speed input: 148007.30 toks/s, output: 144.54 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:05<00:02, 98.97it/s, est. speed input: 146710.16 toks/s, output: 143.27 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:05<00:01, 99.34it/s, est. speed input: 145503.31 toks/s, output: 142.09 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:05<00:01, 99.49it/s, est. speed input: 144344.28 toks/s, output: 140.96 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:06<00:01, 99.64it/s, est. speed input: 143251.57 toks/s, output: 139.89 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:06<00:01, 99.89it/s, est. speed input: 142230.72 toks/s, output: 138.90 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:06<00:01, 99.76it/s, est. speed input: 141223.25 toks/s, output: 137.91 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:06<00:01, 99.71it/s, est. speed input: 140270.09 toks/s, output: 136.98 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:06<00:01, 99.78it/s, est. speed input: 139373.73 toks/s, output: 136.11 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:06<00:00, 101.46it/s, est. speed input: 138692.15 toks/s, output: 135.44 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:07<00:00, 100.95it/s, est. speed input: 137865.45 toks/s, output: 134.63 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:07<00:00, 100.64it/s, est. speed input: 137080.30 toks/s, output: 133.87 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:07<00:00, 101.98it/s, est. speed input: 136478.44 toks/s, output: 133.28 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:07<00:00, 101.42it/s, est. speed input: 135760.50 toks/s, output: 132.58 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:07<00:00, 102.80it/s, est. speed input: 135231.89 toks/s, output: 132.06 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:07<00:00, 102.80it/s, est. speed input: 136024.42 toks/s, output: 132.84 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:07<00:00, 132.83it/s, est. speed input: 136024.42 toks/s, output: 132.84 toks/s]
[rank0]:[W125 18:25:12.962223813 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-25 18:25:15
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:25:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:25:36 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=251273) WARNING 01-25 18:25:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=251273) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=251273) WARNING 01-25 18:25:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 93.59 requests/s, 95931.32 total tokens/s, 93.59 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-25 18:25:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:25:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:25:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:25:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:25:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:25:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:25:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:25:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:25:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:25:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:25:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:25:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:25:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:25:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:25:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:25:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:25:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:25:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:25:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:25:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:25:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:25:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:25:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:25:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:25:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:25:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:25:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:25:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=251273) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=251273) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=251273) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=251273) 
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=251273) [2026-01-25 18:25:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=251273) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  9.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  5.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  6.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.09it/s]
(EngineCore_DP0 pid=251273) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  4.16it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  6.07it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  7.23it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  7.54it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 275.66it/s]
Adding requests:   3%|▎         | 58/2048 [00:00<00:06, 288.89it/s]
Adding requests:   4%|▍         | 89/2048 [00:00<00:06, 298.18it/s]
Adding requests:   6%|▌         | 120/2048 [00:00<00:06, 302.06it/s]
Adding requests:   7%|▋         | 151/2048 [00:00<00:06, 295.16it/s]
Adding requests:   9%|▉         | 182/2048 [00:00<00:06, 296.28it/s]
Adding requests:  10%|█         | 212/2048 [00:00<00:06, 296.40it/s]
Adding requests:  12%|█▏        | 244/2048 [00:00<00:05, 301.37it/s]
Adding requests:  13%|█▎        | 275/2048 [00:00<00:05, 303.12it/s]
Adding requests:  15%|█▍        | 306/2048 [00:01<00:05, 296.77it/s]
Adding requests:  16%|█▋        | 336/2048 [00:01<00:05, 295.96it/s]
Adding requests:  18%|█▊        | 367/2048 [00:01<00:05, 298.42it/s]
Adding requests:  19%|█▉        | 397/2048 [00:01<00:05, 298.00it/s]
Adding requests:  21%|██        | 429/2048 [00:01<00:05, 304.10it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:05, 305.27it/s]
Adding requests:  24%|██▍       | 492/2048 [00:01<00:05, 309.43it/s]
Adding requests:  26%|██▌       | 523/2048 [00:01<00:05, 303.68it/s]
Adding requests:  27%|██▋       | 556/2048 [00:01<00:04, 309.21it/s]
Adding requests:  29%|██▊       | 588/2048 [00:01<00:04, 311.88it/s]
Adding requests:  30%|███       | 623/2048 [00:02<00:04, 320.54it/s]
Adding requests:  32%|███▏      | 657/2048 [00:02<00:04, 325.44it/s]
Adding requests:  34%|███▍      | 692/2048 [00:02<00:04, 332.41it/s]
Adding requests:  36%|███▌      | 728/2048 [00:02<00:03, 338.08it/s]
Adding requests:  37%|███▋      | 763/2048 [00:02<00:03, 340.19it/s]
Adding requests:  39%|███▉      | 798/2048 [00:02<00:03, 339.40it/s]
Adding requests:  41%|████      | 832/2048 [00:02<00:03, 325.95it/s]
Adding requests:  42%|████▏     | 866/2048 [00:02<00:03, 328.49it/s]
Adding requests:  44%|████▍     | 903/2048 [00:02<00:03, 338.98it/s]
Adding requests:  46%|████▌     | 937/2048 [00:02<00:03, 338.93it/s]
Adding requests:  48%|████▊     | 973/2048 [00:03<00:03, 343.26it/s]
Adding requests:  49%|████▉     | 1010/2048 [00:03<00:02, 348.58it/s]
Adding requests:  51%|█████     | 1045/2048 [00:03<00:02, 348.96it/s]
Adding requests:  53%|█████▎    | 1080/2048 [00:03<00:02, 343.06it/s]
Adding requests:  54%|█████▍    | 1115/2048 [00:03<00:02, 341.34it/s]
Adding requests:  56%|█████▌    | 1151/2048 [00:03<00:02, 344.57it/s]
Adding requests:  58%|█████▊    | 1188/2048 [00:03<00:02, 349.18it/s]
Adding requests:  60%|█████▉    | 1224/2048 [00:03<00:02, 351.90it/s]
Adding requests:  62%|██████▏   | 1260/2048 [00:03<00:02, 346.49it/s]
Adding requests:  63%|██████▎   | 1295/2048 [00:04<00:02, 335.40it/s]
Adding requests:  65%|██████▍   | 1329/2048 [00:04<00:02, 336.31it/s]
Adding requests:  67%|██████▋   | 1363/2048 [00:04<00:02, 330.88it/s]
Adding requests:  68%|██████▊   | 1397/2048 [00:04<00:01, 326.73it/s]
Adding requests:  70%|██████▉   | 1431/2048 [00:04<00:01, 327.85it/s]
Adding requests:  72%|███████▏  | 1465/2048 [00:04<00:01, 331.32it/s]
Adding requests:  73%|███████▎  | 1499/2048 [00:04<00:01, 332.58it/s]
Adding requests:  75%|███████▍  | 1533/2048 [00:04<00:01, 332.87it/s]
Adding requests:  77%|███████▋  | 1567/2048 [00:04<00:01, 332.77it/s]
Adding requests:  78%|███████▊  | 1601/2048 [00:04<00:01, 333.79it/s]
Adding requests:  80%|███████▉  | 1635/2048 [00:05<00:01, 332.13it/s]
Adding requests:  81%|████████▏ | 1669/2048 [00:05<00:01, 326.23it/s]
Adding requests:  83%|████████▎ | 1702/2048 [00:05<00:01, 322.91it/s]
Adding requests:  85%|████████▍ | 1735/2048 [00:05<00:00, 323.54it/s]
Adding requests:  86%|████████▋ | 1768/2048 [00:05<00:00, 323.20it/s]
Adding requests:  88%|████████▊ | 1801/2048 [00:05<00:00, 325.03it/s]
Adding requests:  90%|████████▉ | 1835/2048 [00:05<00:00, 328.46it/s]
Adding requests:  91%|█████████ | 1868/2048 [00:05<00:00, 326.08it/s]
Adding requests:  93%|█████████▎| 1903/2048 [00:05<00:00, 330.67it/s]
Adding requests:  95%|█████████▍| 1937/2048 [00:05<00:00, 330.88it/s]
Adding requests:  96%|█████████▌| 1971/2048 [00:06<00:00, 310.86it/s]
Adding requests:  98%|█████████▊| 2005/2048 [00:06<00:00, 318.99it/s]
Adding requests: 100%|█████████▉| 2038/2048 [00:06<00:00, 313.67it/s]
Adding requests: 100%|██████████| 2048/2048 [00:06<00:00, 323.39it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:00<00:00, 2873.96it/s, est. speed input: 2943366.34 toks/s, output: 2874.09 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:03<00:05, 211.55it/s, est. speed input: 265959.59 toks/s, output: 259.72 toks/s]   
Processed prompts:  48%|████▊     | 990/2048 [00:04<00:05, 178.79it/s, est. speed input: 227924.10 toks/s, output: 222.58 toks/s]
Processed prompts:  52%|█████▏    | 1063/2048 [00:05<00:06, 156.57it/s, est. speed input: 207242.04 toks/s, output: 202.38 toks/s]
Processed prompts:  54%|█████▍    | 1111/2048 [00:05<00:06, 146.36it/s, est. speed input: 198338.48 toks/s, output: 193.69 toks/s]
Processed prompts:  56%|█████▌    | 1145/2048 [00:06<00:06, 139.68it/s, est. speed input: 193153.03 toks/s, output: 188.63 toks/s]
Processed prompts:  57%|█████▋    | 1171/2048 [00:06<00:06, 128.39it/s, est. speed input: 187055.92 toks/s, output: 182.67 toks/s]
Processed prompts:  58%|█████▊    | 1191/2048 [00:06<00:06, 126.74it/s, est. speed input: 185190.68 toks/s, output: 180.85 toks/s]
Processed prompts:  59%|█████▉    | 1209/2048 [00:06<00:06, 123.31it/s, est. speed input: 183115.52 toks/s, output: 178.82 toks/s]
Processed prompts:  60%|█████▉    | 1225/2048 [00:06<00:06, 117.92it/s, est. speed input: 180854.57 toks/s, output: 176.62 toks/s]
Processed prompts:  60%|██████    | 1239/2048 [00:07<00:07, 110.53it/s, est. speed input: 178414.22 toks/s, output: 174.23 toks/s]
Processed prompts:  61%|██████    | 1251/2048 [00:07<00:07, 101.38it/s, est. speed input: 175809.27 toks/s, output: 171.69 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:07<00:08, 97.62it/s, est. speed input: 173736.17 toks/s, output: 169.66 toks/s] 
Processed prompts:  63%|██████▎   | 1282/2048 [00:07<00:07, 96.06it/s, est. speed input: 171901.04 toks/s, output: 167.87 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:07<00:07, 94.89it/s, est. speed input: 170155.93 toks/s, output: 166.17 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:07<00:07, 93.96it/s, est. speed input: 168481.70 toks/s, output: 164.53 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:08<00:07, 93.12it/s, est. speed input: 166860.18 toks/s, output: 162.95 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:08<00:07, 92.66it/s, est. speed input: 165326.37 toks/s, output: 161.45 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:08<00:07, 92.43it/s, est. speed input: 163867.99 toks/s, output: 160.03 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:08<00:07, 92.15it/s, est. speed input: 162454.47 toks/s, output: 158.65 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:08<00:07, 91.93it/s, est. speed input: 161094.64 toks/s, output: 157.32 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:09<00:06, 91.82it/s, est. speed input: 159792.45 toks/s, output: 156.05 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:09<00:06, 91.70it/s, est. speed input: 158534.31 toks/s, output: 154.82 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:09<00:06, 91.66it/s, est. speed input: 157328.34 toks/s, output: 153.64 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:09<00:06, 91.57it/s, est. speed input: 156160.50 toks/s, output: 152.50 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:09<00:06, 91.50it/s, est. speed input: 155033.37 toks/s, output: 151.40 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:09<00:06, 91.45it/s, est. speed input: 153945.73 toks/s, output: 150.34 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:10<00:05, 91.20it/s, est. speed input: 152876.14 toks/s, output: 149.29 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:10<00:05, 91.20it/s, est. speed input: 151858.98 toks/s, output: 148.30 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:10<00:05, 91.50it/s, est. speed input: 150903.84 toks/s, output: 147.37 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:10<00:05, 93.56it/s, est. speed input: 150143.20 toks/s, output: 146.62 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:10<00:05, 95.10it/s, est. speed input: 149407.85 toks/s, output: 145.91 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:10<00:04, 96.18it/s, est. speed input: 148693.07 toks/s, output: 145.21 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:11<00:04, 97.00it/s, est. speed input: 148002.41 toks/s, output: 144.53 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:11<00:04, 97.59it/s, est. speed input: 147332.04 toks/s, output: 143.88 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:11<00:04, 98.13it/s, est. speed input: 146689.64 toks/s, output: 143.25 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:11<00:04, 98.45it/s, est. speed input: 146060.63 toks/s, output: 142.64 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:11<00:03, 98.65it/s, est. speed input: 145447.76 toks/s, output: 142.04 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:11<00:03, 98.74it/s, est. speed input: 144847.16 toks/s, output: 141.45 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:12<00:03, 98.76it/s, est. speed input: 144260.87 toks/s, output: 140.88 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:12<00:03, 98.80it/s, est. speed input: 143691.58 toks/s, output: 140.32 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:12<00:03, 98.86it/s, est. speed input: 143139.09 toks/s, output: 139.78 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:12<00:03, 98.95it/s, est. speed input: 142603.40 toks/s, output: 139.26 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:12<00:02, 99.02it/s, est. speed input: 142081.87 toks/s, output: 138.75 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:12<00:02, 98.97it/s, est. speed input: 141567.79 toks/s, output: 138.25 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:13<00:02, 98.91it/s, est. speed input: 141065.10 toks/s, output: 137.76 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:13<00:02, 98.96it/s, est. speed input: 140580.01 toks/s, output: 137.28 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:13<00:02, 98.86it/s, est. speed input: 140098.66 toks/s, output: 136.81 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:13<00:02, 98.83it/s, est. speed input: 139631.11 toks/s, output: 136.36 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:13<00:01, 98.81it/s, est. speed input: 139175.22 toks/s, output: 135.91 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:13<00:01, 100.45it/s, est. speed input: 138818.47 toks/s, output: 135.56 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:13<00:01, 99.47it/s, est. speed input: 138357.62 toks/s, output: 135.11 toks/s] 
Processed prompts:  93%|█████████▎| 1906/2048 [00:14<00:01, 96.78it/s, est. speed input: 137797.40 toks/s, output: 134.57 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:14<00:01, 95.00it/s, est. speed input: 137251.80 toks/s, output: 134.03 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:14<00:01, 93.81it/s, est. speed input: 136720.86 toks/s, output: 133.52 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:14<00:00, 94.70it/s, est. speed input: 136298.22 toks/s, output: 133.10 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:14<00:00, 93.59it/s, est. speed input: 135789.75 toks/s, output: 132.61 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:15<00:00, 92.86it/s, est. speed input: 135294.71 toks/s, output: 132.12 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:15<00:00, 92.34it/s, est. speed input: 134810.64 toks/s, output: 131.65 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:15<00:00, 92.06it/s, est. speed input: 134341.53 toks/s, output: 131.19 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:15<00:00, 93.61it/s, est. speed input: 133976.60 toks/s, output: 130.84 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:15<00:00, 93.61it/s, est. speed input: 134894.53 toks/s, output: 131.73 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:15<00:00, 131.73it/s, est. speed input: 134894.53 toks/s, output: 131.73 toks/s]
[rank0]:[W125 18:26:27.384929524 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-25 18:26:28
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:27:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:27:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=252708) WARNING 01-25 18:27:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=252708) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=252708) WARNING 01-25 18:27:18 [backends.py:609] Failed to read file <frozen os>
Throughput: 93.27 requests/s, 95597.42 total tokens/s, 93.27 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-25 18:27:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:27:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:27:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:27:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:27:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:27:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:27:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:27:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:27:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:27:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:27:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:27:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:27:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:27:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:27:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:27:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:27:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:27:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:27:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:27:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:27:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:27:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:27:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:27:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:27:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:27:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:27:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:27:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=252708) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=252708) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.18it/s]
(EngineCore_DP0 pid=252708) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.18it/s]
(EngineCore_DP0 pid=252708) 
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=252708) [2026-01-25 18:27:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=252708) [rank0]:W0125 18:27:23.805000 252708 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=252708) [rank0]:W0125 18:27:23.885000 252708 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=252708) [rank0]:W0125 18:27:25.063000 252708 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=252708) [rank0]:W0125 18:27:25.187000 252708 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=252708) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  9.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, -19.33it/s]
(EngineCore_DP0 pid=252708) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  6.39it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:01,  4.80it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  6.97it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  6.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  7.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  6.94it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 277.38it/s]
Adding requests:   1%|▏         | 58/4096 [00:00<00:14, 285.76it/s]
Adding requests:   2%|▏         | 88/4096 [00:00<00:13, 290.78it/s]
Adding requests:   3%|▎         | 118/4096 [00:00<00:13, 292.59it/s]
Adding requests:   4%|▎         | 148/4096 [00:00<00:13, 292.99it/s]
Adding requests:   4%|▍         | 178/4096 [00:00<00:13, 288.33it/s]
Adding requests:   5%|▌         | 208/4096 [00:00<00:13, 290.94it/s]
Adding requests:   6%|▌         | 240/4096 [00:00<00:12, 297.65it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:13, 291.84it/s]
Adding requests:   7%|▋         | 300/4096 [00:01<00:13, 287.76it/s]
Adding requests:   8%|▊         | 331/4096 [00:01<00:12, 292.37it/s]
Adding requests:   9%|▉         | 362/4096 [00:01<00:12, 296.14it/s]
Adding requests:  10%|▉         | 392/4096 [00:01<00:12, 295.74it/s]
Adding requests:  10%|█         | 423/4096 [00:01<00:12, 299.48it/s]
Adding requests:  11%|█         | 454/4096 [00:01<00:12, 302.42it/s]
Adding requests:  12%|█▏        | 486/4096 [00:01<00:11, 307.38it/s]
Adding requests:  13%|█▎        | 517/4096 [00:01<00:11, 301.36it/s]
Adding requests:  13%|█▎        | 548/4096 [00:01<00:11, 297.19it/s]
Adding requests:  14%|█▍        | 579/4096 [00:01<00:11, 299.48it/s]
Adding requests:  15%|█▍        | 610/4096 [00:02<00:11, 301.27it/s]
Adding requests:  16%|█▌        | 644/4096 [00:02<00:11, 310.61it/s]
Adding requests:  17%|█▋        | 678/4096 [00:02<00:10, 317.92it/s]
Adding requests:  17%|█▋        | 710/4096 [00:02<00:10, 313.78it/s]
Adding requests:  18%|█▊        | 742/4096 [00:02<00:10, 314.46it/s]
Adding requests:  19%|█▉        | 775/4096 [00:02<00:10, 318.34it/s]
Adding requests:  20%|█▉        | 807/4096 [00:02<00:10, 311.66it/s]
Adding requests:  20%|██        | 839/4096 [00:02<00:10, 312.13it/s]
Adding requests:  21%|██▏       | 873/4096 [00:02<00:10, 318.46it/s]
Adding requests:  22%|██▏       | 906/4096 [00:02<00:09, 320.83it/s]
Adding requests:  23%|██▎       | 939/4096 [00:03<00:09, 318.02it/s]
Adding requests:  24%|██▎       | 972/4096 [00:03<00:09, 320.22it/s]
Adding requests:  25%|██▍       | 1006/4096 [00:03<00:09, 323.66it/s]
Adding requests:  25%|██▌       | 1040/4096 [00:03<00:09, 327.24it/s]
Adding requests:  26%|██▌       | 1073/4096 [00:03<00:09, 322.26it/s]
Adding requests:  27%|██▋       | 1106/4096 [00:03<00:09, 319.64it/s]
Adding requests:  28%|██▊       | 1138/4096 [00:03<00:09, 317.65it/s]
Adding requests:  29%|██▊       | 1170/4096 [00:03<00:09, 313.40it/s]
Adding requests:  29%|██▉       | 1206/4096 [00:03<00:08, 325.89it/s]
Adding requests:  30%|███       | 1239/4096 [00:04<00:08, 326.35it/s]
Adding requests:  31%|███       | 1272/4096 [00:04<00:08, 326.11it/s]
Adding requests:  32%|███▏      | 1306/4096 [00:04<00:08, 329.17it/s]
Adding requests:  33%|███▎      | 1342/4096 [00:04<00:08, 336.14it/s]
Adding requests:  34%|███▎      | 1376/4096 [00:04<00:08, 329.52it/s]
Adding requests:  34%|███▍      | 1411/4096 [00:04<00:08, 333.99it/s]
Adding requests:  35%|███▌      | 1445/4096 [00:04<00:07, 334.89it/s]
Adding requests:  36%|███▌      | 1481/4096 [00:04<00:07, 340.67it/s]
Adding requests:  37%|███▋      | 1519/4096 [00:04<00:07, 349.81it/s]
Adding requests:  38%|███▊      | 1555/4096 [00:04<00:07, 351.24it/s]
Adding requests:  39%|███▉      | 1592/4096 [00:05<00:07, 354.71it/s]
Adding requests:  40%|███▉      | 1629/4096 [00:05<00:06, 358.45it/s]
Adding requests:  41%|████      | 1665/4096 [00:05<00:06, 350.16it/s]
Adding requests:  42%|████▏     | 1701/4096 [00:05<00:06, 348.23it/s]
Adding requests:  42%|████▏     | 1738/4096 [00:05<00:06, 352.28it/s]
Adding requests:  43%|████▎     | 1774/4096 [00:05<00:06, 347.38it/s]
Adding requests:  44%|████▍     | 1810/4096 [00:05<00:06, 349.69it/s]
Adding requests:  45%|████▌     | 1847/4096 [00:05<00:06, 353.27it/s]
Adding requests:  46%|████▌     | 1883/4096 [00:05<00:06, 353.95it/s]
Adding requests:  47%|████▋     | 1919/4096 [00:05<00:06, 347.97it/s]
Adding requests:  48%|████▊     | 1954/4096 [00:06<00:06, 347.38it/s]
Adding requests:  49%|████▊     | 1991/4096 [00:06<00:05, 351.14it/s]
Adding requests:  49%|████▉     | 2027/4096 [00:06<00:05, 351.47it/s]
Adding requests:  50%|█████     | 2063/4096 [00:06<00:05, 343.36it/s]
Adding requests:  51%|█████     | 2098/4096 [00:06<00:05, 333.67it/s]
Adding requests:  52%|█████▏    | 2132/4096 [00:06<00:06, 318.53it/s]
Adding requests:  53%|█████▎    | 2165/4096 [00:06<00:06, 315.57it/s]
Adding requests:  54%|█████▎    | 2197/4096 [00:06<00:06, 314.23it/s]
Adding requests:  54%|█████▍    | 2230/4096 [00:06<00:05, 316.49it/s]
Adding requests:  55%|█████▌    | 2262/4096 [00:07<00:05, 314.03it/s]
Adding requests:  56%|█████▌    | 2296/4096 [00:07<00:05, 319.15it/s]
Adding requests:  57%|█████▋    | 2329/4096 [00:07<00:05, 319.89it/s]
Adding requests:  58%|█████▊    | 2362/4096 [00:07<00:05, 303.66it/s]
Adding requests:  58%|█████▊    | 2394/4096 [00:07<00:05, 305.52it/s]
Adding requests:  59%|█████▉    | 2425/4096 [00:07<00:05, 306.00it/s]
Adding requests:  60%|█████▉    | 2456/4096 [00:07<00:05, 305.45it/s]
Adding requests:  61%|██████    | 2489/4096 [00:07<00:05, 311.11it/s]
Adding requests:  62%|██████▏   | 2522/4096 [00:07<00:04, 315.24it/s]
Adding requests:  62%|██████▏   | 2556/4096 [00:07<00:04, 321.69it/s]
Adding requests:  63%|██████▎   | 2589/4096 [00:08<00:04, 316.30it/s]
Adding requests:  64%|██████▍   | 2622/4096 [00:08<00:04, 320.13it/s]
Adding requests:  65%|██████▍   | 2657/4096 [00:08<00:04, 327.05it/s]
Adding requests:  66%|██████▌   | 2690/4096 [00:08<00:04, 322.72it/s]
Adding requests:  66%|██████▋   | 2723/4096 [00:08<00:04, 320.92it/s]
Adding requests:  67%|██████▋   | 2756/4096 [00:08<00:04, 314.08it/s]
Adding requests:  68%|██████▊   | 2788/4096 [00:08<00:04, 311.46it/s]
Adding requests:  69%|██████▉   | 2820/4096 [00:08<00:04, 310.85it/s]
Adding requests:  70%|██████▉   | 2854/4096 [00:08<00:03, 318.55it/s]
Adding requests:  71%|███████   | 2888/4096 [00:09<00:03, 324.09it/s]
Adding requests:  71%|███████▏  | 2921/4096 [00:09<00:03, 320.22it/s]
Adding requests:  72%|███████▏  | 2954/4096 [00:09<00:03, 321.31it/s]
Adding requests:  73%|███████▎  | 2988/4096 [00:09<00:03, 325.34it/s]
Adding requests:  74%|███████▍  | 3021/4096 [00:09<00:03, 323.42it/s]
Adding requests:  75%|███████▍  | 3054/4096 [00:09<00:03, 322.30it/s]
Adding requests:  75%|███████▌  | 3087/4096 [00:09<00:03, 321.90it/s]
Adding requests:  76%|███████▌  | 3120/4096 [00:09<00:03, 322.68it/s]
Adding requests:  77%|███████▋  | 3153/4096 [00:09<00:03, 311.13it/s]
Adding requests:  78%|███████▊  | 3186/4096 [00:09<00:02, 315.79it/s]
Adding requests:  79%|███████▊  | 3218/4096 [00:10<00:02, 315.29it/s]
Adding requests:  79%|███████▉  | 3250/4096 [00:10<00:02, 314.04it/s]
Adding requests:  80%|████████  | 3282/4096 [00:10<00:02, 314.97it/s]
Adding requests:  81%|████████  | 3315/4096 [00:10<00:02, 317.68it/s]
Adding requests:  82%|████████▏ | 3348/4096 [00:10<00:02, 319.17it/s]
Adding requests:  83%|████████▎ | 3380/4096 [00:10<00:02, 313.76it/s]
Adding requests:  83%|████████▎ | 3412/4096 [00:10<00:02, 311.67it/s]
Adding requests:  84%|████████▍ | 3444/4096 [00:10<00:02, 309.46it/s]
Adding requests:  85%|████████▍ | 3475/4096 [00:10<00:02, 297.37it/s]
Adding requests:  86%|████████▌ | 3508/4096 [00:10<00:01, 306.04it/s]
Adding requests:  86%|████████▋ | 3541/4096 [00:11<00:01, 311.13it/s]
Adding requests:  87%|████████▋ | 3573/4096 [00:11<00:01, 310.47it/s]
Adding requests:  88%|████████▊ | 3606/4096 [00:11<00:01, 315.44it/s]
Adding requests:  89%|████████▉ | 3638/4096 [00:11<00:01, 315.50it/s]
Adding requests:  90%|████████▉ | 3670/4096 [00:11<00:01, 300.39it/s]
Adding requests:  90%|█████████ | 3703/4096 [00:11<00:01, 307.46it/s]
Adding requests:  91%|█████████ | 3735/4096 [00:11<00:01, 309.75it/s]
Adding requests:  92%|█████████▏| 3769/4096 [00:11<00:01, 316.23it/s]
Adding requests:  93%|█████████▎| 3801/4096 [00:11<00:00, 312.82it/s]
Adding requests:  94%|█████████▎| 3834/4096 [00:12<00:00, 315.72it/s]
Adding requests:  94%|█████████▍| 3866/4096 [00:12<00:00, 315.48it/s]
Adding requests:  95%|█████████▌| 3898/4096 [00:12<00:00, 312.55it/s]
Adding requests:  96%|█████████▌| 3931/4096 [00:12<00:00, 315.98it/s]
Adding requests:  97%|█████████▋| 3964/4096 [00:12<00:00, 318.67it/s]
Adding requests:  98%|█████████▊| 3997/4096 [00:12<00:00, 321.67it/s]
Adding requests:  98%|█████████▊| 4032/4096 [00:12<00:00, 327.56it/s]
Adding requests:  99%|█████████▉| 4066/4096 [00:12<00:00, 331.08it/s]
Adding requests: 100%|██████████| 4096/4096 [00:12<00:00, 319.33it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  28%|██▊       | 1153/4096 [00:00<00:00, 3719.47it/s, est. speed input: 3809109.29 toks/s, output: 3719.57 toks/s]
Processed prompts:  37%|███▋      | 1525/4096 [00:04<00:08, 298.16it/s, est. speed input: 385815.64 toks/s, output: 376.77 toks/s]   
Processed prompts:  41%|████      | 1685/4096 [00:05<00:11, 218.39it/s, est. speed input: 297934.15 toks/s, output: 290.95 toks/s]
Processed prompts:  43%|████▎     | 1776/4096 [00:06<00:12, 185.73it/s, est. speed input: 266007.15 toks/s, output: 259.77 toks/s]
Processed prompts:  45%|████▍     | 1835/4096 [00:07<00:13, 168.80it/s, est. speed input: 251160.99 toks/s, output: 245.27 toks/s]
Processed prompts:  46%|████▌     | 1876/4096 [00:07<00:13, 164.58it/s, est. speed input: 246447.52 toks/s, output: 240.67 toks/s]
Processed prompts:  47%|████▋     | 1908/4096 [00:08<00:14, 155.44it/s, est. speed input: 240701.34 toks/s, output: 235.06 toks/s]
Processed prompts:  47%|████▋     | 1933/4096 [00:08<00:15, 142.47it/s, est. speed input: 234551.15 toks/s, output: 229.05 toks/s]
Processed prompts:  48%|████▊     | 1953/4096 [00:08<00:16, 127.50it/s, est. speed input: 228438.22 toks/s, output: 223.08 toks/s]
Processed prompts:  48%|████▊     | 1985/4096 [00:09<00:17, 121.43it/s, est. speed input: 223944.68 toks/s, output: 218.70 toks/s]
Processed prompts:  49%|████▉     | 2017/4096 [00:09<00:17, 116.19it/s, est. speed input: 219749.90 toks/s, output: 214.60 toks/s]
Processed prompts:  50%|█████     | 2049/4096 [00:09<00:18, 110.38it/s, est. speed input: 215438.67 toks/s, output: 210.39 toks/s]
Processed prompts:  51%|█████     | 2081/4096 [00:10<00:19, 105.17it/s, est. speed input: 211228.60 toks/s, output: 206.28 toks/s]
Processed prompts:  52%|█████▏    | 2113/4096 [00:10<00:19, 101.31it/s, est. speed input: 207297.84 toks/s, output: 202.44 toks/s]
Processed prompts:  52%|█████▏    | 2145/4096 [00:10<00:19, 98.49it/s, est. speed input: 203618.56 toks/s, output: 198.85 toks/s] 
Processed prompts:  53%|█████▎    | 2177/4096 [00:11<00:19, 96.46it/s, est. speed input: 200169.14 toks/s, output: 195.48 toks/s]
Processed prompts:  54%|█████▍    | 2209/4096 [00:11<00:19, 96.80it/s, est. speed input: 197308.05 toks/s, output: 192.68 toks/s]
Processed prompts:  55%|█████▍    | 2241/4096 [00:11<00:19, 95.07it/s, est. speed input: 194211.66 toks/s, output: 189.66 toks/s]
Processed prompts:  55%|█████▌    | 2273/4096 [00:12<00:19, 94.80it/s, est. speed input: 191476.68 toks/s, output: 186.99 toks/s]
Processed prompts:  56%|█████▋    | 2305/4096 [00:12<00:19, 93.80it/s, est. speed input: 188742.63 toks/s, output: 184.32 toks/s]
Processed prompts:  57%|█████▋    | 2337/4096 [00:12<00:18, 93.93it/s, est. speed input: 186304.26 toks/s, output: 181.94 toks/s]
Processed prompts:  58%|█████▊    | 2369/4096 [00:13<00:18, 93.14it/s, est. speed input: 183839.24 toks/s, output: 179.53 toks/s]
Processed prompts:  59%|█████▊    | 2401/4096 [00:13<00:18, 92.59it/s, est. speed input: 181502.65 toks/s, output: 177.25 toks/s]
Processed prompts:  59%|█████▉    | 2433/4096 [00:13<00:18, 92.25it/s, est. speed input: 179290.74 toks/s, output: 175.09 toks/s]
Processed prompts:  60%|██████    | 2465/4096 [00:14<00:17, 92.03it/s, est. speed input: 177189.97 toks/s, output: 173.04 toks/s]
Processed prompts:  61%|██████    | 2497/4096 [00:14<00:17, 93.24it/s, est. speed input: 175395.07 toks/s, output: 171.28 toks/s]
Processed prompts:  62%|██████▏   | 2529/4096 [00:14<00:16, 94.77it/s, est. speed input: 173773.10 toks/s, output: 169.70 toks/s]
Processed prompts:  63%|██████▎   | 2561/4096 [00:15<00:15, 96.03it/s, est. speed input: 172239.25 toks/s, output: 168.20 toks/s]
Processed prompts:  63%|██████▎   | 2593/4096 [00:15<00:15, 97.72it/s, est. speed input: 170867.88 toks/s, output: 166.86 toks/s]
Processed prompts:  64%|██████▍   | 2625/4096 [00:15<00:14, 98.13it/s, est. speed input: 169455.73 toks/s, output: 165.48 toks/s]
Processed prompts:  69%|██████▉   | 2817/4096 [00:16<00:05, 240.72it/s, est. speed input: 177960.90 toks/s, output: 173.79 toks/s]
Processed prompts:  70%|██████▉   | 2849/4096 [00:16<00:06, 199.04it/s, est. speed input: 176464.27 toks/s, output: 172.33 toks/s]
Processed prompts:  70%|███████   | 2881/4096 [00:16<00:07, 169.48it/s, est. speed input: 175027.18 toks/s, output: 170.92 toks/s]
Processed prompts:  71%|███████   | 2913/4096 [00:17<00:07, 148.57it/s, est. speed input: 173643.47 toks/s, output: 169.57 toks/s]
Processed prompts:  72%|███████▏  | 2945/4096 [00:17<00:08, 133.79it/s, est. speed input: 172308.82 toks/s, output: 168.27 toks/s]
Processed prompts:  73%|███████▎  | 2977/4096 [00:17<00:09, 120.66it/s, est. speed input: 170786.85 toks/s, output: 166.78 toks/s]
Processed prompts:  73%|███████▎  | 3009/4096 [00:18<00:09, 111.55it/s, est. speed input: 169301.10 toks/s, output: 165.33 toks/s]
Processed prompts:  74%|███████▍  | 3041/4096 [00:18<00:10, 105.35it/s, est. speed input: 167872.86 toks/s, output: 163.94 toks/s]
Processed prompts:  75%|███████▌  | 3073/4096 [00:18<00:10, 101.07it/s, est. speed input: 166494.53 toks/s, output: 162.59 toks/s]
Processed prompts:  76%|███████▌  | 3105/4096 [00:19<00:10, 98.13it/s, est. speed input: 165168.44 toks/s, output: 161.30 toks/s] 
Processed prompts:  77%|███████▋  | 3137/4096 [00:19<00:09, 96.92it/s, est. speed input: 163972.00 toks/s, output: 160.13 toks/s]
Processed prompts:  77%|███████▋  | 3169/4096 [00:19<00:09, 95.26it/s, est. speed input: 162738.51 toks/s, output: 158.92 toks/s]
Processed prompts:  78%|███████▊  | 3201/4096 [00:20<00:09, 94.07it/s, est. speed input: 161543.51 toks/s, output: 157.76 toks/s]
Processed prompts:  79%|███████▉  | 3233/4096 [00:20<00:09, 93.24it/s, est. speed input: 160388.06 toks/s, output: 156.63 toks/s]
Processed prompts:  80%|███████▉  | 3265/4096 [00:20<00:08, 92.68it/s, est. speed input: 159273.95 toks/s, output: 155.54 toks/s]
Processed prompts:  80%|████████  | 3297/4096 [00:21<00:08, 92.24it/s, est. speed input: 158191.32 toks/s, output: 154.48 toks/s]
Processed prompts:  81%|████████▏ | 3329/4096 [00:21<00:08, 92.01it/s, est. speed input: 157150.26 toks/s, output: 153.47 toks/s]
Processed prompts:  82%|████████▏ | 3361/4096 [00:22<00:08, 91.82it/s, est. speed input: 156140.36 toks/s, output: 152.48 toks/s]
Processed prompts:  83%|████████▎ | 3393/4096 [00:22<00:07, 91.86it/s, est. speed input: 155176.93 toks/s, output: 151.54 toks/s]
Processed prompts:  84%|████████▎ | 3425/4096 [00:22<00:07, 91.53it/s, est. speed input: 154212.58 toks/s, output: 150.60 toks/s]
Processed prompts:  84%|████████▍ | 3457/4096 [00:23<00:06, 93.13it/s, est. speed input: 153430.14 toks/s, output: 149.83 toks/s]
Processed prompts:  85%|████████▌ | 3489/4096 [00:23<00:06, 94.79it/s, est. speed input: 152708.79 toks/s, output: 149.13 toks/s]
Processed prompts:  86%|████████▌ | 3521/4096 [00:23<00:05, 95.99it/s, est. speed input: 152007.46 toks/s, output: 148.44 toks/s]
Processed prompts:  87%|████████▋ | 3553/4096 [00:24<00:05, 96.87it/s, est. speed input: 151326.89 toks/s, output: 147.78 toks/s]
Processed prompts:  88%|████████▊ | 3585/4096 [00:24<00:05, 97.48it/s, est. speed input: 150662.55 toks/s, output: 147.13 toks/s]
Processed prompts:  88%|████████▊ | 3617/4096 [00:24<00:04, 97.92it/s, est. speed input: 150016.68 toks/s, output: 146.50 toks/s]
Processed prompts:  89%|████████▉ | 3649/4096 [00:25<00:04, 98.21it/s, est. speed input: 149386.47 toks/s, output: 145.88 toks/s]
Processed prompts:  90%|████████▉ | 3681/4096 [00:25<00:04, 98.46it/s, est. speed input: 148774.36 toks/s, output: 145.29 toks/s]
Processed prompts:  91%|█████████ | 3713/4096 [00:25<00:03, 98.83it/s, est. speed input: 148190.29 toks/s, output: 144.72 toks/s]
Processed prompts:  91%|█████████▏| 3745/4096 [00:26<00:03, 96.45it/s, est. speed input: 147453.69 toks/s, output: 144.00 toks/s]
Processed prompts:  92%|█████████▏| 3777/4096 [00:26<00:03, 94.86it/s, est. speed input: 146736.89 toks/s, output: 143.30 toks/s]
Processed prompts:  93%|█████████▎| 3809/4096 [00:26<00:03, 93.76it/s, est. speed input: 146038.30 toks/s, output: 142.62 toks/s]
Processed prompts:  94%|█████████▍| 3841/4096 [00:27<00:02, 93.01it/s, est. speed input: 145357.62 toks/s, output: 141.95 toks/s]
Processed prompts:  95%|█████████▍| 3873/4096 [00:27<00:02, 92.48it/s, est. speed input: 144693.57 toks/s, output: 141.30 toks/s]
Processed prompts:  95%|█████████▌| 3905/4096 [00:27<00:02, 92.14it/s, est. speed input: 144048.59 toks/s, output: 140.67 toks/s]
Processed prompts:  96%|█████████▌| 3937/4096 [00:28<00:01, 91.92it/s, est. speed input: 143420.27 toks/s, output: 140.06 toks/s]
Processed prompts:  97%|█████████▋| 3969/4096 [00:28<00:01, 91.87it/s, est. speed input: 142813.95 toks/s, output: 139.47 toks/s]
Processed prompts:  98%|█████████▊| 4001/4096 [00:28<00:01, 91.64it/s, est. speed input: 142210.43 toks/s, output: 138.88 toks/s]
Processed prompts:  98%|█████████▊| 4033/4096 [00:29<00:00, 92.33it/s, est. speed input: 141672.96 toks/s, output: 138.35 toks/s]
Processed prompts:  99%|█████████▉| 4065/4096 [00:29<00:00, 92.48it/s, est. speed input: 141128.10 toks/s, output: 137.82 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:29<00:00, 92.48it/s, est. speed input: 142201.21 toks/s, output: 138.87 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:29<00:00, 138.87it/s, est. speed input: 142201.21 toks/s, output: 138.87 toks/s]
[rank0]:[W125 18:28:14.838672364 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-25 18:28:16
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:29:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:29:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=254818) WARNING 01-25 18:29:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=254818) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=254818) WARNING 01-25 18:29:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 59.38 requests/s, 60864.38 total tokens/s, 59.38 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-25 18:29:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:29:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:29:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:29:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:29:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:29:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:29:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:29:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:29:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:29:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:29:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:29:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:29:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=254818) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=254818) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=254818) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=254818) 
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=254818) [2026-01-25 18:29:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=254818) [rank0]:W0125 18:29:35.579000 254818 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=254818) [rank0]:W0125 18:29:35.663000 254818 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=254818) [rank0]:W0125 18:29:36.810000 254818 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=254818) [rank0]:W0125 18:29:36.915000 254818 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=254818) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  6.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  5.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:03,  4.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:02,  5.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:02,  6.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  6.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  9.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  9.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  9.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.51it/s]
(EngineCore_DP0 pid=254818) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.47it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  9.22it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  9.46it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  9.75it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  9.44it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  7.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  7.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.36it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 28/8192 [00:00<00:29, 276.42it/s]
Adding requests:   1%|          | 58/8192 [00:00<00:28, 286.24it/s]
Adding requests:   1%|          | 89/8192 [00:00<00:27, 292.91it/s]
Adding requests:   1%|▏         | 120/8192 [00:00<00:27, 297.05it/s]
Adding requests:   2%|▏         | 151/8192 [00:00<00:27, 297.63it/s]
Adding requests:   2%|▏         | 181/8192 [00:00<00:26, 297.30it/s]
Adding requests:   3%|▎         | 211/8192 [00:00<00:27, 290.86it/s]
Adding requests:   3%|▎         | 241/8192 [00:00<00:28, 280.32it/s]
Adding requests:   3%|▎         | 270/8192 [00:00<00:28, 282.89it/s]
Adding requests:   4%|▎         | 299/8192 [00:01<00:28, 280.07it/s]
Adding requests:   4%|▍         | 328/8192 [00:01<00:28, 276.83it/s]
Adding requests:   4%|▍         | 357/8192 [00:01<00:28, 279.75it/s]
Adding requests:   5%|▍         | 387/8192 [00:01<00:27, 284.42it/s]
Adding requests:   5%|▌         | 416/8192 [00:01<00:27, 284.10it/s]
Adding requests:   5%|▌         | 445/8192 [00:01<00:27, 284.97it/s]
Adding requests:   6%|▌         | 474/8192 [00:01<00:26, 286.11it/s]
Adding requests:   6%|▌         | 503/8192 [00:01<00:27, 283.38it/s]
Adding requests:   6%|▋         | 532/8192 [00:01<00:28, 270.79it/s]
Adding requests:   7%|▋         | 563/8192 [00:01<00:27, 281.49it/s]
Adding requests:   7%|▋         | 592/8192 [00:02<00:26, 283.90it/s]
Adding requests:   8%|▊         | 622/8192 [00:02<00:26, 287.93it/s]
Adding requests:   8%|▊         | 653/8192 [00:02<00:25, 293.15it/s]
Adding requests:   8%|▊         | 685/8192 [00:02<00:25, 300.07it/s]
Adding requests:   9%|▊         | 716/8192 [00:02<00:25, 295.94it/s]
Adding requests:   9%|▉         | 746/8192 [00:02<00:25, 297.11it/s]
Adding requests:   9%|▉         | 776/8192 [00:02<00:25, 295.64it/s]
Adding requests:  10%|▉         | 806/8192 [00:02<00:25, 292.27it/s]
Adding requests:  10%|█         | 836/8192 [00:02<00:25, 291.67it/s]
Adding requests:  11%|█         | 866/8192 [00:03<00:24, 293.66it/s]
Adding requests:  11%|█         | 899/8192 [00:03<00:24, 301.97it/s]
Adding requests:  11%|█▏        | 931/8192 [00:03<00:23, 306.45it/s]
Adding requests:  12%|█▏        | 964/8192 [00:03<00:23, 312.55it/s]
Adding requests:  12%|█▏        | 996/8192 [00:03<00:23, 310.39it/s]
Adding requests:  13%|█▎        | 1028/8192 [00:03<00:22, 312.85it/s]
Adding requests:  13%|█▎        | 1060/8192 [00:03<00:22, 311.27it/s]
Adding requests:  13%|█▎        | 1092/8192 [00:03<00:22, 312.67it/s]
Adding requests:  14%|█▎        | 1124/8192 [00:03<00:22, 309.62it/s]
Adding requests:  14%|█▍        | 1158/8192 [00:03<00:22, 316.02it/s]
Adding requests:  15%|█▍        | 1190/8192 [00:04<00:22, 307.55it/s]
Adding requests:  15%|█▍        | 1221/8192 [00:04<00:22, 305.31it/s]
Adding requests:  15%|█▌        | 1252/8192 [00:04<00:23, 293.12it/s]
Adding requests:  16%|█▌        | 1282/8192 [00:04<00:23, 294.87it/s]
Adding requests:  16%|█▌        | 1312/8192 [00:04<00:23, 295.80it/s]
Adding requests:  16%|█▋        | 1345/8192 [00:04<00:22, 303.62it/s]
Adding requests:  17%|█▋        | 1377/8192 [00:04<00:22, 308.40it/s]
Adding requests:  17%|█▋        | 1410/8192 [00:04<00:21, 313.46it/s]
Adding requests:  18%|█▊        | 1442/8192 [00:04<00:21, 314.34it/s]
Adding requests:  18%|█▊        | 1474/8192 [00:04<00:21, 315.87it/s]
Adding requests:  18%|█▊        | 1506/8192 [00:05<00:21, 315.12it/s]
Adding requests:  19%|█▉        | 1540/8192 [00:05<00:20, 320.23it/s]
Adding requests:  19%|█▉        | 1574/8192 [00:05<00:20, 324.61it/s]
Adding requests:  20%|█▉        | 1610/8192 [00:05<00:19, 334.78it/s]
Adding requests:  20%|██        | 1645/8192 [00:05<00:19, 338.69it/s]
Adding requests:  20%|██        | 1679/8192 [00:05<00:19, 337.89it/s]
Adding requests:  21%|██        | 1714/8192 [00:05<00:19, 339.87it/s]
Adding requests:  21%|██▏       | 1749/8192 [00:05<00:18, 341.92it/s]
Adding requests:  22%|██▏       | 1784/8192 [00:05<00:18, 339.39it/s]
Adding requests:  22%|██▏       | 1820/8192 [00:05<00:18, 343.83it/s]
Adding requests:  23%|██▎       | 1855/8192 [00:06<00:19, 333.11it/s]
Adding requests:  23%|██▎       | 1889/8192 [00:06<00:19, 329.30it/s]
Adding requests:  23%|██▎       | 1923/8192 [00:06<00:18, 330.91it/s]
Adding requests:  24%|██▍       | 1958/8192 [00:06<00:18, 333.95it/s]
Adding requests:  24%|██▍       | 1993/8192 [00:06<00:18, 336.35it/s]
Adding requests:  25%|██▍       | 2029/8192 [00:06<00:17, 342.59it/s]
Adding requests:  25%|██▌       | 2064/8192 [00:06<00:17, 343.10it/s]
Adding requests:  26%|██▌       | 2099/8192 [00:06<00:17, 339.16it/s]
Adding requests:  26%|██▌       | 2134/8192 [00:06<00:17, 339.61it/s]
Adding requests:  26%|██▋       | 2168/8192 [00:07<00:18, 333.96it/s]
Adding requests:  27%|██▋       | 2202/8192 [00:07<00:18, 328.11it/s]
Adding requests:  27%|██▋       | 2236/8192 [00:07<00:18, 330.31it/s]
Adding requests:  28%|██▊       | 2270/8192 [00:07<00:18, 323.19it/s]
Adding requests:  28%|██▊       | 2305/8192 [00:07<00:17, 328.71it/s]
Adding requests:  29%|██▊       | 2338/8192 [00:07<00:17, 326.14it/s]
Adding requests:  29%|██▉       | 2371/8192 [00:07<00:17, 325.87it/s]
Adding requests:  29%|██▉       | 2404/8192 [00:07<00:17, 324.17it/s]
Adding requests:  30%|██▉       | 2437/8192 [00:07<00:18, 317.05it/s]
Adding requests:  30%|███       | 2469/8192 [00:07<00:18, 312.80it/s]
Adding requests:  31%|███       | 2501/8192 [00:08<00:18, 306.90it/s]
Adding requests:  31%|███       | 2532/8192 [00:08<00:18, 307.07it/s]
Adding requests:  31%|███▏      | 2563/8192 [00:08<00:18, 304.03it/s]
Adding requests:  32%|███▏      | 2594/8192 [00:08<00:18, 304.20it/s]
Adding requests:  32%|███▏      | 2625/8192 [00:08<00:18, 305.16it/s]
Adding requests:  32%|███▏      | 2658/8192 [00:08<00:17, 309.90it/s]
Adding requests:  33%|███▎      | 2689/8192 [00:08<00:18, 304.08it/s]
Adding requests:  33%|███▎      | 2720/8192 [00:08<00:17, 304.67it/s]
Adding requests:  34%|███▎      | 2751/8192 [00:08<00:17, 302.71it/s]
Adding requests:  34%|███▍      | 2784/8192 [00:09<00:17, 308.39it/s]
Adding requests:  34%|███▍      | 2815/8192 [00:09<00:17, 307.23it/s]
Adding requests:  35%|███▍      | 2846/8192 [00:09<00:17, 304.64it/s]
Adding requests:  35%|███▌      | 2877/8192 [00:09<00:17, 303.64it/s]
Adding requests:  36%|███▌      | 2909/8192 [00:09<00:17, 306.11it/s]
Adding requests:  36%|███▌      | 2940/8192 [00:09<00:17, 298.86it/s]
Adding requests:  36%|███▋      | 2972/8192 [00:09<00:17, 303.06it/s]
Adding requests:  37%|███▋      | 3003/8192 [00:09<00:17, 298.81it/s]
Adding requests:  37%|███▋      | 3034/8192 [00:09<00:17, 301.84it/s]
Adding requests:  37%|███▋      | 3065/8192 [00:09<00:17, 299.04it/s]
Adding requests:  38%|███▊      | 3097/8192 [00:10<00:16, 304.60it/s]
Adding requests:  38%|███▊      | 3129/8192 [00:10<00:16, 308.13it/s]
Adding requests:  39%|███▊      | 3160/8192 [00:10<00:18, 267.98it/s]
Adding requests:  39%|███▉      | 3188/8192 [00:10<00:20, 245.33it/s]
Adding requests:  39%|███▉      | 3216/8192 [00:10<00:19, 254.04it/s]
Adding requests:  40%|███▉      | 3245/8192 [00:10<00:18, 262.65it/s]
Adding requests:  40%|███▉      | 3274/8192 [00:10<00:18, 269.80it/s]
Adding requests:  40%|████      | 3303/8192 [00:10<00:17, 274.94it/s]
Adding requests:  41%|████      | 3331/8192 [00:10<00:17, 276.24it/s]
Adding requests:  41%|████      | 3361/8192 [00:11<00:17, 282.97it/s]
Adding requests:  41%|████▏     | 3390/8192 [00:11<00:17, 281.09it/s]
Adding requests:  42%|████▏     | 3420/8192 [00:11<00:16, 286.59it/s]
Adding requests:  42%|████▏     | 3449/8192 [00:11<00:16, 285.49it/s]
Adding requests:  42%|████▏     | 3479/8192 [00:11<00:16, 287.45it/s]
Adding requests:  43%|████▎     | 3510/8192 [00:11<00:15, 293.60it/s]
Adding requests:  43%|████▎     | 3540/8192 [00:11<00:15, 293.68it/s]
Adding requests:  44%|████▎     | 3570/8192 [00:11<00:15, 295.27it/s]
Adding requests:  44%|████▍     | 3601/8192 [00:11<00:15, 296.15it/s]
Adding requests:  44%|████▍     | 3631/8192 [00:11<00:15, 295.55it/s]
Adding requests:  45%|████▍     | 3663/8192 [00:12<00:15, 301.91it/s]
Adding requests:  45%|████▌     | 3696/8192 [00:12<00:14, 307.48it/s]
Adding requests:  46%|████▌     | 3728/8192 [00:12<00:14, 308.36it/s]
Adding requests:  46%|████▌     | 3759/8192 [00:12<00:14, 306.64it/s]
Adding requests:  46%|████▋     | 3790/8192 [00:12<00:14, 307.28it/s]
Adding requests:  47%|████▋     | 3821/8192 [00:12<00:14, 306.21it/s]
Adding requests:  47%|████▋     | 3853/8192 [00:12<00:14, 309.87it/s]
Adding requests:  47%|████▋     | 3884/8192 [00:12<00:13, 307.88it/s]
Adding requests:  48%|████▊     | 3918/8192 [00:12<00:13, 315.64it/s]
Adding requests:  48%|████▊     | 3950/8192 [00:12<00:13, 312.96it/s]
Adding requests:  49%|████▊     | 3985/8192 [00:13<00:13, 319.77it/s]
Adding requests:  49%|████▉     | 4020/8192 [00:13<00:12, 328.54it/s]
Adding requests:  49%|████▉     | 4053/8192 [00:13<00:12, 327.68it/s]
Adding requests:  50%|████▉     | 4086/8192 [00:13<00:12, 319.04it/s]
Adding requests:  50%|█████     | 4118/8192 [00:13<00:13, 307.00it/s]
Adding requests:  51%|█████     | 4151/8192 [00:13<00:12, 311.74it/s]
Adding requests:  51%|█████     | 4184/8192 [00:13<00:12, 316.41it/s]
Adding requests:  51%|█████▏    | 4216/8192 [00:13<00:12, 314.29it/s]
Adding requests:  52%|█████▏    | 4249/8192 [00:13<00:12, 316.73it/s]
Adding requests:  52%|█████▏    | 4281/8192 [00:14<00:12, 310.34it/s]
Adding requests:  53%|█████▎    | 4313/8192 [00:14<00:12, 312.91it/s]
Adding requests:  53%|█████▎    | 4346/8192 [00:14<00:12, 317.20it/s]
Adding requests:  53%|█████▎    | 4379/8192 [00:14<00:11, 319.17it/s]
Adding requests:  54%|█████▍    | 4411/8192 [00:14<00:11, 317.26it/s]
Adding requests:  54%|█████▍    | 4443/8192 [00:14<00:11, 313.34it/s]
Adding requests:  55%|█████▍    | 4476/8192 [00:14<00:11, 316.09it/s]
Adding requests:  55%|█████▌    | 4508/8192 [00:14<00:12, 304.96it/s]
Adding requests:  55%|█████▌    | 4540/8192 [00:14<00:11, 307.73it/s]
Adding requests:  56%|█████▌    | 4575/8192 [00:14<00:11, 316.94it/s]
Adding requests:  56%|█████▌    | 4607/8192 [00:15<00:11, 316.84it/s]
Adding requests:  57%|█████▋    | 4639/8192 [00:15<00:11, 317.76it/s]
Adding requests:  57%|█████▋    | 4673/8192 [00:15<00:10, 322.74it/s]
Adding requests:  57%|█████▋    | 4707/8192 [00:15<00:10, 325.53it/s]
Adding requests:  58%|█████▊    | 4741/8192 [00:15<00:10, 329.37it/s]
Adding requests:  58%|█████▊    | 4776/8192 [00:15<00:10, 333.37it/s]
Adding requests:  59%|█████▊    | 4810/8192 [00:15<00:10, 332.67it/s]
Adding requests:  59%|█████▉    | 4845/8192 [00:15<00:09, 335.23it/s]
Adding requests:  60%|█████▉    | 4879/8192 [00:15<00:10, 318.57it/s]
Adding requests:  60%|█████▉    | 4912/8192 [00:15<00:10, 315.26it/s]
Adding requests:  60%|██████    | 4944/8192 [00:16<00:10, 313.79it/s]
Adding requests:  61%|██████    | 4976/8192 [00:16<00:10, 308.76it/s]
Adding requests:  61%|██████    | 5008/8192 [00:16<00:10, 309.46it/s]
Adding requests:  62%|██████▏   | 5041/8192 [00:16<00:10, 312.36it/s]
Adding requests:  62%|██████▏   | 5073/8192 [00:16<00:10, 305.93it/s]
Adding requests:  62%|██████▏   | 5104/8192 [00:16<00:10, 301.15it/s]
Adding requests:  63%|██████▎   | 5135/8192 [00:16<00:10, 300.45it/s]
Adding requests:  63%|██████▎   | 5166/8192 [00:16<00:10, 296.44it/s]
Adding requests:  63%|██████▎   | 5196/8192 [00:16<00:10, 294.27it/s]
Adding requests:  64%|██████▍   | 5226/8192 [00:17<00:10, 288.51it/s]
Adding requests:  64%|██████▍   | 5255/8192 [00:17<00:10, 288.52it/s]
Adding requests:  70%|███████   | 5767/8192 [00:17<00:01, 1681.02it/s]
Adding requests:  72%|███████▏  | 5938/8192 [00:17<00:03, 642.14it/s] 
Adding requests:  74%|███████▍  | 6066/8192 [00:18<00:04, 485.24it/s]
Adding requests:  75%|███████▌  | 6164/8192 [00:18<00:04, 415.12it/s]
Adding requests:  76%|███████▌  | 6241/8192 [00:19<00:05, 377.41it/s]
Adding requests:  77%|███████▋  | 6303/8192 [00:19<00:05, 351.89it/s]
Adding requests:  78%|███████▊  | 6355/8192 [00:19<00:05, 342.83it/s]
Adding requests:  78%|███████▊  | 6400/8192 [00:19<00:05, 333.17it/s]
Adding requests:  79%|███████▊  | 6441/8192 [00:19<00:05, 329.85it/s]
Adding requests:  79%|███████▉  | 6479/8192 [00:19<00:05, 326.31it/s]
Adding requests:  80%|███████▉  | 6515/8192 [00:19<00:05, 323.45it/s]
Adding requests:  80%|███████▉  | 6550/8192 [00:20<00:05, 315.54it/s]
Adding requests:  80%|████████  | 6583/8192 [00:20<00:05, 312.03it/s]
Adding requests:  81%|████████  | 6616/8192 [00:20<00:05, 305.93it/s]
Adding requests:  81%|████████  | 6650/8192 [00:20<00:04, 312.60it/s]
Adding requests:  82%|████████▏ | 6683/8192 [00:20<00:04, 316.89it/s]
Adding requests:  82%|████████▏ | 6718/8192 [00:20<00:04, 324.49it/s]
Adding requests:  82%|████████▏ | 6751/8192 [00:20<00:04, 325.32it/s]
Adding requests:  83%|████████▎ | 6786/8192 [00:20<00:04, 329.54it/s]
Adding requests:  83%|████████▎ | 6820/8192 [00:20<00:04, 331.63it/s]
Adding requests:  84%|████████▎ | 6854/8192 [00:21<00:04, 331.90it/s]
Adding requests:  84%|████████▍ | 6888/8192 [00:21<00:03, 330.87it/s]
Adding requests:  84%|████████▍ | 6922/8192 [00:21<00:03, 327.52it/s]
Adding requests:  85%|████████▍ | 6955/8192 [00:21<00:03, 322.96it/s]
Adding requests:  85%|████████▌ | 6988/8192 [00:21<00:03, 322.67it/s]
Adding requests:  86%|████████▌ | 7021/8192 [00:21<00:03, 317.99it/s]
Adding requests:  86%|████████▌ | 7053/8192 [00:21<00:03, 315.19it/s]
Adding requests:  86%|████████▋ | 7085/8192 [00:21<00:03, 314.91it/s]
Adding requests:  87%|████████▋ | 7117/8192 [00:21<00:03, 314.91it/s]
Adding requests:  87%|████████▋ | 7149/8192 [00:21<00:03, 312.58it/s]
Adding requests:  88%|████████▊ | 7181/8192 [00:22<00:03, 296.26it/s]
Adding requests:  88%|████████▊ | 7213/8192 [00:22<00:03, 302.07it/s]
Adding requests:  88%|████████▊ | 7245/8192 [00:22<00:03, 306.85it/s]
Adding requests:  89%|████████▉ | 7277/8192 [00:22<00:02, 310.42it/s]
Adding requests:  89%|████████▉ | 7311/8192 [00:22<00:02, 317.85it/s]
Adding requests:  90%|████████▉ | 7345/8192 [00:22<00:02, 322.48it/s]
Adding requests:  90%|█████████ | 7378/8192 [00:22<00:02, 322.65it/s]
Adding requests:  91%|█████████ | 7414/8192 [00:22<00:02, 331.16it/s]
Adding requests:  91%|█████████ | 7448/8192 [00:22<00:02, 331.57it/s]
Adding requests:  91%|█████████▏| 7483/8192 [00:22<00:02, 334.41it/s]
Adding requests:  92%|█████████▏| 7517/8192 [00:23<00:02, 332.51it/s]
Adding requests:  92%|█████████▏| 7551/8192 [00:23<00:01, 327.77it/s]
Adding requests:  93%|█████████▎| 7584/8192 [00:23<00:01, 326.86it/s]
Adding requests:  93%|█████████▎| 7617/8192 [00:23<00:01, 322.42it/s]
Adding requests:  93%|█████████▎| 7651/8192 [00:23<00:01, 327.50it/s]
Adding requests:  94%|█████████▍| 7685/8192 [00:23<00:01, 329.75it/s]
Adding requests:  94%|█████████▍| 7718/8192 [00:23<00:01, 326.80it/s]
Adding requests:  95%|█████████▍| 7751/8192 [00:23<00:01, 325.09it/s]
Adding requests:  95%|█████████▌| 7784/8192 [00:23<00:01, 314.41it/s]
Adding requests:  95%|█████████▌| 7816/8192 [00:24<00:01, 311.16it/s]
Adding requests:  96%|█████████▌| 7848/8192 [00:24<00:01, 305.79it/s]
Adding requests:  96%|█████████▌| 7879/8192 [00:24<00:01, 304.43it/s]
Adding requests:  97%|█████████▋| 7910/8192 [00:24<00:00, 298.79it/s]
Adding requests:  97%|█████████▋| 7940/8192 [00:24<00:00, 294.56it/s]
Adding requests:  97%|█████████▋| 7970/8192 [00:24<00:00, 288.96it/s]
Adding requests:  98%|█████████▊| 8000/8192 [00:24<00:00, 289.60it/s]
Adding requests:  98%|█████████▊| 8029/8192 [00:24<00:00, 286.25it/s]
Adding requests:  98%|█████████▊| 8060/8192 [00:24<00:00, 291.13it/s]
Adding requests:  99%|█████████▉| 8091/8192 [00:24<00:00, 294.11it/s]
Adding requests:  99%|█████████▉| 8121/8192 [00:25<00:00, 294.97it/s]
Adding requests:  99%|█████████▉| 8151/8192 [00:25<00:00, 290.33it/s]
Adding requests: 100%|█████████▉| 8181/8192 [00:25<00:00, 288.09it/s]
Adding requests: 100%|██████████| 8192/8192 [00:25<00:00, 323.54it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 1573/8192 [00:00<00:02, 2236.17it/s, est. speed input: 2289940.36 toks/s, output: 2236.20 toks/s]
Processed prompts:  22%|██▏       | 1797/8192 [00:04<00:18, 346.56it/s, est. speed input: 456087.60 toks/s, output: 445.40 toks/s]   
Processed prompts:  23%|██▎       | 1894/8192 [00:06<00:29, 215.22it/s, est. speed input: 316975.09 toks/s, output: 309.55 toks/s]
Processed prompts:  24%|██▍       | 1957/8192 [00:07<00:35, 177.61it/s, est. speed input: 279140.07 toks/s, output: 272.60 toks/s]
Processed prompts:  25%|██▍       | 2021/8192 [00:08<00:42, 145.84it/s, est. speed input: 249710.45 toks/s, output: 243.86 toks/s]
Processed prompts:  25%|██▌       | 2085/8192 [00:09<00:50, 121.62it/s, est. speed input: 227199.33 toks/s, output: 221.87 toks/s]
Processed prompts:  26%|██▌       | 2149/8192 [00:10<00:58, 103.57it/s, est. speed input: 209425.58 toks/s, output: 204.52 toks/s]
Processed prompts:  27%|██▋       | 2213/8192 [00:11<01:05, 91.21it/s, est. speed input: 195646.49 toks/s, output: 191.06 toks/s] 
Processed prompts:  28%|██▊       | 2277/8192 [00:12<01:10, 83.76it/s, est. speed input: 185265.69 toks/s, output: 180.92 toks/s]
Processed prompts:  29%|██▊       | 2341/8192 [00:13<01:15, 77.69it/s, est. speed input: 176122.70 toks/s, output: 171.99 toks/s]
Processed prompts:  29%|██▉       | 2405/8192 [00:14<01:19, 73.09it/s, est. speed input: 168156.63 toks/s, output: 164.22 toks/s]
Processed prompts:  30%|███       | 2469/8192 [00:15<01:23, 68.67it/s, est. speed input: 160651.19 toks/s, output: 156.89 toks/s]
Processed prompts:  31%|███       | 2533/8192 [00:16<01:26, 65.29it/s, est. speed input: 153953.92 toks/s, output: 150.35 toks/s]
Processed prompts:  32%|███▏      | 2597/8192 [00:17<01:28, 62.94it/s, est. speed input: 148078.27 toks/s, output: 144.61 toks/s]
Processed prompts:  32%|███▏      | 2661/8192 [00:19<01:30, 61.31it/s, est. speed input: 142883.09 toks/s, output: 139.53 toks/s]
Processed prompts:  33%|███▎      | 2725/8192 [00:20<01:29, 61.06it/s, est. speed input: 138624.74 toks/s, output: 135.38 toks/s]
Processed prompts:  34%|███▍      | 2789/8192 [00:21<01:27, 61.46it/s, est. speed input: 135004.84 toks/s, output: 131.84 toks/s]
Processed prompts:  35%|███▍      | 2853/8192 [00:22<01:26, 61.72it/s, est. speed input: 131713.73 toks/s, output: 128.63 toks/s]
Processed prompts:  36%|███▋      | 2981/8192 [00:22<00:55, 93.50it/s, est. speed input: 134399.59 toks/s, output: 131.25 toks/s]
Processed prompts:  37%|███▋      | 3045/8192 [00:23<01:03, 81.15it/s, est. speed input: 130900.87 toks/s, output: 127.83 toks/s]
Processed prompts:  38%|███▊      | 3109/8192 [00:24<01:08, 73.87it/s, est. speed input: 127817.33 toks/s, output: 124.82 toks/s]
Processed prompts:  39%|███▊      | 3173/8192 [00:26<01:13, 68.66it/s, est. speed input: 124888.34 toks/s, output: 121.96 toks/s]
Processed prompts:  40%|███▉      | 3237/8192 [00:27<01:15, 65.47it/s, est. speed input: 122262.18 toks/s, output: 119.40 toks/s]
Processed prompts:  40%|████      | 3301/8192 [00:28<01:15, 64.57it/s, est. speed input: 120134.54 toks/s, output: 117.32 toks/s]
Processed prompts:  41%|████      | 3365/8192 [00:29<01:15, 63.93it/s, est. speed input: 118158.82 toks/s, output: 115.39 toks/s]
Processed prompts:  42%|████▏     | 3429/8192 [00:30<01:15, 63.47it/s, est. speed input: 116313.94 toks/s, output: 113.59 toks/s]
Processed prompts:  43%|████▎     | 3493/8192 [00:31<01:15, 62.17it/s, est. speed input: 114393.29 toks/s, output: 111.71 toks/s]
Processed prompts:  43%|████▎     | 3557/8192 [00:32<01:16, 60.72it/s, est. speed input: 112486.68 toks/s, output: 109.85 toks/s]
Processed prompts:  44%|████▍     | 3621/8192 [00:33<01:16, 59.76it/s, est. speed input: 110712.71 toks/s, output: 108.12 toks/s]
Processed prompts:  45%|████▍     | 3685/8192 [00:34<01:15, 59.38it/s, est. speed input: 109104.73 toks/s, output: 106.55 toks/s]
Processed prompts:  46%|████▌     | 3749/8192 [00:35<01:15, 58.81it/s, est. speed input: 107539.56 toks/s, output: 105.02 toks/s]
Processed prompts:  47%|████▋     | 3813/8192 [00:36<01:14, 58.41it/s, est. speed input: 106067.97 toks/s, output: 103.58 toks/s]
Processed prompts:  47%|████▋     | 3877/8192 [00:37<01:12, 59.46it/s, est. speed input: 104910.14 toks/s, output: 102.45 toks/s]
Processed prompts:  48%|████▊     | 3941/8192 [00:38<01:10, 59.92it/s, est. speed input: 103765.80 toks/s, output: 101.33 toks/s]
Processed prompts:  49%|████▉     | 4005/8192 [00:39<01:10, 59.51it/s, est. speed input: 102568.96 toks/s, output: 100.16 toks/s]
Processed prompts:  50%|████▉     | 4069/8192 [00:41<01:09, 58.92it/s, est. speed input: 101390.29 toks/s, output: 99.01 toks/s] 
Processed prompts:  50%|█████     | 4133/8192 [00:42<01:09, 58.49it/s, est. speed input: 100269.43 toks/s, output: 97.92 toks/s]
Processed prompts:  51%|█████     | 4197/8192 [00:43<01:08, 58.49it/s, est. speed input: 99249.64 toks/s, output: 96.92 toks/s] 
Processed prompts:  52%|█████▏    | 4261/8192 [00:44<01:06, 59.29it/s, est. speed input: 98389.20 toks/s, output: 96.08 toks/s]
Processed prompts:  53%|█████▎    | 4325/8192 [00:45<01:03, 60.49it/s, est. speed input: 97646.78 toks/s, output: 95.36 toks/s]
Processed prompts:  54%|█████▎    | 4389/8192 [00:46<01:02, 61.04it/s, est. speed input: 96898.96 toks/s, output: 94.63 toks/s]
Processed prompts:  54%|█████▍    | 4453/8192 [00:47<01:02, 60.28it/s, est. speed input: 96049.77 toks/s, output: 93.80 toks/s]
Processed prompts:  55%|█████▌    | 4517/8192 [00:48<01:01, 59.42it/s, est. speed input: 95198.47 toks/s, output: 92.97 toks/s]
Processed prompts:  56%|█████▌    | 4581/8192 [00:49<01:01, 58.83it/s, est. speed input: 94385.46 toks/s, output: 92.17 toks/s]
Processed prompts:  57%|█████▋    | 4645/8192 [00:50<01:00, 58.44it/s, est. speed input: 93609.50 toks/s, output: 91.42 toks/s]
Processed prompts:  57%|█████▋    | 4709/8192 [00:51<00:59, 58.77it/s, est. speed input: 92933.90 toks/s, output: 90.76 toks/s]
Processed prompts:  58%|█████▊    | 4773/8192 [00:52<00:57, 59.79it/s, est. speed input: 92368.30 toks/s, output: 90.20 toks/s]
Processed prompts:  60%|█████▉    | 4901/8192 [00:53<00:33, 97.57it/s, est. speed input: 94231.36 toks/s, output: 92.02 toks/s]
Processed prompts:  61%|██████    | 4965/8192 [00:54<00:37, 86.06it/s, est. speed input: 93685.42 toks/s, output: 91.49 toks/s]
Processed prompts:  61%|██████▏   | 5029/8192 [00:55<00:41, 76.97it/s, est. speed input: 93042.82 toks/s, output: 90.86 toks/s]
Processed prompts:  62%|██████▏   | 5093/8192 [00:56<00:43, 70.50it/s, est. speed input: 92370.67 toks/s, output: 90.21 toks/s]
Processed prompts:  63%|██████▎   | 5157/8192 [00:57<00:45, 66.31it/s, est. speed input: 91723.97 toks/s, output: 89.57 toks/s]
Processed prompts:  64%|██████▎   | 5221/8192 [00:58<00:46, 63.53it/s, est. speed input: 91100.46 toks/s, output: 88.97 toks/s]
Processed prompts:  65%|██████▍   | 5285/8192 [00:59<00:47, 61.63it/s, est. speed input: 90499.06 toks/s, output: 88.38 toks/s]
Processed prompts:  65%|██████▌   | 5349/8192 [01:00<00:46, 61.63it/s, est. speed input: 90031.12 toks/s, output: 87.92 toks/s]
Processed prompts:  66%|██████▌   | 5413/8192 [01:01<00:44, 61.82it/s, est. speed input: 89595.49 toks/s, output: 87.50 toks/s]
Processed prompts:  67%|██████▋   | 5477/8192 [01:02<00:44, 61.64it/s, est. speed input: 89147.71 toks/s, output: 87.06 toks/s]
Processed prompts:  68%|██████▊   | 5541/8192 [01:04<00:43, 60.70it/s, est. speed input: 88650.07 toks/s, output: 86.57 toks/s]
Processed prompts:  68%|██████▊   | 5605/8192 [01:05<00:43, 59.70it/s, est. speed input: 88140.91 toks/s, output: 86.08 toks/s]
Processed prompts:  69%|██████▉   | 5669/8192 [01:06<00:42, 59.04it/s, est. speed input: 87649.88 toks/s, output: 85.60 toks/s]
Processed prompts:  70%|██████▉   | 5733/8192 [01:07<00:41, 58.58it/s, est. speed input: 87175.32 toks/s, output: 85.13 toks/s]
Processed prompts:  71%|███████   | 5797/8192 [01:08<00:40, 58.81it/s, est. speed input: 86759.66 toks/s, output: 84.73 toks/s]
Processed prompts:  72%|███████▏  | 5861/8192 [01:09<00:38, 59.80it/s, est. speed input: 86418.98 toks/s, output: 84.39 toks/s]
Processed prompts:  72%|███████▏  | 5925/8192 [01:10<00:37, 60.54it/s, est. speed input: 86089.63 toks/s, output: 84.07 toks/s]
Processed prompts:  73%|███████▎  | 5989/8192 [01:11<00:36, 59.94it/s, est. speed input: 85691.09 toks/s, output: 83.68 toks/s]
Processed prompts:  74%|███████▍  | 6053/8192 [01:12<00:36, 59.19it/s, est. speed input: 85280.91 toks/s, output: 83.28 toks/s]
Processed prompts:  75%|███████▍  | 6117/8192 [01:13<00:35, 58.66it/s, est. speed input: 84881.66 toks/s, output: 82.89 toks/s]
Processed prompts:  75%|███████▌  | 6181/8192 [01:14<00:34, 58.31it/s, est. speed input: 84495.46 toks/s, output: 82.52 toks/s]
Processed prompts:  76%|███████▌  | 6245/8192 [01:16<00:33, 58.24it/s, est. speed input: 84132.78 toks/s, output: 82.16 toks/s]
Processed prompts:  77%|███████▋  | 6309/8192 [01:17<00:31, 59.40it/s, est. speed input: 83861.38 toks/s, output: 81.90 toks/s]
Processed prompts:  78%|███████▊  | 6373/8192 [01:18<00:30, 60.25it/s, est. speed input: 83597.83 toks/s, output: 81.64 toks/s]
Processed prompts:  79%|███████▊  | 6437/8192 [01:19<00:29, 60.33it/s, est. speed input: 83308.70 toks/s, output: 81.36 toks/s]
Processed prompts:  79%|███████▉  | 6501/8192 [01:20<00:28, 59.45it/s, est. speed input: 82969.80 toks/s, output: 81.03 toks/s]
Processed prompts:  80%|████████  | 6565/8192 [01:21<00:27, 59.18it/s, est. speed input: 82660.45 toks/s, output: 80.72 toks/s]
Processed prompts:  81%|████████  | 6629/8192 [01:22<00:26, 58.97it/s, est. speed input: 82358.53 toks/s, output: 80.43 toks/s]
Processed prompts:  82%|████████▏ | 6693/8192 [01:23<00:25, 58.52it/s, est. speed input: 82045.31 toks/s, output: 80.12 toks/s]
Processed prompts:  83%|████████▎ | 6821/8192 [01:24<00:14, 91.77it/s, est. speed input: 83146.44 toks/s, output: 81.20 toks/s]
Processed prompts:  84%|████████▍ | 6885/8192 [01:25<00:15, 82.15it/s, est. speed input: 82913.03 toks/s, output: 80.97 toks/s]
Processed prompts:  85%|████████▍ | 6949/8192 [01:26<00:16, 75.86it/s, est. speed input: 82686.10 toks/s, output: 80.75 toks/s]
Processed prompts:  86%|████████▌ | 7013/8192 [01:27<00:16, 71.05it/s, est. speed input: 82438.24 toks/s, output: 80.51 toks/s]
Processed prompts:  86%|████████▋ | 7077/8192 [01:28<00:16, 66.66it/s, est. speed input: 82141.44 toks/s, output: 80.22 toks/s]
Processed prompts:  87%|████████▋ | 7141/8192 [01:29<00:16, 64.11it/s, est. speed input: 81869.33 toks/s, output: 79.95 toks/s]
Processed prompts:  88%|████████▊ | 7205/8192 [01:30<00:15, 62.06it/s, est. speed input: 81587.23 toks/s, output: 79.68 toks/s]
Processed prompts:  89%|████████▊ | 7269/8192 [01:31<00:15, 60.97it/s, est. speed input: 81328.03 toks/s, output: 79.42 toks/s]
Processed prompts:  90%|████████▉ | 7333/8192 [01:32<00:14, 60.34it/s, est. speed input: 81081.21 toks/s, output: 79.18 toks/s]
Processed prompts:  90%|█████████ | 7397/8192 [01:33<00:13, 60.91it/s, est. speed input: 80891.37 toks/s, output: 79.00 toks/s]
Processed prompts:  91%|█████████ | 7461/8192 [01:34<00:11, 61.33it/s, est. speed input: 80706.39 toks/s, output: 78.81 toks/s]
Processed prompts:  92%|█████████▏| 7525/8192 [01:35<00:11, 60.34it/s, est. speed input: 80463.08 toks/s, output: 78.58 toks/s]
Processed prompts:  93%|█████████▎| 7589/8192 [01:36<00:10, 59.46it/s, est. speed input: 80215.10 toks/s, output: 78.34 toks/s]
Processed prompts:  93%|█████████▎| 7653/8192 [01:37<00:09, 58.88it/s, est. speed input: 79973.87 toks/s, output: 78.10 toks/s]
Processed prompts:  94%|█████████▍| 7717/8192 [01:39<00:08, 58.46it/s, est. speed input: 79737.01 toks/s, output: 77.87 toks/s]
Processed prompts:  95%|█████████▍| 7781/8192 [01:40<00:07, 58.21it/s, est. speed input: 79507.36 toks/s, output: 77.64 toks/s]
Processed prompts:  96%|█████████▌| 7845/8192 [01:41<00:05, 59.38it/s, est. speed input: 79347.87 toks/s, output: 77.49 toks/s]
Processed prompts:  97%|█████████▋| 7909/8192 [01:42<00:04, 60.24it/s, est. speed input: 79192.13 toks/s, output: 77.34 toks/s]
Processed prompts:  97%|█████████▋| 7973/8192 [01:43<00:03, 60.20it/s, est. speed input: 79010.26 toks/s, output: 77.16 toks/s]
Processed prompts:  98%|█████████▊| 8037/8192 [01:44<00:02, 59.37it/s, est. speed input: 78796.05 toks/s, output: 76.95 toks/s]
Processed prompts:  99%|█████████▉| 8101/8192 [01:45<00:01, 59.10it/s, est. speed input: 78599.96 toks/s, output: 76.76 toks/s]
Processed prompts: 100%|█████████▉| 8165/8192 [01:46<00:00, 71.10it/s, est. speed input: 78867.14 toks/s, output: 77.02 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:46<00:00, 71.10it/s, est. speed input: 79127.45 toks/s, output: 77.27 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:46<00:00, 77.27it/s, est. speed input: 79127.45 toks/s, output: 77.27 toks/s]
[rank0]:[W125 18:31:58.786941275 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-25 21:15:48
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:15:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 21:15:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=413088) WARNING 01-25 21:16:06 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=413088) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=413088) WARNING 01-25 21:16:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 14.44 requests/s, 7406.25 total tokens/s, 14.44 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 21:15:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:15:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:15:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:15:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:15:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:15:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:15:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:15:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:15:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:15:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:15:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:15:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:15:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:15:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:16:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:16:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:16:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:16:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:16:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:16:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:16:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:16:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:16:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:16:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:16:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:16:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:16:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:16:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=413088) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=413088) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.88s/it]
(EngineCore_DP0 pid=413088) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.88s/it]
(EngineCore_DP0 pid=413088) 
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=413088) [2026-01-25 21:16:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=413088) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.15it/s]
(EngineCore_DP0 pid=413088) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.62it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  42%|████▏     | 54/128 [00:00<00:00, 530.26it/s]
Adding requests:  84%|████████▍ | 108/128 [00:00<00:00, 532.56it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 532.70it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:27,  4.66it/s, est. speed input: 2386.60 toks/s, output: 4.66 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:13,  9.35it/s, est. speed input: 4348.85 toks/s, output: 8.49 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:10, 11.38it/s, est. speed input: 5193.97 toks/s, output: 10.14 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:09, 12.52it/s, est. speed input: 5682.17 toks/s, output: 11.10 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 13.26it/s, est. speed input: 6008.73 toks/s, output: 11.74 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:08, 13.68it/s, est. speed input: 6226.59 toks/s, output: 12.16 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:08, 13.93it/s, est. speed input: 6382.71 toks/s, output: 12.47 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 14.13it/s, est. speed input: 6508.69 toks/s, output: 12.71 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 14.22it/s, est. speed input: 6600.75 toks/s, output: 12.89 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:07, 14.22it/s, est. speed input: 6665.51 toks/s, output: 13.02 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:07, 14.30it/s, est. speed input: 6729.98 toks/s, output: 13.14 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:07, 14.30it/s, est. speed input: 6778.16 toks/s, output: 13.24 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:07, 14.51it/s, est. speed input: 6842.36 toks/s, output: 13.36 toks/s]
Processed prompts:  21%|██        | 27/128 [00:02<00:06, 14.63it/s, est. speed input: 6895.25 toks/s, output: 13.47 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:06, 14.63it/s, est. speed input: 6933.50 toks/s, output: 13.54 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 14.68it/s, est. speed input: 6971.41 toks/s, output: 13.62 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:06, 14.69it/s, est. speed input: 7002.99 toks/s, output: 13.68 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:06, 14.56it/s, est. speed input: 7019.83 toks/s, output: 13.71 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:06, 14.59it/s, est. speed input: 7044.69 toks/s, output: 13.76 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:06, 14.53it/s, est. speed input: 7060.46 toks/s, output: 13.79 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 14.54it/s, est. speed input: 7078.50 toks/s, output: 13.83 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:03<00:05, 14.72it/s, est. speed input: 7107.81 toks/s, output: 13.88 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:03<00:05, 14.81it/s, est. speed input: 7131.81 toks/s, output: 13.93 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:05, 14.73it/s, est. speed input: 7144.85 toks/s, output: 13.95 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:05, 14.74it/s, est. speed input: 7160.90 toks/s, output: 13.99 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:05, 14.76it/s, est. speed input: 7176.28 toks/s, output: 14.02 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:05, 14.89it/s, est. speed input: 7197.43 toks/s, output: 14.06 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 15.11it/s, est. speed input: 7224.32 toks/s, output: 14.11 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:04<00:04, 15.31it/s, est. speed input: 7251.52 toks/s, output: 14.16 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:04<00:04, 15.49it/s, est. speed input: 7278.78 toks/s, output: 14.22 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:04<00:04, 15.56it/s, est. speed input: 7301.68 toks/s, output: 14.26 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:04, 15.56it/s, est. speed input: 7321.34 toks/s, output: 14.30 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:04, 15.56it/s, est. speed input: 7339.44 toks/s, output: 14.33 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 15.63it/s, est. speed input: 7359.96 toks/s, output: 14.37 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 15.69it/s, est. speed input: 7379.33 toks/s, output: 14.41 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 15.66it/s, est. speed input: 7395.15 toks/s, output: 14.44 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:05<00:03, 15.66it/s, est. speed input: 7410.89 toks/s, output: 14.47 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:05<00:03, 15.69it/s, est. speed input: 7426.95 toks/s, output: 14.51 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:05<00:03, 15.69it/s, est. speed input: 7441.78 toks/s, output: 14.53 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:05<00:03, 15.72it/s, est. speed input: 7456.60 toks/s, output: 14.56 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:02, 15.73it/s, est. speed input: 7470.55 toks/s, output: 14.59 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 15.81it/s, est. speed input: 7486.45 toks/s, output: 14.62 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 15.83it/s, est. speed input: 7500.42 toks/s, output: 14.65 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 15.82it/s, est. speed input: 7513.12 toks/s, output: 14.67 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:06<00:02, 15.78it/s, est. speed input: 7523.82 toks/s, output: 14.69 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:06<00:02, 15.85it/s, est. speed input: 7537.66 toks/s, output: 14.72 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:06<00:02, 15.90it/s, est. speed input: 7550.84 toks/s, output: 14.75 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:06<00:02, 15.86it/s, est. speed input: 7561.19 toks/s, output: 14.77 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:01, 15.80it/s, est. speed input: 7570.15 toks/s, output: 14.79 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 15.84it/s, est. speed input: 7581.20 toks/s, output: 14.81 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 15.92it/s, est. speed input: 7593.14 toks/s, output: 14.83 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 15.98it/s, est. speed input: 7605.06 toks/s, output: 14.85 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:07<00:01, 15.90it/s, est. speed input: 7613.07 toks/s, output: 14.87 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:07<00:01, 15.83it/s, est. speed input: 7620.29 toks/s, output: 14.88 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:07<00:01, 15.79it/s, est. speed input: 7627.50 toks/s, output: 14.90 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:07<00:01, 15.66it/s, est. speed input: 7631.85 toks/s, output: 14.91 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:00, 15.28it/s, est. speed input: 7627.59 toks/s, output: 14.90 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 15.08it/s, est. speed input: 7625.22 toks/s, output: 14.89 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 14.93it/s, est. speed input: 7622.40 toks/s, output: 14.89 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 14.85it/s, est. speed input: 7620.49 toks/s, output: 14.88 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:08<00:00, 14.53it/s, est. speed input: 7610.94 toks/s, output: 14.87 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:08<00:00, 14.55it/s, est. speed input: 7608.65 toks/s, output: 14.86 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:08<00:00, 14.54it/s, est. speed input: 7605.82 toks/s, output: 14.86 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:08<00:00, 14.48it/s, est. speed input: 7601.60 toks/s, output: 14.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.48it/s, est. speed input: 7599.15 toks/s, output: 14.84 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.84it/s, est. speed input: 7599.15 toks/s, output: 14.84 toks/s]
[rank0]:[W125 21:16:48.481718166 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 21:16:51
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:17:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 21:17:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=414235) WARNING 01-25 21:17:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=414235) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=414235) WARNING 01-25 21:17:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 14.98 requests/s, 15353.12 total tokens/s, 14.98 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 21:17:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:17:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:17:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:17:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:17:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:17:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:17:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:17:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:17:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:17:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:17:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:17:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:17:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:17:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:17:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:17:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:17:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:17:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:17:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:17:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:17:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:17:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:17:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:17:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:17:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:17:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:17:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:17:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=414235) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=414235) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.09s/it]
(EngineCore_DP0 pid=414235) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.09s/it]
(EngineCore_DP0 pid=414235) 
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=414235) [2026-01-25 21:17:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=414235) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.71it/s]
(EngineCore_DP0 pid=414235) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.62it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 271.77it/s]
Adding requests:  45%|████▍     | 57/128 [00:00<00:00, 278.88it/s]
Adding requests:  68%|██████▊   | 87/128 [00:00<00:00, 286.75it/s]
Adding requests:  91%|█████████ | 116/128 [00:00<00:00, 283.14it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 283.20it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 15.94it/s, est. speed input: 16329.12 toks/s, output: 15.94 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 16.07it/s, est. speed input: 16437.28 toks/s, output: 16.05 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:07, 16.06it/s, est. speed input: 16435.95 toks/s, output: 16.05 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:07, 16.16it/s, est. speed input: 16504.38 toks/s, output: 16.12 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:07, 16.26it/s, est. speed input: 16570.31 toks/s, output: 16.18 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:07, 16.41it/s, est. speed input: 16657.31 toks/s, output: 16.27 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 16.49it/s, est. speed input: 16710.83 toks/s, output: 16.32 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 16.60it/s, est. speed input: 16775.19 toks/s, output: 16.38 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:06, 16.56it/s, est. speed input: 16787.72 toks/s, output: 16.39 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 16.54it/s, est. speed input: 16798.17 toks/s, output: 16.40 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 77.53it/s, est. speed input: 36423.06 toks/s, output: 35.57 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 41.95it/s, est. speed input: 31835.18 toks/s, output: 31.09 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 31.29it/s, est. speed input: 29306.70 toks/s, output: 28.62 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:02<00:02, 26.34it/s, est. speed input: 27765.02 toks/s, output: 27.11 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:02, 23.62it/s, est. speed input: 26785.04 toks/s, output: 26.16 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:02, 22.06it/s, est. speed input: 26183.94 toks/s, output: 25.57 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:03<00:02, 20.53it/s, est. speed input: 25585.93 toks/s, output: 24.99 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:03<00:02, 19.07it/s, est. speed input: 24982.60 toks/s, output: 24.40 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:03<00:02, 17.96it/s, est. speed input: 24441.22 toks/s, output: 23.87 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:03<00:02, 17.30it/s, est. speed input: 24099.52 toks/s, output: 23.53 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:03<00:02, 16.69it/s, est. speed input: 23773.05 toks/s, output: 23.22 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:02, 16.30it/s, est. speed input: 23490.59 toks/s, output: 22.94 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:04<00:02, 16.00it/s, est. speed input: 23226.95 toks/s, output: 22.68 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:04<00:02, 15.73it/s, est. speed input: 22975.16 toks/s, output: 22.44 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:04<00:02, 15.56it/s, est. speed input: 22744.00 toks/s, output: 22.21 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:04<00:02, 15.44it/s, est. speed input: 22526.20 toks/s, output: 22.00 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:04<00:01, 15.35it/s, est. speed input: 22321.61 toks/s, output: 21.80 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:04<00:01, 15.29it/s, est. speed input: 22129.71 toks/s, output: 21.61 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:04<00:01, 15.19it/s, est. speed input: 21940.06 toks/s, output: 21.43 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:04<00:01, 15.22it/s, est. speed input: 21773.29 toks/s, output: 21.26 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:05<00:01, 15.15it/s, est. speed input: 21603.64 toks/s, output: 21.10 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:05<00:01, 15.06it/s, est. speed input: 21438.84 toks/s, output: 20.94 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:05<00:01, 15.14it/s, est. speed input: 21298.07 toks/s, output: 20.80 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:05<00:00, 15.19it/s, est. speed input: 21164.27 toks/s, output: 20.67 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:05<00:00, 15.11it/s, est. speed input: 21023.66 toks/s, output: 20.53 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:05<00:00, 15.08it/s, est. speed input: 20892.44 toks/s, output: 20.40 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:05<00:00, 14.68it/s, est. speed input: 20726.96 toks/s, output: 20.24 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:06<00:00, 14.85it/s, est. speed input: 20615.69 toks/s, output: 20.13 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:06<00:00, 14.99it/s, est. speed input: 20510.86 toks/s, output: 20.03 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:06<00:00, 15.02it/s, est. speed input: 20403.73 toks/s, output: 19.93 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:06<00:00, 14.99it/s, est. speed input: 20296.93 toks/s, output: 19.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 14.99it/s, est. speed input: 20240.69 toks/s, output: 19.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 19.77it/s, est. speed input: 20240.69 toks/s, output: 19.77 toks/s]
[rank0]:[W125 21:17:47.481795259 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 21:17:50
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:18:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 21:18:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=415311) WARNING 01-25 21:18:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=415311) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=415311) WARNING 01-25 21:18:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 28.72 requests/s, 29438.37 total tokens/s, 28.72 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 21:18:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:18:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:18:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:18:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:18:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:18:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:18:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:18:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:18:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:18:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:18:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:18:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:18:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:18:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:18:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:18:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:18:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:18:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:18:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:18:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:18:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:18:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:18:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:18:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:18:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:18:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:18:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:18:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=415311) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=415311) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, -2.39it/s]
(EngineCore_DP0 pid=415311) 
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=415311) [2026-01-25 21:18:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=415311) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.13it/s]
(EngineCore_DP0 pid=415311) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.71it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.57it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 27/256 [00:00<00:00, 263.49it/s]
Adding requests:  23%|██▎       | 58/256 [00:00<00:00, 285.46it/s]
Adding requests:  35%|███▍      | 89/256 [00:00<00:00, 294.95it/s]
Adding requests:  46%|████▋     | 119/256 [00:00<00:00, 284.31it/s]
Adding requests:  58%|█████▊    | 148/256 [00:00<00:00, 283.88it/s]
Adding requests:  70%|██████▉   | 179/256 [00:00<00:00, 290.39it/s]
Adding requests:  82%|████████▏ | 209/256 [00:00<00:00, 292.00it/s]
Adding requests:  95%|█████████▍| 242/256 [00:00<00:00, 301.68it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 293.22it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 177.39it/s, est. speed input: 181701.03 toks/s, output: 177.41 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:00<00:04, 46.04it/s, est. speed input: 53388.38 toks/s, output: 52.14 toks/s]   
Processed prompts:  19%|█▉        | 48/256 [00:01<00:05, 39.40it/s, est. speed input: 46164.34 toks/s, output: 45.08 toks/s]
Processed prompts:  21%|██▏       | 55/256 [00:01<00:05, 38.01it/s, est. speed input: 44347.93 toks/s, output: 43.31 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:01<00:05, 35.46it/s, est. speed input: 42250.25 toks/s, output: 41.26 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:05, 32.09it/s, est. speed input: 39997.22 toks/s, output: 39.06 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:05, 31.40it/s, est. speed input: 39200.09 toks/s, output: 38.28 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:05, 30.79it/s, est. speed input: 38513.11 toks/s, output: 37.61 toks/s]
Processed prompts:  30%|███       | 78/256 [00:02<00:05, 30.36it/s, est. speed input: 37938.79 toks/s, output: 37.05 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:02<00:05, 30.01it/s, est. speed input: 37434.03 toks/s, output: 36.56 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:02<00:05, 29.76it/s, est. speed input: 36992.29 toks/s, output: 36.12 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:05, 29.57it/s, est. speed input: 36598.91 toks/s, output: 35.74 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:02<00:00, 128.80it/s, est. speed input: 55188.07 toks/s, output: 53.89 toks/s]
Processed prompts:  61%|██████▏   | 157/256 [00:03<00:01, 75.82it/s, est. speed input: 52076.38 toks/s, output: 50.86 toks/s] 
Processed prompts:  65%|██████▌   | 167/256 [00:03<00:01, 57.49it/s, est. speed input: 49832.64 toks/s, output: 48.66 toks/s]
Processed prompts:  68%|██████▊   | 175/256 [00:03<00:01, 48.69it/s, est. speed input: 48339.10 toks/s, output: 47.21 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:03<00:01, 41.52it/s, est. speed input: 46786.73 toks/s, output: 45.69 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:04<00:01, 38.37it/s, est. speed input: 45929.77 toks/s, output: 44.85 toks/s]
Processed prompts:  75%|███████▌  | 193/256 [00:04<00:01, 38.07it/s, est. speed input: 45670.01 toks/s, output: 44.60 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 34.76it/s, est. speed input: 44861.48 toks/s, output: 43.81 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:04<00:01, 34.12it/s, est. speed input: 44523.02 toks/s, output: 43.48 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:04<00:01, 33.67it/s, est. speed input: 44218.62 toks/s, output: 43.18 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:04<00:01, 33.20it/s, est. speed input: 43918.08 toks/s, output: 42.89 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:05<00:01, 32.89it/s, est. speed input: 43640.01 toks/s, output: 42.62 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:05<00:01, 32.51it/s, est. speed input: 43358.58 toks/s, output: 42.34 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:05<00:01, 32.36it/s, est. speed input: 43106.82 toks/s, output: 42.10 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 32.35it/s, est. speed input: 42877.08 toks/s, output: 41.87 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:05<00:00, 32.01it/s, est. speed input: 42623.65 toks/s, output: 41.62 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:05<00:00, 31.93it/s, est. speed input: 42397.64 toks/s, output: 41.40 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:05<00:00, 31.81it/s, est. speed input: 42175.49 toks/s, output: 41.19 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:05<00:00, 31.71it/s, est. speed input: 41961.28 toks/s, output: 40.98 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:06<00:00, 31.67it/s, est. speed input: 41759.56 toks/s, output: 40.78 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:06<00:00, 31.53it/s, est. speed input: 41555.35 toks/s, output: 40.58 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:06<00:00, 31.50it/s, est. speed input: 41365.90 toks/s, output: 40.40 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 31.50it/s, est. speed input: 41279.79 toks/s, output: 40.31 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 40.31it/s, est. speed input: 41279.79 toks/s, output: 40.31 toks/s]
[rank0]:[W125 21:18:47.200601960 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 21:18:50
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:19:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 21:19:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=416397) WARNING 01-25 21:19:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=416397) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=416397) WARNING 01-25 21:19:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.83 requests/s, 43904.49 total tokens/s, 42.83 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 21:19:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:19:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:19:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:19:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:19:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:19:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:19:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:19:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:19:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:19:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:19:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:19:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:19:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:19:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:19:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:19:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:19:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:19:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:19:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:19:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:19:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:19:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:19:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:19:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:19:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:19:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:19:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:19:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=416397) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=416397) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.15s/it]
(EngineCore_DP0 pid=416397) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.15s/it]
(EngineCore_DP0 pid=416397) 
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=416397) [2026-01-25 21:19:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=416397) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.69it/s]
(EngineCore_DP0 pid=416397) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.08it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 274.93it/s]
Adding requests:  11%|█▏        | 58/512 [00:00<00:01, 288.74it/s]
Adding requests:  18%|█▊        | 90/512 [00:00<00:01, 299.94it/s]
Adding requests:  24%|██▍       | 122/512 [00:00<00:01, 304.69it/s]
Adding requests:  30%|██▉       | 153/512 [00:00<00:01, 303.08it/s]
Adding requests:  36%|███▌      | 184/512 [00:00<00:01, 303.00it/s]
Adding requests:  42%|████▏     | 216/512 [00:00<00:00, 306.02it/s]
Adding requests:  49%|████▉     | 252/512 [00:00<00:00, 320.50it/s]
Adding requests:  56%|█████▌    | 285/512 [00:00<00:00, 322.32it/s]
Adding requests:  62%|██████▏   | 318/512 [00:01<00:00, 324.02it/s]
Adding requests:  69%|██████▉   | 354/512 [00:01<00:00, 332.64it/s]
Adding requests:  76%|███████▌  | 388/512 [00:01<00:00, 331.61it/s]
Adding requests:  82%|████████▏ | 422/512 [00:01<00:00, 331.10it/s]
Adding requests:  89%|████████▉ | 456/512 [00:01<00:00, 329.66it/s]
Adding requests:  96%|█████████▌| 489/512 [00:01<00:00, 323.85it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 317.67it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:00<00:00, 558.99it/s, est. speed input: 572541.64 toks/s, output: 559.03 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:01<00:05, 76.38it/s, est. speed input: 90532.36 toks/s, output: 88.41 toks/s]  
Processed prompts:  28%|██▊       | 144/512 [00:01<00:05, 66.72it/s, est. speed input: 79437.47 toks/s, output: 77.58 toks/s]
Processed prompts:  31%|███▏      | 161/512 [00:02<00:05, 61.71it/s, est. speed input: 74492.56 toks/s, output: 72.75 toks/s]
Processed prompts:  34%|███▍      | 173/512 [00:02<00:05, 57.27it/s, est. speed input: 70960.10 toks/s, output: 69.30 toks/s]
Processed prompts:  36%|███▌      | 183/512 [00:02<00:06, 51.86it/s, est. speed input: 67418.33 toks/s, output: 65.84 toks/s]
Processed prompts:  37%|███▋      | 191/512 [00:02<00:06, 50.08it/s, est. speed input: 65900.13 toks/s, output: 64.36 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:03<00:01, 134.47it/s, est. speed input: 88151.10 toks/s, output: 86.08 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:03<00:02, 88.70it/s, est. speed input: 81238.86 toks/s, output: 79.33 toks/s] 
Processed prompts:  60%|██████    | 308/512 [00:04<00:02, 75.54it/s, est. speed input: 78215.93 toks/s, output: 76.38 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:04<00:02, 63.43it/s, est. speed input: 74773.83 toks/s, output: 73.02 toks/s]
Processed prompts:  65%|██████▌   | 333/512 [00:04<00:02, 62.45it/s, est. speed input: 74144.40 toks/s, output: 72.41 toks/s]
Processed prompts:  67%|██████▋   | 343/512 [00:04<00:03, 54.73it/s, est. speed input: 71926.91 toks/s, output: 70.24 toks/s]
Processed prompts:  69%|██████▊   | 351/512 [00:05<00:03, 52.10it/s, est. speed input: 70860.97 toks/s, output: 69.20 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:05<00:03, 48.56it/s, est. speed input: 69669.86 toks/s, output: 68.04 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:05<00:03, 46.96it/s, est. speed input: 68746.06 toks/s, output: 67.13 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:05<00:03, 45.71it/s, est. speed input: 67889.26 toks/s, output: 66.30 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:05<00:02, 44.77it/s, est. speed input: 67089.19 toks/s, output: 65.52 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:06<00:02, 44.04it/s, est. speed input: 66336.23 toks/s, output: 64.78 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:06<00:02, 43.51it/s, est. speed input: 65630.12 toks/s, output: 64.09 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:06<00:02, 43.21it/s, est. speed input: 64978.19 toks/s, output: 63.46 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:06<00:02, 42.97it/s, est. speed input: 64360.56 toks/s, output: 62.85 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:06<00:02, 43.49it/s, est. speed input: 63872.59 toks/s, output: 62.38 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:06<00:01, 44.18it/s, est. speed input: 63450.10 toks/s, output: 61.96 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:07<00:01, 44.70it/s, est. speed input: 63050.13 toks/s, output: 61.57 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:07<00:01, 45.05it/s, est. speed input: 62666.17 toks/s, output: 61.20 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:07<00:01, 45.29it/s, est. speed input: 62299.56 toks/s, output: 60.84 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:07<00:01, 45.45it/s, est. speed input: 61948.29 toks/s, output: 60.50 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:07<00:00, 45.52it/s, est. speed input: 61607.67 toks/s, output: 60.16 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:07<00:00, 45.62it/s, est. speed input: 61287.73 toks/s, output: 59.85 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:08<00:00, 45.67it/s, est. speed input: 60979.50 toks/s, output: 59.55 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:08<00:00, 45.72it/s, est. speed input: 60685.27 toks/s, output: 59.26 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:08<00:00, 45.74it/s, est. speed input: 60402.13 toks/s, output: 58.99 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:08<00:00, 47.18it/s, est. speed input: 60252.45 toks/s, output: 58.84 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 47.18it/s, est. speed input: 60487.33 toks/s, output: 59.07 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 59.07it/s, est. speed input: 60487.33 toks/s, output: 59.07 toks/s]
[rank0]:[W125 21:19:51.963699421 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 21:19:54
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:20:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 21:20:09 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=417527) WARNING 01-25 21:20:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=417527) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=417527) WARNING 01-25 21:20:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.76 requests/s, 42801.18 total tokens/s, 41.76 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-25 21:20:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:20:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:20:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:20:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:20:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:20:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:20:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:20:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:20:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:20:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:20:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:20:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:20:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:20:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:20:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:20:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:20:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:20:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:20:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:20:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:20:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:20:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:20:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:20:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:20:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:20:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:20:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:20:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:16] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:16] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=417527) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=417527) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.16s/it]
(EngineCore_DP0 pid=417527) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.16s/it]
(EngineCore_DP0 pid=417527) 
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=417527) [2026-01-25 21:20:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=417527) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:02,  2.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  3.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  4.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  4.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  3.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  3.50it/s]
(EngineCore_DP0 pid=417527) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  5.25it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  4.20it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  5.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  6.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  5.63it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 30/1024 [00:00<00:03, 296.66it/s]
Adding requests:   6%|▌         | 62/1024 [00:00<00:03, 307.57it/s]
Adding requests:   9%|▉         | 93/1024 [00:00<00:03, 301.02it/s]
Adding requests:  12%|█▏        | 127/1024 [00:00<00:02, 314.25it/s]
Adding requests:  16%|█▌        | 160/1024 [00:00<00:02, 318.12it/s]
Adding requests:  19%|█▉        | 192/1024 [00:00<00:02, 316.83it/s]
Adding requests:  22%|██▏       | 226/1024 [00:00<00:02, 321.73it/s]
Adding requests:  25%|██▌       | 259/1024 [00:00<00:02, 317.11it/s]
Adding requests:  28%|██▊       | 291/1024 [00:00<00:02, 317.31it/s]
Adding requests:  32%|███▏      | 323/1024 [00:01<00:02, 315.01it/s]
Adding requests:  35%|███▍      | 356/1024 [00:01<00:02, 316.66it/s]
Adding requests:  38%|███▊      | 388/1024 [00:01<00:02, 315.57it/s]
Adding requests:  41%|████      | 420/1024 [00:01<00:01, 314.69it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 320.04it/s]
Adding requests:  48%|████▊     | 487/1024 [00:01<00:01, 322.94it/s]
Adding requests:  51%|█████     | 520/1024 [00:01<00:01, 319.30it/s]
Adding requests:  54%|█████▍    | 552/1024 [00:01<00:01, 318.12it/s]
Adding requests:  57%|█████▋    | 587/1024 [00:01<00:01, 326.21it/s]
Adding requests:  61%|██████    | 622/1024 [00:01<00:01, 330.12it/s]
Adding requests:  64%|██████▍   | 657/1024 [00:02<00:01, 335.58it/s]
Adding requests:  68%|██████▊   | 693/1024 [00:02<00:00, 342.38it/s]
Adding requests:  71%|███████   | 728/1024 [00:02<00:00, 343.92it/s]
Adding requests:  75%|███████▍  | 763/1024 [00:02<00:00, 340.48it/s]
Adding requests:  78%|███████▊  | 798/1024 [00:02<00:00, 329.99it/s]
Adding requests:  81%|████████▏ | 832/1024 [00:02<00:00, 318.76it/s]
Adding requests:  84%|████████▍ | 865/1024 [00:02<00:00, 317.93it/s]
Adding requests:  88%|████████▊ | 898/1024 [00:02<00:00, 320.09it/s]
Adding requests:  91%|█████████ | 931/1024 [00:02<00:00, 317.48it/s]
Adding requests:  94%|█████████▍| 963/1024 [00:03<00:00, 314.71it/s]
Adding requests:  97%|█████████▋| 995/1024 [00:03<00:00, 311.18it/s]
Adding requests: 100%|██████████| 1024/1024 [00:03<00:00, 319.84it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:00<00:00, 1169.36it/s, est. speed input: 1197789.08 toks/s, output: 1169.47 toks/s]
Processed prompts:  23%|██▎       | 239/1024 [00:02<00:10, 72.80it/s, est. speed input: 87044.12 toks/s, output: 85.00 toks/s]      
Processed prompts:  28%|██▊       | 290/1024 [00:04<00:12, 58.38it/s, est. speed input: 71281.24 toks/s, output: 69.61 toks/s]
Processed prompts:  31%|███▏      | 320/1024 [00:04<00:12, 57.03it/s, est. speed input: 69030.74 toks/s, output: 67.41 toks/s]
Processed prompts:  33%|███▎      | 340/1024 [00:05<00:12, 52.70it/s, est. speed input: 65780.89 toks/s, output: 64.24 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:05<00:13, 50.47it/s, est. speed input: 64163.51 toks/s, output: 62.66 toks/s]
Processed prompts:  36%|███▌      | 365/1024 [00:05<00:12, 51.65it/s, est. speed input: 64132.50 toks/s, output: 62.63 toks/s]
Processed prompts:  37%|███▋      | 375/1024 [00:06<00:12, 52.23it/s, est. speed input: 63932.61 toks/s, output: 62.43 toks/s]
Processed prompts:  37%|███▋      | 383/1024 [00:06<00:12, 51.05it/s, est. speed input: 63409.17 toks/s, output: 61.92 toks/s]
Processed prompts:  38%|███▊      | 390/1024 [00:06<00:12, 48.87it/s, est. speed input: 62756.68 toks/s, output: 61.29 toks/s]
Processed prompts:  39%|███▊      | 396/1024 [00:06<00:13, 45.68it/s, est. speed input: 61981.42 toks/s, output: 60.53 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:06<00:14, 42.91it/s, est. speed input: 61248.60 toks/s, output: 59.81 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:06<00:14, 43.37it/s, est. speed input: 60849.70 toks/s, output: 59.42 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:13, 43.74it/s, est. speed input: 60472.24 toks/s, output: 59.05 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:07<00:13, 43.65it/s, est. speed input: 60065.80 toks/s, output: 58.66 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:07<00:13, 42.97it/s, est. speed input: 59605.40 toks/s, output: 58.21 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:07<00:13, 42.49it/s, est. speed input: 59168.10 toks/s, output: 57.78 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:07<00:13, 42.17it/s, est. speed input: 58755.34 toks/s, output: 57.38 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:13, 41.94it/s, est. speed input: 58361.90 toks/s, output: 56.99 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:08<00:13, 41.80it/s, est. speed input: 57988.91 toks/s, output: 56.63 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:08<00:13, 41.71it/s, est. speed input: 57633.96 toks/s, output: 56.28 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:08<00:13, 41.61it/s, est. speed input: 57291.85 toks/s, output: 55.95 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:08<00:12, 41.52it/s, est. speed input: 56962.35 toks/s, output: 55.63 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:12, 41.46it/s, est. speed input: 56647.24 toks/s, output: 55.32 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:09<00:12, 41.44it/s, est. speed input: 56347.31 toks/s, output: 55.03 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:09<00:12, 41.42it/s, est. speed input: 56059.72 toks/s, output: 54.75 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:09<00:12, 41.38it/s, est. speed input: 55781.28 toks/s, output: 54.47 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:09<00:11, 41.34it/s, est. speed input: 55512.00 toks/s, output: 54.21 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:09<00:11, 41.32it/s, est. speed input: 55254.94 toks/s, output: 53.96 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:10<00:11, 41.31it/s, est. speed input: 55007.74 toks/s, output: 53.72 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:10<00:11, 41.30it/s, est. speed input: 54768.85 toks/s, output: 53.49 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:10<00:11, 41.27it/s, est. speed input: 54537.12 toks/s, output: 53.26 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:10<00:11, 41.26it/s, est. speed input: 54315.47 toks/s, output: 53.04 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:10<00:10, 41.24it/s, est. speed input: 54099.56 toks/s, output: 52.83 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:11<00:10, 41.27it/s, est. speed input: 53894.88 toks/s, output: 52.63 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:11<00:10, 41.27it/s, est. speed input: 53695.81 toks/s, output: 52.44 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:11<00:10, 41.29it/s, est. speed input: 53505.16 toks/s, output: 52.25 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:11<00:10, 41.30it/s, est. speed input: 53320.10 toks/s, output: 52.07 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:11<00:09, 41.31it/s, est. speed input: 53141.10 toks/s, output: 51.90 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:12<00:09, 41.30it/s, est. speed input: 52967.21 toks/s, output: 51.73 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:12<00:09, 41.30it/s, est. speed input: 52798.67 toks/s, output: 51.56 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:12<00:09, 41.28it/s, est. speed input: 52634.82 toks/s, output: 51.40 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:12<00:09, 41.25it/s, est. speed input: 52474.65 toks/s, output: 51.24 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:12<00:08, 42.21it/s, est. speed input: 52379.79 toks/s, output: 51.15 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:13<00:08, 42.91it/s, est. speed input: 52288.53 toks/s, output: 51.06 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:13<00:08, 43.44it/s, est. speed input: 52200.65 toks/s, output: 50.98 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:13<00:07, 43.81it/s, est. speed input: 52114.84 toks/s, output: 50.89 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:13<00:07, 44.07it/s, est. speed input: 52030.87 toks/s, output: 50.81 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:13<00:07, 44.24it/s, est. speed input: 51948.91 toks/s, output: 50.73 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:13<00:07, 44.37it/s, est. speed input: 51869.27 toks/s, output: 50.65 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:14<00:06, 44.47it/s, est. speed input: 51791.85 toks/s, output: 50.58 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:14<00:06, 44.56it/s, est. speed input: 51717.65 toks/s, output: 50.51 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:14<00:06, 44.60it/s, est. speed input: 51643.91 toks/s, output: 50.43 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:14<00:06, 44.61it/s, est. speed input: 51571.43 toks/s, output: 50.36 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:14<00:06, 44.64it/s, est. speed input: 51501.68 toks/s, output: 50.29 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:15<00:06, 44.65it/s, est. speed input: 51433.00 toks/s, output: 50.23 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:15<00:05, 44.66it/s, est. speed input: 51365.93 toks/s, output: 50.16 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:15<00:05, 43.79it/s, est. speed input: 51260.80 toks/s, output: 50.06 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:15<00:05, 42.96it/s, est. speed input: 51146.67 toks/s, output: 49.95 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:15<00:05, 42.36it/s, est. speed input: 51033.64 toks/s, output: 49.84 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:15<00:05, 41.99it/s, est. speed input: 50925.50 toks/s, output: 49.73 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:16<00:05, 41.74it/s, est. speed input: 50820.00 toks/s, output: 49.63 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:16<00:05, 41.58it/s, est. speed input: 50717.50 toks/s, output: 49.53 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:16<00:04, 41.45it/s, est. speed input: 50616.73 toks/s, output: 49.43 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:16<00:04, 41.37it/s, est. speed input: 50518.86 toks/s, output: 49.33 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:16<00:04, 41.32it/s, est. speed input: 50423.44 toks/s, output: 49.24 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:17<00:04, 41.26it/s, est. speed input: 50328.86 toks/s, output: 49.15 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:17<00:04, 41.23it/s, est. speed input: 50237.70 toks/s, output: 49.06 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:17<00:04, 41.20it/s, est. speed input: 50147.26 toks/s, output: 48.97 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:17<00:03, 41.20it/s, est. speed input: 50060.34 toks/s, output: 48.89 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:17<00:03, 41.22it/s, est. speed input: 49975.15 toks/s, output: 48.80 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:18<00:03, 41.24it/s, est. speed input: 49892.84 toks/s, output: 48.72 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:18<00:03, 41.21it/s, est. speed input: 49810.16 toks/s, output: 48.64 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:18<00:03, 41.24it/s, est. speed input: 49731.63 toks/s, output: 48.57 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:18<00:02, 41.22it/s, est. speed input: 49652.90 toks/s, output: 48.49 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:18<00:02, 41.22it/s, est. speed input: 49576.28 toks/s, output: 48.41 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:19<00:02, 41.17it/s, est. speed input: 49499.68 toks/s, output: 48.34 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:19<00:02, 41.16it/s, est. speed input: 49424.92 toks/s, output: 48.27 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:19<00:02, 41.17it/s, est. speed input: 49352.64 toks/s, output: 48.20 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:19<00:01, 41.14it/s, est. speed input: 49280.39 toks/s, output: 48.13 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:19<00:01, 41.13it/s, est. speed input: 49209.93 toks/s, output: 48.06 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:20<00:01, 41.14it/s, est. speed input: 49141.60 toks/s, output: 47.99 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:20<00:01, 41.15it/s, est. speed input: 49074.48 toks/s, output: 47.92 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:20<00:01, 41.16it/s, est. speed input: 49008.92 toks/s, output: 47.86 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:20<00:00, 41.66it/s, est. speed input: 48962.70 toks/s, output: 47.82 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:20<00:00, 42.49it/s, est. speed input: 48933.87 toks/s, output: 47.79 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:20<00:00, 43.09it/s, est. speed input: 48905.68 toks/s, output: 47.76 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:21<00:00, 43.53it/s, est. speed input: 48878.18 toks/s, output: 47.73 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:21<00:00, 45.34it/s, est. speed input: 48897.05 toks/s, output: 47.75 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:21<00:00, 45.34it/s, est. speed input: 49184.55 toks/s, output: 48.03 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:21<00:00, 48.03it/s, est. speed input: 49184.55 toks/s, output: 48.03 toks/s]
[rank0]:[W125 21:21:11.590046919 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-25 21:21:14
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:21:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 21:21:35 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=418973) WARNING 01-25 21:21:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=418973) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=418973) WARNING 01-25 21:21:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.94 requests/s, 42984.38 total tokens/s, 41.94 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-25 21:21:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:21:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:21:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:21:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:21:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:21:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:21:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:21:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:21:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:21:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:21:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:21:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:21:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:21:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:21:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:21:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:21:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:21:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:21:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:21:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:21:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:21:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:21:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:21:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:21:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:21:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:21:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:21:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=418973) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=418973) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.16s/it]
(EngineCore_DP0 pid=418973) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.16s/it]
(EngineCore_DP0 pid=418973) 
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=418973) [2026-01-25 21:21:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=418973) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.99it/s]
(EngineCore_DP0 pid=418973) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.51it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.16it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  7.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  5.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  6.16it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 32/2048 [00:00<00:06, 315.49it/s]
Adding requests:   3%|▎         | 66/2048 [00:00<00:06, 328.11it/s]
Adding requests:   5%|▍         | 99/2048 [00:00<00:05, 327.33it/s]
Adding requests:   7%|▋         | 134/2048 [00:00<00:05, 333.28it/s]
Adding requests:   8%|▊         | 168/2048 [00:00<00:05, 329.67it/s]
Adding requests:  10%|▉         | 202/2048 [00:00<00:05, 332.43it/s]
Adding requests:  12%|█▏        | 237/2048 [00:00<00:05, 336.79it/s]
Adding requests:  13%|█▎        | 271/2048 [00:00<00:05, 327.17it/s]
Adding requests:  15%|█▍        | 304/2048 [00:00<00:05, 324.79it/s]
Adding requests:  16%|█▋        | 337/2048 [00:01<00:05, 323.91it/s]
Adding requests:  18%|█▊        | 371/2048 [00:01<00:05, 325.70it/s]
Adding requests:  20%|█▉        | 405/2048 [00:01<00:04, 328.71it/s]
Adding requests:  21%|██▏       | 439/2048 [00:01<00:04, 329.31it/s]
Adding requests:  23%|██▎       | 472/2048 [00:01<00:04, 328.18it/s]
Adding requests:  25%|██▍       | 506/2048 [00:01<00:04, 331.62it/s]
Adding requests:  26%|██▋       | 540/2048 [00:01<00:04, 318.04it/s]
Adding requests:  28%|██▊       | 573/2048 [00:01<00:04, 319.23it/s]
Adding requests:  30%|██▉       | 606/2048 [00:01<00:04, 321.74it/s]
Adding requests:  31%|███▏      | 642/2048 [00:01<00:04, 331.77it/s]
Adding requests:  33%|███▎      | 677/2048 [00:02<00:04, 335.86it/s]
Adding requests:  35%|███▍      | 711/2048 [00:02<00:03, 336.80it/s]
Adding requests:  36%|███▋      | 745/2048 [00:02<00:03, 330.55it/s]
Adding requests:  38%|███▊      | 779/2048 [00:02<00:03, 332.94it/s]
Adding requests:  40%|███▉      | 813/2048 [00:02<00:03, 328.22it/s]
Adding requests:  41%|████▏     | 846/2048 [00:02<00:03, 323.10it/s]
Adding requests:  43%|████▎     | 883/2048 [00:02<00:03, 335.54it/s]
Adding requests:  45%|████▍     | 919/2048 [00:02<00:03, 341.77it/s]
Adding requests:  47%|████▋     | 954/2048 [00:02<00:03, 343.31it/s]
Adding requests:  48%|████▊     | 991/2048 [00:02<00:03, 349.39it/s]
Adding requests:  50%|█████     | 1028/2048 [00:03<00:02, 355.45it/s]
Adding requests:  52%|█████▏    | 1064/2048 [00:03<00:02, 347.37it/s]
Adding requests:  54%|█████▎    | 1099/2048 [00:03<00:02, 336.40it/s]
Adding requests:  55%|█████▌    | 1133/2048 [00:03<00:02, 330.62it/s]
Adding requests:  57%|█████▋    | 1169/2048 [00:03<00:02, 336.30it/s]
Adding requests:  59%|█████▉    | 1204/2048 [00:03<00:02, 339.68it/s]
Adding requests:  60%|██████    | 1239/2048 [00:03<00:02, 329.18it/s]
Adding requests:  62%|██████▏   | 1273/2048 [00:03<00:02, 319.84it/s]
Adding requests:  64%|██████▍   | 1306/2048 [00:03<00:02, 319.83it/s]
Adding requests:  65%|██████▌   | 1339/2048 [00:04<00:02, 321.52it/s]
Adding requests:  67%|██████▋   | 1372/2048 [00:04<00:02, 322.33it/s]
Adding requests:  69%|██████▊   | 1406/2048 [00:04<00:01, 326.26it/s]
Adding requests:  70%|███████   | 1439/2048 [00:04<00:01, 315.00it/s]
Adding requests:  72%|███████▏  | 1472/2048 [00:04<00:01, 317.64it/s]
Adding requests:  74%|███████▎  | 1506/2048 [00:04<00:01, 323.35it/s]
Adding requests:  75%|███████▌  | 1539/2048 [00:04<00:01, 324.55it/s]
Adding requests:  77%|███████▋  | 1572/2048 [00:04<00:01, 324.55it/s]
Adding requests:  78%|███████▊  | 1606/2048 [00:04<00:01, 328.17it/s]
Adding requests:  80%|████████  | 1639/2048 [00:04<00:01, 326.43it/s]
Adding requests:  82%|████████▏ | 1672/2048 [00:05<00:01, 321.47it/s]
Adding requests:  83%|████████▎ | 1705/2048 [00:05<00:01, 319.68it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 448.92it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:00<00:02, 637.46it/s, est. speed input: 652805.19 toks/s, output: 637.47 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:01<00:12, 132.78it/s, est. speed input: 167924.86 toks/s, output: 163.99 toks/s]
Processed prompts:  17%|█▋        | 351/2048 [00:02<00:14, 119.22it/s, est. speed input: 152721.26 toks/s, output: 149.14 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:03<00:20, 81.41it/s, est. speed input: 120942.33 toks/s, output: 118.11 toks/s] 
Processed prompts:  19%|█▉        | 386/2048 [00:03<00:22, 72.48it/s, est. speed input: 112219.86 toks/s, output: 109.59 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:03<00:24, 66.12it/s, est. speed input: 106026.23 toks/s, output: 103.54 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:04<00:26, 60.83it/s, est. speed input: 100896.44 toks/s, output: 98.53 toks/s] 
Processed prompts:  21%|██        | 434/2048 [00:04<00:28, 56.60it/s, est. speed input: 96568.31 toks/s, output: 94.30 toks/s] 
Processed prompts:  22%|██▏       | 450/2048 [00:04<00:29, 53.33it/s, est. speed input: 92867.87 toks/s, output: 90.69 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:05<00:31, 50.86it/s, est. speed input: 89666.39 toks/s, output: 87.56 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:05<00:31, 49.05it/s, est. speed input: 86875.13 toks/s, output: 84.84 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:06<00:32, 47.45it/s, est. speed input: 84311.65 toks/s, output: 82.34 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:06<00:32, 46.58it/s, est. speed input: 82134.48 toks/s, output: 80.21 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:06<00:33, 45.41it/s, est. speed input: 80014.12 toks/s, output: 78.14 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:07<00:34, 44.04it/s, est. speed input: 77944.89 toks/s, output: 76.12 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:07<00:34, 43.11it/s, est. speed input: 76088.63 toks/s, output: 74.31 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:07<00:34, 42.45it/s, est. speed input: 74411.12 toks/s, output: 72.67 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:08<00:34, 42.01it/s, est. speed input: 72894.12 toks/s, output: 71.19 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:08<00:34, 41.69it/s, est. speed input: 71510.72 toks/s, output: 69.83 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:09<00:34, 41.48it/s, est. speed input: 70246.96 toks/s, output: 68.60 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:09<00:34, 41.32it/s, est. speed input: 69084.89 toks/s, output: 67.47 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:09<00:33, 41.22it/s, est. speed input: 68015.13 toks/s, output: 66.42 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:10<00:33, 41.14it/s, est. speed input: 67027.26 toks/s, output: 65.46 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:10<00:33, 41.09it/s, est. speed input: 66110.16 toks/s, output: 64.56 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:11<00:32, 41.04it/s, est. speed input: 65256.29 toks/s, output: 63.73 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:11<00:32, 41.02it/s, est. speed input: 64462.54 toks/s, output: 62.95 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:11<00:31, 41.00it/s, est. speed input: 63720.07 toks/s, output: 62.23 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:12<00:31, 40.96it/s, est. speed input: 63021.95 toks/s, output: 61.54 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:12<00:31, 40.96it/s, est. speed input: 62370.40 toks/s, output: 60.91 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:13<00:30, 41.14it/s, est. speed input: 61785.41 toks/s, output: 60.34 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:13<00:29, 42.05it/s, est. speed input: 61343.58 toks/s, output: 59.91 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:13<00:28, 42.70it/s, est. speed input: 60924.20 toks/s, output: 59.50 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:14<00:28, 43.18it/s, est. speed input: 60526.54 toks/s, output: 59.11 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:14<00:27, 43.51it/s, est. speed input: 60148.83 toks/s, output: 58.74 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:14<00:27, 43.23it/s, est. speed input: 59730.10 toks/s, output: 58.33 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:15<00:27, 42.51it/s, est. speed input: 59272.11 toks/s, output: 57.88 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:15<00:27, 42.01it/s, est. speed input: 58837.12 toks/s, output: 57.46 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:16<00:27, 41.68it/s, est. speed input: 58423.93 toks/s, output: 57.05 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:16<00:26, 41.44it/s, est. speed input: 58029.22 toks/s, output: 56.67 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:16<00:26, 41.28it/s, est. speed input: 57653.45 toks/s, output: 56.30 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:17<00:26, 41.17it/s, est. speed input: 57294.75 toks/s, output: 55.95 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:17<00:26, 41.08it/s, est. speed input: 56951.06 toks/s, output: 55.62 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:17<00:25, 41.02it/s, est. speed input: 56622.52 toks/s, output: 55.30 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:18<00:25, 40.97it/s, est. speed input: 56307.24 toks/s, output: 54.99 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:18<00:24, 40.94it/s, est. speed input: 56005.19 toks/s, output: 54.69 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:19<00:24, 40.91it/s, est. speed input: 55714.95 toks/s, output: 54.41 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:19<00:24, 40.89it/s, est. speed input: 55437.10 toks/s, output: 54.14 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:19<00:23, 40.88it/s, est. speed input: 55169.67 toks/s, output: 53.88 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:20<00:22, 41.77it/s, est. speed input: 54988.82 toks/s, output: 53.70 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:20<00:22, 42.49it/s, est. speed input: 54819.47 toks/s, output: 53.53 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:21<00:21, 43.00it/s, est. speed input: 54655.93 toks/s, output: 53.37 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:21<00:20, 43.37it/s, est. speed input: 54497.96 toks/s, output: 53.22 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:21<00:20, 44.31it/s, est. speed input: 54392.42 toks/s, output: 53.12 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:22<00:19, 44.29it/s, est. speed input: 54243.64 toks/s, output: 52.97 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:22<00:19, 44.27it/s, est. speed input: 54099.45 toks/s, output: 52.83 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:22<00:19, 43.89it/s, est. speed input: 53935.77 toks/s, output: 52.67 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:23<00:19, 42.92it/s, est. speed input: 53730.46 toks/s, output: 52.47 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:23<00:19, 42.27it/s, est. speed input: 53532.55 toks/s, output: 52.28 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:23<00:19, 41.83it/s, est. speed input: 53341.17 toks/s, output: 52.09 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:24<00:18, 41.52it/s, est. speed input: 53155.85 toks/s, output: 51.91 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:24<00:18, 41.30it/s, est. speed input: 52975.59 toks/s, output: 51.73 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:25<00:18, 41.16it/s, est. speed input: 52802.02 toks/s, output: 51.56 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:25<00:17, 41.06it/s, est. speed input: 52633.15 toks/s, output: 51.40 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:25<00:17, 40.99it/s, est. speed input: 52469.65 toks/s, output: 51.24 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:26<00:17, 40.94it/s, est. speed input: 52310.92 toks/s, output: 51.08 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:26<00:16, 40.90it/s, est. speed input: 52156.76 toks/s, output: 50.93 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:27<00:16, 40.88it/s, est. speed input: 52007.14 toks/s, output: 50.79 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:27<00:15, 41.72it/s, est. speed input: 51912.60 toks/s, output: 50.70 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:27<00:15, 42.43it/s, est. speed input: 51825.91 toks/s, output: 50.61 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:28<00:14, 42.96it/s, est. speed input: 51742.40 toks/s, output: 50.53 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:28<00:13, 43.32it/s, est. speed input: 51660.17 toks/s, output: 50.45 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:28<00:13, 43.59it/s, est. speed input: 51580.68 toks/s, output: 50.37 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:29<00:13, 43.77it/s, est. speed input: 51502.65 toks/s, output: 50.30 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:29<00:12, 43.91it/s, est. speed input: 51426.74 toks/s, output: 50.22 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:30<00:12, 43.99it/s, est. speed input: 51352.23 toks/s, output: 50.15 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:30<00:11, 44.05it/s, est. speed input: 51279.69 toks/s, output: 50.08 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:30<00:11, 43.96it/s, est. speed input: 51202.61 toks/s, output: 50.00 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:31<00:11, 42.97it/s, est. speed input: 51084.33 toks/s, output: 49.89 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:31<00:04, 99.63it/s, est. speed input: 53186.70 toks/s, output: 51.94 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:31<00:04, 80.67it/s, est. speed input: 53046.40 toks/s, output: 51.80 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:32<00:05, 68.11it/s, est. speed input: 52909.69 toks/s, output: 51.67 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:32<00:06, 59.64it/s, est. speed input: 52776.26 toks/s, output: 51.54 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:33<00:06, 53.86it/s, est. speed input: 52646.26 toks/s, output: 51.41 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:33<00:06, 49.89it/s, est. speed input: 52519.19 toks/s, output: 51.29 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:33<00:06, 47.14it/s, est. speed input: 52395.11 toks/s, output: 51.17 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:34<00:06, 45.22it/s, est. speed input: 52273.50 toks/s, output: 51.05 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:34<00:06, 43.90it/s, est. speed input: 52155.05 toks/s, output: 50.93 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:34<00:06, 42.97it/s, est. speed input: 52039.09 toks/s, output: 50.82 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:35<00:05, 42.43it/s, est. speed input: 51930.45 toks/s, output: 50.71 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:35<00:05, 42.94it/s, est. speed input: 51863.30 toks/s, output: 50.65 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:36<00:05, 43.32it/s, est. speed input: 51797.67 toks/s, output: 50.58 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:36<00:04, 43.60it/s, est. speed input: 51733.60 toks/s, output: 50.52 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:36<00:04, 43.78it/s, est. speed input: 51670.23 toks/s, output: 50.46 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:37<00:03, 44.67it/s, est. speed input: 51636.86 toks/s, output: 50.43 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:37<00:03, 44.50it/s, est. speed input: 51574.38 toks/s, output: 50.37 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:37<00:03, 44.42it/s, est. speed input: 51514.28 toks/s, output: 50.31 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:38<00:02, 44.36it/s, est. speed input: 51455.25 toks/s, output: 50.25 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:38<00:02, 44.31it/s, est. speed input: 51397.34 toks/s, output: 50.19 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:38<00:02, 43.57it/s, est. speed input: 51314.31 toks/s, output: 50.11 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:39<00:01, 42.71it/s, est. speed input: 51219.74 toks/s, output: 50.02 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:39<00:01, 42.13it/s, est. speed input: 51127.02 toks/s, output: 49.93 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:40<00:01, 41.73it/s, est. speed input: 51035.97 toks/s, output: 49.84 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:40<00:00, 41.46it/s, est. speed input: 50947.16 toks/s, output: 49.75 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:40<00:00, 42.05it/s, est. speed input: 50889.39 toks/s, output: 49.70 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:40<00:00, 42.05it/s, est. speed input: 51239.17 toks/s, output: 50.04 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:40<00:00, 50.04it/s, est. speed input: 51239.17 toks/s, output: 50.04 toks/s]
[rank0]:[W125 21:23:00.949763106 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-25 21:23:03
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:23:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 21:23:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=420925) WARNING 01-25 21:23:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=420925) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=420925) WARNING 01-25 21:23:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.94 requests/s, 42991.38 total tokens/s, 41.94 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-25 21:23:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:23:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:23:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:23:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:23:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:23:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:23:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:23:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:23:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:23:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:23:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:23:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:23:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:23:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:23:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:23:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:23:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:23:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:23:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:23:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:23:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:23:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:23:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:23:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:23:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:23:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:23:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:23:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=420925) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=420925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.20s/it]
(EngineCore_DP0 pid=420925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.20s/it]
(EngineCore_DP0 pid=420925) 
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=420925) [2026-01-25 21:23:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=420925) [rank0]:W0125 21:24:05.463000 420925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=420925) [rank0]:W0125 21:24:05.592000 420925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=420925) [rank0]:W0125 21:24:07.261000 420925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=420925) [rank0]:W0125 21:24:07.447000 420925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=420925) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  7.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  6.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.19it/s]
(EngineCore_DP0 pid=420925) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:02,  2.52it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:01,  3.89it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:01,  3.74it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:01<00:00,  4.37it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:01<00:00,  5.37it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:01<00:00,  6.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  7.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  5.26it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 278.72it/s]
Adding requests:   1%|▏         | 59/4096 [00:00<00:13, 292.80it/s]
Adding requests:   2%|▏         | 91/4096 [00:00<00:13, 302.45it/s]
Adding requests:   3%|▎         | 122/4096 [00:00<00:13, 300.23it/s]
Adding requests:   4%|▎         | 153/4096 [00:00<00:13, 293.94it/s]
Adding requests:   4%|▍         | 183/4096 [00:00<00:13, 294.64it/s]
Adding requests:   5%|▌         | 213/4096 [00:00<00:13, 295.83it/s]
Adding requests:   6%|▌         | 243/4096 [00:00<00:12, 296.52it/s]
Adding requests:   7%|▋         | 274/4096 [00:00<00:12, 300.53it/s]
Adding requests:   7%|▋         | 305/4096 [00:01<00:12, 297.63it/s]
Adding requests:   8%|▊         | 335/4096 [00:01<00:12, 291.69it/s]
Adding requests:   9%|▉         | 367/4096 [00:01<00:12, 297.18it/s]
Adding requests:  10%|▉         | 400/4096 [00:01<00:12, 304.42it/s]
Adding requests:  11%|█         | 433/4096 [00:01<00:11, 310.74it/s]
Adding requests:  11%|█▏        | 465/4096 [00:01<00:11, 310.73it/s]
Adding requests:  12%|█▏        | 497/4096 [00:01<00:11, 313.13it/s]
Adding requests:  13%|█▎        | 529/4096 [00:01<00:11, 307.88it/s]
Adding requests:  14%|█▍        | 564/4096 [00:01<00:11, 317.69it/s]
Adding requests:  15%|█▍        | 597/4096 [00:01<00:10, 320.30it/s]
Adding requests:  15%|█▌        | 630/4096 [00:02<00:10, 320.28it/s]
Adding requests:  16%|█▌        | 663/4096 [00:02<00:10, 319.12it/s]
Adding requests:  17%|█▋        | 696/4096 [00:02<00:10, 321.13it/s]
Adding requests:  18%|█▊        | 730/4096 [00:02<00:10, 326.21it/s]
Adding requests:  19%|█▊        | 763/4096 [00:02<00:10, 325.73it/s]
Adding requests:  19%|█▉        | 796/4096 [00:02<00:10, 324.57it/s]
Adding requests:  20%|██        | 829/4096 [00:02<00:10, 319.04it/s]
Adding requests:  21%|██        | 861/4096 [00:02<00:10, 312.50it/s]
Adding requests:  22%|██▏       | 893/4096 [00:02<00:10, 308.04it/s]
Adding requests:  23%|██▎       | 926/4096 [00:02<00:10, 314.31it/s]
Adding requests:  23%|██▎       | 959/4096 [00:03<00:09, 317.86it/s]
Adding requests:  24%|██▍       | 991/4096 [00:03<00:09, 315.11it/s]
Adding requests:  25%|██▌       | 1024/4096 [00:03<00:09, 318.92it/s]
Adding requests:  26%|██▌       | 1057/4096 [00:03<00:09, 320.59it/s]
Adding requests:  27%|██▋       | 1090/4096 [00:03<00:09, 309.97it/s]
Adding requests:  27%|██▋       | 1122/4096 [00:03<00:09, 307.14it/s]
Adding requests:  28%|██▊       | 1156/4096 [00:03<00:09, 314.32it/s]
Adding requests:  29%|██▉       | 1190/4096 [00:03<00:09, 321.22it/s]
Adding requests:  30%|██▉       | 1224/4096 [00:03<00:08, 323.71it/s]
Adding requests:  31%|███       | 1257/4096 [00:04<00:08, 316.70it/s]
Adding requests:  31%|███▏      | 1289/4096 [00:04<00:08, 317.62it/s]
Adding requests:  32%|███▏      | 1322/4096 [00:04<00:08, 320.68it/s]
Adding requests:  33%|███▎      | 1355/4096 [00:04<00:08, 320.28it/s]
Adding requests:  34%|███▍      | 1388/4096 [00:04<00:08, 320.25it/s]
Adding requests:  35%|███▍      | 1422/4096 [00:04<00:08, 325.80it/s]
Adding requests:  48%|████▊     | 1981/4096 [00:04<00:01, 1867.18it/s]
Adding requests:  53%|█████▎    | 2168/4096 [00:05<00:02, 769.47it/s] 
Adding requests:  56%|█████▋    | 2309/4096 [00:05<00:03, 572.84it/s]
Adding requests:  59%|█████▉    | 2417/4096 [00:05<00:03, 500.68it/s]
Adding requests:  61%|██████    | 2503/4096 [00:06<00:03, 465.89it/s]
Adding requests:  63%|██████▎   | 2574/4096 [00:06<00:03, 440.55it/s]
Adding requests:  64%|██████▍   | 2635/4096 [00:06<00:03, 420.94it/s]
Adding requests:  66%|██████▌   | 2688/4096 [00:06<00:03, 408.67it/s]
Adding requests:  67%|██████▋   | 2736/4096 [00:06<00:03, 394.56it/s]
Adding requests:  68%|██████▊   | 2780/4096 [00:07<00:03, 384.78it/s]
Adding requests:  69%|██████▉   | 2822/4096 [00:07<00:03, 377.46it/s]
Adding requests:  70%|██████▉   | 2862/4096 [00:07<00:03, 372.49it/s]
Adding requests:  71%|███████   | 2901/4096 [00:07<00:03, 370.01it/s]
Adding requests:  72%|███████▏  | 2939/4096 [00:07<00:03, 357.39it/s]
Adding requests:  73%|███████▎  | 2976/4096 [00:07<00:03, 353.28it/s]
Adding requests:  74%|███████▎  | 3012/4096 [00:07<00:03, 351.83it/s]
Adding requests:  74%|███████▍  | 3048/4096 [00:07<00:03, 344.76it/s]
Adding requests:  75%|███████▌  | 3083/4096 [00:07<00:02, 343.54it/s]
Adding requests:  76%|███████▌  | 3119/4096 [00:07<00:02, 344.78it/s]
Adding requests:  77%|███████▋  | 3154/4096 [00:08<00:02, 335.73it/s]
Adding requests:  78%|███████▊  | 3188/4096 [00:08<00:02, 332.87it/s]
Adding requests:  79%|███████▊  | 3222/4096 [00:08<00:02, 329.46it/s]
Adding requests:  79%|███████▉  | 3256/4096 [00:08<00:02, 330.21it/s]
Adding requests:  80%|████████  | 3290/4096 [00:08<00:02, 325.60it/s]
Adding requests:  81%|████████  | 3323/4096 [00:08<00:02, 321.06it/s]
Adding requests:  82%|████████▏ | 3356/4096 [00:08<00:02, 321.58it/s]
Adding requests:  83%|████████▎ | 3389/4096 [00:08<00:02, 318.75it/s]
Adding requests:  84%|████████▎ | 3422/4096 [00:08<00:02, 320.48it/s]
Adding requests:  84%|████████▍ | 3455/4096 [00:09<00:02, 313.36it/s]
Adding requests:  85%|████████▌ | 3487/4096 [00:09<00:01, 313.50it/s]
Adding requests:  86%|████████▌ | 3521/4096 [00:09<00:01, 318.68it/s]
Adding requests:  87%|████████▋ | 3555/4096 [00:09<00:01, 322.44it/s]
Adding requests:  88%|████████▊ | 3588/4096 [00:09<00:01, 316.22it/s]
Adding requests:  88%|████████▊ | 3620/4096 [00:09<00:01, 310.79it/s]
Adding requests:  89%|████████▉ | 3652/4096 [00:09<00:01, 301.09it/s]
Adding requests:  90%|████████▉ | 3684/4096 [00:09<00:01, 304.65it/s]
Adding requests:  91%|█████████ | 3718/4096 [00:09<00:01, 312.29it/s]
Adding requests:  92%|█████████▏| 3751/4096 [00:10<00:01, 315.64it/s]
Adding requests:  92%|█████████▏| 3785/4096 [00:10<00:00, 321.02it/s]
Adding requests:  93%|█████████▎| 3818/4096 [00:10<00:00, 323.27it/s]
Adding requests:  94%|█████████▍| 3852/4096 [00:10<00:00, 326.14it/s]
Adding requests:  95%|█████████▍| 3885/4096 [00:10<00:00, 325.21it/s]
Adding requests:  96%|█████████▌| 3918/4096 [00:10<00:00, 323.34it/s]
Adding requests:  96%|█████████▋| 3951/4096 [00:10<00:00, 321.74it/s]
Adding requests:  97%|█████████▋| 3984/4096 [00:10<00:00, 321.10it/s]
Adding requests:  98%|█████████▊| 4017/4096 [00:10<00:00, 320.25it/s]
Adding requests:  99%|█████████▉| 4050/4096 [00:10<00:00, 315.87it/s]
Adding requests: 100%|█████████▉| 4082/4096 [00:11<00:00, 310.23it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 369.58it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:00<00:04, 722.80it/s, est. speed input: 740223.33 toks/s, output: 722.81 toks/s]
Processed prompts:  14%|█▍        | 587/4096 [00:02<00:16, 210.91it/s, est. speed input: 265335.70 toks/s, output: 259.12 toks/s]
Processed prompts:  15%|█▌        | 620/4096 [00:03<00:22, 153.64it/s, est. speed input: 210526.83 toks/s, output: 205.59 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:03<00:30, 114.48it/s, est. speed input: 176187.08 toks/s, output: 172.06 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:04<00:36, 92.54it/s, est. speed input: 155095.65 toks/s, output: 151.46 toks/s] 
Processed prompts:  17%|█▋        | 706/4096 [00:05<00:43, 77.05it/s, est. speed input: 139358.34 toks/s, output: 136.09 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:05<00:51, 65.50it/s, est. speed input: 126688.55 toks/s, output: 123.72 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:06<00:57, 57.83it/s, est. speed input: 116930.59 toks/s, output: 114.19 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:07<01:02, 52.64it/s, est. speed input: 109185.95 toks/s, output: 106.63 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:08<01:06, 49.11it/s, est. speed input: 102901.56 toks/s, output: 100.49 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:09<01:09, 46.68it/s, est. speed input: 97693.63 toks/s, output: 95.40 toks/s]  
Processed prompts:  22%|██▏       | 898/4096 [00:09<01:11, 44.98it/s, est. speed input: 93300.83 toks/s, output: 91.11 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:10<01:11, 44.06it/s, est. speed input: 89675.64 toks/s, output: 87.57 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:11<01:10, 44.19it/s, est. speed input: 86879.76 toks/s, output: 84.84 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:12<01:10, 44.28it/s, est. speed input: 84416.68 toks/s, output: 82.44 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:12<01:09, 44.34it/s, est. speed input: 82229.81 toks/s, output: 80.30 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:13<01:09, 43.58it/s, est. speed input: 80007.60 toks/s, output: 78.13 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:14<01:10, 42.80it/s, est. speed input: 77942.97 toks/s, output: 76.12 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:15<01:10, 42.27it/s, est. speed input: 76089.28 toks/s, output: 74.31 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:15<01:09, 42.26it/s, est. speed input: 74521.57 toks/s, output: 72.77 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:16<01:09, 41.89it/s, est. speed input: 72999.59 toks/s, output: 71.29 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:17<01:09, 41.64it/s, est. speed input: 71613.35 toks/s, output: 69.93 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:18<01:08, 41.60it/s, est. speed input: 70380.20 toks/s, output: 68.73 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:18<01:06, 42.42it/s, est. speed input: 69435.05 toks/s, output: 67.81 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:19<01:04, 43.02it/s, est. speed input: 68560.17 toks/s, output: 66.95 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:20<01:03, 43.42it/s, est. speed input: 67741.34 toks/s, output: 66.15 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:21<01:02, 43.62it/s, est. speed input: 66963.44 toks/s, output: 65.39 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:21<01:02, 42.82it/s, est. speed input: 66074.99 toks/s, output: 64.53 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:22<01:02, 42.27it/s, est. speed input: 65246.71 toks/s, output: 63.72 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:23<01:02, 41.90it/s, est. speed input: 64473.52 toks/s, output: 62.96 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:24<01:02, 41.64it/s, est. speed input: 63749.73 toks/s, output: 62.26 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:24<00:35, 70.64it/s, est. speed input: 66111.91 toks/s, output: 64.56 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:25<00:40, 61.31it/s, est. speed input: 65379.50 toks/s, output: 63.85 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:26<00:44, 55.05it/s, est. speed input: 64693.49 toks/s, output: 63.18 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:27<00:46, 51.98it/s, est. speed input: 64185.42 toks/s, output: 62.68 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:27<00:47, 49.78it/s, est. speed input: 63703.65 toks/s, output: 62.21 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:28<00:48, 48.22it/s, est. speed input: 63245.96 toks/s, output: 61.76 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:29<00:49, 46.67it/s, est. speed input: 62761.39 toks/s, output: 61.29 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:30<00:50, 44.92it/s, est. speed input: 62224.62 toks/s, output: 60.77 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:30<00:50, 44.12it/s, est. speed input: 61760.07 toks/s, output: 60.31 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:31<00:51, 43.19it/s, est. speed input: 61274.28 toks/s, output: 59.84 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:32<00:51, 42.54it/s, est. speed input: 60811.76 toks/s, output: 59.39 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:33<00:50, 42.09it/s, est. speed input: 60370.52 toks/s, output: 58.96 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:33<00:50, 41.77it/s, est. speed input: 59948.43 toks/s, output: 58.54 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:34<00:50, 41.56it/s, est. speed input: 59546.47 toks/s, output: 58.15 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:35<00:48, 41.99it/s, est. speed input: 59222.57 toks/s, output: 57.83 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:36<00:47, 42.71it/s, est. speed input: 58950.69 toks/s, output: 57.57 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:36<00:45, 43.23it/s, est. speed input: 58689.43 toks/s, output: 57.31 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:37<00:45, 42.80it/s, est. speed input: 58367.58 toks/s, output: 57.00 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:38<00:45, 42.26it/s, est. speed input: 58036.46 toks/s, output: 56.68 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:39<00:45, 41.90it/s, est. speed input: 57718.59 toks/s, output: 56.37 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:39<00:44, 41.65it/s, est. speed input: 57413.53 toks/s, output: 56.07 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:40<00:43, 41.47it/s, est. speed input: 57119.90 toks/s, output: 55.78 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:41<00:43, 41.35it/s, est. speed input: 56837.73 toks/s, output: 55.51 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:42<00:42, 41.27it/s, est. speed input: 56565.54 toks/s, output: 55.24 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:43<00:41, 41.91it/s, est. speed input: 56360.01 toks/s, output: 55.04 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:43<00:39, 42.65it/s, est. speed input: 56182.13 toks/s, output: 54.87 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:44<00:38, 43.18it/s, est. speed input: 56010.37 toks/s, output: 54.70 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:45<00:37, 43.36it/s, est. speed input: 55829.58 toks/s, output: 54.52 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:45<00:37, 43.03it/s, est. speed input: 55623.48 toks/s, output: 54.32 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:46<00:36, 42.42it/s, est. speed input: 55397.16 toks/s, output: 54.10 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:47<00:36, 42.01it/s, est. speed input: 55178.47 toks/s, output: 53.89 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:48<00:36, 41.72it/s, est. speed input: 54966.84 toks/s, output: 53.68 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:49<00:35, 41.53it/s, est. speed input: 54762.20 toks/s, output: 53.48 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:49<00:34, 41.39it/s, est. speed input: 54563.54 toks/s, output: 53.28 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:50<00:33, 42.17it/s, est. speed input: 54428.89 toks/s, output: 53.15 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:51<00:32, 42.84it/s, est. speed input: 54304.68 toks/s, output: 53.03 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:52<00:30, 43.32it/s, est. speed input: 54183.87 toks/s, output: 52.91 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:52<00:30, 43.67it/s, est. speed input: 54066.27 toks/s, output: 52.80 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:53<00:29, 43.35it/s, est. speed input: 53920.08 toks/s, output: 52.66 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:54<00:29, 42.63it/s, est. speed input: 53749.54 toks/s, output: 52.49 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:55<00:28, 42.15it/s, est. speed input: 53584.19 toks/s, output: 52.33 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:55<00:15, 70.77it/s, est. speed input: 54732.85 toks/s, output: 53.45 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:56<00:17, 61.39it/s, est. speed input: 54558.11 toks/s, output: 53.28 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:57<00:19, 55.07it/s, est. speed input: 54388.00 toks/s, output: 53.11 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:58<00:20, 50.76it/s, est. speed input: 54222.05 toks/s, output: 52.95 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:58<00:20, 48.61it/s, est. speed input: 54100.06 toks/s, output: 52.83 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:59<00:20, 47.42it/s, est. speed input: 53997.00 toks/s, output: 52.73 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:00<00:19, 46.56it/s, est. speed input: 53896.40 toks/s, output: 52.63 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:00<00:19, 45.94it/s, est. speed input: 53797.67 toks/s, output: 52.54 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:01<00:19, 44.70it/s, est. speed input: 53663.34 toks/s, output: 52.41 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:02<00:19, 43.58it/s, est. speed input: 53518.73 toks/s, output: 52.26 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:03<00:18, 42.80it/s, est. speed input: 53377.22 toks/s, output: 52.13 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:04<00:18, 42.27it/s, est. speed input: 53239.17 toks/s, output: 51.99 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:04<00:17, 41.90it/s, est. speed input: 53104.70 toks/s, output: 51.86 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:05<00:16, 41.65it/s, est. speed input: 52973.30 toks/s, output: 51.73 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:06<00:16, 41.46it/s, est. speed input: 52844.74 toks/s, output: 51.61 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:07<00:15, 41.34it/s, est. speed input: 52719.51 toks/s, output: 51.48 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:07<00:14, 42.04it/s, est. speed input: 52634.47 toks/s, output: 51.40 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:08<00:13, 42.74it/s, est. speed input: 52559.86 toks/s, output: 51.33 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:09<00:12, 43.03it/s, est. speed input: 52477.56 toks/s, output: 51.25 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:10<00:12, 42.41it/s, est. speed input: 52361.53 toks/s, output: 51.13 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:10<00:11, 42.00it/s, est. speed input: 52248.16 toks/s, output: 51.02 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:11<00:10, 41.72it/s, est. speed input: 52137.37 toks/s, output: 50.92 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:12<00:09, 41.52it/s, est. speed input: 52028.84 toks/s, output: 50.81 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:13<00:09, 41.38it/s, est. speed input: 51922.58 toks/s, output: 50.71 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:14<00:08, 41.28it/s, est. speed input: 51818.65 toks/s, output: 50.60 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:14<00:07, 41.57it/s, est. speed input: 51732.08 toks/s, output: 50.52 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:15<00:06, 42.40it/s, est. speed input: 51673.17 toks/s, output: 50.46 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:16<00:05, 43.01it/s, est. speed input: 51615.37 toks/s, output: 50.41 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:16<00:05, 43.44it/s, est. speed input: 51558.61 toks/s, output: 50.35 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:17<00:04, 43.08it/s, est. speed input: 51478.03 toks/s, output: 50.27 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:18<00:03, 42.46it/s, est. speed input: 51384.44 toks/s, output: 50.18 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:19<00:02, 42.03it/s, est. speed input: 51292.68 toks/s, output: 50.09 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:20<00:02, 41.74it/s, est. speed input: 51202.77 toks/s, output: 50.00 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:20<00:01, 41.91it/s, est. speed input: 51129.02 toks/s, output: 49.93 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:21<00:00, 42.13it/s, est. speed input: 51060.57 toks/s, output: 49.86 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:21<00:00, 42.13it/s, est. speed input: 51436.93 toks/s, output: 50.23 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:21<00:00, 50.23it/s, est. speed input: 51436.93 toks/s, output: 50.23 toks/s]
[rank0]:[W125 21:25:51.462009555 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-25 21:25:54
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:26:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 21:26:53 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=423899) WARNING 01-25 21:27:00 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=423899) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=423899) WARNING 01-25 21:27:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 14.05 requests/s, 14402.48 total tokens/s, 14.05 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-25 21:26:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:26:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:26:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:26:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:26:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:26:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:26:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:26:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:26:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:26:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:26:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:26:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:26:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:26:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:26:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:26:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 21:26:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 21:26:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:26:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:26:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:26:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:26:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 21:26:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 21:26:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:26:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:26:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:26:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:26:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=423899) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=423899) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.18s/it]
(EngineCore_DP0 pid=423899) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.18s/it]
(EngineCore_DP0 pid=423899) 
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=423899) [2026-01-25 21:27:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=423899) [rank0]:W0125 21:27:21.602000 423899 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=423899) [rank0]:W0125 21:27:21.720000 423899 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=423899) [rank0]:W0125 21:27:23.551000 423899 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=423899) [rank0]:W0125 21:27:23.746000 423899 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=423899) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:05,  3.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  4.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:03,  4.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:04,  3.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:03,  3.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:02<00:03,  3.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:02<00:02,  3.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:02<00:02,  4.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:02<00:01,  5.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:02<00:01,  6.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:01,  6.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  7.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  7.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:03<00:00,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:03<00:00,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:03<00:00,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  7.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  5.44it/s]
(EngineCore_DP0 pid=423899) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.65it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  7.38it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:01,  7.69it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  7.03it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:01,  4.27it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:01<00:00,  5.06it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:01<00:00,  5.40it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:01<00:00,  3.99it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  4.73it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  5.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  6.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  5.50it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 28/8192 [00:00<00:29, 274.29it/s]
Adding requests:   1%|          | 59/8192 [00:00<00:27, 292.97it/s]
Adding requests:   1%|          | 91/8192 [00:00<00:26, 302.72it/s]
Adding requests:   1%|▏         | 122/8192 [00:00<00:27, 292.31it/s]
Adding requests:   2%|▏         | 152/8192 [00:00<00:28, 281.91it/s]
Adding requests:   2%|▏         | 182/8192 [00:00<00:28, 284.89it/s]
Adding requests:   3%|▎         | 212/8192 [00:00<00:27, 286.84it/s]
Adding requests:   3%|▎         | 243/8192 [00:00<00:27, 291.87it/s]
Adding requests:   3%|▎         | 273/8192 [00:00<00:26, 294.19it/s]
Adding requests:   4%|▎         | 303/8192 [00:01<00:27, 290.40it/s]
Adding requests:   4%|▍         | 333/8192 [00:01<00:27, 286.39it/s]
Adding requests:   4%|▍         | 362/8192 [00:01<00:27, 284.80it/s]
Adding requests:   5%|▍         | 393/8192 [00:01<00:26, 291.40it/s]
Adding requests:   5%|▌         | 424/8192 [00:01<00:26, 294.24it/s]
Adding requests:   6%|▌         | 454/8192 [00:01<00:26, 290.02it/s]
Adding requests:   6%|▌         | 488/8192 [00:01<00:25, 302.68it/s]
Adding requests:   6%|▋         | 520/8192 [00:01<00:25, 305.14it/s]
Adding requests:   7%|▋         | 555/8192 [00:01<00:24, 318.21it/s]
Adding requests:   7%|▋         | 591/8192 [00:01<00:23, 329.47it/s]
Adding requests:   8%|▊         | 625/8192 [00:02<00:22, 331.93it/s]
Adding requests:   8%|▊         | 660/8192 [00:02<00:22, 337.03it/s]
Adding requests:   8%|▊         | 696/8192 [00:02<00:21, 341.13it/s]
Adding requests:   9%|▉         | 731/8192 [00:02<00:22, 331.37it/s]
Adding requests:   9%|▉         | 765/8192 [00:02<00:22, 329.47it/s]
Adding requests:  10%|▉         | 799/8192 [00:02<00:22, 327.08it/s]
Adding requests:  10%|█         | 832/8192 [00:02<00:22, 320.69it/s]
Adding requests:  11%|█         | 865/8192 [00:02<00:22, 321.61it/s]
Adding requests:  11%|█         | 901/8192 [00:02<00:21, 331.70it/s]
Adding requests:  11%|█▏        | 935/8192 [00:03<00:22, 329.54it/s]
Adding requests:  12%|█▏        | 969/8192 [00:03<00:21, 332.09it/s]
Adding requests:  12%|█▏        | 1004/8192 [00:03<00:21, 335.55it/s]
Adding requests:  13%|█▎        | 1038/8192 [00:03<00:21, 334.68it/s]
Adding requests:  13%|█▎        | 1072/8192 [00:03<00:21, 334.78it/s]
Adding requests:  14%|█▎        | 1107/8192 [00:03<00:20, 338.57it/s]
Adding requests:  14%|█▍        | 1141/8192 [00:03<00:21, 330.99it/s]
Adding requests:  14%|█▍        | 1175/8192 [00:03<00:21, 332.16it/s]
Adding requests:  15%|█▍        | 1209/8192 [00:03<00:21, 331.15it/s]
Adding requests:  15%|█▌        | 1243/8192 [00:03<00:21, 321.79it/s]
Adding requests:  16%|█▌        | 1276/8192 [00:04<00:21, 317.30it/s]
Adding requests:  16%|█▌        | 1309/8192 [00:04<00:21, 318.00it/s]
Adding requests:  16%|█▋        | 1341/8192 [00:04<00:21, 317.26it/s]
Adding requests:  17%|█▋        | 1373/8192 [00:04<00:21, 313.76it/s]
Adding requests:  17%|█▋        | 1407/8192 [00:04<00:21, 319.13it/s]
Adding requests:  18%|█▊        | 1439/8192 [00:04<00:21, 314.06it/s]
Adding requests:  18%|█▊        | 1473/8192 [00:04<00:21, 318.82it/s]
Adding requests:  18%|█▊        | 1506/8192 [00:04<00:20, 320.77it/s]
Adding requests:  19%|█▉        | 1539/8192 [00:04<00:20, 322.34it/s]
Adding requests:  19%|█▉        | 1572/8192 [00:04<00:20, 319.96it/s]
Adding requests:  20%|█▉        | 1607/8192 [00:05<00:20, 325.96it/s]
Adding requests:  20%|██        | 1641/8192 [00:05<00:19, 329.71it/s]
Adding requests:  20%|██        | 1674/8192 [00:05<00:20, 322.31it/s]
Adding requests:  21%|██        | 1707/8192 [00:05<00:20, 322.96it/s]
Adding requests:  21%|██▏       | 1741/8192 [00:05<00:19, 326.29it/s]
Adding requests:  22%|██▏       | 1774/8192 [00:05<00:19, 324.49it/s]
Adding requests:  22%|██▏       | 1807/8192 [00:05<00:19, 324.29it/s]
Adding requests:  22%|██▏       | 1840/8192 [00:05<00:19, 320.93it/s]
Adding requests:  23%|██▎       | 1873/8192 [00:05<00:20, 311.33it/s]
Adding requests:  23%|██▎       | 1905/8192 [00:06<00:21, 295.74it/s]
Adding requests:  24%|██▎       | 1938/8192 [00:06<00:20, 304.03it/s]
Adding requests:  24%|██▍       | 1969/8192 [00:06<00:20, 305.05it/s]
Adding requests:  24%|██▍       | 2001/8192 [00:06<00:20, 307.66it/s]
Adding requests:  25%|██▍       | 2034/8192 [00:06<00:19, 311.78it/s]
Adding requests:  25%|██▌       | 2066/8192 [00:06<00:19, 314.05it/s]
Adding requests:  26%|██▌       | 2098/8192 [00:06<00:19, 308.52it/s]
Adding requests:  26%|██▌       | 2129/8192 [00:06<00:19, 308.30it/s]
Adding requests:  26%|██▋       | 2161/8192 [00:06<00:19, 310.68it/s]
Adding requests:  27%|██▋       | 2193/8192 [00:06<00:20, 297.60it/s]
Adding requests:  27%|██▋       | 2225/8192 [00:07<00:19, 302.71it/s]
Adding requests:  28%|██▊       | 2258/8192 [00:07<00:19, 309.35it/s]
Adding requests:  28%|██▊       | 2290/8192 [00:07<00:19, 304.98it/s]
Adding requests:  28%|██▊       | 2322/8192 [00:07<00:19, 307.25it/s]
Adding requests:  29%|██▊       | 2354/8192 [00:07<00:18, 309.29it/s]
Adding requests:  29%|██▉       | 2386/8192 [00:07<00:18, 309.19it/s]
Adding requests:  30%|██▉       | 2417/8192 [00:07<00:18, 308.05it/s]
Adding requests:  30%|██▉       | 2449/8192 [00:07<00:18, 308.81it/s]
Adding requests:  30%|███       | 2482/8192 [00:07<00:18, 312.40it/s]
Adding requests:  31%|███       | 2514/8192 [00:08<00:18, 307.55it/s]
Adding requests:  31%|███       | 2547/8192 [00:08<00:18, 312.25it/s]
Adding requests:  32%|███▏      | 2581/8192 [00:08<00:17, 317.93it/s]
Adding requests:  32%|███▏      | 2613/8192 [00:08<00:17, 314.90it/s]
Adding requests:  32%|███▏      | 2645/8192 [00:08<00:17, 315.91it/s]
Adding requests:  33%|███▎      | 2678/8192 [00:08<00:17, 318.18it/s]
Adding requests:  33%|███▎      | 2710/8192 [00:08<00:17, 316.35it/s]
Adding requests:  33%|███▎      | 2742/8192 [00:08<00:17, 314.30it/s]
Adding requests:  34%|███▍      | 2776/8192 [00:08<00:16, 319.82it/s]
Adding requests:  34%|███▍      | 2808/8192 [00:08<00:17, 315.65it/s]
Adding requests:  35%|███▍      | 2840/8192 [00:09<00:16, 315.75it/s]
Adding requests:  35%|███▌      | 2872/8192 [00:09<00:17, 306.96it/s]
Adding requests:  35%|███▌      | 2903/8192 [00:09<00:17, 302.16it/s]
Adding requests:  36%|███▌      | 2936/8192 [00:09<00:17, 308.33it/s]
Adding requests:  36%|███▋      | 2972/8192 [00:09<00:16, 321.14it/s]
Adding requests:  37%|███▋      | 3005/8192 [00:09<00:16, 323.58it/s]
Adding requests:  37%|███▋      | 3038/8192 [00:09<00:15, 324.49it/s]
Adding requests:  38%|███▊      | 3072/8192 [00:09<00:15, 329.00it/s]
Adding requests:  38%|███▊      | 3106/8192 [00:09<00:15, 330.03it/s]
Adding requests:  38%|███▊      | 3140/8192 [00:09<00:15, 327.23it/s]
Adding requests:  39%|███▊      | 3173/8192 [00:10<00:15, 327.93it/s]
Adding requests:  39%|███▉      | 3207/8192 [00:10<00:15, 329.42it/s]
Adding requests:  40%|███▉      | 3240/8192 [00:10<00:15, 317.46it/s]
Adding requests:  40%|███▉      | 3274/8192 [00:10<00:15, 323.02it/s]
Adding requests:  40%|████      | 3308/8192 [00:10<00:14, 327.33it/s]
Adding requests:  41%|████      | 3342/8192 [00:10<00:14, 327.80it/s]
Adding requests:  41%|████      | 3376/8192 [00:10<00:14, 329.58it/s]
Adding requests:  42%|████▏     | 3409/8192 [00:10<00:14, 324.33it/s]
Adding requests:  42%|████▏     | 3443/8192 [00:10<00:14, 324.30it/s]
Adding requests:  42%|████▏     | 3476/8192 [00:11<00:14, 316.13it/s]
Adding requests:  43%|████▎     | 3509/8192 [00:11<00:14, 317.74it/s]
Adding requests:  43%|████▎     | 3543/8192 [00:11<00:14, 323.93it/s]
Adding requests:  44%|████▎     | 3578/8192 [00:11<00:14, 329.46it/s]
Adding requests:  44%|████▍     | 3613/8192 [00:11<00:13, 333.41it/s]
Adding requests:  45%|████▍     | 3648/8192 [00:11<00:13, 335.84it/s]
Adding requests:  45%|████▍     | 3684/8192 [00:11<00:13, 341.43it/s]
Adding requests:  45%|████▌     | 3719/8192 [00:11<00:13, 339.37it/s]
Adding requests:  46%|████▌     | 3753/8192 [00:11<00:13, 336.49it/s]
Adding requests:  46%|████▌     | 3787/8192 [00:11<00:13, 332.69it/s]
Adding requests:  47%|████▋     | 3821/8192 [00:12<00:13, 322.77it/s]
Adding requests:  47%|████▋     | 3854/8192 [00:12<00:13, 318.37it/s]
Adding requests:  47%|████▋     | 3886/8192 [00:12<00:13, 309.91it/s]
Adding requests:  48%|████▊     | 3918/8192 [00:12<00:13, 311.02it/s]
Adding requests:  48%|████▊     | 3950/8192 [00:12<00:13, 307.59it/s]
Adding requests:  49%|████▊     | 3982/8192 [00:12<00:13, 308.31it/s]
Adding requests:  49%|████▉     | 4015/8192 [00:12<00:13, 312.42it/s]
Adding requests:  49%|████▉     | 4047/8192 [00:12<00:13, 301.36it/s]
Adding requests:  50%|████▉     | 4079/8192 [00:12<00:13, 304.31it/s]
Adding requests:  50%|█████     | 4112/8192 [00:13<00:13, 309.80it/s]
Adding requests:  51%|█████     | 4144/8192 [00:13<00:13, 307.58it/s]
Adding requests:  51%|█████     | 4175/8192 [00:13<00:13, 304.22it/s]
Adding requests:  51%|█████▏    | 4207/8192 [00:13<00:13, 304.83it/s]
Adding requests:  52%|█████▏    | 4239/8192 [00:13<00:12, 306.93it/s]
Adding requests:  52%|█████▏    | 4271/8192 [00:13<00:12, 307.80it/s]
Adding requests:  53%|█████▎    | 4303/8192 [00:13<00:12, 310.97it/s]
Adding requests:  53%|█████▎    | 4336/8192 [00:13<00:12, 313.75it/s]
Adding requests:  53%|█████▎    | 4368/8192 [00:13<00:12, 312.78it/s]
Adding requests:  54%|█████▎    | 4400/8192 [00:13<00:12, 311.36it/s]
Adding requests:  54%|█████▍    | 4432/8192 [00:14<00:12, 302.97it/s]
Adding requests:  54%|█████▍    | 4464/8192 [00:14<00:12, 307.44it/s]
Adding requests:  55%|█████▍    | 4496/8192 [00:14<00:11, 308.72it/s]
Adding requests:  55%|█████▌    | 4527/8192 [00:14<00:12, 299.17it/s]
Adding requests:  56%|█████▌    | 4557/8192 [00:14<00:12, 296.20it/s]
Adding requests:  56%|█████▌    | 4589/8192 [00:14<00:11, 302.97it/s]
Adding requests:  56%|█████▋    | 4620/8192 [00:14<00:12, 291.60it/s]
Adding requests:  57%|█████▋    | 4652/8192 [00:14<00:11, 299.28it/s]
Adding requests:  57%|█████▋    | 4683/8192 [00:14<00:11, 299.68it/s]
Adding requests:  58%|█████▊    | 4714/8192 [00:14<00:11, 300.86it/s]
Adding requests:  58%|█████▊    | 4746/8192 [00:15<00:11, 303.82it/s]
Adding requests:  58%|█████▊    | 4778/8192 [00:15<00:11, 307.54it/s]
Adding requests:  59%|█████▊    | 4810/8192 [00:15<00:10, 309.97it/s]
Adding requests:  59%|█████▉    | 4842/8192 [00:15<00:10, 312.42it/s]
Adding requests:  59%|█████▉    | 4874/8192 [00:15<00:10, 303.99it/s]
Adding requests:  60%|█████▉    | 4905/8192 [00:15<00:10, 305.10it/s]
Adding requests:  60%|██████    | 4938/8192 [00:15<00:10, 309.47it/s]
Adding requests:  61%|██████    | 4969/8192 [00:15<00:10, 305.78it/s]
Adding requests:  61%|██████    | 5000/8192 [00:15<00:10, 306.16it/s]
Adding requests:  61%|██████▏   | 5033/8192 [00:16<00:10, 312.75it/s]
Adding requests:  62%|██████▏   | 5065/8192 [00:16<00:10, 304.64it/s]
Adding requests:  62%|██████▏   | 5097/8192 [00:16<00:10, 308.62it/s]
Adding requests:  63%|██████▎   | 5129/8192 [00:16<00:09, 310.86it/s]
Adding requests:  63%|██████▎   | 5161/8192 [00:16<00:09, 309.11it/s]
Adding requests:  63%|██████▎   | 5192/8192 [00:16<00:09, 306.41it/s]
Adding requests:  64%|██████▍   | 5223/8192 [00:16<00:09, 306.92it/s]
Adding requests:  64%|██████▍   | 5254/8192 [00:16<00:09, 302.34it/s]
Adding requests:  65%|██████▍   | 5285/8192 [00:16<00:09, 302.86it/s]
Adding requests:  65%|██████▍   | 5319/8192 [00:16<00:09, 313.08it/s]
Adding requests:  65%|██████▌   | 5353/8192 [00:17<00:08, 319.18it/s]
Adding requests:  66%|██████▌   | 5385/8192 [00:17<00:08, 318.73it/s]
Adding requests:  66%|██████▌   | 5420/8192 [00:17<00:08, 325.94it/s]
Adding requests:  67%|██████▋   | 5454/8192 [00:17<00:08, 327.40it/s]
Adding requests:  67%|██████▋   | 5487/8192 [00:17<00:08, 325.40it/s]
Adding requests:  67%|██████▋   | 5520/8192 [00:17<00:08, 323.58it/s]
Adding requests:  68%|██████▊   | 5555/8192 [00:17<00:07, 330.65it/s]
Adding requests:  68%|██████▊   | 5590/8192 [00:17<00:07, 331.95it/s]
Adding requests:  69%|██████▊   | 5625/8192 [00:17<00:07, 335.29it/s]
Adding requests:  69%|██████▉   | 5659/8192 [00:17<00:07, 334.68it/s]
Adding requests:  70%|██████▉   | 5694/8192 [00:18<00:07, 337.53it/s]
Adding requests:  70%|██████▉   | 5728/8192 [00:18<00:07, 337.29it/s]
Adding requests:  70%|███████   | 5762/8192 [00:18<00:07, 336.23it/s]
Adding requests:  71%|███████   | 5796/8192 [00:18<00:07, 335.17it/s]
Adding requests:  71%|███████   | 5831/8192 [00:18<00:07, 336.26it/s]
Adding requests:  72%|███████▏  | 5865/8192 [00:18<00:07, 332.07it/s]
Adding requests:  72%|███████▏  | 5899/8192 [00:18<00:06, 330.84it/s]
Adding requests:  72%|███████▏  | 5933/8192 [00:18<00:06, 331.83it/s]
Adding requests:  73%|███████▎  | 5967/8192 [00:18<00:06, 331.58it/s]
Adding requests:  73%|███████▎  | 6001/8192 [00:19<00:06, 313.96it/s]
Adding requests:  74%|███████▎  | 6036/8192 [00:19<00:06, 323.33it/s]
Adding requests:  74%|███████▍  | 6070/8192 [00:19<00:06, 326.78it/s]
Adding requests:  74%|███████▍  | 6103/8192 [00:19<00:06, 326.73it/s]
Adding requests:  75%|███████▍  | 6137/8192 [00:19<00:06, 329.40it/s]
Adding requests:  75%|███████▌  | 6171/8192 [00:19<00:06, 330.95it/s]
Adding requests:  76%|███████▌  | 6205/8192 [00:19<00:06, 330.52it/s]
Adding requests:  76%|███████▌  | 6240/8192 [00:19<00:05, 333.82it/s]
Adding requests:  77%|███████▋  | 6274/8192 [00:19<00:05, 332.99it/s]
Adding requests:  77%|███████▋  | 6308/8192 [00:19<00:05, 322.14it/s]
Adding requests:  77%|███████▋  | 6341/8192 [00:20<00:05, 320.41it/s]
Adding requests:  78%|███████▊  | 6374/8192 [00:20<00:05, 319.86it/s]
Adding requests:  78%|███████▊  | 6407/8192 [00:20<00:05, 309.91it/s]
Adding requests:  79%|███████▊  | 6439/8192 [00:20<00:05, 312.48it/s]
Adding requests:  79%|███████▉  | 6473/8192 [00:20<00:05, 318.38it/s]
Adding requests:  79%|███████▉  | 6506/8192 [00:20<00:05, 320.09it/s]
Adding requests:  80%|███████▉  | 6539/8192 [00:20<00:05, 322.46it/s]
Adding requests:  80%|████████  | 6572/8192 [00:20<00:05, 322.73it/s]
Adding requests:  81%|████████  | 6605/8192 [00:20<00:04, 318.28it/s]
Adding requests:  81%|████████  | 6637/8192 [00:20<00:04, 316.42it/s]
Adding requests:  81%|████████▏ | 6669/8192 [00:21<00:04, 315.05it/s]
Adding requests:  82%|████████▏ | 6701/8192 [00:21<00:04, 314.12it/s]
Adding requests:  82%|████████▏ | 6733/8192 [00:21<00:04, 314.23it/s]
Adding requests:  83%|████████▎ | 6765/8192 [00:21<00:04, 315.10it/s]
Adding requests:  83%|████████▎ | 6798/8192 [00:21<00:04, 319.37it/s]
Adding requests:  83%|████████▎ | 6830/8192 [00:21<00:04, 318.45it/s]
Adding requests:  84%|████████▍ | 6863/8192 [00:21<00:04, 320.02it/s]
Adding requests:  84%|████████▍ | 6896/8192 [00:21<00:04, 318.56it/s]
Adding requests:  85%|████████▍ | 6928/8192 [00:21<00:04, 314.12it/s]
Adding requests:  85%|████████▍ | 6960/8192 [00:22<00:03, 313.57it/s]
Adding requests:  85%|████████▌ | 6992/8192 [00:22<00:03, 315.04it/s]
Adding requests:  86%|████████▌ | 7024/8192 [00:22<00:03, 309.68it/s]
Adding requests:  86%|████████▌ | 7055/8192 [00:22<00:03, 303.28it/s]
Adding requests:  87%|████████▋ | 7087/8192 [00:22<00:03, 306.34it/s]
Adding requests:  87%|████████▋ | 7119/8192 [00:22<00:03, 307.85it/s]
Adding requests:  87%|████████▋ | 7150/8192 [00:22<00:03, 306.57it/s]
Adding requests:  88%|████████▊ | 7181/8192 [00:22<00:03, 307.23it/s]
Adding requests:  88%|████████▊ | 7213/8192 [00:22<00:03, 309.25it/s]
Adding requests:  88%|████████▊ | 7244/8192 [00:22<00:03, 303.70it/s]
Adding requests:  89%|████████▉ | 7275/8192 [00:23<00:03, 304.22it/s]
Adding requests:  89%|████████▉ | 7306/8192 [00:23<00:02, 303.72it/s]
Adding requests:  96%|█████████▌| 7865/8192 [00:23<00:00, 1856.22it/s]
Adding requests:  98%|█████████▊| 8053/8192 [00:23<00:00, 721.47it/s] 
Adding requests: 100%|██████████| 8192/8192 [00:24<00:00, 336.81it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 322/8192 [00:02<01:01, 127.79it/s, est. speed input: 130856.96 toks/s, output: 127.79 toks/s]
Processed prompts:   5%|▍         | 386/8192 [00:07<02:51, 45.54it/s, est. speed input: 55582.75 toks/s, output: 54.28 toks/s]   
Processed prompts:   5%|▌         | 450/8192 [00:11<04:20, 29.74it/s, est. speed input: 39635.15 toks/s, output: 38.71 toks/s]
Processed prompts:   6%|▋         | 514/8192 [00:16<05:35, 22.88it/s, est. speed input: 32277.97 toks/s, output: 31.52 toks/s]
Processed prompts:   7%|▋         | 578/8192 [00:20<06:25, 19.76it/s, est. speed input: 28516.13 toks/s, output: 27.85 toks/s]
Processed prompts:   8%|▊         | 642/8192 [00:25<07:05, 17.73it/s, est. speed input: 25944.38 toks/s, output: 25.34 toks/s]
Processed prompts:   9%|▊         | 706/8192 [00:28<06:35, 18.92it/s, est. speed input: 25684.69 toks/s, output: 25.08 toks/s]
Processed prompts:   9%|▉         | 770/8192 [00:32<07:15, 17.06it/s, est. speed input: 24032.24 toks/s, output: 23.47 toks/s]
Processed prompts:  10%|█         | 834/8192 [00:37<07:34, 16.19it/s, est. speed input: 22925.14 toks/s, output: 22.39 toks/s]
Processed prompts:  11%|█         | 898/8192 [00:41<07:54, 15.37it/s, est. speed input: 21933.90 toks/s, output: 21.42 toks/s]
Processed prompts:  12%|█▏        | 962/8192 [00:46<08:02, 14.97it/s, est. speed input: 21201.65 toks/s, output: 20.70 toks/s]
Processed prompts:  13%|█▎        | 1026/8192 [00:50<08:05, 14.75it/s, est. speed input: 20620.48 toks/s, output: 20.14 toks/s]
Processed prompts:  13%|█▎        | 1090/8192 [00:55<08:10, 14.47it/s, est. speed input: 20083.86 toks/s, output: 19.61 toks/s]
Processed prompts:  14%|█▍        | 1154/8192 [01:00<08:06, 14.47it/s, est. speed input: 19694.92 toks/s, output: 19.23 toks/s]
Processed prompts:  15%|█▍        | 1218/8192 [01:02<07:15, 16.01it/s, est. speed input: 19797.61 toks/s, output: 19.33 toks/s]
Processed prompts:  16%|█▌        | 1282/8192 [01:07<07:24, 15.54it/s, est. speed input: 19477.71 toks/s, output: 19.02 toks/s]
Processed prompts:  16%|█▋        | 1346/8192 [01:12<07:37, 14.97it/s, est. speed input: 19131.48 toks/s, output: 18.68 toks/s]
Processed prompts:  17%|█▋        | 1410/8192 [01:16<07:42, 14.68it/s, est. speed input: 18848.33 toks/s, output: 18.41 toks/s]
Processed prompts:  18%|█▊        | 1474/8192 [01:21<07:47, 14.37it/s, est. speed input: 18570.67 toks/s, output: 18.14 toks/s]
Processed prompts:  19%|█▉        | 1538/8192 [01:25<07:43, 14.36it/s, est. speed input: 18368.48 toks/s, output: 17.94 toks/s]
Processed prompts:  20%|█▉        | 1602/8192 [01:30<07:42, 14.26it/s, est. speed input: 18165.56 toks/s, output: 17.74 toks/s]
Processed prompts:  20%|██        | 1666/8192 [01:32<06:42, 16.21it/s, est. speed input: 18345.42 toks/s, output: 17.92 toks/s]
Processed prompts:  21%|██        | 1730/8192 [01:37<06:59, 15.39it/s, est. speed input: 18142.97 toks/s, output: 17.72 toks/s]
Processed prompts:  22%|██▏       | 1794/8192 [01:42<07:05, 15.02it/s, est. speed input: 17985.83 toks/s, output: 17.56 toks/s]
Processed prompts:  23%|██▎       | 1858/8192 [01:46<07:07, 14.80it/s, est. speed input: 17846.38 toks/s, output: 17.43 toks/s]
Processed prompts:  23%|██▎       | 1922/8192 [01:51<07:10, 14.56it/s, est. speed input: 17703.39 toks/s, output: 17.29 toks/s]
Processed prompts:  24%|██▍       | 1986/8192 [01:55<07:09, 14.46it/s, est. speed input: 17582.00 toks/s, output: 17.17 toks/s]
Processed prompts:  25%|██▌       | 2050/8192 [02:00<07:10, 14.26it/s, est. speed input: 17449.08 toks/s, output: 17.04 toks/s]
Processed prompts:  26%|██▌       | 2114/8192 [02:03<06:16, 16.14it/s, est. speed input: 17592.07 toks/s, output: 17.18 toks/s]
Processed prompts:  27%|██▋       | 2178/8192 [02:07<06:30, 15.40it/s, est. speed input: 17472.33 toks/s, output: 17.06 toks/s]
Processed prompts:  27%|██▋       | 2242/8192 [02:12<06:35, 15.05it/s, est. speed input: 17376.43 toks/s, output: 16.97 toks/s]
Processed prompts:  28%|██▊       | 2306/8192 [02:16<06:42, 14.62it/s, est. speed input: 17261.54 toks/s, output: 16.86 toks/s]
Processed prompts:  29%|██▉       | 2370/8192 [02:21<06:40, 14.53it/s, est. speed input: 17180.11 toks/s, output: 16.78 toks/s]
Processed prompts:  30%|██▉       | 2434/8192 [02:25<06:42, 14.30it/s, est. speed input: 17082.65 toks/s, output: 16.68 toks/s]
Processed prompts:  30%|███       | 2498/8192 [02:30<06:35, 14.40it/s, est. speed input: 17021.68 toks/s, output: 16.62 toks/s]
Processed prompts:  31%|███▏      | 2562/8192 [02:33<05:54, 15.89it/s, est. speed input: 17110.18 toks/s, output: 16.71 toks/s]
Processed prompts:  32%|███▏      | 2626/8192 [02:37<06:01, 15.38it/s, est. speed input: 17040.84 toks/s, output: 16.64 toks/s]
Processed prompts:  33%|███▎      | 2690/8192 [02:42<06:10, 14.87it/s, est. speed input: 16957.22 toks/s, output: 16.56 toks/s]
Processed prompts:  34%|███▎      | 2754/8192 [02:46<06:11, 14.64it/s, est. speed input: 16889.82 toks/s, output: 16.49 toks/s]
Processed prompts:  34%|███▍      | 2818/8192 [02:51<06:11, 14.48it/s, est. speed input: 16825.66 toks/s, output: 16.43 toks/s]
Processed prompts:  35%|███▌      | 2882/8192 [02:56<06:11, 14.29it/s, est. speed input: 16756.36 toks/s, output: 16.36 toks/s]
Processed prompts:  36%|███▌      | 2946/8192 [03:00<06:06, 14.33it/s, est. speed input: 16707.50 toks/s, output: 16.32 toks/s]
Processed prompts:  37%|███▋      | 3010/8192 [03:03<05:25, 15.93it/s, est. speed input: 16794.49 toks/s, output: 16.40 toks/s]
Processed prompts:  38%|███▊      | 3074/8192 [03:08<05:32, 15.39it/s, est. speed input: 16742.28 toks/s, output: 16.35 toks/s]
Processed prompts:  38%|███▊      | 3138/8192 [03:12<05:40, 14.86it/s, est. speed input: 16677.87 toks/s, output: 16.29 toks/s]
Processed prompts:  39%|███▉      | 3202/8192 [03:17<05:40, 14.66it/s, est. speed input: 16629.22 toks/s, output: 16.24 toks/s]
Processed prompts:  40%|███▉      | 3266/8192 [03:21<05:43, 14.36it/s, est. speed input: 16569.15 toks/s, output: 16.18 toks/s]
Processed prompts:  41%|████      | 3330/8192 [03:26<05:38, 14.34it/s, est. speed input: 16527.75 toks/s, output: 16.14 toks/s]
Processed prompts:  41%|████▏     | 3394/8192 [03:30<05:36, 14.27it/s, est. speed input: 16482.70 toks/s, output: 16.10 toks/s]
Processed prompts:  42%|████▏     | 3458/8192 [03:33<04:55, 16.01it/s, est. speed input: 16568.75 toks/s, output: 16.18 toks/s]
Processed prompts:  43%|████▎     | 3522/8192 [03:38<05:05, 15.28it/s, est. speed input: 16517.28 toks/s, output: 16.13 toks/s]
Processed prompts:  44%|████▍     | 3586/8192 [03:42<05:08, 14.94it/s, est. speed input: 16477.66 toks/s, output: 16.09 toks/s]
Processed prompts:  45%|████▍     | 3650/8192 [03:47<05:10, 14.61it/s, est. speed input: 16431.80 toks/s, output: 16.05 toks/s]
Processed prompts:  45%|████▌     | 3714/8192 [03:52<05:11, 14.39it/s, est. speed input: 16388.48 toks/s, output: 16.00 toks/s]
Processed prompts:  46%|████▌     | 3778/8192 [03:56<05:06, 14.41it/s, est. speed input: 16358.76 toks/s, output: 15.98 toks/s]
Processed prompts:  47%|████▋     | 3842/8192 [04:01<05:06, 14.18it/s, est. speed input: 16312.78 toks/s, output: 15.93 toks/s]
Processed prompts:  48%|████▊     | 3906/8192 [04:03<04:26, 16.07it/s, est. speed input: 16398.03 toks/s, output: 16.01 toks/s]
Processed prompts:  48%|████▊     | 3970/8192 [04:08<04:36, 15.27it/s, est. speed input: 16353.25 toks/s, output: 15.97 toks/s]
Processed prompts:  49%|████▉     | 4034/8192 [04:12<04:33, 15.20it/s, est. speed input: 16337.35 toks/s, output: 15.95 toks/s]
Processed prompts:  50%|█████     | 4098/8192 [04:17<04:38, 14.72it/s, est. speed input: 16295.19 toks/s, output: 15.91 toks/s]
Processed prompts:  51%|█████     | 4162/8192 [04:22<04:37, 14.52it/s, est. speed input: 16262.62 toks/s, output: 15.88 toks/s]
Processed prompts:  52%|█████▏    | 4226/8192 [04:26<04:35, 14.41it/s, est. speed input: 16232.61 toks/s, output: 15.85 toks/s]
Processed prompts:  52%|█████▏    | 4290/8192 [04:31<04:31, 14.35it/s, est. speed input: 16205.15 toks/s, output: 15.83 toks/s]
Processed prompts:  53%|█████▎    | 4354/8192 [04:35<04:28, 14.29it/s, est. speed input: 16177.01 toks/s, output: 15.80 toks/s]
Processed prompts:  54%|█████▍    | 4418/8192 [04:38<03:53, 16.16it/s, est. speed input: 16252.21 toks/s, output: 15.87 toks/s]
Processed prompts:  55%|█████▍    | 4482/8192 [04:42<04:00, 15.40it/s, est. speed input: 16219.11 toks/s, output: 15.84 toks/s]
Processed prompts:  55%|█████▌    | 4546/8192 [04:47<04:03, 15.00it/s, est. speed input: 16191.60 toks/s, output: 15.81 toks/s]
Processed prompts:  56%|█████▋    | 4610/8192 [04:52<04:03, 14.72it/s, est. speed input: 16164.80 toks/s, output: 15.79 toks/s]
Processed prompts:  57%|█████▋    | 4674/8192 [04:56<04:03, 14.46it/s, est. speed input: 16134.46 toks/s, output: 15.76 toks/s]
Processed prompts:  58%|█████▊    | 4738/8192 [05:01<03:59, 14.44it/s, est. speed input: 16113.72 toks/s, output: 15.74 toks/s]
Processed prompts:  59%|█████▊    | 4802/8192 [05:05<03:58, 14.21it/s, est. speed input: 16082.11 toks/s, output: 15.71 toks/s]
Processed prompts:  59%|█████▉    | 4866/8192 [05:08<03:25, 16.19it/s, est. speed input: 16155.42 toks/s, output: 15.78 toks/s]
Processed prompts:  60%|██████    | 4930/8192 [05:13<03:32, 15.38it/s, est. speed input: 16125.14 toks/s, output: 15.75 toks/s]
Processed prompts:  61%|██████    | 4994/8192 [05:17<03:30, 15.17it/s, est. speed input: 16110.08 toks/s, output: 15.73 toks/s]
Processed prompts:  62%|██████▏   | 5058/8192 [05:22<03:33, 14.69it/s, est. speed input: 16079.51 toks/s, output: 15.70 toks/s]
Processed prompts:  63%|██████▎   | 5122/8192 [05:26<03:30, 14.59it/s, est. speed input: 16061.00 toks/s, output: 15.68 toks/s]
Processed prompts:  63%|██████▎   | 5186/8192 [05:31<03:28, 14.38it/s, est. speed input: 16035.94 toks/s, output: 15.66 toks/s]
Processed prompts:  64%|██████▍   | 5250/8192 [05:35<03:25, 14.32it/s, est. speed input: 16015.33 toks/s, output: 15.64 toks/s]
Processed prompts:  65%|██████▍   | 5314/8192 [05:38<03:01, 15.82it/s, est. speed input: 16064.20 toks/s, output: 15.69 toks/s]
Processed prompts:  66%|██████▌   | 5378/8192 [05:43<03:03, 15.31it/s, est. speed input: 16044.97 toks/s, output: 15.67 toks/s]
Processed prompts:  66%|██████▋   | 5442/8192 [05:47<03:04, 14.88it/s, est. speed input: 16021.74 toks/s, output: 15.65 toks/s]
Processed prompts:  67%|██████▋   | 5506/8192 [05:52<03:04, 14.60it/s, est. speed input: 15999.60 toks/s, output: 15.62 toks/s]
Processed prompts:  68%|██████▊   | 5570/8192 [05:56<03:00, 14.50it/s, est. speed input: 15982.33 toks/s, output: 15.61 toks/s]
Processed prompts:  69%|██████▉   | 5634/8192 [06:01<02:59, 14.26it/s, est. speed input: 15957.31 toks/s, output: 15.58 toks/s]
Processed prompts:  70%|██████▉   | 5698/8192 [06:05<02:54, 14.31it/s, est. speed input: 15943.22 toks/s, output: 15.57 toks/s]
Processed prompts:  70%|███████   | 5762/8192 [06:08<02:32, 15.91it/s, est. speed input: 15992.36 toks/s, output: 15.62 toks/s]
Processed prompts:  71%|███████   | 5826/8192 [06:13<02:33, 15.38it/s, est. speed input: 15975.82 toks/s, output: 15.60 toks/s]
Processed prompts:  72%|███████▏  | 5890/8192 [06:18<02:35, 14.83it/s, est. speed input: 15951.66 toks/s, output: 15.58 toks/s]
Processed prompts:  73%|███████▎  | 5954/8192 [06:22<02:32, 14.66it/s, est. speed input: 15936.04 toks/s, output: 15.56 toks/s]
Processed prompts:  73%|███████▎  | 6018/8192 [06:27<02:30, 14.41it/s, est. speed input: 15915.33 toks/s, output: 15.54 toks/s]
Processed prompts:  74%|███████▍  | 6082/8192 [06:31<02:27, 14.35it/s, est. speed input: 15899.73 toks/s, output: 15.53 toks/s]
Processed prompts:  75%|███████▌  | 6146/8192 [06:36<02:22, 14.33it/s, est. speed input: 15885.21 toks/s, output: 15.51 toks/s]
Processed prompts:  76%|███████▌  | 6210/8192 [06:39<02:04, 15.98it/s, est. speed input: 15932.79 toks/s, output: 15.56 toks/s]
Processed prompts:  77%|███████▋  | 6274/8192 [06:43<02:05, 15.28it/s, est. speed input: 15912.82 toks/s, output: 15.54 toks/s]
Processed prompts:  77%|███████▋  | 6338/8192 [06:48<02:04, 14.90it/s, est. speed input: 15896.13 toks/s, output: 15.52 toks/s]
Processed prompts:  78%|███████▊  | 6402/8192 [06:52<02:02, 14.66it/s, est. speed input: 15880.50 toks/s, output: 15.51 toks/s]
Processed prompts:  79%|███████▉  | 6466/8192 [06:57<01:59, 14.40it/s, est. speed input: 15861.43 toks/s, output: 15.49 toks/s]
Processed prompts:  80%|███████▉  | 6530/8192 [07:01<01:55, 14.44it/s, est. speed input: 15851.33 toks/s, output: 15.48 toks/s]
Processed prompts:  80%|████████  | 6594/8192 [07:06<01:51, 14.30it/s, est. speed input: 15834.89 toks/s, output: 15.46 toks/s]
Processed prompts:  81%|████████▏ | 6658/8192 [07:09<01:34, 16.25it/s, est. speed input: 15888.55 toks/s, output: 15.52 toks/s]
Processed prompts:  82%|████████▏ | 6722/8192 [07:13<01:35, 15.39it/s, est. speed input: 15868.47 toks/s, output: 15.50 toks/s]
Processed prompts:  83%|████████▎ | 6786/8192 [07:18<01:33, 15.11it/s, est. speed input: 15858.13 toks/s, output: 15.49 toks/s]
Processed prompts:  84%|████████▎ | 6850/8192 [07:22<01:31, 14.64it/s, est. speed input: 15838.24 toks/s, output: 15.47 toks/s]
Processed prompts:  84%|████████▍ | 6914/8192 [07:27<01:28, 14.49it/s, est. speed input: 15824.48 toks/s, output: 15.45 toks/s]
Processed prompts:  85%|████████▌ | 6978/8192 [07:31<01:24, 14.34it/s, est. speed input: 15809.71 toks/s, output: 15.44 toks/s]
Processed prompts:  86%|████████▌ | 7042/8192 [07:36<01:20, 14.26it/s, est. speed input: 15795.82 toks/s, output: 15.43 toks/s]
Processed prompts:  87%|████████▋ | 7106/8192 [07:40<01:16, 14.28it/s, est. speed input: 15784.69 toks/s, output: 15.41 toks/s]
Processed prompts:  88%|████████▊ | 7170/8192 [07:43<01:03, 16.19it/s, est. speed input: 15833.44 toks/s, output: 15.46 toks/s]
Processed prompts:  88%|████████▊ | 7234/8192 [07:48<01:01, 15.49it/s, est. speed input: 15819.78 toks/s, output: 15.45 toks/s]
Processed prompts:  89%|████████▉ | 7298/8192 [07:52<00:59, 15.01it/s, est. speed input: 15805.38 toks/s, output: 15.43 toks/s]
Processed prompts:  90%|████████▉ | 7362/8192 [07:57<00:56, 14.74it/s, est. speed input: 15792.94 toks/s, output: 15.42 toks/s]
Processed prompts:  91%|█████████ | 7426/8192 [08:02<00:53, 14.43it/s, est. speed input: 15776.37 toks/s, output: 15.41 toks/s]
Processed prompts:  91%|█████████▏| 7490/8192 [08:06<00:48, 14.41it/s, est. speed input: 15766.57 toks/s, output: 15.40 toks/s]
Processed prompts:  92%|█████████▏| 7554/8192 [08:11<00:44, 14.20it/s, est. speed input: 15750.36 toks/s, output: 15.38 toks/s]
Processed prompts:  93%|█████████▎| 7618/8192 [08:13<00:35, 16.14it/s, est. speed input: 15797.04 toks/s, output: 15.43 toks/s]
Processed prompts:  94%|█████████▍| 7682/8192 [08:18<00:33, 15.31it/s, est. speed input: 15779.98 toks/s, output: 15.41 toks/s]
Processed prompts:  95%|█████████▍| 7746/8192 [08:22<00:29, 14.99it/s, est. speed input: 15769.91 toks/s, output: 15.40 toks/s]
Processed prompts:  95%|█████████▌| 7810/8192 [08:27<00:26, 14.60it/s, est. speed input: 15754.46 toks/s, output: 15.39 toks/s]
Processed prompts:  96%|█████████▌| 7874/8192 [08:32<00:21, 14.48it/s, est. speed input: 15743.85 toks/s, output: 15.37 toks/s]
Processed prompts:  97%|█████████▋| 7938/8192 [08:36<00:17, 14.36it/s, est. speed input: 15732.38 toks/s, output: 15.36 toks/s]
Processed prompts:  98%|█████████▊| 8002/8192 [08:41<00:13, 14.23it/s, est. speed input: 15719.51 toks/s, output: 15.35 toks/s]
Processed prompts:  98%|█████████▊| 8066/8192 [08:44<00:07, 15.99it/s, est. speed input: 15759.01 toks/s, output: 15.39 toks/s]
Processed prompts:  99%|█████████▉| 8130/8192 [08:48<00:04, 15.50it/s, est. speed input: 15751.08 toks/s, output: 15.38 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [08:48<00:00, 15.50it/s, est. speed input: 15871.17 toks/s, output: 15.50 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [08:48<00:00, 15.50it/s, est. speed input: 15871.17 toks/s, output: 15.50 toks/s]
[rank0]:[W125 21:36:49.988387467 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 00:36:55
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 00:37:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 00:37:06 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=604100) WARNING 01-26 00:37:13 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=604100) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=604100) WARNING 01-26 00:37:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.09 requests/s, 7742.41 total tokens/s, 15.09 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 00:37:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:37:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:37:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:37:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:37:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:37:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:37:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:37:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 00:37:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:37:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:37:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:37:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:37:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:37:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:37:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:37:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:37:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=604100) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=604100) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.32s/it]
(EngineCore_DP0 pid=604100) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.31s/it]
(EngineCore_DP0 pid=604100) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.16s/it]
(EngineCore_DP0 pid=604100) 
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=604100) [2026-01-26 00:37:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=604100) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.01s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
(EngineCore_DP0 pid=604100) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.45it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  34%|███▍      | 44/128 [00:00<00:00, 437.40it/s]
Adding requests:  70%|███████   | 90/128 [00:00<00:00, 449.43it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 449.24it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:29,  4.28it/s, est. speed input: 2192.78 toks/s, output: 4.28 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:22,  5.60it/s, est. speed input: 2740.78 toks/s, output: 5.35 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:18,  6.82it/s, est. speed input: 3186.45 toks/s, output: 6.22 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:12,  9.92it/s, est. speed input: 4173.38 toks/s, output: 8.15 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:10, 11.68it/s, est. speed input: 4796.01 toks/s, output: 9.37 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:09, 12.92it/s, est. speed input: 5258.06 toks/s, output: 10.27 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:01<00:08, 13.79it/s, est. speed input: 5610.42 toks/s, output: 10.96 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:08, 14.37it/s, est. speed input: 5882.20 toks/s, output: 11.49 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 14.71it/s, est. speed input: 6091.69 toks/s, output: 11.90 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 14.95it/s, est. speed input: 6261.69 toks/s, output: 12.23 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:07, 15.17it/s, est. speed input: 6410.29 toks/s, output: 12.52 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 15.37it/s, est. speed input: 6541.05 toks/s, output: 12.78 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 15.42it/s, est. speed input: 6643.58 toks/s, output: 12.98 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 15.44it/s, est. speed input: 6730.86 toks/s, output: 13.15 toks/s]
Processed prompts:  21%|██        | 27/128 [00:02<00:06, 15.62it/s, est. speed input: 6822.34 toks/s, output: 13.32 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:06, 15.73it/s, est. speed input: 6902.02 toks/s, output: 13.48 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 15.77it/s, est. speed input: 6969.41 toks/s, output: 13.61 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:06, 15.83it/s, est. speed input: 7032.26 toks/s, output: 13.73 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 15.86it/s, est. speed input: 7088.10 toks/s, output: 13.84 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 15.90it/s, est. speed input: 7139.86 toks/s, output: 13.94 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 15.93it/s, est. speed input: 7187.26 toks/s, output: 14.04 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 15.88it/s, est. speed input: 7225.64 toks/s, output: 14.11 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:03<00:05, 15.74it/s, est. speed input: 7254.49 toks/s, output: 14.17 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:03<00:05, 15.68it/s, est. speed input: 7282.82 toks/s, output: 14.22 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:05, 15.60it/s, est. speed input: 7306.92 toks/s, output: 14.27 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:05, 15.47it/s, est. speed input: 7325.10 toks/s, output: 14.31 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 15.47it/s, est. speed input: 7346.61 toks/s, output: 14.35 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 15.47it/s, est. speed input: 7366.80 toks/s, output: 14.39 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 15.44it/s, est. speed input: 7384.15 toks/s, output: 14.42 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 15.43it/s, est. speed input: 7400.84 toks/s, output: 14.45 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:04<00:04, 15.41it/s, est. speed input: 7415.54 toks/s, output: 14.48 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:04<00:04, 15.64it/s, est. speed input: 7441.40 toks/s, output: 14.53 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:04, 15.85it/s, est. speed input: 7467.69 toks/s, output: 14.59 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:03, 15.92it/s, est. speed input: 7489.54 toks/s, output: 14.63 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.02it/s, est. speed input: 7512.03 toks/s, output: 14.67 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.13it/s, est. speed input: 7534.81 toks/s, output: 14.72 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.13it/s, est. speed input: 7553.45 toks/s, output: 14.75 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.13it/s, est. speed input: 7571.24 toks/s, output: 14.79 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:05<00:03, 16.04it/s, est. speed input: 7584.69 toks/s, output: 14.81 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:05<00:03, 15.98it/s, est. speed input: 7597.47 toks/s, output: 14.84 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:05<00:03, 15.98it/s, est. speed input: 7611.16 toks/s, output: 14.87 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:02, 15.95it/s, est. speed input: 7623.13 toks/s, output: 14.89 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 15.91it/s, est. speed input: 7633.96 toks/s, output: 14.91 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.20it/s, est. speed input: 7655.33 toks/s, output: 14.95 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.52it/s, est. speed input: 7679.37 toks/s, output: 15.00 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.77it/s, est. speed input: 7703.12 toks/s, output: 15.05 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:06<00:02, 16.95it/s, est. speed input: 7725.89 toks/s, output: 15.09 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:06<00:02, 17.06it/s, est. speed input: 7747.53 toks/s, output: 15.13 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:06<00:01, 17.14it/s, est. speed input: 7768.17 toks/s, output: 15.17 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:01, 17.23it/s, est. speed input: 7788.99 toks/s, output: 15.21 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 17.22it/s, est. speed input: 7807.20 toks/s, output: 15.25 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 17.27it/s, est. speed input: 7826.30 toks/s, output: 15.29 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 17.25it/s, est. speed input: 7843.22 toks/s, output: 15.32 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 17.24it/s, est. speed input: 7859.81 toks/s, output: 15.35 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 17.19it/s, est. speed input: 7874.58 toks/s, output: 15.38 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:07<00:01, 17.11it/s, est. speed input: 7887.87 toks/s, output: 15.41 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:07<00:00, 17.08it/s, est. speed input: 7901.32 toks/s, output: 15.43 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:00, 17.00it/s, est. speed input: 7912.72 toks/s, output: 15.45 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 16.97it/s, est. speed input: 7924.55 toks/s, output: 15.48 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 16.89it/s, est. speed input: 7934.49 toks/s, output: 15.50 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 16.94it/s, est. speed input: 7946.81 toks/s, output: 15.52 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 17.04it/s, est. speed input: 7960.10 toks/s, output: 15.55 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 17.06it/s, est. speed input: 7971.83 toks/s, output: 15.57 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:08<00:00, 17.10it/s, est. speed input: 7983.92 toks/s, output: 15.59 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:08<00:00, 16.99it/s, est. speed input: 7992.69 toks/s, output: 15.61 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 16.99it/s, est. speed input: 7997.39 toks/s, output: 15.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.62it/s, est. speed input: 7997.39 toks/s, output: 15.62 toks/s]
[rank0]:[W126 00:38:06.504344762 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 00:38:09
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 00:38:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 00:38:18 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=605220) WARNING 01-26 00:38:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=605220) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=605220) WARNING 01-26 00:38:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.09 requests/s, 15470.73 total tokens/s, 15.09 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 00:38:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:38:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:38:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:38:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:38:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:38:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:38:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:38:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 00:38:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:38:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:38:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:38:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:38:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:38:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:38:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:38:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:38:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=605220) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=605220) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.03s/it]
(EngineCore_DP0 pid=605220) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.51s/it]
(EngineCore_DP0 pid=605220) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.43s/it]
(EngineCore_DP0 pid=605220) 
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=605220) [2026-01-26 00:38:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=605220) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  2.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]
(EngineCore_DP0 pid=605220) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  18%|█▊        | 23/128 [00:00<00:00, 229.91it/s]
Adding requests:  38%|███▊      | 48/128 [00:00<00:00, 240.01it/s]
Adding requests:  58%|█████▊    | 74/128 [00:00<00:00, 248.70it/s]
Adding requests:  77%|███████▋  | 99/128 [00:00<00:00, 242.44it/s]
Adding requests:  98%|█████████▊| 126/128 [00:00<00:00, 248.86it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 245.40it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 16.15it/s, est. speed input: 16546.28 toks/s, output: 16.16 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 16.43it/s, est. speed input: 16783.54 toks/s, output: 16.39 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:07, 16.55it/s, est. speed input: 16886.49 toks/s, output: 16.49 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:07, 16.63it/s, est. speed input: 16952.02 toks/s, output: 16.55 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:07, 16.66it/s, est. speed input: 16983.96 toks/s, output: 16.59 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:06, 16.66it/s, est. speed input: 16996.38 toks/s, output: 16.60 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 16.66it/s, est. speed input: 17006.26 toks/s, output: 16.61 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 16.65it/s, est. speed input: 17007.46 toks/s, output: 16.61 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:06, 16.65it/s, est. speed input: 17011.15 toks/s, output: 16.61 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 16.72it/s, est. speed input: 17040.22 toks/s, output: 16.64 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 16.62it/s, est. speed input: 17017.91 toks/s, output: 16.62 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 16.48it/s, est. speed input: 16978.76 toks/s, output: 16.58 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:06, 16.70it/s, est. speed input: 17027.79 toks/s, output: 16.63 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 16.81it/s, est. speed input: 17059.78 toks/s, output: 16.66 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 16.54it/s, est. speed input: 17008.18 toks/s, output: 16.61 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 16.23it/s, est. speed input: 16936.20 toks/s, output: 16.54 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:02<00:05, 16.01it/s, est. speed input: 16871.31 toks/s, output: 16.48 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:05, 15.90it/s, est. speed input: 16822.01 toks/s, output: 16.43 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 15.84it/s, est. speed input: 16781.73 toks/s, output: 16.39 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 15.80it/s, est. speed input: 16744.41 toks/s, output: 16.35 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 15.84it/s, est. speed input: 16723.10 toks/s, output: 16.33 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:05, 15.83it/s, est. speed input: 16698.25 toks/s, output: 16.31 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:05, 15.82it/s, est. speed input: 16674.92 toks/s, output: 16.28 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:05, 15.85it/s, est. speed input: 16658.66 toks/s, output: 16.27 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:03<00:04, 15.73it/s, est. speed input: 16624.37 toks/s, output: 16.23 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:04, 15.63it/s, est. speed input: 16589.60 toks/s, output: 16.20 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 15.54it/s, est. speed input: 16554.51 toks/s, output: 16.17 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 15.47it/s, est. speed input: 16522.31 toks/s, output: 16.13 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 15.49it/s, est. speed input: 16499.45 toks/s, output: 16.11 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:04, 15.44it/s, est. speed input: 16471.56 toks/s, output: 16.09 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:04, 15.45it/s, est. speed input: 16450.77 toks/s, output: 16.07 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:04, 15.40it/s, est. speed input: 16424.47 toks/s, output: 16.04 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:04<00:04, 15.41it/s, est. speed input: 16404.79 toks/s, output: 16.02 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:03, 15.51it/s, est. speed input: 16396.30 toks/s, output: 16.01 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 15.55it/s, est. speed input: 16385.91 toks/s, output: 16.00 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 15.54it/s, est. speed input: 16371.53 toks/s, output: 15.99 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 15.69it/s, est. speed input: 16373.00 toks/s, output: 15.99 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 15.79it/s, est. speed input: 16374.10 toks/s, output: 15.99 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:03, 15.89it/s, est. speed input: 16377.62 toks/s, output: 15.99 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:05<00:03, 15.75it/s, est. speed input: 16362.87 toks/s, output: 15.98 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:05<00:02, 15.66it/s, est. speed input: 16349.31 toks/s, output: 15.97 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:05<00:02, 15.60it/s, est. speed input: 16336.56 toks/s, output: 15.95 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 15.59it/s, est. speed input: 16327.30 toks/s, output: 15.94 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 15.57it/s, est. speed input: 16317.56 toks/s, output: 15.94 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 15.61it/s, est. speed input: 16311.58 toks/s, output: 15.93 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 15.57it/s, est. speed input: 16301.62 toks/s, output: 15.92 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:02, 15.44it/s, est. speed input: 16283.53 toks/s, output: 15.90 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:06<00:02, 15.42it/s, est. speed input: 16272.31 toks/s, output: 15.89 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:06<00:01, 15.42it/s, est. speed input: 16262.23 toks/s, output: 15.88 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:06<00:01, 15.63it/s, est. speed input: 16267.41 toks/s, output: 15.89 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:06<00:01, 16.03it/s, est. speed input: 16289.17 toks/s, output: 15.91 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 16.27it/s, est. speed input: 16306.61 toks/s, output: 15.92 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 16.43it/s, est. speed input: 16322.93 toks/s, output: 15.94 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 16.53it/s, est. speed input: 16338.15 toks/s, output: 15.96 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 16.59it/s, est. speed input: 16352.01 toks/s, output: 15.97 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:07<00:00, 16.67it/s, est. speed input: 16367.52 toks/s, output: 15.98 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:07<00:00, 16.69it/s, est. speed input: 16380.52 toks/s, output: 16.00 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:07<00:00, 16.70it/s, est. speed input: 16392.75 toks/s, output: 16.01 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:07<00:00, 16.73it/s, est. speed input: 16405.99 toks/s, output: 16.02 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:07<00:00, 16.76it/s, est. speed input: 16419.22 toks/s, output: 16.03 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 16.86it/s, est. speed input: 16435.66 toks/s, output: 16.05 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 16.81it/s, est. speed input: 16446.05 toks/s, output: 16.06 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 16.81it/s, est. speed input: 16457.71 toks/s, output: 16.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.88it/s, est. speed input: 16472.18 toks/s, output: 16.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.88it/s, est. speed input: 16472.18 toks/s, output: 16.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.09it/s, est. speed input: 16472.18 toks/s, output: 16.09 toks/s]
[rank0]:[W126 00:39:09.680895417 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 00:39:10
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 00:39:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 00:39:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=606198) WARNING 01-26 00:39:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=606198) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=606198) WARNING 01-26 00:39:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 20.44 requests/s, 20955.33 total tokens/s, 20.44 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 00:39:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:39:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:39:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:39:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:39:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:39:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:39:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:39:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 00:39:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:39:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:39:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:39:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:39:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:39:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:39:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:39:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:39:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=606198) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=606198) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.04s/it]
(EngineCore_DP0 pid=606198) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.34s/it]
(EngineCore_DP0 pid=606198) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.30s/it]
(EngineCore_DP0 pid=606198) 
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=606198) [2026-01-26 00:39:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=606198) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.80it/s]
(EngineCore_DP0 pid=606198) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  6.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.68it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 21/256 [00:00<00:01, 200.66it/s]
Adding requests:  17%|█▋        | 44/256 [00:00<00:00, 216.15it/s]
Adding requests:  27%|██▋       | 69/256 [00:00<00:00, 229.36it/s]
Adding requests:  36%|███▌      | 92/256 [00:00<00:00, 228.69it/s]
Adding requests:  46%|████▌     | 117/256 [00:00<00:00, 233.90it/s]
Adding requests:  55%|█████▌    | 141/256 [00:00<00:00, 235.45it/s]
Adding requests:  65%|██████▍   | 166/256 [00:00<00:00, 238.24it/s]
Adding requests:  75%|███████▌  | 193/256 [00:00<00:00, 247.18it/s]
Adding requests:  85%|████████▌ | 218/256 [00:00<00:00, 246.69it/s]
Adding requests:  95%|█████████▍| 243/256 [00:01<00:00, 242.36it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 236.67it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:01, 122.28it/s, est. speed input: 125243.27 toks/s, output: 122.29 toks/s]
Processed prompts:  11%|█         | 27/256 [00:00<00:06, 34.50it/s, est. speed input: 39764.17 toks/s, output: 38.83 toks/s]   
Processed prompts:  13%|█▎        | 34/256 [00:01<00:08, 27.03it/s, est. speed input: 32172.78 toks/s, output: 31.42 toks/s]
Processed prompts:  15%|█▌        | 39/256 [00:01<00:08, 26.73it/s, est. speed input: 31302.47 toks/s, output: 30.57 toks/s]
Processed prompts:  17%|█▋        | 43/256 [00:01<00:08, 25.16it/s, est. speed input: 29968.22 toks/s, output: 29.27 toks/s]
Processed prompts:  18%|█▊        | 47/256 [00:01<00:08, 23.95it/s, est. speed input: 28948.34 toks/s, output: 28.27 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:01<00:09, 21.60it/s, est. speed input: 27586.35 toks/s, output: 26.94 toks/s]
Processed prompts:  21%|██        | 54/256 [00:02<00:09, 21.73it/s, est. speed input: 27143.55 toks/s, output: 26.51 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:02<00:09, 21.91it/s, est. speed input: 26803.08 toks/s, output: 26.17 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:02<00:08, 22.02it/s, est. speed input: 26507.02 toks/s, output: 25.89 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:02<00:08, 22.10it/s, est. speed input: 26249.00 toks/s, output: 25.63 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:02<00:08, 22.17it/s, est. speed input: 26030.76 toks/s, output: 25.42 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:02<00:08, 22.20it/s, est. speed input: 25831.92 toks/s, output: 25.23 toks/s]
Processed prompts:  30%|███       | 78/256 [00:03<00:08, 22.24it/s, est. speed input: 25662.17 toks/s, output: 25.06 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:03<00:07, 22.29it/s, est. speed input: 25515.18 toks/s, output: 24.92 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:03<00:07, 22.30it/s, est. speed input: 25377.25 toks/s, output: 24.78 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:03<00:07, 22.31it/s, est. speed input: 25254.94 toks/s, output: 24.66 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:03<00:07, 22.31it/s, est. speed input: 25141.17 toks/s, output: 24.55 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:04<00:07, 22.31it/s, est. speed input: 25039.64 toks/s, output: 24.45 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:04<00:06, 22.30it/s, est. speed input: 24943.95 toks/s, output: 24.36 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:04<00:06, 22.32it/s, est. speed input: 24859.85 toks/s, output: 24.28 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:04<00:06, 22.18it/s, est. speed input: 24760.80 toks/s, output: 24.18 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:04<00:06, 21.68it/s, est. speed input: 24609.58 toks/s, output: 24.03 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:04<00:06, 21.34it/s, est. speed input: 24471.75 toks/s, output: 23.90 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:05<00:06, 21.11it/s, est. speed input: 24343.61 toks/s, output: 23.77 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:05<00:06, 20.95it/s, est. speed input: 24224.29 toks/s, output: 23.66 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:05<00:06, 20.83it/s, est. speed input: 24112.01 toks/s, output: 23.55 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:05<00:05, 20.79it/s, est. speed input: 24013.91 toks/s, output: 23.45 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:05<00:05, 20.77it/s, est. speed input: 23922.19 toks/s, output: 23.36 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:06<00:05, 20.73it/s, est. speed input: 23833.78 toks/s, output: 23.28 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:06<00:05, 20.69it/s, est. speed input: 23749.08 toks/s, output: 23.19 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:06<00:05, 20.67it/s, est. speed input: 23670.04 toks/s, output: 23.12 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:06<00:04, 20.64it/s, est. speed input: 23594.18 toks/s, output: 23.04 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:06<00:04, 20.63it/s, est. speed input: 23524.01 toks/s, output: 22.97 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:07<00:04, 20.63it/s, est. speed input: 23457.97 toks/s, output: 22.91 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:07<00:04, 20.62it/s, est. speed input: 23394.42 toks/s, output: 22.85 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:07<00:04, 20.61it/s, est. speed input: 23334.31 toks/s, output: 22.79 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:07<00:03, 20.59it/s, est. speed input: 23275.79 toks/s, output: 22.73 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:07<00:03, 20.59it/s, est. speed input: 23221.42 toks/s, output: 22.68 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:08<00:03, 20.59it/s, est. speed input: 23169.59 toks/s, output: 22.63 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:08<00:03, 20.56it/s, est. speed input: 23118.14 toks/s, output: 22.58 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:08<00:03, 20.54it/s, est. speed input: 23068.97 toks/s, output: 22.53 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:08<00:03, 20.55it/s, est. speed input: 23024.08 toks/s, output: 22.48 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:08<00:02, 20.55it/s, est. speed input: 22979.94 toks/s, output: 22.44 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:09<00:02, 20.58it/s, est. speed input: 22940.68 toks/s, output: 22.40 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:09<00:02, 20.59it/s, est. speed input: 22902.18 toks/s, output: 22.37 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:09<00:02, 20.60it/s, est. speed input: 22865.16 toks/s, output: 22.33 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:09<00:01, 21.03it/s, est. speed input: 22860.78 toks/s, output: 22.32 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:09<00:01, 21.41it/s, est. speed input: 22861.73 toks/s, output: 22.33 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:09<00:01, 21.63it/s, est. speed input: 22858.51 toks/s, output: 22.32 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:10<00:01, 21.84it/s, est. speed input: 22858.91 toks/s, output: 22.32 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:10<00:01, 21.97it/s, est. speed input: 22858.31 toks/s, output: 22.32 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:10<00:00, 22.01it/s, est. speed input: 22854.57 toks/s, output: 22.32 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:10<00:00, 22.12it/s, est. speed input: 22855.35 toks/s, output: 22.32 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:10<00:00, 22.17it/s, est. speed input: 22855.06 toks/s, output: 22.32 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:11<00:00, 22.21it/s, est. speed input: 22854.74 toks/s, output: 22.32 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 22.21it/s, est. speed input: 26663.35 toks/s, output: 26.04 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 26.04it/s, est. speed input: 26663.35 toks/s, output: 26.04 toks/s]
[rank0]:[W126 00:40:12.973436235 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 00:40:15
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 00:40:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 00:40:29 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=607188) WARNING 01-26 00:40:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=607188) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=607188) WARNING 01-26 00:40:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 21.42 requests/s, 21954.22 total tokens/s, 21.42 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 00:40:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:40:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:40:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:40:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:40:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:40:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:40:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:40:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 00:40:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:40:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:40:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:40:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:40:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:40:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:40:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:40:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:40:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=607188) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=607188) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.15s/it]
(EngineCore_DP0 pid=607188) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.48s/it]
(EngineCore_DP0 pid=607188) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.43s/it]
(EngineCore_DP0 pid=607188) 
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=607188) [2026-01-26 00:40:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=607188) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  7.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.98it/s]
(EngineCore_DP0 pid=607188) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  7.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  7.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  7.30it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 24/512 [00:00<00:02, 235.20it/s]
Adding requests:  10%|▉         | 51/512 [00:00<00:01, 251.20it/s]
Adding requests:  15%|█▌        | 79/512 [00:00<00:01, 261.32it/s]
Adding requests:  21%|██        | 106/512 [00:00<00:01, 263.35it/s]
Adding requests:  26%|██▌       | 133/512 [00:00<00:01, 261.58it/s]
Adding requests:  31%|███▏      | 160/512 [00:00<00:01, 260.01it/s]
Adding requests:  37%|███▋      | 187/512 [00:00<00:01, 259.37it/s]
Adding requests:  42%|████▏     | 213/512 [00:00<00:01, 258.27it/s]
Adding requests:  47%|████▋     | 239/512 [00:00<00:01, 255.44it/s]
Adding requests:  52%|█████▏    | 265/512 [00:01<00:01, 241.10it/s]
Adding requests:  57%|█████▋    | 290/512 [00:01<00:00, 239.47it/s]
Adding requests:  62%|██████▏   | 316/512 [00:01<00:00, 242.70it/s]
Adding requests:  67%|██████▋   | 341/512 [00:01<00:00, 242.95it/s]
Adding requests:  71%|███████▏  | 366/512 [00:01<00:00, 243.91it/s]
Adding requests:  76%|███████▋  | 391/512 [00:01<00:00, 245.57it/s]
Adding requests:  81%|████████▏ | 417/512 [00:01<00:00, 247.95it/s]
Adding requests:  86%|████████▋ | 442/512 [00:01<00:00, 243.77it/s]
Adding requests:  91%|█████████▏| 468/512 [00:01<00:00, 248.09it/s]
Adding requests:  97%|█████████▋| 495/512 [00:01<00:00, 252.86it/s]
Adding requests: 100%|██████████| 512/512 [00:02<00:00, 249.96it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:00<00:02, 178.68it/s, est. speed input: 182999.00 toks/s, output: 178.69 toks/s]
Processed prompts:  11%|█         | 56/512 [00:00<00:09, 49.78it/s, est. speed input: 59752.43 toks/s, output: 58.35 toks/s]   
Processed prompts:  13%|█▎        | 65/512 [00:01<00:11, 40.19it/s, est. speed input: 49920.51 toks/s, output: 48.75 toks/s]
Processed prompts:  14%|█▍        | 71/512 [00:01<00:13, 31.81it/s, est. speed input: 42604.50 toks/s, output: 41.61 toks/s]
Processed prompts:  15%|█▍        | 76/512 [00:01<00:14, 30.81it/s, est. speed input: 41106.52 toks/s, output: 40.14 toks/s]
Processed prompts:  16%|█▌        | 80/512 [00:02<00:15, 28.75it/s, est. speed input: 39397.06 toks/s, output: 38.47 toks/s]
Processed prompts:  16%|█▋        | 84/512 [00:02<00:15, 27.00it/s, est. speed input: 37962.60 toks/s, output: 37.07 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:02<00:05, 73.40it/s, est. speed input: 51203.13 toks/s, output: 50.00 toks/s]
Processed prompts:  26%|██▌       | 131/512 [00:02<00:07, 52.35it/s, est. speed input: 47673.45 toks/s, output: 46.56 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:03<00:09, 39.63it/s, est. speed input: 44337.57 toks/s, output: 43.30 toks/s]
Processed prompts:  28%|██▊       | 144/512 [00:03<00:09, 38.04it/s, est. speed input: 43705.40 toks/s, output: 42.68 toks/s]
Processed prompts:  29%|██▉       | 149/512 [00:03<00:10, 35.41it/s, est. speed input: 42849.43 toks/s, output: 41.85 toks/s]
Processed prompts:  30%|███       | 154/512 [00:03<00:13, 27.03it/s, est. speed input: 40232.94 toks/s, output: 39.29 toks/s]
Processed prompts:  31%|███       | 158/512 [00:04<00:13, 26.22it/s, est. speed input: 39537.85 toks/s, output: 38.61 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:04<00:13, 25.51it/s, est. speed input: 38899.01 toks/s, output: 37.99 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:04<00:13, 24.92it/s, est. speed input: 38307.14 toks/s, output: 37.41 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:04<00:13, 24.47it/s, est. speed input: 37762.75 toks/s, output: 36.88 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:04<00:14, 24.11it/s, est. speed input: 37254.57 toks/s, output: 36.38 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:04<00:14, 23.84it/s, est. speed input: 36782.11 toks/s, output: 35.92 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:05<00:13, 23.66it/s, est. speed input: 36343.77 toks/s, output: 35.49 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:05<00:13, 23.52it/s, est. speed input: 35933.25 toks/s, output: 35.09 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:05<00:13, 23.41it/s, est. speed input: 35547.88 toks/s, output: 34.71 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:05<00:13, 23.34it/s, est. speed input: 35185.96 toks/s, output: 34.36 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:05<00:13, 23.28it/s, est. speed input: 34844.24 toks/s, output: 34.03 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:05<00:13, 23.24it/s, est. speed input: 34523.49 toks/s, output: 33.71 toks/s]
Processed prompts:  40%|████      | 206/512 [00:06<00:13, 23.22it/s, est. speed input: 34221.56 toks/s, output: 33.42 toks/s]
Processed prompts:  41%|████      | 210/512 [00:06<00:12, 23.24it/s, est. speed input: 33940.04 toks/s, output: 33.14 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:06<00:12, 23.25it/s, est. speed input: 33672.62 toks/s, output: 32.88 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:06<00:12, 22.83it/s, est. speed input: 33366.31 toks/s, output: 32.58 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:06<00:12, 22.39it/s, est. speed input: 33055.52 toks/s, output: 32.28 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:07<00:12, 22.08it/s, est. speed input: 32760.63 toks/s, output: 31.99 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:07<00:12, 21.86it/s, est. speed input: 32479.21 toks/s, output: 31.72 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:07<00:12, 21.71it/s, est. speed input: 32213.18 toks/s, output: 31.46 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:07<00:12, 21.61it/s, est. speed input: 31959.71 toks/s, output: 31.21 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:07<00:12, 21.54it/s, est. speed input: 31718.35 toks/s, output: 30.97 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:08<00:12, 21.48it/s, est. speed input: 31487.44 toks/s, output: 30.75 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:08<00:12, 21.45it/s, est. speed input: 31268.14 toks/s, output: 30.54 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:08<00:12, 21.42it/s, est. speed input: 31057.94 toks/s, output: 30.33 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:08<00:11, 21.41it/s, est. speed input: 30857.44 toks/s, output: 30.13 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:08<00:11, 21.39it/s, est. speed input: 30665.22 toks/s, output: 29.95 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:08<00:11, 21.38it/s, est. speed input: 30481.03 toks/s, output: 29.77 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:09<00:11, 21.37it/s, est. speed input: 30303.77 toks/s, output: 29.59 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:09<00:11, 21.40it/s, est. speed input: 30137.13 toks/s, output: 29.43 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:09<00:10, 21.40it/s, est. speed input: 29975.56 toks/s, output: 29.27 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:09<00:10, 21.40it/s, est. speed input: 29820.07 toks/s, output: 29.12 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:09<00:10, 21.40it/s, est. speed input: 29669.83 toks/s, output: 28.97 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:10<00:10, 21.37it/s, est. speed input: 29523.07 toks/s, output: 28.83 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:10<00:10, 21.35it/s, est. speed input: 29382.05 toks/s, output: 28.69 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:10<00:10, 21.35it/s, est. speed input: 29246.91 toks/s, output: 28.56 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:10<00:09, 21.35it/s, est. speed input: 29116.60 toks/s, output: 28.43 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:10<00:09, 21.35it/s, est. speed input: 28990.65 toks/s, output: 28.31 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:10<00:09, 21.35it/s, est. speed input: 28869.47 toks/s, output: 28.19 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:11<00:09, 21.35it/s, est. speed input: 28751.85 toks/s, output: 28.08 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:11<00:09, 21.34it/s, est. speed input: 28637.49 toks/s, output: 27.97 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:11<00:08, 21.33it/s, est. speed input: 28527.00 toks/s, output: 27.86 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:11<00:08, 21.34it/s, est. speed input: 28420.77 toks/s, output: 27.75 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:11<00:08, 21.34it/s, est. speed input: 28317.66 toks/s, output: 27.65 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:12<00:08, 21.31it/s, est. speed input: 28215.72 toks/s, output: 27.55 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:12<00:08, 21.32it/s, est. speed input: 28118.55 toks/s, output: 27.46 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:12<00:07, 21.44it/s, est. speed input: 28032.39 toks/s, output: 27.38 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:12<00:07, 21.91it/s, est. speed input: 27972.15 toks/s, output: 27.32 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:12<00:07, 22.28it/s, est. speed input: 27915.25 toks/s, output: 27.26 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:13<00:07, 22.54it/s, est. speed input: 27859.62 toks/s, output: 27.21 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:13<00:06, 22.71it/s, est. speed input: 27804.90 toks/s, output: 27.15 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:13<00:06, 22.85it/s, est. speed input: 27752.40 toks/s, output: 27.10 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:13<00:06, 22.92it/s, est. speed input: 27699.88 toks/s, output: 27.05 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:13<00:06, 22.98it/s, est. speed input: 27648.86 toks/s, output: 27.00 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:13<00:05, 23.01it/s, est. speed input: 27598.87 toks/s, output: 26.95 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:14<00:05, 23.05it/s, est. speed input: 27550.71 toks/s, output: 26.90 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:14<00:05, 23.06it/s, est. speed input: 27503.01 toks/s, output: 26.86 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:14<00:05, 23.06it/s, est. speed input: 27456.37 toks/s, output: 26.81 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:14<00:05, 23.08it/s, est. speed input: 27411.54 toks/s, output: 26.77 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:14<00:05, 22.53it/s, est. speed input: 27340.72 toks/s, output: 26.70 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:14<00:05, 22.17it/s, est. speed input: 27272.30 toks/s, output: 26.63 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:15<00:05, 21.90it/s, est. speed input: 27204.41 toks/s, output: 26.57 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:15<00:04, 21.72it/s, est. speed input: 27138.60 toks/s, output: 26.50 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:15<00:04, 21.59it/s, est. speed input: 27074.14 toks/s, output: 26.44 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:15<00:04, 21.53it/s, est. speed input: 27012.24 toks/s, output: 26.38 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:15<00:04, 21.47it/s, est. speed input: 26951.25 toks/s, output: 26.32 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:16<00:04, 21.43it/s, est. speed input: 26891.64 toks/s, output: 26.26 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:16<00:04, 21.38it/s, est. speed input: 26832.59 toks/s, output: 26.20 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:16<00:03, 21.35it/s, est. speed input: 26775.11 toks/s, output: 26.15 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:16<00:03, 21.31it/s, est. speed input: 26717.95 toks/s, output: 26.09 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:16<00:03, 21.31it/s, est. speed input: 26662.95 toks/s, output: 26.04 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:17<00:03, 21.30it/s, est. speed input: 26609.38 toks/s, output: 25.99 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:17<00:03, 21.26it/s, est. speed input: 26554.83 toks/s, output: 25.93 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:17<00:02, 21.27it/s, est. speed input: 26503.46 toks/s, output: 25.88 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:17<00:02, 21.26it/s, est. speed input: 26452.61 toks/s, output: 25.83 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:17<00:02, 21.25it/s, est. speed input: 26402.52 toks/s, output: 25.78 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:17<00:02, 21.25it/s, est. speed input: 26354.02 toks/s, output: 25.74 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:18<00:02, 21.26it/s, est. speed input: 26306.66 toks/s, output: 25.69 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:18<00:01, 21.25it/s, est. speed input: 26259.84 toks/s, output: 25.64 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:18<00:01, 21.25it/s, est. speed input: 26214.05 toks/s, output: 25.60 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:18<00:01, 21.24it/s, est. speed input: 26168.78 toks/s, output: 25.56 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:18<00:01, 21.24it/s, est. speed input: 26124.83 toks/s, output: 25.51 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:19<00:01, 21.26it/s, est. speed input: 26082.46 toks/s, output: 25.47 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:19<00:01, 21.28it/s, est. speed input: 26040.91 toks/s, output: 25.43 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:19<00:00, 21.28it/s, est. speed input: 26000.09 toks/s, output: 25.39 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:19<00:00, 21.28it/s, est. speed input: 25959.88 toks/s, output: 25.35 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:19<00:00, 21.26it/s, est. speed input: 25919.67 toks/s, output: 25.31 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:20<00:00, 21.26it/s, est. speed input: 25880.73 toks/s, output: 25.27 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:20<00:00, 23.08it/s, est. speed input: 25905.82 toks/s, output: 25.30 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:20<00:00, 23.08it/s, est. speed input: 26007.12 toks/s, output: 25.40 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:20<00:00, 25.40it/s, est. speed input: 26007.12 toks/s, output: 25.40 toks/s]
[rank0]:[W126 00:41:33.274662135 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 00:41:36
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 00:41:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 00:41:52 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=608340) WARNING 01-26 00:42:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=608340) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=608340) WARNING 01-26 00:42:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 21.80 requests/s, 22339.96 total tokens/s, 21.80 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 00:41:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:41:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:41:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:41:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:41:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:41:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:41:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:41:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:41:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:41:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:41:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:41:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:41:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:41:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 00:42:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:42:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:42:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:42:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:42:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:42:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:42:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:42:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:42:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:42:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:42:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:42:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:42:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:42:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=608340) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=608340) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.12s/it]
(EngineCore_DP0 pid=608340) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.40s/it]
(EngineCore_DP0 pid=608340) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.36s/it]
(EngineCore_DP0 pid=608340) 
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=608340) [2026-01-26 00:42:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=608340) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  3.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  4.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:01<00:00,  4.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  2.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  3.05it/s]
(EngineCore_DP0 pid=608340) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  5.62it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  5.92it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  4.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  3.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  4.20it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 21/1024 [00:00<00:04, 209.76it/s]
Adding requests:   4%|▍         | 44/1024 [00:00<00:04, 221.17it/s]
Adding requests:   7%|▋         | 70/1024 [00:00<00:04, 233.82it/s]
Adding requests:   9%|▉         | 94/1024 [00:00<00:04, 223.26it/s]
Adding requests:  12%|█▏        | 118/1024 [00:00<00:04, 225.88it/s]
Adding requests:  14%|█▍        | 141/1024 [00:00<00:03, 226.28it/s]
Adding requests:  16%|█▌        | 166/1024 [00:00<00:03, 231.84it/s]
Adding requests:  19%|█▉        | 193/1024 [00:00<00:03, 242.07it/s]
Adding requests:  21%|██▏       | 218/1024 [00:00<00:03, 242.34it/s]
Adding requests:  24%|██▍       | 244/1024 [00:01<00:03, 246.98it/s]
Adding requests:  26%|██▋       | 271/1024 [00:01<00:02, 251.19it/s]
Adding requests:  29%|██▉       | 298/1024 [00:01<00:02, 254.99it/s]
Adding requests:  32%|███▏      | 325/1024 [00:01<00:02, 257.96it/s]
Adding requests:  34%|███▍      | 353/1024 [00:01<00:02, 262.47it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:02, 262.50it/s]
Adding requests:  40%|███▉      | 408/1024 [00:01<00:02, 267.12it/s]
Adding requests:  43%|████▎     | 436/1024 [00:01<00:02, 269.41it/s]
Adding requests:  45%|████▌     | 464/1024 [00:01<00:02, 270.59it/s]
Adding requests:  48%|████▊     | 495/1024 [00:01<00:01, 281.42it/s]
Adding requests:  51%|█████▏    | 526/1024 [00:02<00:01, 289.34it/s]
Adding requests:  54%|█████▍    | 555/1024 [00:02<00:01, 283.62it/s]
Adding requests:  57%|█████▋    | 584/1024 [00:02<00:01, 278.95it/s]
Adding requests:  60%|█████▉    | 612/1024 [00:02<00:01, 274.99it/s]
Adding requests:  62%|██████▎   | 640/1024 [00:02<00:01, 265.33it/s]
Adding requests:  65%|██████▌   | 667/1024 [00:02<00:01, 258.09it/s]
Adding requests:  68%|██████▊   | 695/1024 [00:02<00:01, 261.78it/s]
Adding requests:  71%|███████   | 722/1024 [00:02<00:01, 260.27it/s]
Adding requests:  73%|███████▎  | 749/1024 [00:02<00:01, 256.06it/s]
Adding requests:  76%|███████▌  | 776/1024 [00:03<00:00, 259.82it/s]
Adding requests:  78%|███████▊  | 803/1024 [00:03<00:00, 261.78it/s]
Adding requests:  81%|████████  | 831/1024 [00:03<00:00, 266.62it/s]
Adding requests:  84%|████████▍ | 858/1024 [00:03<00:00, 266.65it/s]
Adding requests:  86%|████████▋ | 885/1024 [00:03<00:00, 267.57it/s]
Adding requests:  89%|████████▉ | 912/1024 [00:03<00:00, 265.90it/s]
Adding requests:  92%|█████████▏| 939/1024 [00:03<00:00, 256.07it/s]
Adding requests:  94%|█████████▍| 965/1024 [00:03<00:00, 253.31it/s]
Adding requests:  97%|█████████▋| 991/1024 [00:03<00:00, 246.53it/s]
Adding requests:  99%|█████████▉| 1016/1024 [00:03<00:00, 235.76it/s]
Adding requests: 100%|██████████| 1024/1024 [00:04<00:00, 255.48it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:00<00:03, 278.14it/s, est. speed input: 284857.09 toks/s, output: 278.15 toks/s]
Processed prompts:   9%|▉         | 94/1024 [00:01<00:15, 58.19it/s, est. speed input: 71492.50 toks/s, output: 69.82 toks/s]   
Processed prompts:  10%|█         | 107/1024 [00:02<00:23, 39.82it/s, est. speed input: 52566.28 toks/s, output: 51.33 toks/s]
Processed prompts:  11%|█         | 115/1024 [00:02<00:25, 35.39it/s, est. speed input: 47994.72 toks/s, output: 46.87 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:02<00:28, 31.14it/s, est. speed input: 44260.95 toks/s, output: 43.22 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:03<00:31, 28.58it/s, est. speed input: 41703.65 toks/s, output: 40.73 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:03<00:33, 26.65it/s, est. speed input: 39678.02 toks/s, output: 38.75 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:03<00:34, 25.22it/s, est. speed input: 38026.29 toks/s, output: 37.13 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:04<00:35, 24.57it/s, est. speed input: 36836.28 toks/s, output: 35.97 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:04<00:35, 24.25it/s, est. speed input: 35887.98 toks/s, output: 35.05 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:04<00:35, 24.02it/s, est. speed input: 35070.53 toks/s, output: 34.25 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:05<00:35, 23.85it/s, est. speed input: 34358.90 toks/s, output: 33.55 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:05<00:35, 23.72it/s, est. speed input: 33729.64 toks/s, output: 32.94 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:05<00:35, 23.63it/s, est. speed input: 33172.90 toks/s, output: 32.40 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:06<00:34, 23.57it/s, est. speed input: 32677.48 toks/s, output: 31.91 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:06<00:34, 23.53it/s, est. speed input: 32232.90 toks/s, output: 31.48 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:07<00:34, 23.49it/s, est. speed input: 31829.88 toks/s, output: 31.08 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:07<00:34, 23.46it/s, est. speed input: 31464.31 toks/s, output: 30.73 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:07<00:13, 55.27it/s, est. speed input: 36142.94 toks/s, output: 35.30 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:08<00:16, 45.19it/s, est. speed input: 35524.41 toks/s, output: 34.69 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:08<00:19, 38.15it/s, est. speed input: 34957.79 toks/s, output: 34.14 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:08<00:21, 33.24it/s, est. speed input: 34436.84 toks/s, output: 33.63 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:09<00:24, 29.81it/s, est. speed input: 33957.93 toks/s, output: 33.16 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:09<00:25, 27.41it/s, est. speed input: 33514.25 toks/s, output: 32.73 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:09<00:27, 25.73it/s, est. speed input: 33103.84 toks/s, output: 32.33 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:10<00:28, 24.56it/s, est. speed input: 32722.55 toks/s, output: 31.96 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:10<00:28, 23.74it/s, est. speed input: 32366.63 toks/s, output: 31.61 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:11<00:29, 23.15it/s, est. speed input: 32033.05 toks/s, output: 31.28 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:11<00:29, 22.76it/s, est. speed input: 31722.71 toks/s, output: 30.98 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:11<00:29, 22.48it/s, est. speed input: 31431.07 toks/s, output: 30.69 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:12<00:29, 22.27it/s, est. speed input: 31155.60 toks/s, output: 30.43 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:12<00:29, 22.13it/s, est. speed input: 30896.64 toks/s, output: 30.17 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:12<00:28, 22.12it/s, est. speed input: 30664.43 toks/s, output: 29.95 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:13<00:28, 22.46it/s, est. speed input: 30488.26 toks/s, output: 29.77 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:13<00:27, 22.70it/s, est. speed input: 30320.54 toks/s, output: 29.61 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:13<00:26, 22.87it/s, est. speed input: 30160.25 toks/s, output: 29.45 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:14<00:26, 22.99it/s, est. speed input: 30008.09 toks/s, output: 29.30 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:14<00:25, 23.08it/s, est. speed input: 29862.94 toks/s, output: 29.16 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:14<00:25, 23.13it/s, est. speed input: 29723.40 toks/s, output: 29.03 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:15<00:25, 23.16it/s, est. speed input: 29590.44 toks/s, output: 28.90 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:15<00:24, 23.15it/s, est. speed input: 29459.11 toks/s, output: 28.77 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:16<00:25, 22.62it/s, est. speed input: 29285.22 toks/s, output: 28.60 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:16<00:25, 22.25it/s, est. speed input: 29118.64 toks/s, output: 28.44 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:16<00:24, 22.01it/s, est. speed input: 28959.31 toks/s, output: 28.28 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:17<00:24, 21.83it/s, est. speed input: 28806.88 toks/s, output: 28.13 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:17<00:24, 21.72it/s, est. speed input: 28661.23 toks/s, output: 27.99 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:17<00:24, 21.63it/s, est. speed input: 28521.19 toks/s, output: 27.85 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:18<00:24, 21.58it/s, est. speed input: 28387.20 toks/s, output: 27.72 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:18<00:23, 21.54it/s, est. speed input: 28258.66 toks/s, output: 27.60 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:18<00:23, 21.51it/s, est. speed input: 28135.08 toks/s, output: 27.48 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:19<00:22, 21.49it/s, est. speed input: 28015.72 toks/s, output: 27.36 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:19<00:22, 21.47it/s, est. speed input: 27900.58 toks/s, output: 27.25 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:20<00:22, 21.46it/s, est. speed input: 27790.19 toks/s, output: 27.14 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:20<00:21, 21.45it/s, est. speed input: 27683.59 toks/s, output: 27.03 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:20<00:21, 21.44it/s, est. speed input: 27580.65 toks/s, output: 26.93 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:21<00:21, 21.43it/s, est. speed input: 27481.34 toks/s, output: 26.84 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:21<00:20, 21.43it/s, est. speed input: 27385.49 toks/s, output: 26.74 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:21<00:20, 21.43it/s, est. speed input: 27292.79 toks/s, output: 26.65 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:22<00:19, 21.78it/s, est. speed input: 27228.24 toks/s, output: 26.59 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:22<00:19, 22.19it/s, est. speed input: 27175.55 toks/s, output: 26.54 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:23<00:18, 22.49it/s, est. speed input: 27124.43 toks/s, output: 26.49 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:23<00:17, 22.70it/s, est. speed input: 27074.91 toks/s, output: 26.44 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:23<00:17, 22.73it/s, est. speed input: 27019.85 toks/s, output: 26.39 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:24<00:17, 22.32it/s, est. speed input: 26941.01 toks/s, output: 26.31 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:24<00:17, 22.04it/s, est. speed input: 26864.74 toks/s, output: 26.24 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:24<00:17, 21.87it/s, est. speed input: 26791.42 toks/s, output: 26.16 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:25<00:16, 21.73it/s, est. speed input: 26719.62 toks/s, output: 26.09 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:25<00:16, 21.64it/s, est. speed input: 26649.93 toks/s, output: 26.03 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:25<00:16, 21.58it/s, est. speed input: 26582.44 toks/s, output: 25.96 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:26<00:15, 21.54it/s, est. speed input: 26516.88 toks/s, output: 25.90 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:26<00:15, 21.50it/s, est. speed input: 26452.64 toks/s, output: 25.83 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:27<00:15, 21.48it/s, est. speed input: 26390.41 toks/s, output: 25.77 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:27<00:14, 21.46it/s, est. speed input: 26329.91 toks/s, output: 25.71 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:27<00:14, 21.45it/s, est. speed input: 26270.99 toks/s, output: 25.66 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:28<00:14, 21.43it/s, est. speed input: 26213.31 toks/s, output: 25.60 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:28<00:13, 21.43it/s, est. speed input: 26157.26 toks/s, output: 25.54 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:28<00:13, 21.47it/s, est. speed input: 26105.25 toks/s, output: 25.49 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:29<00:12, 21.97it/s, est. speed input: 26077.71 toks/s, output: 25.47 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:29<00:12, 22.32it/s, est. speed input: 26050.81 toks/s, output: 25.44 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:29<00:11, 22.58it/s, est. speed input: 26024.56 toks/s, output: 25.41 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:30<00:11, 22.78it/s, est. speed input: 25999.17 toks/s, output: 25.39 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:30<00:10, 22.91it/s, est. speed input: 25974.24 toks/s, output: 25.37 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:30<00:10, 23.65it/s, est. speed input: 25976.64 toks/s, output: 25.37 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:31<00:09, 23.52it/s, est. speed input: 25952.21 toks/s, output: 25.34 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:31<00:09, 23.43it/s, est. speed input: 25928.52 toks/s, output: 25.32 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:32<00:09, 22.81it/s, est. speed input: 25882.90 toks/s, output: 25.28 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:32<00:09, 22.38it/s, est. speed input: 25837.65 toks/s, output: 25.23 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:32<00:08, 22.08it/s, est. speed input: 25793.03 toks/s, output: 25.19 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:33<00:08, 21.88it/s, est. speed input: 25749.55 toks/s, output: 25.15 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:33<00:08, 21.74it/s, est. speed input: 25707.21 toks/s, output: 25.10 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:33<00:08, 21.64it/s, est. speed input: 25665.63 toks/s, output: 25.06 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:34<00:07, 21.58it/s, est. speed input: 25625.28 toks/s, output: 25.02 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:34<00:07, 21.54it/s, est. speed input: 25585.70 toks/s, output: 24.99 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:35<00:06, 21.50it/s, est. speed input: 25546.62 toks/s, output: 24.95 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:35<00:06, 21.48it/s, est. speed input: 25508.58 toks/s, output: 24.91 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:35<00:06, 21.46it/s, est. speed input: 25471.22 toks/s, output: 24.87 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:36<00:05, 21.45it/s, est. speed input: 25434.78 toks/s, output: 24.84 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:36<00:05, 21.43it/s, est. speed input: 25398.73 toks/s, output: 24.80 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:36<00:05, 21.86it/s, est. speed input: 25380.56 toks/s, output: 24.79 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:37<00:04, 22.24it/s, est. speed input: 25365.43 toks/s, output: 24.77 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:37<00:04, 22.53it/s, est. speed input: 25350.90 toks/s, output: 24.76 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:37<00:03, 22.73it/s, est. speed input: 25336.50 toks/s, output: 24.74 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:38<00:00, 63.32it/s, est. speed input: 26343.88 toks/s, output: 25.73 toks/s]
Processed prompts:  97%|█████████▋| 991/1024 [00:38<00:00, 54.45it/s, est. speed input: 26454.27 toks/s, output: 25.83 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:39<00:00, 35.28it/s, est. speed input: 26275.80 toks/s, output: 25.66 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:39<00:00, 32.10it/s, est. speed input: 26253.55 toks/s, output: 25.64 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:39<00:00, 30.66it/s, est. speed input: 26258.62 toks/s, output: 25.64 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:39<00:00, 30.66it/s, est. speed input: 26413.22 toks/s, output: 25.79 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:39<00:00, 25.79it/s, est. speed input: 26413.22 toks/s, output: 25.79 toks/s]
[rank0]:[W126 00:43:20.847356498 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 00:43:24
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 00:43:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 00:43:49 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=609891) WARNING 01-26 00:43:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=609891) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=609891) WARNING 01-26 00:44:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 10.48 requests/s, 10743.04 total tokens/s, 10.48 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 00:43:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:43:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:43:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:43:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:43:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:43:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:43:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:43:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 00:43:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:43:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:43:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:43:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:43:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:43:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:43:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:43:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:43:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=609891) [2026-01-26 00:43:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=609891) [2026-01-26 00:43:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=609891) [2026-01-26 00:43:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=609891) [2026-01-26 00:43:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=609891) [2026-01-26 00:43:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=609891) [2026-01-26 00:43:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=609891) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=609891) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.04s/it]
(EngineCore_DP0 pid=609891) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.36s/it]
(EngineCore_DP0 pid=609891) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.31s/it]
(EngineCore_DP0 pid=609891) 
(EngineCore_DP0 pid=609891) [2026-01-26 00:44:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=609891) [2026-01-26 00:44:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=609891) [2026-01-26 00:44:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=609891) [2026-01-26 00:44:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=609891) [2026-01-26 00:44:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=609891) [2026-01-26 00:44:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=609891) [2026-01-26 00:44:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=609891) [2026-01-26 00:44:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=609891) [rank0]:W0126 00:44:19.598000 609891 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=609891) [rank0]:W0126 00:44:19.716000 609891 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=609891) [rank0]:W0126 00:44:20.095000 609891 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=609891) [rank0]:W0126 00:44:20.284000 609891 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=609891) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:03,  1.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:02,  2.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:01<00:02,  2.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:01<00:01,  2.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:01<00:00,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  4.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:02<00:00,  4.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:02<00:00,  3.37it/s]
(EngineCore_DP0 pid=609891) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.93it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  7.90it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.20it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.17it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 22/2048 [00:00<00:09, 219.12it/s]
Adding requests:   2%|▏         | 48/2048 [00:00<00:08, 238.60it/s]
Adding requests:   4%|▎         | 74/2048 [00:00<00:08, 245.64it/s]
Adding requests:   5%|▍         | 99/2048 [00:00<00:07, 246.87it/s]
Adding requests:   6%|▌         | 125/2048 [00:00<00:07, 251.43it/s]
Adding requests:   7%|▋         | 151/2048 [00:00<00:07, 244.87it/s]
Adding requests:   9%|▊         | 177/2048 [00:00<00:07, 247.09it/s]
Adding requests:  10%|▉         | 204/2048 [00:00<00:07, 251.13it/s]
Adding requests:  11%|█▏        | 231/2048 [00:00<00:07, 255.29it/s]
Adding requests:  13%|█▎        | 257/2048 [00:01<00:07, 251.63it/s]
Adding requests:  14%|█▍        | 283/2048 [00:01<00:07, 251.07it/s]
Adding requests:  15%|█▌        | 309/2048 [00:01<00:06, 252.72it/s]
Adding requests:  16%|█▋        | 335/2048 [00:01<00:06, 252.89it/s]
Adding requests:  18%|█▊        | 361/2048 [00:01<00:06, 248.66it/s]
Adding requests:  19%|█▉        | 386/2048 [00:01<00:06, 245.29it/s]
Adding requests:  20%|██        | 411/2048 [00:01<00:06, 241.38it/s]
Adding requests:  21%|██▏       | 436/2048 [00:01<00:06, 235.33it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:06, 229.93it/s]
Adding requests:  24%|██▎       | 485/2048 [00:01<00:06, 233.81it/s]
Adding requests:  25%|██▍       | 509/2048 [00:02<00:06, 232.32it/s]
Adding requests:  26%|██▌       | 533/2048 [00:02<00:06, 232.12it/s]
Adding requests:  27%|██▋       | 557/2048 [00:02<00:06, 227.72it/s]
Adding requests:  28%|██▊       | 580/2048 [00:02<00:06, 224.53it/s]
Adding requests:  29%|██▉       | 603/2048 [00:02<00:06, 216.81it/s]
Adding requests:  31%|███       | 626/2048 [00:02<00:06, 219.54it/s]
Adding requests:  32%|███▏      | 649/2048 [00:02<00:06, 214.92it/s]
Adding requests:  33%|███▎      | 671/2048 [00:02<00:06, 214.20it/s]
Adding requests:  34%|███▍      | 694/2048 [00:02<00:06, 216.90it/s]
Adding requests:  35%|███▌      | 717/2048 [00:03<00:06, 218.56it/s]
Adding requests:  36%|███▌      | 739/2048 [00:03<00:06, 214.68it/s]
Adding requests:  37%|███▋      | 761/2048 [00:03<00:06, 212.14it/s]
Adding requests:  38%|███▊      | 785/2048 [00:03<00:05, 218.58it/s]
Adding requests:  39%|███▉      | 807/2048 [00:03<00:05, 215.98it/s]
Adding requests:  41%|████      | 831/2048 [00:03<00:05, 220.72it/s]
Adding requests:  42%|████▏     | 854/2048 [00:03<00:05, 212.14it/s]
Adding requests:  43%|████▎     | 877/2048 [00:03<00:05, 215.08it/s]
Adding requests:  44%|████▍     | 902/2048 [00:03<00:05, 223.95it/s]
Adding requests:  45%|████▌     | 925/2048 [00:04<00:05, 221.92it/s]
Adding requests:  46%|████▋     | 948/2048 [00:04<00:04, 222.98it/s]
Adding requests:  47%|████▋     | 971/2048 [00:04<00:04, 224.52it/s]
Adding requests:  49%|████▊     | 994/2048 [00:04<00:04, 221.27it/s]
Adding requests:  50%|████▉     | 1017/2048 [00:04<00:04, 223.75it/s]
Adding requests:  51%|█████     | 1041/2048 [00:04<00:04, 224.23it/s]
Adding requests:  52%|█████▏    | 1064/2048 [00:04<00:04, 221.40it/s]
Adding requests:  53%|█████▎    | 1087/2048 [00:04<00:04, 218.73it/s]
Adding requests:  54%|█████▍    | 1109/2048 [00:04<00:04, 217.80it/s]
Adding requests:  55%|█████▌    | 1133/2048 [00:04<00:04, 223.88it/s]
Adding requests:  56%|█████▋    | 1156/2048 [00:05<00:04, 216.22it/s]
Adding requests:  58%|█████▊    | 1178/2048 [00:05<00:04, 216.31it/s]
Adding requests:  59%|█████▊    | 1200/2048 [00:05<00:03, 214.74it/s]
Adding requests:  60%|█████▉    | 1225/2048 [00:05<00:03, 222.76it/s]
Adding requests:  61%|██████    | 1248/2048 [00:05<00:03, 219.04it/s]
Adding requests:  62%|██████▏   | 1270/2048 [00:05<00:03, 218.30it/s]
Adding requests:  63%|██████▎   | 1292/2048 [00:05<00:03, 213.55it/s]
Adding requests:  64%|██████▍   | 1315/2048 [00:05<00:03, 216.30it/s]
Adding requests:  65%|██████▌   | 1338/2048 [00:05<00:03, 217.76it/s]
Adding requests:  67%|██████▋   | 1362/2048 [00:05<00:03, 223.04it/s]
Adding requests:  68%|██████▊   | 1385/2048 [00:06<00:02, 221.27it/s]
Adding requests:  69%|██████▉   | 1408/2048 [00:06<00:02, 220.30it/s]
Adding requests:  70%|██████▉   | 1431/2048 [00:06<00:02, 219.21it/s]
Adding requests:  71%|███████   | 1454/2048 [00:06<00:02, 221.41it/s]
Adding requests:  72%|███████▏  | 1477/2048 [00:06<00:02, 223.59it/s]
Adding requests:  73%|███████▎  | 1500/2048 [00:06<00:02, 223.75it/s]
Adding requests:  74%|███████▍  | 1523/2048 [00:06<00:02, 223.66it/s]
Adding requests:  75%|███████▌  | 1546/2048 [00:06<00:02, 216.76it/s]
Adding requests:  77%|███████▋  | 1568/2048 [00:06<00:02, 212.13it/s]
Adding requests:  78%|███████▊  | 1592/2048 [00:07<00:02, 219.06it/s]
Adding requests:  79%|███████▉  | 1616/2048 [00:07<00:01, 224.65it/s]
Adding requests:  80%|████████  | 1639/2048 [00:07<00:01, 225.97it/s]
Adding requests:  81%|████████  | 1662/2048 [00:07<00:01, 225.00it/s]
Adding requests:  82%|████████▏ | 1687/2048 [00:07<00:01, 231.76it/s]
Adding requests:  84%|████████▎ | 1714/2048 [00:07<00:01, 240.99it/s]
Adding requests:  85%|████████▍ | 1739/2048 [00:07<00:01, 242.31it/s]
Adding requests:  86%|████████▌ | 1766/2048 [00:07<00:01, 249.36it/s]
Adding requests:  87%|████████▋ | 1791/2048 [00:07<00:01, 245.02it/s]
Adding requests:  89%|████████▊ | 1816/2048 [00:07<00:00, 243.07it/s]
Adding requests:  90%|████████▉ | 1841/2048 [00:08<00:00, 241.92it/s]
Adding requests:  91%|█████████ | 1866/2048 [00:08<00:00, 241.83it/s]
Adding requests:  92%|█████████▏| 1891/2048 [00:08<00:00, 242.14it/s]
Adding requests:  94%|█████████▎| 1916/2048 [00:08<00:00, 241.44it/s]
Adding requests:  95%|█████████▍| 1941/2048 [00:08<00:00, 242.62it/s]
Adding requests:  96%|█████████▌| 1966/2048 [00:08<00:00, 241.62it/s]
Adding requests:  97%|█████████▋| 1991/2048 [00:08<00:00, 240.70it/s]
Adding requests:  98%|█████████▊| 2016/2048 [00:08<00:00, 237.01it/s]
Adding requests: 100%|█████████▉| 2040/2048 [00:08<00:00, 227.46it/s]
Adding requests: 100%|██████████| 2048/2048 [00:08<00:00, 228.78it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 82/2048 [00:00<00:16, 121.82it/s, est. speed input: 124753.17 toks/s, output: 121.83 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:02<00:54, 36.01it/s, est. speed input: 44800.32 toks/s, output: 43.75 toks/s]   
Processed prompts:   6%|▌         | 114/2048 [00:03<01:25, 22.51it/s, est. speed input: 30667.00 toks/s, output: 29.95 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:05<01:51, 17.24it/s, est. speed input: 24769.14 toks/s, output: 24.19 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:06<02:07, 14.94it/s, est. speed input: 21857.18 toks/s, output: 21.34 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:08<02:18, 13.63it/s, est. speed input: 20019.56 toks/s, output: 19.55 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:09<02:29, 12.54it/s, est. speed input: 18560.55 toks/s, output: 18.13 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:11<01:58, 15.54it/s, est. speed input: 19083.01 toks/s, output: 18.64 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:12<02:12, 13.80it/s, est. speed input: 18028.32 toks/s, output: 17.61 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:14<02:21, 12.75it/s, est. speed input: 17246.57 toks/s, output: 16.84 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:15<02:26, 12.24it/s, est. speed input: 16703.63 toks/s, output: 16.31 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:17<02:30, 11.78it/s, est. speed input: 16210.48 toks/s, output: 15.83 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:18<02:35, 11.28it/s, est. speed input: 15730.84 toks/s, output: 15.36 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:20<02:39, 10.93it/s, est. speed input: 15319.04 toks/s, output: 14.96 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:22<02:41, 10.71it/s, est. speed input: 14971.78 toks/s, output: 14.62 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:23<02:40, 10.69it/s, est. speed input: 14711.27 toks/s, output: 14.37 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:24<02:37, 10.76it/s, est. speed input: 14505.06 toks/s, output: 14.17 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:26<02:38, 10.58it/s, est. speed input: 14264.57 toks/s, output: 13.93 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:28<02:38, 10.46it/s, est. speed input: 14050.95 toks/s, output: 13.72 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:29<02:38, 10.38it/s, est. speed input: 13860.02 toks/s, output: 13.54 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:31<02:35, 10.49it/s, est. speed input: 13725.22 toks/s, output: 13.40 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:32<02:31, 10.65it/s, est. speed input: 13617.48 toks/s, output: 13.30 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:34<02:31, 10.54it/s, est. speed input: 13477.64 toks/s, output: 13.16 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:35<02:31, 10.43it/s, est. speed input: 13343.89 toks/s, output: 13.03 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:37<02:31, 10.36it/s, est. speed input: 13221.23 toks/s, output: 12.91 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:38<02:27, 10.53it/s, est. speed input: 13145.07 toks/s, output: 12.84 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:40<02:23, 10.67it/s, est. speed input: 13078.56 toks/s, output: 12.77 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:41<02:22, 10.63it/s, est. speed input: 12994.61 toks/s, output: 12.69 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:43<01:46, 13.89it/s, est. speed input: 13308.22 toks/s, output: 13.00 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:44<01:55, 12.74it/s, est. speed input: 13207.26 toks/s, output: 12.90 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:46<02:00, 12.05it/s, est. speed input: 13124.65 toks/s, output: 12.82 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:47<02:02, 11.76it/s, est. speed input: 13069.19 toks/s, output: 12.76 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:49<02:03, 11.47it/s, est. speed input: 13008.79 toks/s, output: 12.70 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:50<02:07, 11.07it/s, est. speed input: 12928.81 toks/s, output: 12.63 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:52<02:08, 10.80it/s, est. speed input: 12853.72 toks/s, output: 12.55 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:53<02:09, 10.61it/s, est. speed input: 12783.07 toks/s, output: 12.48 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:55<02:08, 10.55it/s, est. speed input: 12724.02 toks/s, output: 12.43 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:56<02:05, 10.67it/s, est. speed input: 12685.99 toks/s, output: 12.39 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:58<02:06, 10.52it/s, est. speed input: 12625.30 toks/s, output: 12.33 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [01:00<02:05, 10.42it/s, est. speed input: 12567.71 toks/s, output: 12.27 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [01:01<02:05, 10.34it/s, est. speed input: 12513.10 toks/s, output: 12.22 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [01:03<02:01, 10.50it/s, est. speed input: 12481.74 toks/s, output: 12.19 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [01:04<01:53, 11.09it/s, est. speed input: 12493.04 toks/s, output: 12.20 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [01:05<01:54, 10.89it/s, est. speed input: 12451.48 toks/s, output: 12.16 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [01:07<01:55, 10.67it/s, est. speed input: 12404.33 toks/s, output: 12.11 toks/s]
Processed prompts:  41%|████      | 834/2048 [01:09<01:55, 10.52it/s, est. speed input: 12359.25 toks/s, output: 12.07 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [01:10<01:53, 10.55it/s, est. speed input: 12327.78 toks/s, output: 12.04 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [01:12<01:50, 10.69it/s, est. speed input: 12306.91 toks/s, output: 12.02 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [01:13<01:18, 14.62it/s, est. speed input: 12544.33 toks/s, output: 12.25 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [01:14<01:25, 13.29it/s, est. speed input: 12505.44 toks/s, output: 12.21 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [01:16<01:30, 12.39it/s, est. speed input: 12468.06 toks/s, output: 12.18 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:17<01:33, 11.78it/s, est. speed input: 12432.21 toks/s, output: 12.14 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:19<01:34, 11.46it/s, est. speed input: 12404.64 toks/s, output: 12.11 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:20<01:34, 11.33it/s, est. speed input: 12384.67 toks/s, output: 12.09 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:22<01:35, 10.99it/s, est. speed input: 12348.74 toks/s, output: 12.06 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:23<01:36, 10.74it/s, est. speed input: 12312.72 toks/s, output: 12.02 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:25<01:36, 10.57it/s, est. speed input: 12277.92 toks/s, output: 11.99 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:27<01:36, 10.45it/s, est. speed input: 12244.46 toks/s, output: 11.96 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:28<01:33, 10.61it/s, est. speed input: 12228.58 toks/s, output: 11.94 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:30<01:32, 10.55it/s, est. speed input: 12201.85 toks/s, output: 11.92 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:31<01:31, 10.44it/s, est. speed input: 12171.28 toks/s, output: 11.89 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:33<01:30, 10.36it/s, est. speed input: 12141.82 toks/s, output: 11.86 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:34<01:29, 10.36it/s, est. speed input: 12116.96 toks/s, output: 11.83 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:36<01:26, 10.55it/s, est. speed input: 12104.48 toks/s, output: 11.82 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:37<01:24, 10.57it/s, est. speed input: 12085.52 toks/s, output: 11.80 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:39<01:24, 10.45it/s, est. speed input: 12059.19 toks/s, output: 11.78 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:40<01:23, 10.37it/s, est. speed input: 12033.65 toks/s, output: 11.75 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:42<01:21, 10.37it/s, est. speed input: 12012.29 toks/s, output: 11.73 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:43<00:57, 14.17it/s, est. speed input: 12178.56 toks/s, output: 11.89 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:45<01:00, 13.21it/s, est. speed input: 12165.16 toks/s, output: 11.88 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:46<01:03, 12.25it/s, est. speed input: 12139.53 toks/s, output: 11.86 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:48<01:05, 11.61it/s, est. speed input: 12114.66 toks/s, output: 11.83 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:49<01:07, 11.17it/s, est. speed input: 12090.46 toks/s, output: 11.81 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:51<01:06, 11.04it/s, est. speed input: 12075.45 toks/s, output: 11.79 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:52<01:05, 11.03it/s, est. speed input: 12065.38 toks/s, output: 11.78 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:54<01:04, 10.81it/s, est. speed input: 12044.98 toks/s, output: 11.76 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:56<01:04, 10.62it/s, est. speed input: 12022.97 toks/s, output: 11.74 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:57<01:03, 10.48it/s, est. speed input: 12001.56 toks/s, output: 11.72 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:59<01:02, 10.48it/s, est. speed input: 11985.26 toks/s, output: 11.70 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [02:00<00:59, 10.64it/s, est. speed input: 11976.89 toks/s, output: 11.70 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [02:02<00:58, 10.60it/s, est. speed input: 11961.89 toks/s, output: 11.68 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [02:03<00:57, 10.47it/s, est. speed input: 11942.30 toks/s, output: 11.66 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [02:05<00:56, 10.38it/s, est. speed input: 11923.22 toks/s, output: 11.64 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [02:06<00:55, 10.37it/s, est. speed input: 11907.08 toks/s, output: 11.63 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [02:08<00:52, 10.56it/s, est. speed input: 11900.08 toks/s, output: 11.62 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [02:09<00:51, 10.61it/s, est. speed input: 11889.67 toks/s, output: 11.61 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [02:11<00:50, 10.48it/s, est. speed input: 11872.05 toks/s, output: 11.59 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [02:12<00:49, 10.39it/s, est. speed input: 11854.88 toks/s, output: 11.58 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [02:14<00:47, 10.32it/s, est. speed input: 11838.08 toks/s, output: 11.56 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [02:15<00:32, 14.18it/s, est. speed input: 11968.48 toks/s, output: 11.69 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [02:17<00:33, 13.24it/s, est. speed input: 11961.27 toks/s, output: 11.68 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [02:18<00:33, 12.84it/s, est. speed input: 11962.81 toks/s, output: 11.68 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [02:20<00:34, 11.99it/s, est. speed input: 11945.48 toks/s, output: 11.67 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [02:21<00:34, 11.42it/s, est. speed input: 11928.56 toks/s, output: 11.65 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [02:23<00:34, 11.08it/s, est. speed input: 11913.90 toks/s, output: 11.63 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [02:24<00:33, 11.07it/s, est. speed input: 11907.64 toks/s, output: 11.63 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [02:26<00:32, 10.90it/s, est. speed input: 11895.80 toks/s, output: 11.62 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [02:27<00:31, 10.68it/s, est. speed input: 11880.12 toks/s, output: 11.60 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [02:29<00:30, 10.52it/s, est. speed input: 11864.77 toks/s, output: 11.59 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [02:30<00:28, 10.44it/s, est. speed input: 11850.66 toks/s, output: 11.57 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [02:32<00:26, 10.61it/s, est. speed input: 11845.34 toks/s, output: 11.57 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [02:33<00:25, 10.65it/s, est. speed input: 11837.10 toks/s, output: 11.56 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [02:35<00:24, 10.50it/s, est. speed input: 11822.80 toks/s, output: 11.55 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [02:36<00:22, 10.40it/s, est. speed input: 11808.79 toks/s, output: 11.53 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [02:38<00:21, 10.33it/s, est. speed input: 11795.02 toks/s, output: 11.52 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [02:39<00:19, 10.53it/s, est. speed input: 11790.33 toks/s, output: 11.51 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [02:41<00:17, 10.66it/s, est. speed input: 11785.24 toks/s, output: 11.51 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [02:43<00:16, 10.51it/s, est. speed input: 11772.11 toks/s, output: 11.50 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [02:44<00:15, 10.41it/s, est. speed input: 11759.23 toks/s, output: 11.48 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [02:46<00:09, 13.46it/s, est. speed input: 11845.70 toks/s, output: 11.57 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [02:47<00:08, 12.64it/s, est. speed input: 11837.62 toks/s, output: 11.56 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [02:49<00:07, 12.18it/s, est. speed input: 11832.92 toks/s, output: 11.56 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [02:50<00:06, 11.61it/s, est. speed input: 11821.44 toks/s, output: 11.54 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [02:52<00:05, 11.17it/s, est. speed input: 11808.64 toks/s, output: 11.53 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [02:53<00:04, 10.87it/s, est. speed input: 11796.09 toks/s, output: 11.52 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [02:55<00:02, 10.69it/s, est. speed input: 11784.80 toks/s, output: 11.51 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [02:56<00:01, 11.25it/s, est. speed input: 11794.74 toks/s, output: 11.52 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:56<00:00, 11.25it/s, est. speed input: 11875.90 toks/s, output: 11.60 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:56<00:00, 11.60it/s, est. speed input: 11875.90 toks/s, output: 11.60 toks/s]
[rank0]:[W126 00:47:36.498821457 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 00:47:40
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 00:48:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 00:48:20 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=613648) WARNING 01-26 00:48:28 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=613648) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=613648) WARNING 01-26 00:48:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 5.22 requests/s, 5354.36 total tokens/s, 5.22 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 00:48:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:48:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:48:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:48:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:48:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:48:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:48:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:48:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 00:48:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 00:48:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:48:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 00:48:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 00:48:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 00:48:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 00:48:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 00:48:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 00:48:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=613648) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=613648) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.09s/it]
(EngineCore_DP0 pid=613648) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.39s/it]
(EngineCore_DP0 pid=613648) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.35s/it]
(EngineCore_DP0 pid=613648) 
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=613648) [2026-01-26 00:48:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=613648) [rank0]:W0126 00:48:52.219000 613648 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=613648) [rank0]:W0126 00:48:52.338000 613648 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=613648) [rank0]:W0126 00:48:54.509000 613648 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=613648) [rank0]:W0126 00:48:54.698000 613648 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=613648) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:07,  1.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:01<00:04,  1.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:01<00:04,  1.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:01<00:02,  2.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:02<00:01,  3.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:02<00:01,  4.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:02<00:00,  5.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:02<00:00,  5.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:02<00:00,  6.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:02<00:00,  6.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:02<00:00,  6.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:02<00:00,  3.97it/s]
(EngineCore_DP0 pid=613648) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  6.82it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  7.55it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.07it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  6.78it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:01<00:00,  3.31it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:01<00:00,  4.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  4.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  4.89it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 23/4096 [00:00<00:17, 228.17it/s]
Adding requests:   1%|          | 49/4096 [00:00<00:16, 243.66it/s]
Adding requests:   2%|▏         | 76/4096 [00:00<00:15, 252.33it/s]
Adding requests:   2%|▏         | 102/4096 [00:00<00:15, 252.65it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:15, 252.08it/s]
Adding requests:   4%|▍         | 154/4096 [00:00<00:15, 251.49it/s]
Adding requests:   4%|▍         | 182/4096 [00:00<00:15, 259.69it/s]
Adding requests:   5%|▌         | 208/4096 [00:00<00:14, 259.66it/s]
Adding requests:   6%|▌         | 234/4096 [00:00<00:15, 251.28it/s]
Adding requests:   6%|▋         | 260/4096 [00:01<00:15, 241.34it/s]
Adding requests:   7%|▋         | 285/4096 [00:01<00:15, 241.71it/s]
Adding requests:   8%|▊         | 310/4096 [00:01<00:15, 243.53it/s]
Adding requests:   8%|▊         | 336/4096 [00:01<00:15, 247.22it/s]
Adding requests:   9%|▉         | 361/4096 [00:01<00:15, 243.59it/s]
Adding requests:   9%|▉         | 386/4096 [00:01<00:15, 245.04it/s]
Adding requests:  10%|█         | 413/4096 [00:01<00:14, 249.77it/s]
Adding requests:  11%|█         | 439/4096 [00:01<00:15, 238.60it/s]
Adding requests:  11%|█▏        | 463/4096 [00:01<00:16, 224.18it/s]
Adding requests:  12%|█▏        | 491/4096 [00:02<00:15, 236.51it/s]
Adding requests:  13%|█▎        | 515/4096 [00:02<00:15, 232.58it/s]
Adding requests:  13%|█▎        | 541/4096 [00:02<00:15, 236.69it/s]
Adding requests:  14%|█▍        | 565/4096 [00:02<00:15, 233.30it/s]
Adding requests:  14%|█▍        | 589/4096 [00:02<00:15, 231.10it/s]
Adding requests:  15%|█▍        | 614/4096 [00:02<00:14, 235.82it/s]
Adding requests:  16%|█▌        | 640/4096 [00:02<00:14, 240.52it/s]
Adding requests:  16%|█▌        | 665/4096 [00:02<00:15, 225.60it/s]
Adding requests:  17%|█▋        | 690/4096 [00:02<00:14, 231.85it/s]
Adding requests:  17%|█▋        | 714/4096 [00:02<00:14, 234.06it/s]
Adding requests:  18%|█▊        | 738/4096 [00:03<00:14, 234.34it/s]
Adding requests:  19%|█▊        | 762/4096 [00:03<00:15, 216.90it/s]
Adding requests:  19%|█▉        | 785/4096 [00:03<00:15, 219.30it/s]
Adding requests:  20%|█▉        | 809/4096 [00:03<00:14, 222.88it/s]
Adding requests:  20%|██        | 836/4096 [00:03<00:13, 235.69it/s]
Adding requests:  21%|██        | 860/4096 [00:03<00:14, 224.41it/s]
Adding requests:  22%|██▏       | 883/4096 [00:03<00:14, 221.30it/s]
Adding requests:  22%|██▏       | 908/4096 [00:03<00:13, 228.17it/s]
Adding requests:  23%|██▎       | 931/4096 [00:03<00:13, 228.17it/s]
Adding requests:  23%|██▎       | 954/4096 [00:04<00:13, 226.04it/s]
Adding requests:  24%|██▍       | 977/4096 [00:04<00:14, 216.22it/s]
Adding requests:  24%|██▍       | 999/4096 [00:04<00:14, 217.17it/s]
Adding requests:  25%|██▍       | 1023/4096 [00:04<00:13, 223.04it/s]
Adding requests:  26%|██▌       | 1046/4096 [00:04<00:13, 219.75it/s]
Adding requests:  26%|██▌       | 1069/4096 [00:04<00:14, 210.99it/s]
Adding requests:  27%|██▋       | 1092/4096 [00:04<00:13, 214.99it/s]
Adding requests:  27%|██▋       | 1117/4096 [00:04<00:13, 222.60it/s]
Adding requests:  28%|██▊       | 1142/4096 [00:04<00:12, 228.97it/s]
Adding requests:  28%|██▊       | 1165/4096 [00:05<00:13, 210.92it/s]
Adding requests:  29%|██▉       | 1188/4096 [00:05<00:13, 214.86it/s]
Adding requests:  30%|██▉       | 1211/4096 [00:05<00:13, 218.45it/s]
Adding requests:  30%|███       | 1235/4096 [00:05<00:12, 221.03it/s]
Adding requests:  31%|███       | 1258/4096 [00:05<00:13, 213.68it/s]
Adding requests:  31%|███▏      | 1280/4096 [00:05<00:13, 213.82it/s]
Adding requests:  32%|███▏      | 1303/4096 [00:05<00:12, 218.06it/s]
Adding requests:  32%|███▏      | 1328/4096 [00:05<00:12, 225.25it/s]
Adding requests:  33%|███▎      | 1351/4096 [00:05<00:12, 220.99it/s]
Adding requests:  34%|███▎      | 1374/4096 [00:05<00:12, 221.48it/s]
Adding requests:  34%|███▍      | 1399/4096 [00:06<00:11, 229.65it/s]
Adding requests:  35%|███▍      | 1424/4096 [00:06<00:11, 233.33it/s]
Adding requests:  35%|███▌      | 1448/4096 [00:06<00:11, 228.75it/s]
Adding requests:  36%|███▌      | 1471/4096 [00:06<00:11, 223.19it/s]
Adding requests:  37%|███▋      | 1497/4096 [00:06<00:11, 232.61it/s]
Adding requests:  37%|███▋      | 1523/4096 [00:06<00:10, 237.91it/s]
Adding requests:  38%|███▊      | 1547/4096 [00:06<00:10, 234.73it/s]
Adding requests:  38%|███▊      | 1571/4096 [00:06<00:11, 213.74it/s]
Adding requests:  39%|███▉      | 1596/4096 [00:06<00:11, 220.89it/s]
Adding requests:  40%|███▉      | 1619/4096 [00:07<00:11, 221.90it/s]
Adding requests:  40%|████      | 1642/4096 [00:07<00:11, 219.38it/s]
Adding requests:  41%|████      | 1665/4096 [00:07<00:11, 208.51it/s]
Adding requests:  41%|████▏     | 1693/4096 [00:07<00:10, 226.12it/s]
Adding requests:  42%|████▏     | 1720/4096 [00:07<00:09, 237.63it/s]
Adding requests:  43%|████▎     | 1745/4096 [00:07<00:09, 240.85it/s]
Adding requests:  43%|████▎     | 1770/4096 [00:07<00:09, 237.33it/s]
Adding requests:  44%|████▍     | 1795/4096 [00:07<00:09, 240.77it/s]
Adding requests:  44%|████▍     | 1820/4096 [00:07<00:09, 242.52it/s]
Adding requests:  45%|████▌     | 1847/4096 [00:07<00:09, 248.79it/s]
Adding requests:  46%|████▌     | 1872/4096 [00:08<00:09, 240.81it/s]
Adding requests:  46%|████▋     | 1898/4096 [00:08<00:08, 245.37it/s]
Adding requests:  47%|████▋     | 1926/4096 [00:08<00:08, 255.00it/s]
Adding requests:  48%|████▊     | 1954/4096 [00:08<00:08, 260.78it/s]
Adding requests:  48%|████▊     | 1981/4096 [00:08<00:08, 245.44it/s]
Adding requests:  49%|████▉     | 2006/4096 [00:08<00:08, 245.96it/s]
Adding requests:  50%|████▉     | 2031/4096 [00:08<00:08, 245.23it/s]
Adding requests:  50%|█████     | 2056/4096 [00:08<00:08, 241.06it/s]
Adding requests:  51%|█████     | 2081/4096 [00:08<00:09, 222.99it/s]
Adding requests:  51%|█████▏    | 2108/4096 [00:09<00:08, 233.45it/s]
Adding requests:  52%|█████▏    | 2132/4096 [00:09<00:08, 231.69it/s]
Adding requests:  53%|█████▎    | 2156/4096 [00:09<00:08, 231.72it/s]
Adding requests:  53%|█████▎    | 2180/4096 [00:09<00:08, 221.72it/s]
Adding requests:  54%|█████▍    | 2204/4096 [00:09<00:08, 225.10it/s]
Adding requests:  54%|█████▍    | 2229/4096 [00:09<00:08, 229.87it/s]
Adding requests:  55%|█████▌    | 2255/4096 [00:09<00:07, 237.82it/s]
Adding requests:  56%|█████▌    | 2279/4096 [00:09<00:08, 223.05it/s]
Adding requests:  56%|█████▋    | 2304/4096 [00:09<00:07, 229.67it/s]
Adding requests:  57%|█████▋    | 2330/4096 [00:10<00:07, 235.28it/s]
Adding requests:  58%|█████▊    | 2357/4096 [00:10<00:07, 241.87it/s]
Adding requests:  58%|█████▊    | 2382/4096 [00:10<00:07, 233.78it/s]
Adding requests:  59%|█████▊    | 2406/4096 [00:10<00:07, 230.95it/s]
Adding requests:  59%|█████▉    | 2432/4096 [00:10<00:06, 238.51it/s]
Adding requests:  60%|█████▉    | 2456/4096 [00:10<00:07, 228.82it/s]
Adding requests:  61%|██████    | 2480/4096 [00:10<00:07, 219.75it/s]
Adding requests:  61%|██████    | 2504/4096 [00:10<00:07, 223.09it/s]
Adding requests:  62%|██████▏   | 2531/4096 [00:10<00:06, 235.87it/s]
Adding requests:  62%|██████▏   | 2559/4096 [00:11<00:06, 246.29it/s]
Adding requests:  63%|██████▎   | 2584/4096 [00:11<00:06, 236.51it/s]
Adding requests:  64%|██████▎   | 2608/4096 [00:11<00:06, 227.00it/s]
Adding requests:  64%|██████▍   | 2632/4096 [00:11<00:06, 229.50it/s]
Adding requests:  65%|██████▍   | 2657/4096 [00:11<00:06, 234.82it/s]
Adding requests:  65%|██████▌   | 2681/4096 [00:11<00:06, 226.14it/s]
Adding requests:  66%|██████▌   | 2704/4096 [00:11<00:06, 217.59it/s]
Adding requests:  67%|██████▋   | 2728/4096 [00:11<00:06, 222.60it/s]
Adding requests:  67%|██████▋   | 2754/4096 [00:11<00:05, 233.15it/s]
Adding requests:  68%|██████▊   | 2778/4096 [00:11<00:05, 230.21it/s]
Adding requests:  68%|██████▊   | 2802/4096 [00:12<00:05, 232.64it/s]
Adding requests:  69%|██████▉   | 2828/4096 [00:12<00:05, 239.96it/s]
Adding requests:  70%|██████▉   | 2855/4096 [00:12<00:05, 247.99it/s]
Adding requests:  70%|███████   | 2881/4096 [00:12<00:04, 247.96it/s]
Adding requests:  71%|███████   | 2906/4096 [00:12<00:05, 235.54it/s]
Adding requests:  72%|███████▏  | 2931/4096 [00:12<00:04, 236.99it/s]
Adding requests:  72%|███████▏  | 2959/4096 [00:12<00:04, 247.27it/s]
Adding requests:  73%|███████▎  | 2984/4096 [00:12<00:04, 243.45it/s]
Adding requests:  73%|███████▎  | 3009/4096 [00:12<00:04, 232.45it/s]
Adding requests:  74%|███████▍  | 3033/4096 [00:13<00:04, 233.73it/s]
Adding requests:  75%|███████▍  | 3059/4096 [00:13<00:04, 240.06it/s]
Adding requests:  75%|███████▌  | 3085/4096 [00:13<00:04, 244.99it/s]
Adding requests:  76%|███████▌  | 3110/4096 [00:13<00:04, 233.62it/s]
Adding requests:  77%|███████▋  | 3134/4096 [00:13<00:04, 234.34it/s]
Adding requests:  77%|███████▋  | 3158/4096 [00:13<00:04, 231.64it/s]
Adding requests:  78%|███████▊  | 3183/4096 [00:13<00:03, 236.79it/s]
Adding requests:  78%|███████▊  | 3207/4096 [00:13<00:03, 230.54it/s]
Adding requests:  79%|███████▉  | 3231/4096 [00:13<00:03, 227.00it/s]
Adding requests:  79%|███████▉  | 3255/4096 [00:14<00:03, 230.55it/s]
Adding requests:  80%|████████  | 3279/4096 [00:14<00:03, 233.24it/s]
Adding requests:  81%|████████  | 3303/4096 [00:14<00:03, 234.36it/s]
Adding requests:  81%|████████  | 3327/4096 [00:14<00:03, 226.38it/s]
Adding requests:  82%|████████▏ | 3353/4096 [00:14<00:03, 234.96it/s]
Adding requests:  83%|████████▎ | 3381/4096 [00:14<00:02, 245.23it/s]
Adding requests:  83%|████████▎ | 3407/4096 [00:14<00:02, 248.03it/s]
Adding requests:  84%|████████▍ | 3432/4096 [00:14<00:02, 240.98it/s]
Adding requests:  84%|████████▍ | 3459/4096 [00:14<00:02, 248.46it/s]
Adding requests:  85%|████████▌ | 3486/4096 [00:14<00:02, 251.29it/s]
Adding requests:  86%|████████▌ | 3513/4096 [00:15<00:02, 255.78it/s]
Adding requests:  86%|████████▋ | 3539/4096 [00:15<00:02, 249.51it/s]
Adding requests:  87%|████████▋ | 3567/4096 [00:15<00:02, 258.21it/s]
Adding requests:  88%|████████▊ | 3595/4096 [00:15<00:01, 262.53it/s]
Adding requests:  88%|████████▊ | 3622/4096 [00:15<00:01, 261.85it/s]
Adding requests:  89%|████████▉ | 3649/4096 [00:15<00:01, 249.58it/s]
Adding requests:  90%|████████▉ | 3676/4096 [00:15<00:01, 252.96it/s]
Adding requests:  90%|█████████ | 3705/4096 [00:15<00:01, 263.33it/s]
Adding requests:  91%|█████████ | 3732/4096 [00:15<00:01, 255.98it/s]
Adding requests:  92%|█████████▏| 3758/4096 [00:16<00:01, 240.86it/s]
Adding requests:  92%|█████████▏| 3783/4096 [00:16<00:01, 242.21it/s]
Adding requests:  93%|█████████▎| 3808/4096 [00:16<00:01, 240.27it/s]
Adding requests:  94%|█████████▎| 3833/4096 [00:16<00:01, 222.39it/s]
Adding requests:  94%|█████████▍| 3857/4096 [00:16<00:01, 226.55it/s]
Adding requests:  95%|█████████▍| 3884/4096 [00:16<00:00, 236.95it/s]
Adding requests:  95%|█████████▌| 3910/4096 [00:16<00:00, 241.79it/s]
Adding requests:  96%|█████████▌| 3935/4096 [00:16<00:00, 229.99it/s]
Adding requests:  97%|█████████▋| 3959/4096 [00:16<00:00, 230.23it/s]
Adding requests:  97%|█████████▋| 3983/4096 [00:16<00:00, 232.32it/s]
Adding requests:  98%|█████████▊| 4008/4096 [00:17<00:00, 235.11it/s]
Adding requests:  98%|█████████▊| 4032/4096 [00:17<00:00, 221.69it/s]
Adding requests:  99%|█████████▉| 4055/4096 [00:17<00:00, 215.99it/s]
Adding requests: 100%|█████████▉| 4079/4096 [00:17<00:00, 222.23it/s]
Adding requests: 100%|██████████| 4096/4096 [00:17<00:00, 233.90it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 66/4096 [00:02<02:42, 24.78it/s, est. speed input: 25377.77 toks/s, output: 24.78 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:06<05:10, 12.89it/s, est. speed input: 14614.62 toks/s, output: 14.27 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:13<07:58,  8.29it/s, est. speed input: 10172.32 toks/s, output: 9.93 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:19<09:29,  6.90it/s, est. speed input: 8645.65 toks/s, output: 8.44 toks/s] 
Processed prompts:   5%|▍         | 194/4096 [00:25<10:22,  6.26it/s, est. speed input: 7864.84 toks/s, output: 7.68 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:31<10:54,  5.91it/s, est. speed input: 7387.96 toks/s, output: 7.21 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:35<10:18,  6.21it/s, est. speed input: 7356.96 toks/s, output: 7.18 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:41<10:44,  5.91it/s, est. speed input: 7085.50 toks/s, output: 6.92 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:47<11:03,  5.69it/s, est. speed input: 6869.63 toks/s, output: 6.71 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:54<11:17,  5.52it/s, est. speed input: 6690.69 toks/s, output: 6.53 toks/s]
Processed prompts:   9%|▉         | 386/4096 [01:00<11:25,  5.41it/s, est. speed input: 6548.14 toks/s, output: 6.39 toks/s]
Processed prompts:  10%|█         | 418/4096 [01:06<11:24,  5.37it/s, est. speed input: 6444.29 toks/s, output: 6.29 toks/s]
Processed prompts:  11%|█         | 450/4096 [01:10<10:30,  5.78it/s, est. speed input: 6492.19 toks/s, output: 6.34 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [01:17<10:49,  5.57it/s, est. speed input: 6391.56 toks/s, output: 6.24 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [01:23<10:55,  5.47it/s, est. speed input: 6317.33 toks/s, output: 6.17 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [01:29<10:56,  5.41it/s, est. speed input: 6254.59 toks/s, output: 6.11 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [01:35<10:55,  5.37it/s, est. speed input: 6201.07 toks/s, output: 6.06 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [01:40<10:04,  5.77it/s, est. speed input: 6244.08 toks/s, output: 6.10 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [01:46<10:15,  5.61it/s, est. speed input: 6195.39 toks/s, output: 6.05 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [01:52<10:23,  5.49it/s, est. speed input: 6150.08 toks/s, output: 6.01 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [01:58<10:28,  5.39it/s, est. speed input: 6105.57 toks/s, output: 5.96 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [02:04<10:29,  5.33it/s, est. speed input: 6066.87 toks/s, output: 5.92 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [02:08<09:26,  5.87it/s, est. speed input: 6125.56 toks/s, output: 5.98 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [02:14<09:42,  5.65it/s, est. speed input: 6088.50 toks/s, output: 5.95 toks/s]
Processed prompts:  20%|██        | 834/4096 [02:21<09:53,  5.49it/s, est. speed input: 6052.80 toks/s, output: 5.91 toks/s]
Processed prompts:  21%|██        | 866/4096 [02:27<09:57,  5.41it/s, est. speed input: 6023.74 toks/s, output: 5.88 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [02:33<09:55,  5.37it/s, est. speed input: 5999.95 toks/s, output: 5.86 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [02:39<09:53,  5.34it/s, est. speed input: 5976.06 toks/s, output: 5.84 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [02:43<09:04,  5.76it/s, est. speed input: 6010.45 toks/s, output: 5.87 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [02:49<09:11,  5.62it/s, est. speed input: 5991.02 toks/s, output: 5.85 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [02:56<09:19,  5.49it/s, est. speed input: 5967.39 toks/s, output: 5.83 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [03:02<09:24,  5.38it/s, est. speed input: 5943.68 toks/s, output: 5.80 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [03:08<09:24,  5.33it/s, est. speed input: 5923.89 toks/s, output: 5.79 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [03:12<08:27,  5.86it/s, est. speed input: 5965.55 toks/s, output: 5.83 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [03:18<08:41,  5.64it/s, est. speed input: 5944.65 toks/s, output: 5.81 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [03:24<08:49,  5.49it/s, est. speed input: 5925.30 toks/s, output: 5.79 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [03:31<08:50,  5.42it/s, est. speed input: 5910.02 toks/s, output: 5.77 toks/s]
Processed prompts:  31%|███       | 1250/4096 [03:37<08:49,  5.38it/s, est. speed input: 5895.92 toks/s, output: 5.76 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [03:41<08:05,  5.80it/s, est. speed input: 5923.75 toks/s, output: 5.78 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [03:47<08:14,  5.63it/s, est. speed input: 5909.63 toks/s, output: 5.77 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [03:53<08:19,  5.51it/s, est. speed input: 5895.76 toks/s, output: 5.76 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [03:59<08:21,  5.42it/s, est. speed input: 5882.14 toks/s, output: 5.74 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [04:06<08:22,  5.34it/s, est. speed input: 5866.88 toks/s, output: 5.73 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [04:12<08:19,  5.31it/s, est. speed input: 5854.78 toks/s, output: 5.72 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [04:16<07:36,  5.75it/s, est. speed input: 5879.84 toks/s, output: 5.74 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [04:22<07:46,  5.55it/s, est. speed input: 5865.19 toks/s, output: 5.73 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [04:29<07:50,  5.44it/s, est. speed input: 5852.49 toks/s, output: 5.72 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [04:35<07:49,  5.39it/s, est. speed input: 5842.49 toks/s, output: 5.71 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [04:40<07:40,  5.42it/s, est. speed input: 5838.19 toks/s, output: 5.70 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [04:45<07:04,  5.80it/s, est. speed input: 5858.66 toks/s, output: 5.72 toks/s]
Processed prompts:  41%|████      | 1666/4096 [04:51<07:11,  5.63it/s, est. speed input: 5848.75 toks/s, output: 5.71 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [04:57<07:15,  5.51it/s, est. speed input: 5838.91 toks/s, output: 5.70 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [05:03<07:16,  5.42it/s, est. speed input: 5829.02 toks/s, output: 5.69 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [05:10<07:15,  5.35it/s, est. speed input: 5819.26 toks/s, output: 5.68 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [05:14<06:36,  5.80it/s, est. speed input: 5841.27 toks/s, output: 5.70 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [05:20<06:43,  5.63it/s, est. speed input: 5832.81 toks/s, output: 5.70 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [05:26<06:49,  5.46it/s, est. speed input: 5821.08 toks/s, output: 5.68 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [05:32<06:49,  5.38it/s, est. speed input: 5812.21 toks/s, output: 5.68 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [05:39<06:45,  5.36it/s, est. speed input: 5805.23 toks/s, output: 5.67 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [05:45<06:41,  5.34it/s, est. speed input: 5798.56 toks/s, output: 5.66 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [05:49<06:07,  5.74it/s, est. speed input: 5816.27 toks/s, output: 5.68 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [05:55<06:10,  5.61it/s, est. speed input: 5809.81 toks/s, output: 5.67 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [06:01<06:13,  5.48it/s, est. speed input: 5801.43 toks/s, output: 5.67 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [06:07<06:13,  5.39it/s, est. speed input: 5793.67 toks/s, output: 5.66 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [06:14<06:12,  5.32it/s, est. speed input: 5785.36 toks/s, output: 5.65 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [06:18<05:33,  5.85it/s, est. speed input: 5807.51 toks/s, output: 5.67 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [06:24<05:35,  5.71it/s, est. speed input: 5803.41 toks/s, output: 5.67 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [06:30<05:41,  5.52it/s, est. speed input: 5794.74 toks/s, output: 5.66 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [06:36<05:40,  5.44it/s, est. speed input: 5788.48 toks/s, output: 5.65 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [06:42<05:37,  5.39it/s, est. speed input: 5782.65 toks/s, output: 5.65 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [06:47<05:07,  5.83it/s, est. speed input: 5799.99 toks/s, output: 5.66 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [06:53<05:12,  5.63it/s, est. speed input: 5793.26 toks/s, output: 5.66 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [06:59<05:12,  5.52it/s, est. speed input: 5787.43 toks/s, output: 5.65 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [07:05<05:11,  5.44it/s, est. speed input: 5781.81 toks/s, output: 5.65 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [07:11<05:10,  5.36it/s, est. speed input: 5774.84 toks/s, output: 5.64 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [07:17<05:07,  5.31it/s, est. speed input: 5768.41 toks/s, output: 5.63 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [07:22<04:38,  5.74it/s, est. speed input: 5783.74 toks/s, output: 5.65 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [07:28<04:41,  5.57it/s, est. speed input: 5777.34 toks/s, output: 5.64 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [07:34<04:42,  5.43it/s, est. speed input: 5770.33 toks/s, output: 5.64 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [07:40<04:39,  5.38it/s, est. speed input: 5765.40 toks/s, output: 5.63 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [07:46<04:34,  5.36it/s, est. speed input: 5761.06 toks/s, output: 5.63 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [07:51<04:09,  5.76it/s, est. speed input: 5774.52 toks/s, output: 5.64 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [07:57<04:11,  5.59it/s, est. speed input: 5769.07 toks/s, output: 5.63 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [08:03<04:10,  5.48it/s, est. speed input: 5764.05 toks/s, output: 5.63 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [08:09<04:07,  5.42it/s, est. speed input: 5759.66 toks/s, output: 5.62 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [08:15<04:04,  5.36it/s, est. speed input: 5754.63 toks/s, output: 5.62 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [08:20<03:41,  5.78it/s, est. speed input: 5768.15 toks/s, output: 5.63 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [08:26<03:42,  5.60it/s, est. speed input: 5763.16 toks/s, output: 5.63 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [08:32<03:42,  5.47it/s, est. speed input: 5757.50 toks/s, output: 5.62 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [08:38<03:39,  5.39it/s, est. speed input: 5752.42 toks/s, output: 5.62 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [08:44<03:34,  5.35it/s, est. speed input: 5748.44 toks/s, output: 5.61 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [08:49<03:12,  5.82it/s, est. speed input: 5762.75 toks/s, output: 5.63 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [08:55<03:13,  5.60it/s, est. speed input: 5757.20 toks/s, output: 5.62 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [09:01<03:11,  5.49it/s, est. speed input: 5752.82 toks/s, output: 5.62 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [09:07<03:09,  5.40it/s, est. speed input: 5747.99 toks/s, output: 5.61 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [09:13<03:04,  5.36it/s, est. speed input: 5744.22 toks/s, output: 5.61 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [09:19<03:00,  5.30it/s, est. speed input: 5739.14 toks/s, output: 5.60 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [09:24<02:39,  5.82it/s, est. speed input: 5754.13 toks/s, output: 5.62 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [09:30<02:38,  5.65it/s, est. speed input: 5750.58 toks/s, output: 5.62 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [09:36<02:37,  5.48it/s, est. speed input: 5745.10 toks/s, output: 5.61 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [09:42<02:34,  5.39it/s, est. speed input: 5740.41 toks/s, output: 5.61 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [09:48<02:29,  5.35it/s, est. speed input: 5736.78 toks/s, output: 5.60 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [09:53<02:11,  5.82it/s, est. speed input: 5749.77 toks/s, output: 5.62 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [09:59<02:11,  5.60it/s, est. speed input: 5744.70 toks/s, output: 5.61 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [10:05<02:07,  5.49it/s, est. speed input: 5741.17 toks/s, output: 5.61 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [10:11<02:03,  5.42it/s, est. speed input: 5737.63 toks/s, output: 5.60 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [10:17<01:58,  5.37it/s, est. speed input: 5734.08 toks/s, output: 5.60 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [10:22<01:45,  5.75it/s, est. speed input: 5744.00 toks/s, output: 5.61 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [10:28<01:42,  5.60it/s, est. speed input: 5740.60 toks/s, output: 5.61 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [10:34<01:38,  5.49it/s, est. speed input: 5736.99 toks/s, output: 5.60 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [10:40<01:34,  5.38it/s, est. speed input: 5732.57 toks/s, output: 5.60 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [10:46<01:29,  5.32it/s, est. speed input: 5728.54 toks/s, output: 5.59 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [10:52<01:23,  5.31it/s, est. speed input: 5725.71 toks/s, output: 5.59 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [10:57<01:11,  5.83it/s, est. speed input: 5738.62 toks/s, output: 5.60 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [11:03<01:08,  5.61it/s, est. speed input: 5734.35 toks/s, output: 5.60 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [11:09<01:03,  5.49it/s, est. speed input: 5730.85 toks/s, output: 5.60 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [11:15<00:58,  5.42it/s, est. speed input: 5727.82 toks/s, output: 5.59 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [11:21<00:53,  5.38it/s, est. speed input: 5724.93 toks/s, output: 5.59 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [11:26<00:43,  5.78it/s, est. speed input: 5734.50 toks/s, output: 5.60 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [11:32<00:39,  5.62it/s, est. speed input: 5731.54 toks/s, output: 5.60 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [11:38<00:34,  5.55it/s, est. speed input: 5729.76 toks/s, output: 5.60 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [11:44<00:29,  5.43it/s, est. speed input: 5725.85 toks/s, output: 5.59 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [11:50<00:23,  5.37it/s, est. speed input: 5722.66 toks/s, output: 5.59 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [11:54<00:16,  5.83it/s, est. speed input: 5733.49 toks/s, output: 5.60 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [12:00<00:11,  5.64it/s, est. speed input: 5730.26 toks/s, output: 5.60 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [12:06<00:05,  5.56it/s, est. speed input: 5728.57 toks/s, output: 5.59 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [12:06<00:00,  5.56it/s, est. speed input: 5770.83 toks/s, output: 5.64 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [12:06<00:00,  5.64it/s, est. speed input: 5770.83 toks/s, output: 5.64 toks/s]
[rank0]:[W126 01:01:29.287561086 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 01:01:33
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 01:02:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 01:02:48 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=625703) WARNING 01-26 01:02:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=625703) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=625703) WARNING 01-26 01:03:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     def forward(
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     raise e
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/tmp/torchinductor_root/eq/ceqgg2b5od57a2ayhnybjtujmzypeuhpekg6vjyjbhdblz52tdmg.py", line 1093, in call
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 10)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/RTX4090_cc89_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 369, in quant_slide_int8_triton
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) ERROR 01-26 01:03:22 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered

STDERR:
[2026-01-26 01:02:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 01:02:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 01:02:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 01:02:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 01:02:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 01:02:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 01:02:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 01:02:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 01:02:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 01:02:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 01:02:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 01:02:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 01:02:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 01:02:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 01:02:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 01:02:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 01:02:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=625703) [2026-01-26 01:02:57] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=625703) [2026-01-26 01:02:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=625703) [2026-01-26 01:02:57] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=625703) [2026-01-26 01:02:57] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=625703) [2026-01-26 01:02:57] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=625703) [2026-01-26 01:02:57] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=625703) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=625703) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.19s/it]
(EngineCore_DP0 pid=625703) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.42s/it]
(EngineCore_DP0 pid=625703) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.39s/it]
(EngineCore_DP0 pid=625703) 
(EngineCore_DP0 pid=625703) [2026-01-26 01:03:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=625703) [2026-01-26 01:03:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=625703) [2026-01-26 01:03:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=625703) [2026-01-26 01:03:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=625703) [2026-01-26 01:03:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=625703) [2026-01-26 01:03:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=625703) [2026-01-26 01:03:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=625703) [2026-01-26 01:03:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=625703) [rank0]:W0126 01:03:19.912000 625703 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=625703) [rank0]:W0126 01:03:20.030000 625703 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=625703) [rank0]:W0126 01:03:21.744000 625703 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=625703) [rank0]:W0126 01:03:20.241000 625703 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=625703) Process EngineCore_DP0:
(EngineCore_DP0 pid=625703) Traceback (most recent call last):
(EngineCore_DP0 pid=625703)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=625703)     self.run()
(EngineCore_DP0 pid=625703)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=625703)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=625703)     raise e
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=625703)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=625703)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=625703)     super().__init__(
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=625703)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=625703)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=625703)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=625703)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=625703)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=625703)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=625703)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=625703)     return func(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=625703)     return func(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=625703)     self.model_runner.profile_run()
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=625703)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=625703)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=625703)     return func(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=625703)     outputs = self.model(
(EngineCore_DP0 pid=625703)               ^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=625703)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=625703)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=625703)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=625703)     hidden_states = self.model(
(EngineCore_DP0 pid=625703)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=625703)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=625703)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=625703)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=625703)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=625703)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=625703)     def forward(
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=625703)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=625703)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=625703)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=625703)     raise e
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=625703)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=625703)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=625703)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=625703)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=625703)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=625703)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=625703)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=625703)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=625703)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=625703)     return compiled_fn(full_args)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=625703)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=625703)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=625703)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=625703)                             ^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=625703)     outs = compiled_fn(args)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=625703)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=625703)     return self.current_callable(inputs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=625703)     out = model(new_inputs)
(EngineCore_DP0 pid=625703)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/tmp/torchinductor_root/eq/ceqgg2b5od57a2ayhnybjtujmzypeuhpekg6vjyjbhdblz52tdmg.py", line 1093, in call
(EngineCore_DP0 pid=625703)     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 10)
(EngineCore_DP0 pid=625703)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=625703)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=625703)     return fn(input, L)
(EngineCore_DP0 pid=625703)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/RTX4090_cc89_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 369, in quant_slide_int8_triton
(EngineCore_DP0 pid=625703)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=625703)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=625703)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=625703)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=625703)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=625703)     self._init_handles()
(EngineCore_DP0 pid=625703)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=625703)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=625703)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=625703) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 01:03:23.399185798 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 02:48:07
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:48:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:48:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=81483) WARNING 01-26 02:48:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=81483) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=81483) WARNING 01-26 02:48:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.04 requests/s, 8740.49 total tokens/s, 17.04 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 02:48:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:48:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:48:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:48:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:48:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:48:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:48:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:48:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:48:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:48:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:48:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:48:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:48:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:48:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:48:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:48:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:48:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=81483) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=81483) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.57s/it]
(EngineCore_DP0 pid=81483) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.61s/it]
(EngineCore_DP0 pid=81483) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.45s/it]
(EngineCore_DP0 pid=81483) 
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=81483) [2026-01-26 02:48:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=81483) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.13it/s]
(EngineCore_DP0 pid=81483) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  35%|███▌      | 45/128 [00:00<00:00, 442.48it/s]
Adding requests:  73%|███████▎  | 94/128 [00:00<00:00, 467.44it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 467.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 25.58it/s, est. speed input: 13101.26 toks/s, output: 25.59 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:06, 19.99it/s, est. speed input: 10584.88 toks/s, output: 20.67 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:06, 18.77it/s, est. speed input: 9980.47 toks/s, output: 19.49 toks/s] 
Processed prompts:   9%|▊         | 11/128 [00:00<00:06, 18.34it/s, est. speed input: 9768.11 toks/s, output: 19.08 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:06, 18.10it/s, est. speed input: 9640.30 toks/s, output: 18.83 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:06, 17.95it/s, est. speed input: 9553.35 toks/s, output: 18.66 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:06, 17.84it/s, est. speed input: 9485.33 toks/s, output: 18.53 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 17.85it/s, est. speed input: 9447.97 toks/s, output: 18.45 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:05, 17.89it/s, est. speed input: 9424.65 toks/s, output: 18.41 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 17.91it/s, est. speed input: 9404.09 toks/s, output: 18.37 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.94it/s, est. speed input: 9390.01 toks/s, output: 18.34 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.93it/s, est. speed input: 9372.23 toks/s, output: 18.30 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.88it/s, est. speed input: 9353.26 toks/s, output: 18.27 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.88it/s, est. speed input: 9339.74 toks/s, output: 18.24 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 17.91it/s, est. speed input: 9331.68 toks/s, output: 18.23 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 17.70it/s, est. speed input: 9301.31 toks/s, output: 18.17 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 17.57it/s, est. speed input: 9275.29 toks/s, output: 18.12 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.52it/s, est. speed input: 9255.63 toks/s, output: 18.08 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:04, 17.53it/s, est. speed input: 9242.03 toks/s, output: 18.05 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.53it/s, est. speed input: 9229.24 toks/s, output: 18.03 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.55it/s, est. speed input: 9218.99 toks/s, output: 18.01 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 17.54it/s, est. speed input: 9208.32 toks/s, output: 17.98 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 17.44it/s, est. speed input: 9191.37 toks/s, output: 17.95 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 17.44it/s, est. speed input: 9181.00 toks/s, output: 17.93 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:04, 17.36it/s, est. speed input: 9165.84 toks/s, output: 17.90 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 17.37it/s, est. speed input: 9155.79 toks/s, output: 17.88 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 17.40it/s, est. speed input: 9148.51 toks/s, output: 17.87 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 17.47it/s, est. speed input: 9144.37 toks/s, output: 17.86 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 17.51it/s, est. speed input: 9140.03 toks/s, output: 17.85 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 17.52it/s, est. speed input: 9134.76 toks/s, output: 17.84 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 17.51it/s, est. speed input: 9129.19 toks/s, output: 17.83 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 17.52it/s, est. speed input: 9124.68 toks/s, output: 17.82 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:03, 17.48it/s, est. speed input: 9117.91 toks/s, output: 17.81 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:03<00:03, 17.33it/s, est. speed input: 9105.85 toks/s, output: 17.78 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 17.25it/s, est. speed input: 9095.43 toks/s, output: 17.76 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 17.23it/s, est. speed input: 9087.04 toks/s, output: 17.75 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.23it/s, est. speed input: 9079.94 toks/s, output: 17.73 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 17.43it/s, est. speed input: 9082.22 toks/s, output: 17.74 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 17.53it/s, est. speed input: 9082.64 toks/s, output: 17.74 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 17.57it/s, est. speed input: 9081.74 toks/s, output: 17.74 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 17.57it/s, est. speed input: 9079.56 toks/s, output: 17.73 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:02, 17.63it/s, est. speed input: 9079.93 toks/s, output: 17.73 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 17.63it/s, est. speed input: 9078.88 toks/s, output: 17.73 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 17.49it/s, est. speed input: 9072.30 toks/s, output: 17.72 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 17.42it/s, est. speed input: 9067.01 toks/s, output: 17.71 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 17.36it/s, est. speed input: 9061.62 toks/s, output: 17.70 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 17.38it/s, est. speed input: 9058.74 toks/s, output: 17.69 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 17.42it/s, est. speed input: 9056.80 toks/s, output: 17.69 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 17.42it/s, est. speed input: 9054.12 toks/s, output: 17.68 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:05<00:01, 17.43it/s, est. speed input: 9051.75 toks/s, output: 17.68 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:05<00:01, 17.51it/s, est. speed input: 9051.88 toks/s, output: 17.68 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 17.56it/s, est. speed input: 9051.80 toks/s, output: 17.68 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 17.51it/s, est. speed input: 9049.18 toks/s, output: 17.67 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:00, 17.53it/s, est. speed input: 9048.12 toks/s, output: 17.67 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 17.62it/s, est. speed input: 9049.76 toks/s, output: 17.68 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 17.69it/s, est. speed input: 9051.29 toks/s, output: 17.68 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 17.72it/s, est. speed input: 9052.39 toks/s, output: 17.68 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 17.72it/s, est. speed input: 9052.77 toks/s, output: 17.68 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:06<00:00, 17.67it/s, est. speed input: 9051.54 toks/s, output: 17.68 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:06<00:00, 17.75it/s, est. speed input: 9053.81 toks/s, output: 17.68 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 17.79it/s, est. speed input: 9055.49 toks/s, output: 17.69 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 17.78it/s, est. speed input: 9056.03 toks/s, output: 17.69 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.78it/s, est. speed input: 9055.39 toks/s, output: 17.69 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.69it/s, est. speed input: 9055.39 toks/s, output: 17.69 toks/s]
[rank0]:[W126 02:49:02.680598404 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 02:49:04
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:49:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:49:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=82534) WARNING 01-26 02:49:20 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=82534) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=82534) WARNING 01-26 02:49:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.60 requests/s, 17014.08 total tokens/s, 16.60 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 02:49:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:49:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:49:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:49:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:49:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:49:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:49:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:49:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:49:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:49:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:49:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:49:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=82534) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=82534) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.13s/it]
(EngineCore_DP0 pid=82534) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.42s/it]
(EngineCore_DP0 pid=82534) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.38s/it]
(EngineCore_DP0 pid=82534) 
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=82534) [2026-01-26 02:49:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=82534) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.23it/s]
(EngineCore_DP0 pid=82534) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  19%|█▉        | 24/128 [00:00<00:00, 233.75it/s]
Adding requests:  39%|███▉      | 50/128 [00:00<00:00, 242.70it/s]
Adding requests:  61%|██████    | 78/128 [00:00<00:00, 255.22it/s]
Adding requests:  81%|████████▏ | 104/128 [00:00<00:00, 254.92it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 250.98it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:02, 57.17it/s, est. speed input: 58558.65 toks/s, output: 57.18 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:00, 139.85it/s, est. speed input: 130809.15 toks/s, output: 127.73 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 36.44it/s, est. speed input: 44468.06 toks/s, output: 43.42 toks/s]   
Processed prompts:  43%|████▎     | 55/128 [00:01<00:02, 27.90it/s, est. speed input: 35479.75 toks/s, output: 34.65 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:02, 24.73it/s, est. speed input: 32241.78 toks/s, output: 31.49 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:02<00:02, 22.90it/s, est. speed input: 30411.80 toks/s, output: 29.70 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:02<00:02, 21.57it/s, est. speed input: 29184.02 toks/s, output: 28.50 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:02, 20.80it/s, est. speed input: 28459.97 toks/s, output: 27.79 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:02, 20.04it/s, est. speed input: 27799.13 toks/s, output: 27.15 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:02, 19.41it/s, est. speed input: 27217.85 toks/s, output: 26.58 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:03<00:02, 18.90it/s, est. speed input: 26702.51 toks/s, output: 26.08 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:03<00:02, 18.63it/s, est. speed input: 26395.53 toks/s, output: 25.78 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:02, 18.26it/s, est. speed input: 26080.58 toks/s, output: 25.47 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:03<00:02, 17.98it/s, est. speed input: 25793.30 toks/s, output: 25.19 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:03<00:02, 17.78it/s, est. speed input: 25528.86 toks/s, output: 24.93 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:03<00:02, 17.64it/s, est. speed input: 25283.71 toks/s, output: 24.69 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:03<00:01, 17.47it/s, est. speed input: 25043.92 toks/s, output: 24.46 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:03<00:01, 17.37it/s, est. speed input: 24822.13 toks/s, output: 24.24 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:04<00:01, 17.23it/s, est. speed input: 24603.98 toks/s, output: 24.03 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:04<00:01, 17.13it/s, est. speed input: 24398.21 toks/s, output: 23.83 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:04<00:01, 17.16it/s, est. speed input: 24216.84 toks/s, output: 23.65 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:04<00:01, 16.19it/s, est. speed input: 23913.55 toks/s, output: 23.35 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:04<00:01, 16.50it/s, est. speed input: 23755.61 toks/s, output: 23.20 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:04<00:01, 16.71it/s, est. speed input: 23604.14 toks/s, output: 23.05 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:04<00:01, 16.88it/s, est. speed input: 23462.39 toks/s, output: 22.91 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:04<00:00, 16.97it/s, est. speed input: 23323.35 toks/s, output: 22.78 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:05<00:00, 17.02it/s, est. speed input: 23190.13 toks/s, output: 22.65 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:05<00:00, 17.12it/s, est. speed input: 23068.92 toks/s, output: 22.53 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:05<00:00, 17.11it/s, est. speed input: 22945.10 toks/s, output: 22.41 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:05<00:00, 17.21it/s, est. speed input: 22836.25 toks/s, output: 22.30 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:05<00:00, 17.18it/s, est. speed input: 22723.47 toks/s, output: 22.19 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:05<00:00, 17.22it/s, est. speed input: 22621.09 toks/s, output: 22.09 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:05<00:00, 17.32it/s, est. speed input: 22528.12 toks/s, output: 22.00 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 17.35it/s, est. speed input: 22436.14 toks/s, output: 21.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 17.35it/s, est. speed input: 22436.14 toks/s, output: 21.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 21.91it/s, est. speed input: 22436.14 toks/s, output: 21.91 toks/s]
[rank0]:[W126 02:49:52.157029500 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 02:49:54
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:50:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:50:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=83448) WARNING 01-26 02:50:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=83448) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=83448) WARNING 01-26 02:50:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 21.00 requests/s, 21525.39 total tokens/s, 21.00 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 02:50:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:50:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:50:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:50:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:50:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:50:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:50:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:50:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:50:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:50:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:50:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:50:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:50:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:50:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:50:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:50:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=83448) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=83448) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.03it/s]
(EngineCore_DP0 pid=83448) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]
(EngineCore_DP0 pid=83448) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]
(EngineCore_DP0 pid=83448) 
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=83448) [2026-01-26 02:50:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=83448) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.94it/s]
(EngineCore_DP0 pid=83448) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.22it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   9%|▉         | 23/256 [00:00<00:01, 224.66it/s]
Adding requests:  19%|█▉        | 49/256 [00:00<00:00, 243.88it/s]
Adding requests:  30%|██▉       | 76/256 [00:00<00:00, 253.51it/s]
Adding requests:  40%|███▉      | 102/256 [00:00<00:00, 251.26it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 249.65it/s]
Adding requests:  60%|██████    | 154/256 [00:00<00:00, 251.13it/s]
Adding requests:  71%|███████   | 182/256 [00:00<00:00, 259.07it/s]
Adding requests:  82%|████████▏ | 210/256 [00:00<00:00, 263.78it/s]
Adding requests:  93%|█████████▎| 237/256 [00:00<00:00, 258.18it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 254.16it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:01, 136.91it/s, est. speed input: 140227.80 toks/s, output: 136.92 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:00<00:06, 35.69it/s, est. speed input: 41761.43 toks/s, output: 40.78 toks/s]   
Processed prompts:  15%|█▌        | 39/256 [00:01<00:06, 31.94it/s, est. speed input: 37496.15 toks/s, output: 36.62 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:01<00:07, 27.25it/s, est. speed input: 33494.78 toks/s, output: 32.71 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:01<00:08, 25.86it/s, est. speed input: 32093.68 toks/s, output: 31.34 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:08, 24.73it/s, est. speed input: 30990.02 toks/s, output: 30.26 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:01<00:08, 23.83it/s, est. speed input: 30096.36 toks/s, output: 29.39 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:02<00:08, 23.15it/s, est. speed input: 29364.94 toks/s, output: 28.68 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:02<00:08, 22.65it/s, est. speed input: 28751.73 toks/s, output: 28.08 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:02<00:08, 22.30it/s, est. speed input: 28235.05 toks/s, output: 27.57 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:02<00:08, 22.04it/s, est. speed input: 27790.11 toks/s, output: 27.14 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:02<00:08, 21.84it/s, est. speed input: 27399.91 toks/s, output: 26.76 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:03<00:08, 21.73it/s, est. speed input: 27066.20 toks/s, output: 26.43 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:03<00:07, 21.61it/s, est. speed input: 26761.94 toks/s, output: 26.13 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:03<00:07, 21.57it/s, est. speed input: 26501.31 toks/s, output: 25.88 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:03<00:07, 21.56it/s, est. speed input: 26269.95 toks/s, output: 25.65 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:03<00:07, 21.52it/s, est. speed input: 26056.03 toks/s, output: 25.45 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:03<00:07, 21.50it/s, est. speed input: 25862.54 toks/s, output: 25.26 toks/s]
Processed prompts:  41%|████      | 104/256 [00:04<00:07, 21.45it/s, est. speed input: 25680.80 toks/s, output: 25.08 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:04<00:06, 21.44it/s, est. speed input: 25519.56 toks/s, output: 24.92 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:04<00:06, 21.43it/s, est. speed input: 25371.45 toks/s, output: 24.78 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:04<00:06, 21.42it/s, est. speed input: 25234.21 toks/s, output: 24.64 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:04<00:06, 21.41it/s, est. speed input: 25105.77 toks/s, output: 24.52 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:05<00:06, 21.42it/s, est. speed input: 24989.99 toks/s, output: 24.40 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:05<00:05, 21.44it/s, est. speed input: 24884.73 toks/s, output: 24.30 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:05<00:05, 21.40it/s, est. speed input: 24778.97 toks/s, output: 24.20 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:05<00:05, 21.40it/s, est. speed input: 24683.74 toks/s, output: 24.11 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:05<00:05, 21.38it/s, est. speed input: 24592.49 toks/s, output: 24.02 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:06<00:05, 21.40it/s, est. speed input: 24510.82 toks/s, output: 23.94 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:06<00:05, 21.41it/s, est. speed input: 24434.36 toks/s, output: 23.86 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:06<00:04, 21.41it/s, est. speed input: 24360.56 toks/s, output: 23.79 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:06<00:04, 21.42it/s, est. speed input: 24292.78 toks/s, output: 23.72 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:06<00:04, 21.45it/s, est. speed input: 24230.08 toks/s, output: 23.66 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:06<00:04, 21.42it/s, est. speed input: 24166.69 toks/s, output: 23.60 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:07<00:04, 21.44it/s, est. speed input: 24109.67 toks/s, output: 23.54 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:07<00:03, 21.46it/s, est. speed input: 24056.91 toks/s, output: 23.49 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:07<00:03, 21.45it/s, est. speed input: 24004.32 toks/s, output: 23.44 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:07<00:03, 21.46it/s, est. speed input: 23955.83 toks/s, output: 23.39 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:07<00:03, 21.45it/s, est. speed input: 23908.23 toks/s, output: 23.35 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:08<00:03, 21.46it/s, est. speed input: 23863.69 toks/s, output: 23.30 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:08<00:02, 21.44it/s, est. speed input: 23819.95 toks/s, output: 23.26 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:08<00:02, 21.46it/s, est. speed input: 23779.95 toks/s, output: 23.22 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:08<00:02, 21.42it/s, est. speed input: 23738.01 toks/s, output: 23.18 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:08<00:02, 22.54it/s, est. speed input: 23783.30 toks/s, output: 23.23 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:08<00:02, 22.15it/s, est. speed input: 23741.58 toks/s, output: 23.19 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:09<00:02, 21.91it/s, est. speed input: 23703.81 toks/s, output: 23.15 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:09<00:01, 21.77it/s, est. speed input: 23668.80 toks/s, output: 23.11 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:09<00:01, 21.65it/s, est. speed input: 23633.90 toks/s, output: 23.08 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:09<00:01, 21.58it/s, est. speed input: 23601.57 toks/s, output: 23.05 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:09<00:01, 21.52it/s, est. speed input: 23568.79 toks/s, output: 23.02 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:10<00:01, 21.49it/s, est. speed input: 23539.05 toks/s, output: 22.99 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:10<00:00, 21.45it/s, est. speed input: 23508.15 toks/s, output: 22.96 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:10<00:00, 21.43it/s, est. speed input: 23479.39 toks/s, output: 22.93 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:10<00:00, 21.41it/s, est. speed input: 23451.07 toks/s, output: 22.90 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:10<00:00, 21.41it/s, est. speed input: 23424.74 toks/s, output: 22.88 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:11<00:00, 21.39it/s, est. speed input: 23398.45 toks/s, output: 22.85 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:11<00:00, 22.64it/s, est. speed input: 23445.39 toks/s, output: 22.90 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:11<00:00, 22.64it/s, est. speed input: 23445.39 toks/s, output: 22.90 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:11<00:00, 22.90it/s, est. speed input: 23445.39 toks/s, output: 22.90 toks/s]
[rank0]:[W126 02:50:48.874105024 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 02:50:49
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:51:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:51:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=84470) WARNING 01-26 02:51:09 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=84470) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=84470) WARNING 01-26 02:51:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 21.86 requests/s, 22411.25 total tokens/s, 21.86 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 02:51:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:51:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:51:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:51:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:51:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:51:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:51:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:51:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:51:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:51:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:51:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:51:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=84470) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=84470) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.01it/s]
(EngineCore_DP0 pid=84470) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]
(EngineCore_DP0 pid=84470) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.21s/it]
(EngineCore_DP0 pid=84470) 
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=84470) [2026-01-26 02:51:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=84470) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.20it/s]
(EngineCore_DP0 pid=84470) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.57it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 23/512 [00:00<00:02, 228.84it/s]
Adding requests:  10%|▉         | 49/512 [00:00<00:01, 241.91it/s]
Adding requests:  15%|█▍        | 75/512 [00:00<00:01, 247.15it/s]
Adding requests:  20%|█▉        | 101/512 [00:00<00:01, 248.45it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:01, 256.48it/s]
Adding requests:  30%|███       | 156/512 [00:00<00:01, 260.23it/s]
Adding requests:  36%|███▌      | 185/512 [00:00<00:01, 268.36it/s]
Adding requests:  41%|████▏     | 212/512 [00:00<00:01, 268.04it/s]
Adding requests:  47%|████▋     | 239/512 [00:00<00:01, 268.14it/s]
Adding requests:  52%|█████▏    | 266/512 [00:01<00:00, 268.64it/s]
Adding requests:  57%|█████▋    | 293/512 [00:01<00:00, 267.78it/s]
Adding requests:  63%|██████▎   | 322/512 [00:01<00:00, 271.72it/s]
Adding requests:  68%|██████▊   | 350/512 [00:01<00:00, 271.11it/s]
Adding requests:  74%|███████▍  | 378/512 [00:01<00:00, 272.92it/s]
Adding requests:  80%|███████▉  | 408/512 [00:01<00:00, 279.24it/s]
Adding requests:  85%|████████▌ | 436/512 [00:01<00:00, 277.06it/s]
Adding requests:  91%|█████████ | 464/512 [00:01<00:00, 275.36it/s]
Adding requests:  97%|█████████▋| 495/512 [00:01<00:00, 284.54it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 268.94it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:00<00:01, 316.70it/s, est. speed input: 324380.33 toks/s, output: 316.72 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:01<00:12, 36.90it/s, est. speed input: 43757.56 toks/s, output: 42.73 toks/s]   
Processed prompts:  16%|█▌        | 81/512 [00:02<00:12, 33.84it/s, est. speed input: 39815.29 toks/s, output: 38.88 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:02<00:14, 28.86it/s, est. speed input: 35527.91 toks/s, output: 34.70 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:02<00:15, 26.65it/s, est. speed input: 33654.11 toks/s, output: 32.87 toks/s]
Processed prompts:  20%|██        | 103/512 [00:03<00:15, 26.82it/s, est. speed input: 33363.16 toks/s, output: 32.58 toks/s]
Processed prompts:  21%|██        | 108/512 [00:03<00:14, 26.99it/s, est. speed input: 33097.86 toks/s, output: 32.32 toks/s]
Processed prompts:  22%|██▏       | 112/512 [00:03<00:15, 26.06it/s, est. speed input: 32573.21 toks/s, output: 31.81 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:03<00:15, 25.22it/s, est. speed input: 32096.52 toks/s, output: 31.34 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:03<00:16, 23.19it/s, est. speed input: 31402.98 toks/s, output: 30.67 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:04<00:18, 21.55it/s, est. speed input: 30770.77 toks/s, output: 30.05 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:04<00:17, 21.73it/s, est. speed input: 30431.56 toks/s, output: 29.72 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:04<00:17, 21.87it/s, est. speed input: 30119.74 toks/s, output: 29.41 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:04<00:17, 21.97it/s, est. speed input: 29830.92 toks/s, output: 29.13 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:04<00:16, 22.04it/s, est. speed input: 29565.14 toks/s, output: 28.87 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:04<00:16, 22.10it/s, est. speed input: 29318.44 toks/s, output: 28.63 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:05<00:16, 22.14it/s, est. speed input: 29089.48 toks/s, output: 28.41 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:05<00:16, 22.18it/s, est. speed input: 28877.68 toks/s, output: 28.20 toks/s]
Processed prompts:  30%|███       | 154/512 [00:05<00:16, 22.19it/s, est. speed input: 28676.28 toks/s, output: 28.00 toks/s]
Processed prompts:  31%|███       | 158/512 [00:05<00:15, 22.21it/s, est. speed input: 28490.37 toks/s, output: 27.82 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:05<00:15, 22.22it/s, est. speed input: 28314.95 toks/s, output: 27.65 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:06<00:15, 22.23it/s, est. speed input: 28149.85 toks/s, output: 27.49 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:06<00:15, 22.22it/s, est. speed input: 27992.80 toks/s, output: 27.34 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:06<00:15, 22.23it/s, est. speed input: 27846.25 toks/s, output: 27.19 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:06<00:15, 22.22it/s, est. speed input: 27706.61 toks/s, output: 27.06 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:06<00:14, 22.23it/s, est. speed input: 27575.18 toks/s, output: 26.93 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:06<00:14, 22.23it/s, est. speed input: 27450.78 toks/s, output: 26.81 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:07<00:14, 22.23it/s, est. speed input: 27331.86 toks/s, output: 26.69 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:07<00:14, 22.22it/s, est. speed input: 27218.41 toks/s, output: 26.58 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:07<00:14, 22.19it/s, est. speed input: 27108.31 toks/s, output: 26.47 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:07<00:13, 23.49it/s, est. speed input: 27122.46 toks/s, output: 26.49 toks/s]
Processed prompts:  40%|████      | 206/512 [00:07<00:13, 23.08it/s, est. speed input: 27020.37 toks/s, output: 26.39 toks/s]
Processed prompts:  41%|████      | 210/512 [00:07<00:13, 22.80it/s, est. speed input: 26923.20 toks/s, output: 26.29 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:08<00:13, 22.63it/s, est. speed input: 26831.89 toks/s, output: 26.20 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:08<00:13, 22.51it/s, est. speed input: 26743.92 toks/s, output: 26.12 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:08<00:12, 22.40it/s, est. speed input: 26658.42 toks/s, output: 26.03 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:08<00:12, 22.34it/s, est. speed input: 26577.03 toks/s, output: 25.95 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:08<00:12, 22.30it/s, est. speed input: 26499.10 toks/s, output: 25.88 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:09<00:12, 22.28it/s, est. speed input: 26425.35 toks/s, output: 25.81 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:09<00:12, 22.26it/s, est. speed input: 26353.55 toks/s, output: 25.74 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:09<00:12, 22.23it/s, est. speed input: 26283.72 toks/s, output: 25.67 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:09<00:11, 22.22it/s, est. speed input: 26216.94 toks/s, output: 25.60 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:09<00:11, 22.20it/s, est. speed input: 26151.74 toks/s, output: 25.54 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:09<00:11, 22.20it/s, est. speed input: 26089.96 toks/s, output: 25.48 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:10<00:11, 22.22it/s, est. speed input: 26032.04 toks/s, output: 25.42 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:10<00:11, 22.23it/s, est. speed input: 25975.07 toks/s, output: 25.37 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:10<00:11, 22.22it/s, est. speed input: 25919.75 toks/s, output: 25.31 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:10<00:10, 22.20it/s, est. speed input: 25864.69 toks/s, output: 25.26 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:10<00:10, 22.17it/s, est. speed input: 25811.37 toks/s, output: 25.21 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:11<00:10, 22.17it/s, est. speed input: 25760.62 toks/s, output: 25.16 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:11<00:10, 22.18it/s, est. speed input: 25711.93 toks/s, output: 25.11 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:11<00:10, 22.18it/s, est. speed input: 25664.53 toks/s, output: 25.06 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:11<00:10, 22.18it/s, est. speed input: 25618.59 toks/s, output: 25.02 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:11<00:09, 22.18it/s, est. speed input: 25573.86 toks/s, output: 24.97 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:11<00:09, 22.17it/s, est. speed input: 25530.16 toks/s, output: 24.93 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:12<00:09, 22.14it/s, est. speed input: 25486.69 toks/s, output: 24.89 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:12<00:09, 22.13it/s, est. speed input: 25444.76 toks/s, output: 24.85 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:12<00:09, 22.13it/s, est. speed input: 25404.24 toks/s, output: 24.81 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:12<00:08, 22.13it/s, est. speed input: 25365.27 toks/s, output: 24.77 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:12<00:08, 22.12it/s, est. speed input: 25326.52 toks/s, output: 24.73 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:13<00:08, 22.12it/s, est. speed input: 25289.43 toks/s, output: 24.70 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:13<00:08, 22.11it/s, est. speed input: 25252.82 toks/s, output: 24.66 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:13<00:08, 22.10it/s, est. speed input: 25217.23 toks/s, output: 24.63 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:13<00:08, 22.10it/s, est. speed input: 25182.45 toks/s, output: 24.59 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:13<00:07, 22.10it/s, est. speed input: 25149.03 toks/s, output: 24.56 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:13<00:07, 22.07it/s, est. speed input: 25114.98 toks/s, output: 24.53 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:14<00:07, 22.08it/s, est. speed input: 25083.22 toks/s, output: 24.50 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:14<00:07, 22.09it/s, est. speed input: 25052.29 toks/s, output: 24.47 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:14<00:07, 22.09it/s, est. speed input: 25021.76 toks/s, output: 24.44 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:14<00:06, 22.09it/s, est. speed input: 24992.17 toks/s, output: 24.41 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:14<00:06, 22.08it/s, est. speed input: 24962.86 toks/s, output: 24.38 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:15<00:06, 22.07it/s, est. speed input: 24934.01 toks/s, output: 24.35 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:15<00:06, 22.08it/s, est. speed input: 24906.49 toks/s, output: 24.32 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:15<00:06, 22.09it/s, est. speed input: 24879.93 toks/s, output: 24.30 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:15<00:06, 22.09it/s, est. speed input: 24853.87 toks/s, output: 24.27 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:15<00:05, 22.10it/s, est. speed input: 24828.54 toks/s, output: 24.25 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:16<00:01, 66.09it/s, est. speed input: 26721.34 toks/s, output: 26.10 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [00:16<00:01, 57.06it/s, est. speed input: 26801.79 toks/s, output: 26.17 toks/s]
Processed prompts:  84%|████████▍ | 429/512 [00:16<00:01, 48.79it/s, est. speed input: 26818.17 toks/s, output: 26.19 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:16<00:02, 33.21it/s, est. speed input: 26544.37 toks/s, output: 25.92 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:16<00:02, 30.52it/s, est. speed input: 26502.21 toks/s, output: 25.88 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:17<00:02, 28.35it/s, est. speed input: 26460.79 toks/s, output: 25.84 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:17<00:02, 26.66it/s, est. speed input: 26420.11 toks/s, output: 25.80 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:17<00:02, 25.38it/s, est. speed input: 26380.13 toks/s, output: 25.76 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:17<00:02, 24.43it/s, est. speed input: 26340.69 toks/s, output: 25.72 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:17<00:02, 23.77it/s, est. speed input: 26303.38 toks/s, output: 25.69 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:18<00:02, 23.27it/s, est. speed input: 26266.23 toks/s, output: 25.65 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:18<00:02, 22.93it/s, est. speed input: 26229.94 toks/s, output: 25.62 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:18<00:01, 22.67it/s, est. speed input: 26194.13 toks/s, output: 25.58 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:18<00:01, 22.49it/s, est. speed input: 26158.71 toks/s, output: 25.55 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:18<00:01, 22.35it/s, est. speed input: 26123.68 toks/s, output: 25.51 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:18<00:01, 22.25it/s, est. speed input: 26089.54 toks/s, output: 25.48 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:19<00:01, 22.19it/s, est. speed input: 26056.01 toks/s, output: 25.45 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:19<00:00, 22.14it/s, est. speed input: 26023.02 toks/s, output: 25.41 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:19<00:00, 22.12it/s, est. speed input: 25991.11 toks/s, output: 25.38 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:19<00:00, 22.09it/s, est. speed input: 25959.16 toks/s, output: 25.35 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:19<00:00, 22.10it/s, est. speed input: 25929.02 toks/s, output: 25.32 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:20<00:00, 22.10it/s, est. speed input: 25899.36 toks/s, output: 25.29 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:20<00:00, 23.74it/s, est. speed input: 25923.63 toks/s, output: 25.32 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:20<00:00, 23.74it/s, est. speed input: 26025.03 toks/s, output: 25.42 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:20<00:00, 25.41it/s, est. speed input: 26025.03 toks/s, output: 25.42 toks/s]
[rank0]:[W126 02:51:55.883123550 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 02:51:57
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:52:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:52:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=85695) WARNING 01-26 02:52:21 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=85695) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=85695) WARNING 01-26 02:52:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 22.26 requests/s, 22819.14 total tokens/s, 22.26 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 02:52:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:52:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:52:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:52:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:52:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:52:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:52:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:52:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:52:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:52:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:52:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:52:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=85695) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=85695) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=85695) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]
(EngineCore_DP0 pid=85695) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.18s/it]
(EngineCore_DP0 pid=85695) 
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=85695) [2026-01-26 02:52:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=85695) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.16it/s]
(EngineCore_DP0 pid=85695) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.26it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.80it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 22/1024 [00:00<00:04, 215.82it/s]
Adding requests:   5%|▍         | 48/1024 [00:00<00:04, 236.82it/s]
Adding requests:   7%|▋         | 75/1024 [00:00<00:03, 249.56it/s]
Adding requests:  10%|▉         | 102/1024 [00:00<00:03, 255.99it/s]
Adding requests:  13%|█▎        | 129/1024 [00:00<00:03, 257.24it/s]
Adding requests:  15%|█▌        | 155/1024 [00:00<00:03, 257.53it/s]
Adding requests:  18%|█▊        | 184/1024 [00:00<00:03, 266.85it/s]
Adding requests:  21%|██        | 212/1024 [00:00<00:03, 269.61it/s]
Adding requests:  23%|██▎       | 240/1024 [00:00<00:02, 269.92it/s]
Adding requests:  26%|██▌       | 267/1024 [00:01<00:02, 269.59it/s]
Adding requests:  29%|██▊       | 294/1024 [00:01<00:02, 269.35it/s]
Adding requests:  31%|███▏      | 322/1024 [00:01<00:02, 271.24it/s]
Adding requests:  34%|███▍      | 350/1024 [00:01<00:02, 270.98it/s]
Adding requests:  37%|███▋      | 378/1024 [00:01<00:02, 271.30it/s]
Adding requests:  40%|███▉      | 408/1024 [00:01<00:02, 277.00it/s]
Adding requests:  43%|████▎     | 436/1024 [00:01<00:02, 277.45it/s]
Adding requests:  45%|████▌     | 464/1024 [00:01<00:02, 272.07it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 283.25it/s]
Adding requests:  52%|█████▏    | 528/1024 [00:01<00:01, 289.81it/s]
Adding requests:  54%|█████▍    | 557/1024 [00:02<00:01, 282.92it/s]
Adding requests:  57%|█████▋    | 586/1024 [00:02<00:01, 277.38it/s]
Adding requests:  60%|█████▉    | 614/1024 [00:02<00:01, 274.77it/s]
Adding requests:  63%|██████▎   | 642/1024 [00:02<00:01, 270.26it/s]
Adding requests:  65%|██████▌   | 670/1024 [00:02<00:01, 265.85it/s]
Adding requests:  68%|██████▊   | 699/1024 [00:02<00:01, 271.17it/s]
Adding requests:  71%|███████   | 727/1024 [00:02<00:01, 261.15it/s]
Adding requests:  74%|███████▎  | 754/1024 [00:02<00:01, 260.11it/s]
Adding requests:  76%|███████▋  | 782/1024 [00:02<00:00, 263.55it/s]
Adding requests:  79%|███████▉  | 809/1024 [00:03<00:00, 262.94it/s]
Adding requests:  82%|████████▏ | 837/1024 [00:03<00:00, 267.54it/s]
Adding requests:  84%|████████▍ | 864/1024 [00:03<00:00, 266.21it/s]
Adding requests:  87%|████████▋ | 892/1024 [00:03<00:00, 268.10it/s]
Adding requests:  90%|████████▉ | 919/1024 [00:03<00:00, 263.73it/s]
Adding requests:  92%|█████████▏| 946/1024 [00:03<00:00, 263.65it/s]
Adding requests:  95%|█████████▌| 975/1024 [00:03<00:00, 268.46it/s]
Adding requests:  98%|█████████▊| 1002/1024 [00:03<00:00, 262.86it/s]
Adding requests: 100%|██████████| 1024/1024 [00:03<00:00, 267.21it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:00<00:03, 283.64it/s, est. speed input: 290477.90 toks/s, output: 283.65 toks/s]
Processed prompts:  11%|█         | 111/1024 [00:01<00:13, 67.99it/s, est. speed input: 83730.04 toks/s, output: 81.77 toks/s]  
Processed prompts:  12%|█▏        | 125/1024 [00:02<00:19, 46.50it/s, est. speed input: 61848.15 toks/s, output: 60.40 toks/s]
Processed prompts:  13%|█▎        | 134/1024 [00:02<00:21, 41.39it/s, est. speed input: 56568.08 toks/s, output: 55.24 toks/s]
Processed prompts:  14%|█▎        | 140/1024 [00:02<00:25, 35.10it/s, est. speed input: 51536.43 toks/s, output: 50.33 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:03<00:29, 30.21it/s, est. speed input: 47642.68 toks/s, output: 46.53 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:03<00:31, 28.06it/s, est. speed input: 45131.32 toks/s, output: 44.07 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:03<00:16, 49.54it/s, est. speed input: 50794.09 toks/s, output: 49.60 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:04<00:19, 41.97it/s, est. speed input: 48465.24 toks/s, output: 47.33 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:04<00:22, 36.40it/s, est. speed input: 46496.89 toks/s, output: 45.41 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:04<00:24, 32.35it/s, est. speed input: 44809.81 toks/s, output: 43.76 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:05<00:27, 29.45it/s, est. speed input: 43349.10 toks/s, output: 42.33 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:05<00:28, 27.37it/s, est. speed input: 42069.91 toks/s, output: 41.08 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:06<00:30, 25.89it/s, est. speed input: 40939.83 toks/s, output: 39.98 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:06<00:31, 24.85it/s, est. speed input: 39935.34 toks/s, output: 39.00 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:06<00:31, 24.12it/s, est. speed input: 39037.47 toks/s, output: 38.12 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:07<00:32, 23.59it/s, est. speed input: 38226.75 toks/s, output: 37.33 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:07<00:32, 23.22it/s, est. speed input: 37494.30 toks/s, output: 36.62 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:07<00:32, 22.96it/s, est. speed input: 36827.84 toks/s, output: 35.96 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:08<00:32, 22.78it/s, est. speed input: 36219.90 toks/s, output: 35.37 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:08<00:32, 22.65it/s, est. speed input: 35661.71 toks/s, output: 34.83 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:08<00:31, 22.56it/s, est. speed input: 35148.57 toks/s, output: 34.32 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:09<00:31, 22.49it/s, est. speed input: 34674.04 toks/s, output: 33.86 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:09<00:31, 22.45it/s, est. speed input: 34235.89 toks/s, output: 33.43 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:09<00:30, 22.41it/s, est. speed input: 33827.57 toks/s, output: 33.03 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:10<00:30, 22.38it/s, est. speed input: 33447.45 toks/s, output: 32.66 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:10<00:30, 22.36it/s, est. speed input: 33092.14 toks/s, output: 32.32 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:11<00:29, 22.34it/s, est. speed input: 32759.42 toks/s, output: 31.99 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:11<00:29, 22.34it/s, est. speed input: 32448.96 toks/s, output: 31.69 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:11<00:29, 22.33it/s, est. speed input: 32156.74 toks/s, output: 31.40 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:12<00:28, 22.33it/s, est. speed input: 31882.06 toks/s, output: 31.13 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:12<00:28, 22.32it/s, est. speed input: 31622.50 toks/s, output: 30.88 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:12<00:28, 22.31it/s, est. speed input: 31377.58 toks/s, output: 30.64 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:13<00:27, 22.31it/s, est. speed input: 31145.65 toks/s, output: 30.42 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:13<00:27, 22.30it/s, est. speed input: 30925.30 toks/s, output: 30.20 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:13<00:27, 22.29it/s, est. speed input: 30716.05 toks/s, output: 30.00 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:14<00:26, 22.29it/s, est. speed input: 30517.33 toks/s, output: 29.80 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:14<00:26, 22.28it/s, est. speed input: 30327.98 toks/s, output: 29.62 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:15<00:26, 22.27it/s, est. speed input: 30147.79 toks/s, output: 29.44 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:15<00:25, 22.27it/s, est. speed input: 29976.06 toks/s, output: 29.27 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:15<00:25, 22.27it/s, est. speed input: 29812.43 toks/s, output: 29.11 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:16<00:25, 22.27it/s, est. speed input: 29656.13 toks/s, output: 28.96 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:16<00:24, 22.27it/s, est. speed input: 29505.88 toks/s, output: 28.81 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:16<00:24, 22.27it/s, est. speed input: 29362.47 toks/s, output: 28.67 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:17<00:23, 22.26it/s, est. speed input: 29224.78 toks/s, output: 28.54 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:17<00:23, 22.26it/s, est. speed input: 29092.64 toks/s, output: 28.41 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:17<00:23, 22.26it/s, est. speed input: 28966.10 toks/s, output: 28.29 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:18<00:22, 22.27it/s, est. speed input: 28845.19 toks/s, output: 28.17 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:18<00:22, 22.27it/s, est. speed input: 28728.65 toks/s, output: 28.06 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:18<00:22, 22.26it/s, est. speed input: 28616.08 toks/s, output: 27.95 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:19<00:21, 22.26it/s, est. speed input: 28507.50 toks/s, output: 27.84 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:19<00:21, 22.26it/s, est. speed input: 28403.10 toks/s, output: 27.74 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:20<00:21, 22.26it/s, est. speed input: 28302.29 toks/s, output: 27.64 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:20<00:20, 22.26it/s, est. speed input: 28205.32 toks/s, output: 27.54 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:20<00:20, 22.26it/s, est. speed input: 28111.73 toks/s, output: 27.45 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:21<00:20, 22.26it/s, est. speed input: 28021.51 toks/s, output: 27.36 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:21<00:19, 22.27it/s, est. speed input: 27934.58 toks/s, output: 27.28 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:21<00:19, 22.27it/s, est. speed input: 27850.39 toks/s, output: 27.20 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:22<00:18, 22.27it/s, est. speed input: 27768.55 toks/s, output: 27.12 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:22<00:18, 22.27it/s, est. speed input: 27689.87 toks/s, output: 27.04 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:22<00:18, 22.27it/s, est. speed input: 27613.33 toks/s, output: 26.97 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:23<00:17, 22.28it/s, est. speed input: 27539.68 toks/s, output: 26.89 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:23<00:17, 22.28it/s, est. speed input: 27467.83 toks/s, output: 26.82 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:23<00:17, 22.28it/s, est. speed input: 27398.13 toks/s, output: 26.76 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:24<00:16, 22.27it/s, est. speed input: 27330.15 toks/s, output: 26.69 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:24<00:16, 22.27it/s, est. speed input: 27264.41 toks/s, output: 26.63 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:25<00:16, 22.27it/s, est. speed input: 27200.68 toks/s, output: 26.56 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:25<00:15, 22.28it/s, est. speed input: 27138.87 toks/s, output: 26.50 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:25<00:15, 22.28it/s, est. speed input: 27078.69 toks/s, output: 26.44 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:26<00:14, 22.27it/s, est. speed input: 27019.61 toks/s, output: 26.39 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:26<00:14, 22.27it/s, est. speed input: 26962.36 toks/s, output: 26.33 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:26<00:14, 22.27it/s, est. speed input: 26906.77 toks/s, output: 26.28 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:27<00:13, 22.27it/s, est. speed input: 26852.60 toks/s, output: 26.22 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:27<00:13, 22.27it/s, est. speed input: 26799.79 toks/s, output: 26.17 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:27<00:13, 22.27it/s, est. speed input: 26748.54 toks/s, output: 26.12 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:28<00:12, 22.28it/s, est. speed input: 26699.00 toks/s, output: 26.07 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:28<00:12, 22.28it/s, est. speed input: 26650.27 toks/s, output: 26.03 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:29<00:12, 22.28it/s, est. speed input: 26602.94 toks/s, output: 25.98 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:29<00:11, 22.28it/s, est. speed input: 26556.50 toks/s, output: 25.93 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:29<00:11, 22.28it/s, est. speed input: 26511.54 toks/s, output: 25.89 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:30<00:11, 22.28it/s, est. speed input: 26467.54 toks/s, output: 25.85 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:30<00:10, 22.91it/s, est. speed input: 26453.13 toks/s, output: 25.83 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:30<00:10, 22.73it/s, est. speed input: 26410.92 toks/s, output: 25.79 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:31<00:09, 22.59it/s, est. speed input: 26369.42 toks/s, output: 25.75 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:31<00:09, 22.50it/s, est. speed input: 26329.05 toks/s, output: 25.71 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:31<00:09, 22.43it/s, est. speed input: 26289.54 toks/s, output: 25.67 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:32<00:08, 22.39it/s, est. speed input: 26250.92 toks/s, output: 25.64 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:32<00:08, 22.35it/s, est. speed input: 26212.93 toks/s, output: 25.60 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:32<00:08, 22.34it/s, est. speed input: 26176.08 toks/s, output: 25.56 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:33<00:07, 22.32it/s, est. speed input: 26139.76 toks/s, output: 25.53 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:33<00:07, 22.31it/s, est. speed input: 26104.27 toks/s, output: 25.49 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:34<00:07, 22.30it/s, est. speed input: 26069.50 toks/s, output: 25.46 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:34<00:06, 22.30it/s, est. speed input: 26035.70 toks/s, output: 25.43 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:34<00:06, 22.29it/s, est. speed input: 26002.47 toks/s, output: 25.39 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:35<00:02, 46.08it/s, est. speed input: 26844.93 toks/s, output: 26.22 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:35<00:02, 39.25it/s, est. speed input: 26804.32 toks/s, output: 26.18 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:35<00:02, 34.31it/s, est. speed input: 26764.44 toks/s, output: 26.14 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:36<00:02, 30.79it/s, est. speed input: 26725.49 toks/s, output: 26.10 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:36<00:02, 28.27it/s, est. speed input: 26687.03 toks/s, output: 26.06 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:36<00:02, 26.49it/s, est. speed input: 26649.21 toks/s, output: 26.02 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:37<00:02, 25.23it/s, est. speed input: 26612.29 toks/s, output: 25.99 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:37<00:01, 24.35it/s, est. speed input: 26576.05 toks/s, output: 25.95 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:38<00:01, 23.73it/s, est. speed input: 26540.62 toks/s, output: 25.92 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:38<00:01, 23.30it/s, est. speed input: 26505.85 toks/s, output: 25.88 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:38<00:00, 22.99it/s, est. speed input: 26471.40 toks/s, output: 25.85 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:39<00:00, 22.78it/s, est. speed input: 26437.91 toks/s, output: 25.82 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:39<00:00, 23.46it/s, est. speed input: 26433.25 toks/s, output: 25.81 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:39<00:00, 23.46it/s, est. speed input: 26588.85 toks/s, output: 25.97 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:39<00:00, 25.97it/s, est. speed input: 26588.85 toks/s, output: 25.97 toks/s]
[rank0]:[W126 02:53:29.750317527 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 02:53:31
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:53:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:53:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=87342) WARNING 01-26 02:54:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=87342) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=87342) WARNING 01-26 02:54:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 8.23 requests/s, 8438.58 total tokens/s, 8.23 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 02:53:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:53:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:53:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:53:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:53:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:53:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:53:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:53:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:53:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:54:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:54:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:54:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:54:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:54:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:54:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:54:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:54:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=87342) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=87342) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=87342) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]
(EngineCore_DP0 pid=87342) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.20s/it]
(EngineCore_DP0 pid=87342) 
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=87342) [2026-01-26 02:54:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=87342) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  7.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.76it/s]
(EngineCore_DP0 pid=87342) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.65it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  7.85it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.43it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 23/2048 [00:00<00:08, 225.44it/s]
Adding requests:   2%|▏         | 49/2048 [00:00<00:08, 241.41it/s]
Adding requests:   4%|▎         | 76/2048 [00:00<00:07, 250.88it/s]
Adding requests:   5%|▍         | 102/2048 [00:00<00:08, 241.31it/s]
Adding requests:   6%|▌         | 127/2048 [00:00<00:07, 240.43it/s]
Adding requests:   7%|▋         | 152/2048 [00:00<00:07, 238.09it/s]
Adding requests:   9%|▊         | 177/2048 [00:00<00:07, 240.90it/s]
Adding requests:  10%|▉         | 203/2048 [00:00<00:07, 244.35it/s]
Adding requests:  11%|█         | 229/2048 [00:00<00:07, 247.35it/s]
Adding requests:  12%|█▏        | 254/2048 [00:01<00:07, 239.85it/s]
Adding requests:  14%|█▎        | 279/2048 [00:01<00:07, 241.47it/s]
Adding requests:  15%|█▍        | 304/2048 [00:01<00:07, 240.98it/s]
Adding requests:  16%|█▌        | 330/2048 [00:01<00:07, 244.98it/s]
Adding requests:  17%|█▋        | 357/2048 [00:01<00:06, 251.34it/s]
Adding requests:  19%|█▊        | 383/2048 [00:01<00:06, 251.27it/s]
Adding requests:  20%|██        | 410/2048 [00:01<00:06, 254.64it/s]
Adding requests:  21%|██▏       | 436/2048 [00:01<00:06, 250.67it/s]
Adding requests:  23%|██▎       | 462/2048 [00:01<00:06, 251.81it/s]
Adding requests:  24%|██▍       | 490/2048 [00:01<00:06, 259.33it/s]
Adding requests:  25%|██▌       | 517/2048 [00:02<00:05, 261.55it/s]
Adding requests:  27%|██▋       | 544/2048 [00:02<00:05, 263.36it/s]
Adding requests:  28%|██▊       | 571/2048 [00:02<00:05, 259.73it/s]
Adding requests:  29%|██▉       | 597/2048 [00:02<00:05, 248.75it/s]
Adding requests:  30%|███       | 623/2048 [00:02<00:05, 248.49it/s]
Adding requests:  32%|███▏      | 648/2048 [00:02<00:05, 245.97it/s]
Adding requests:  33%|███▎      | 673/2048 [00:02<00:05, 244.02it/s]
Adding requests:  34%|███▍      | 701/2048 [00:02<00:05, 252.84it/s]
Adding requests:  35%|███▌      | 727/2048 [00:02<00:05, 249.53it/s]
Adding requests:  37%|███▋      | 752/2048 [00:03<00:05, 248.97it/s]
Adding requests:  38%|███▊      | 777/2048 [00:03<00:05, 245.62it/s]
Adding requests:  39%|███▉      | 802/2048 [00:03<00:05, 244.62it/s]
Adding requests:  40%|████      | 828/2048 [00:03<00:04, 248.80it/s]
Adding requests:  42%|████▏     | 853/2048 [00:03<00:04, 246.43it/s]
Adding requests:  43%|████▎     | 879/2048 [00:03<00:04, 248.56it/s]
Adding requests:  44%|████▍     | 906/2048 [00:03<00:04, 252.70it/s]
Adding requests:  46%|████▌     | 932/2048 [00:03<00:04, 241.72it/s]
Adding requests:  47%|████▋     | 957/2048 [00:03<00:04, 240.74it/s]
Adding requests:  48%|████▊     | 982/2048 [00:03<00:04, 242.05it/s]
Adding requests:  49%|████▉     | 1007/2048 [00:04<00:04, 236.29it/s]
Adding requests:  50%|█████     | 1031/2048 [00:04<00:04, 237.20it/s]
Adding requests:  52%|█████▏    | 1055/2048 [00:04<00:04, 237.79it/s]
Adding requests:  53%|█████▎    | 1079/2048 [00:04<00:04, 237.85it/s]
Adding requests:  54%|█████▍    | 1103/2048 [00:04<00:04, 234.22it/s]
Adding requests:  55%|█████▌    | 1130/2048 [00:04<00:03, 243.97it/s]
Adding requests:  56%|█████▋    | 1155/2048 [00:04<00:03, 240.41it/s]
Adding requests:  58%|█████▊    | 1182/2048 [00:04<00:03, 246.82it/s]
Adding requests:  59%|█████▉    | 1208/2048 [00:04<00:03, 249.14it/s]
Adding requests:  60%|██████    | 1235/2048 [00:05<00:03, 252.36it/s]
Adding requests:  62%|██████▏   | 1261/2048 [00:05<00:03, 248.51it/s]
Adding requests:  63%|██████▎   | 1286/2048 [00:05<00:03, 247.89it/s]
Adding requests:  64%|██████▍   | 1311/2048 [00:05<00:02, 248.43it/s]
Adding requests:  65%|██████▌   | 1336/2048 [00:05<00:02, 245.03it/s]
Adding requests:  67%|██████▋   | 1363/2048 [00:05<00:02, 248.50it/s]
Adding requests:  68%|██████▊   | 1388/2048 [00:05<00:02, 248.28it/s]
Adding requests:  69%|██████▉   | 1414/2048 [00:05<00:02, 248.79it/s]
Adding requests:  70%|███████   | 1440/2048 [00:05<00:02, 249.32it/s]
Adding requests:  72%|███████▏  | 1466/2048 [00:05<00:02, 250.89it/s]
Adding requests:  73%|███████▎  | 1492/2048 [00:06<00:02, 252.47it/s]
Adding requests:  74%|███████▍  | 1518/2048 [00:06<00:02, 254.34it/s]
Adding requests:  75%|███████▌  | 1544/2048 [00:06<00:02, 250.99it/s]
Adding requests:  77%|███████▋  | 1570/2048 [00:06<00:01, 244.26it/s]
Adding requests:  78%|███████▊  | 1596/2048 [00:06<00:01, 246.18it/s]
Adding requests:  79%|███████▉  | 1621/2048 [00:06<00:01, 236.67it/s]
Adding requests:  80%|████████  | 1645/2048 [00:06<00:01, 236.90it/s]
Adding requests:  81%|████████▏ | 1669/2048 [00:06<00:01, 231.29it/s]
Adding requests:  83%|████████▎ | 1695/2048 [00:06<00:01, 236.70it/s]
Adding requests:  84%|████████▍ | 1720/2048 [00:06<00:01, 238.80it/s]
Adding requests:  85%|████████▌ | 1746/2048 [00:07<00:01, 242.71it/s]
Adding requests:  87%|████████▋ | 1772/2048 [00:07<00:01, 245.48it/s]
Adding requests:  88%|████████▊ | 1797/2048 [00:07<00:01, 241.50it/s]
Adding requests:  89%|████████▉ | 1822/2048 [00:07<00:00, 242.28it/s]
Adding requests:  90%|█████████ | 1848/2048 [00:07<00:00, 244.29it/s]
Adding requests:  91%|█████████▏| 1873/2048 [00:07<00:00, 243.11it/s]
Adding requests:  93%|█████████▎| 1898/2048 [00:07<00:00, 244.36it/s]
Adding requests:  94%|█████████▍| 1925/2048 [00:07<00:00, 250.59it/s]
Adding requests:  95%|█████████▌| 1951/2048 [00:07<00:00, 241.76it/s]
Adding requests:  96%|█████████▋| 1976/2048 [00:08<00:00, 243.19it/s]
Adding requests:  98%|█████████▊| 2001/2048 [00:08<00:00, 244.78it/s]
Adding requests:  99%|█████████▉| 2026/2048 [00:08<00:00, 238.74it/s]
Adding requests: 100%|██████████| 2048/2048 [00:08<00:00, 245.55it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 66/2048 [00:01<00:51, 38.53it/s, est. speed input: 39450.68 toks/s, output: 38.53 toks/s]
Processed prompts:   4%|▍         | 82/2048 [00:03<01:40, 19.64it/s, est. speed input: 22810.11 toks/s, output: 22.28 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:05<02:17, 14.23it/s, est. speed input: 17762.67 toks/s, output: 17.35 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:07<02:44, 11.77it/s, est. speed input: 15323.13 toks/s, output: 14.96 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:09<03:03, 10.45it/s, est. speed input: 13888.31 toks/s, output: 13.56 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:11<03:16,  9.67it/s, est. speed input: 12949.16 toks/s, output: 12.65 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:13<03:25,  9.17it/s, est. speed input: 12283.47 toks/s, output: 12.00 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:15<03:31,  8.85it/s, est. speed input: 11786.03 toks/s, output: 11.51 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:17<03:34,  8.64it/s, est. speed input: 11401.02 toks/s, output: 11.13 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:19<03:36,  8.49it/s, est. speed input: 11095.06 toks/s, output: 10.84 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:21<03:36,  8.40it/s, est. speed input: 10845.30 toks/s, output: 10.59 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:23<03:36,  8.33it/s, est. speed input: 10637.30 toks/s, output: 10.39 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:23<02:49, 10.56it/s, est. speed input: 11072.83 toks/s, output: 10.81 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:25<03:02,  9.71it/s, est. speed input: 10867.60 toks/s, output: 10.61 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:27<03:11,  9.19it/s, est. speed input: 10691.23 toks/s, output: 10.44 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:29<03:16,  8.86it/s, est. speed input: 10538.00 toks/s, output: 10.29 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:31<03:19,  8.64it/s, est. speed input: 10403.71 toks/s, output: 10.16 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:33<03:21,  8.49it/s, est. speed input: 10284.95 toks/s, output: 10.04 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:35<03:21,  8.39it/s, est. speed input: 10179.31 toks/s, output: 9.94 toks/s] 
Processed prompts:  18%|█▊        | 370/2048 [00:37<03:21,  8.32it/s, est. speed input: 10084.73 toks/s, output: 9.85 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:39<03:20,  8.28it/s, est. speed input: 9999.37 toks/s, output: 9.77 toks/s] 
Processed prompts:  20%|█▉        | 402/2048 [00:41<03:19,  8.24it/s, est. speed input: 9922.02 toks/s, output: 9.69 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:43<03:18,  8.22it/s, est. speed input: 9851.76 toks/s, output: 9.62 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:45<03:16,  8.20it/s, est. speed input: 9787.39 toks/s, output: 9.56 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:47<03:15,  8.19it/s, est. speed input: 9728.46 toks/s, output: 9.50 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:49<03:13,  8.18it/s, est. speed input: 9674.06 toks/s, output: 9.45 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:51<03:11,  8.18it/s, est. speed input: 9623.77 toks/s, output: 9.40 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:53<03:09,  8.17it/s, est. speed input: 9577.19 toks/s, output: 9.35 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:53<02:28, 10.35it/s, est. speed input: 9777.17 toks/s, output: 9.55 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:55<02:38,  9.58it/s, est. speed input: 9727.36 toks/s, output: 9.50 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:57<02:44,  9.11it/s, est. speed input: 9680.96 toks/s, output: 9.45 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:59<02:48,  8.80it/s, est. speed input: 9637.57 toks/s, output: 9.41 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [01:01<02:50,  8.60it/s, est. speed input: 9596.90 toks/s, output: 9.37 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [01:03<02:51,  8.46it/s, est. speed input: 9558.80 toks/s, output: 9.33 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [01:05<02:51,  8.37it/s, est. speed input: 9522.93 toks/s, output: 9.30 toks/s]
Processed prompts:  31%|███       | 626/2048 [01:07<02:51,  8.31it/s, est. speed input: 9489.14 toks/s, output: 9.27 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [01:09<02:50,  8.26it/s, est. speed input: 9457.22 toks/s, output: 9.24 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [01:11<02:48,  8.23it/s, est. speed input: 9427.11 toks/s, output: 9.21 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [01:13<02:47,  8.21it/s, est. speed input: 9398.45 toks/s, output: 9.18 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [01:15<02:45,  8.20it/s, est. speed input: 9371.45 toks/s, output: 9.15 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [01:17<02:43,  8.19it/s, est. speed input: 9345.77 toks/s, output: 9.13 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [01:19<02:42,  8.18it/s, est. speed input: 9321.31 toks/s, output: 9.10 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [01:21<02:40,  8.17it/s, est. speed input: 9298.05 toks/s, output: 9.08 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [01:23<02:38,  8.17it/s, est. speed input: 9275.86 toks/s, output: 9.06 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [01:25<02:36,  8.17it/s, est. speed input: 9254.74 toks/s, output: 9.04 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [01:26<01:53, 10.98it/s, est. speed input: 9446.05 toks/s, output: 9.22 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [01:28<02:01, 10.11it/s, est. speed input: 9422.01 toks/s, output: 9.20 toks/s]
Processed prompts:  41%|████      | 834/2048 [01:30<02:07,  9.52it/s, est. speed input: 9399.08 toks/s, output: 9.18 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [01:32<02:11,  9.11it/s, est. speed input: 9377.12 toks/s, output: 9.16 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [01:34<02:13,  8.82it/s, est. speed input: 9356.02 toks/s, output: 9.14 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [01:36<02:15,  8.62it/s, est. speed input: 9335.82 toks/s, output: 9.12 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [01:38<02:15,  8.48it/s, est. speed input: 9316.39 toks/s, output: 9.10 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [01:40<02:15,  8.39it/s, est. speed input: 9297.73 toks/s, output: 9.08 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [01:42<02:14,  8.32it/s, est. speed input: 9279.78 toks/s, output: 9.06 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:44<02:13,  8.27it/s, est. speed input: 9262.54 toks/s, output: 9.05 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:46<02:11,  8.24it/s, est. speed input: 9245.93 toks/s, output: 9.03 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:48<02:10,  8.22it/s, est. speed input: 9229.88 toks/s, output: 9.01 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:50<02:08,  8.20it/s, est. speed input: 9214.36 toks/s, output: 9.00 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:52<02:06,  8.19it/s, est. speed input: 9199.42 toks/s, output: 8.98 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:54<02:04,  8.18it/s, est. speed input: 9184.98 toks/s, output: 8.97 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:56<02:03,  8.17it/s, est. speed input: 9171.05 toks/s, output: 8.96 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:56<01:35, 10.34it/s, est. speed input: 9265.00 toks/s, output: 9.05 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:58<01:41,  9.58it/s, est. speed input: 9250.06 toks/s, output: 9.03 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [02:00<01:45,  9.10it/s, est. speed input: 9235.60 toks/s, output: 9.02 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [02:02<01:47,  8.80it/s, est. speed input: 9221.60 toks/s, output: 9.01 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [02:04<01:47,  8.60it/s, est. speed input: 9208.05 toks/s, output: 8.99 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [02:06<01:47,  8.46it/s, est. speed input: 9194.89 toks/s, output: 8.98 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [02:08<01:46,  8.37it/s, est. speed input: 9182.14 toks/s, output: 8.97 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [02:10<01:45,  8.31it/s, est. speed input: 9169.77 toks/s, output: 8.95 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [02:12<01:44,  8.26it/s, est. speed input: 9157.83 toks/s, output: 8.94 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [02:14<01:42,  8.23it/s, est. speed input: 9146.10 toks/s, output: 8.93 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [02:16<01:41,  8.21it/s, est. speed input: 9134.81 toks/s, output: 8.92 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [02:18<01:39,  8.20it/s, est. speed input: 9123.80 toks/s, output: 8.91 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [02:20<01:37,  8.19it/s, est. speed input: 9113.08 toks/s, output: 8.90 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [02:22<01:35,  8.18it/s, est. speed input: 9102.70 toks/s, output: 8.89 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [02:24<01:33,  8.17it/s, est. speed input: 9092.61 toks/s, output: 8.88 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [02:26<01:31,  8.17it/s, est. speed input: 9082.74 toks/s, output: 8.87 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [02:26<01:10, 10.34it/s, est. speed input: 9157.93 toks/s, output: 8.94 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [02:28<01:14,  9.57it/s, est. speed input: 9147.37 toks/s, output: 8.93 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [02:30<01:17,  9.10it/s, est. speed input: 9137.10 toks/s, output: 8.92 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [02:32<01:17,  8.80it/s, est. speed input: 9127.09 toks/s, output: 8.91 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [02:34<01:17,  8.60it/s, est. speed input: 9117.35 toks/s, output: 8.90 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [02:36<01:17,  8.46it/s, est. speed input: 9107.86 toks/s, output: 8.89 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [02:38<01:16,  8.37it/s, est. speed input: 9098.59 toks/s, output: 8.89 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [02:40<01:14,  8.31it/s, est. speed input: 9089.55 toks/s, output: 8.88 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [02:42<01:13,  8.26it/s, est. speed input: 9080.77 toks/s, output: 8.87 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [02:44<01:11,  8.23it/s, est. speed input: 9072.15 toks/s, output: 8.86 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [02:46<01:09,  8.21it/s, est. speed input: 9063.76 toks/s, output: 8.85 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [02:48<01:08,  8.20it/s, est. speed input: 9055.50 toks/s, output: 8.84 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [02:50<01:06,  8.19it/s, est. speed input: 9047.50 toks/s, output: 8.84 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [02:52<01:04,  8.18it/s, est. speed input: 9039.65 toks/s, output: 8.83 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [02:54<01:02,  8.17it/s, est. speed input: 9032.00 toks/s, output: 8.82 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [02:56<01:00,  8.17it/s, est. speed input: 9024.51 toks/s, output: 8.81 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [02:58<00:58,  8.17it/s, est. speed input: 9017.18 toks/s, output: 8.81 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [02:58<00:44, 10.33it/s, est. speed input: 9078.91 toks/s, output: 8.87 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [03:00<00:46,  9.57it/s, est. speed input: 9071.10 toks/s, output: 8.86 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [03:01<00:40, 10.55it/s, est. speed input: 9103.59 toks/s, output: 8.89 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [03:03<00:42,  9.70it/s, est. speed input: 9095.66 toks/s, output: 8.88 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [03:05<00:43,  9.18it/s, est. speed input: 9087.90 toks/s, output: 8.87 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [03:07<00:43,  8.85it/s, est. speed input: 9080.25 toks/s, output: 8.87 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [03:09<00:42,  8.63it/s, est. speed input: 9072.78 toks/s, output: 8.86 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [03:11<00:41,  8.48it/s, est. speed input: 9065.46 toks/s, output: 8.85 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [03:13<00:39,  8.38it/s, est. speed input: 9058.30 toks/s, output: 8.85 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [03:15<00:38,  8.32it/s, est. speed input: 9051.25 toks/s, output: 8.84 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [03:17<00:36,  8.27it/s, est. speed input: 9044.35 toks/s, output: 8.83 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [03:19<00:34,  8.24it/s, est. speed input: 9037.62 toks/s, output: 8.83 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [03:21<00:32,  8.21it/s, est. speed input: 9030.99 toks/s, output: 8.82 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [03:23<00:30,  8.20it/s, est. speed input: 9024.48 toks/s, output: 8.81 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [03:25<00:29,  8.19it/s, est. speed input: 9018.11 toks/s, output: 8.81 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [03:27<00:27,  8.18it/s, est. speed input: 9011.87 toks/s, output: 8.80 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [03:29<00:25,  8.17it/s, est. speed input: 9005.74 toks/s, output: 8.79 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [03:30<00:18, 10.34it/s, est. speed input: 9058.52 toks/s, output: 8.85 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [03:31<00:18,  9.58it/s, est. speed input: 9052.08 toks/s, output: 8.84 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [03:33<00:17,  9.11it/s, est. speed input: 9045.77 toks/s, output: 8.83 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [03:35<00:16,  8.80it/s, est. speed input: 9039.56 toks/s, output: 8.83 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [03:37<00:14,  8.60it/s, est. speed input: 9033.47 toks/s, output: 8.82 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [03:39<00:12,  8.46it/s, est. speed input: 9027.39 toks/s, output: 8.82 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [03:41<00:11,  8.37it/s, est. speed input: 9021.38 toks/s, output: 8.81 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [03:43<00:09,  8.30it/s, est. speed input: 9015.50 toks/s, output: 8.80 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [03:45<00:07,  8.26it/s, est. speed input: 9009.73 toks/s, output: 8.80 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [03:47<00:05,  8.22it/s, est. speed input: 9003.98 toks/s, output: 8.79 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [03:49<00:03,  8.20it/s, est. speed input: 8998.33 toks/s, output: 8.79 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [03:50<00:01,  9.36it/s, est. speed input: 9024.72 toks/s, output: 8.81 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [03:50<00:00,  9.36it/s, est. speed input: 9086.82 toks/s, output: 8.87 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [03:50<00:00,  8.87it/s, est. speed input: 9086.82 toks/s, output: 8.87 toks/s]
[rank0]:[W126 02:58:25.197754202 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 02:58:27
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:59:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:59:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=91990) WARNING 01-26 02:59:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=91990) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=91990) WARNING 01-26 02:59:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 5.28 requests/s, 5409.50 total tokens/s, 5.28 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 02:59:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:59:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:59:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:59:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:59:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:59:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:59:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:59:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:59:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:59:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:59:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:59:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:59:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:59:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:59:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:59:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=91990) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=91990) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.03it/s]
(EngineCore_DP0 pid=91990) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]
(EngineCore_DP0 pid=91990) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.21s/it]
(EngineCore_DP0 pid=91990) 
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=91990) [2026-01-26 02:59:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=91990) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  8.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  9.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  9.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.71it/s]
(EngineCore_DP0 pid=91990) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.29it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.10it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  9.24it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.02it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/4096 [00:00<00:16, 240.61it/s]
Adding requests:   1%|▏         | 52/4096 [00:00<00:16, 251.39it/s]
Adding requests:   2%|▏         | 79/4096 [00:00<00:15, 259.36it/s]
Adding requests:   3%|▎         | 107/4096 [00:00<00:15, 264.13it/s]
Adding requests:   3%|▎         | 134/4096 [00:00<00:15, 259.58it/s]
Adding requests:   4%|▍         | 160/4096 [00:00<00:15, 250.84it/s]
Adding requests:   5%|▍         | 186/4096 [00:00<00:15, 245.58it/s]
Adding requests:   5%|▌         | 214/4096 [00:00<00:15, 254.78it/s]
Adding requests:   6%|▌         | 242/4096 [00:00<00:14, 260.16it/s]
Adding requests:   7%|▋         | 269/4096 [00:01<00:14, 256.24it/s]
Adding requests:   7%|▋         | 295/4096 [00:01<00:15, 244.13it/s]
Adding requests:   8%|▊         | 323/4096 [00:01<00:14, 252.92it/s]
Adding requests:   9%|▊         | 352/4096 [00:01<00:14, 262.07it/s]
Adding requests:   9%|▉         | 379/4096 [00:01<00:14, 259.70it/s]
Adding requests:  10%|▉         | 406/4096 [00:01<00:14, 250.61it/s]
Adding requests:  11%|█         | 432/4096 [00:01<00:14, 252.38it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:14, 256.66it/s]
Adding requests:  12%|█▏        | 488/4096 [00:01<00:13, 263.64it/s]
Adding requests:  13%|█▎        | 515/4096 [00:02<00:14, 251.54it/s]
Adding requests:  13%|█▎        | 544/4096 [00:02<00:13, 260.28it/s]
Adding requests:  14%|█▍        | 574/4096 [00:02<00:13, 269.83it/s]
Adding requests:  15%|█▍        | 602/4096 [00:02<00:13, 263.20it/s]
Adding requests:  15%|█▌        | 629/4096 [00:02<00:13, 252.34it/s]
Adding requests:  16%|█▌        | 655/4096 [00:02<00:13, 251.93it/s]
Adding requests:  17%|█▋        | 682/4096 [00:02<00:13, 255.56it/s]
Adding requests:  17%|█▋        | 709/4096 [00:02<00:13, 259.36it/s]
Adding requests:  18%|█▊        | 736/4096 [00:02<00:14, 239.56it/s]
Adding requests:  19%|█▊        | 763/4096 [00:03<00:13, 246.01it/s]
Adding requests:  19%|█▉        | 792/4096 [00:03<00:12, 254.50it/s]
Adding requests:  20%|█▉        | 818/4096 [00:03<00:12, 254.82it/s]
Adding requests:  21%|██        | 844/4096 [00:03<00:13, 249.84it/s]
Adding requests:  21%|██▏       | 872/4096 [00:03<00:12, 257.61it/s]
Adding requests:  22%|██▏       | 900/4096 [00:03<00:12, 261.88it/s]
Adding requests:  23%|██▎       | 927/4096 [00:03<00:12, 255.12it/s]
Adding requests:  23%|██▎       | 953/4096 [00:03<00:12, 247.21it/s]
Adding requests:  24%|██▍       | 979/4096 [00:03<00:12, 250.39it/s]
Adding requests:  25%|██▍       | 1005/4096 [00:03<00:12, 250.60it/s]
Adding requests:  25%|██▌       | 1033/4096 [00:04<00:11, 257.24it/s]
Adding requests:  26%|██▌       | 1059/4096 [00:04<00:12, 243.49it/s]
Adding requests:  26%|██▋       | 1085/4096 [00:04<00:12, 246.21it/s]
Adding requests:  27%|██▋       | 1110/4096 [00:04<00:12, 243.39it/s]
Adding requests:  28%|██▊       | 1139/4096 [00:04<00:11, 256.03it/s]
Adding requests:  28%|██▊       | 1165/4096 [00:04<00:12, 238.84it/s]
Adding requests:  29%|██▉       | 1190/4096 [00:04<00:12, 240.39it/s]
Adding requests:  30%|██▉       | 1219/4096 [00:04<00:11, 253.13it/s]
Adding requests:  30%|███       | 1245/4096 [00:04<00:11, 253.70it/s]
Adding requests:  31%|███       | 1271/4096 [00:05<00:11, 240.85it/s]
Adding requests:  32%|███▏      | 1296/4096 [00:05<00:11, 240.24it/s]
Adding requests:  32%|███▏      | 1322/4096 [00:05<00:11, 244.54it/s]
Adding requests:  33%|███▎      | 1351/4096 [00:05<00:10, 254.52it/s]
Adding requests:  34%|███▎      | 1377/4096 [00:05<00:10, 249.22it/s]
Adding requests:  34%|███▍      | 1403/4096 [00:05<00:10, 248.32it/s]
Adding requests:  35%|███▍      | 1431/4096 [00:05<00:10, 256.31it/s]
Adding requests:  36%|███▌      | 1459/4096 [00:05<00:10, 263.15it/s]
Adding requests:  36%|███▋      | 1486/4096 [00:05<00:10, 257.55it/s]
Adding requests:  37%|███▋      | 1512/4096 [00:05<00:10, 256.42it/s]
Adding requests:  38%|███▊      | 1540/4096 [00:06<00:09, 260.66it/s]
Adding requests:  38%|███▊      | 1567/4096 [00:06<00:09, 261.75it/s]
Adding requests:  39%|███▉      | 1594/4096 [00:06<00:10, 246.84it/s]
Adding requests:  40%|███▉      | 1619/4096 [00:06<00:10, 241.24it/s]
Adding requests:  40%|████      | 1645/4096 [00:06<00:09, 245.37it/s]
Adding requests:  41%|████      | 1671/4096 [00:06<00:09, 247.81it/s]
Adding requests:  41%|████▏     | 1696/4096 [00:06<00:09, 242.24it/s]
Adding requests:  42%|████▏     | 1721/4096 [00:06<00:09, 240.85it/s]
Adding requests:  43%|████▎     | 1750/4096 [00:06<00:09, 254.20it/s]
Adding requests:  43%|████▎     | 1779/4096 [00:07<00:08, 263.29it/s]
Adding requests:  44%|████▍     | 1806/4096 [00:07<00:08, 256.89it/s]
Adding requests:  45%|████▍     | 1832/4096 [00:07<00:09, 249.11it/s]
Adding requests:  45%|████▌     | 1860/4096 [00:07<00:08, 256.43it/s]
Adding requests:  46%|████▌     | 1889/4096 [00:07<00:08, 262.78it/s]
Adding requests:  47%|████▋     | 1916/4096 [00:07<00:08, 259.24it/s]
Adding requests:  47%|████▋     | 1942/4096 [00:07<00:08, 251.56it/s]
Adding requests:  48%|████▊     | 1969/4096 [00:07<00:08, 255.90it/s]
Adding requests:  49%|████▊     | 1996/4096 [00:07<00:08, 258.21it/s]
Adding requests:  49%|████▉     | 2022/4096 [00:08<00:08, 249.81it/s]
Adding requests:  50%|█████     | 2048/4096 [00:08<00:08, 240.04it/s]
Adding requests:  51%|█████     | 2073/4096 [00:08<00:08, 239.39it/s]
Adding requests:  51%|█████▏    | 2100/4096 [00:08<00:08, 247.59it/s]
Adding requests:  52%|█████▏    | 2126/4096 [00:08<00:07, 250.48it/s]
Adding requests:  53%|█████▎    | 2152/4096 [00:08<00:08, 241.70it/s]
Adding requests:  53%|█████▎    | 2177/4096 [00:08<00:07, 242.06it/s]
Adding requests:  54%|█████▍    | 2203/4096 [00:08<00:07, 246.69it/s]
Adding requests:  54%|█████▍    | 2228/4096 [00:08<00:07, 247.36it/s]
Adding requests:  55%|█████▌    | 2253/4096 [00:08<00:07, 243.12it/s]
Adding requests:  56%|█████▌    | 2281/4096 [00:09<00:07, 251.63it/s]
Adding requests:  56%|█████▋    | 2309/4096 [00:09<00:06, 257.81it/s]
Adding requests:  57%|█████▋    | 2335/4096 [00:09<00:06, 254.75it/s]
Adding requests:  58%|█████▊    | 2361/4096 [00:09<00:07, 245.00it/s]
Adding requests:  58%|█████▊    | 2390/4096 [00:09<00:06, 256.16it/s]
Adding requests:  59%|█████▉    | 2418/4096 [00:09<00:06, 260.75it/s]
Adding requests:  60%|█████▉    | 2445/4096 [00:09<00:06, 261.75it/s]
Adding requests:  60%|██████    | 2472/4096 [00:09<00:06, 249.58it/s]
Adding requests:  61%|██████    | 2499/4096 [00:09<00:06, 253.71it/s]
Adding requests:  62%|██████▏   | 2527/4096 [00:10<00:06, 259.94it/s]
Adding requests:  62%|██████▏   | 2559/4096 [00:10<00:05, 273.86it/s]
Adding requests:  63%|██████▎   | 2587/4096 [00:10<00:05, 264.98it/s]
Adding requests:  64%|██████▍   | 2614/4096 [00:10<00:05, 262.01it/s]
Adding requests:  64%|██████▍   | 2641/4096 [00:10<00:05, 262.37it/s]
Adding requests:  65%|██████▌   | 2669/4096 [00:10<00:05, 266.42it/s]
Adding requests:  66%|██████▌   | 2696/4096 [00:10<00:05, 252.12it/s]
Adding requests:  66%|██████▋   | 2722/4096 [00:10<00:05, 251.16it/s]
Adding requests:  67%|██████▋   | 2749/4096 [00:10<00:05, 255.07it/s]
Adding requests:  68%|██████▊   | 2777/4096 [00:10<00:05, 260.20it/s]
Adding requests:  68%|██████▊   | 2804/4096 [00:11<00:05, 256.24it/s]
Adding requests:  69%|██████▉   | 2830/4096 [00:11<00:05, 251.50it/s]
Adding requests:  70%|██████▉   | 2858/4096 [00:11<00:04, 257.89it/s]
Adding requests:  70%|███████   | 2885/4096 [00:11<00:04, 259.50it/s]
Adding requests:  71%|███████   | 2911/4096 [00:11<00:04, 249.40it/s]
Adding requests:  72%|███████▏  | 2937/4096 [00:11<00:04, 251.74it/s]
Adding requests:  72%|███████▏  | 2965/4096 [00:11<00:04, 258.32it/s]
Adding requests:  73%|███████▎  | 2994/4096 [00:11<00:04, 267.30it/s]
Adding requests:  74%|███████▍  | 3021/4096 [00:11<00:04, 262.95it/s]
Adding requests:  74%|███████▍  | 3048/4096 [00:12<00:04, 255.89it/s]
Adding requests:  75%|███████▌  | 3078/4096 [00:12<00:03, 266.88it/s]
Adding requests:  76%|███████▌  | 3106/4096 [00:12<00:03, 268.20it/s]
Adding requests:  76%|███████▋  | 3133/4096 [00:12<00:03, 264.61it/s]
Adding requests:  77%|███████▋  | 3160/4096 [00:12<00:03, 250.35it/s]
Adding requests:  78%|███████▊  | 3189/4096 [00:12<00:03, 259.80it/s]
Adding requests:  79%|███████▊  | 3216/4096 [00:12<00:03, 261.59it/s]
Adding requests:  79%|███████▉  | 3243/4096 [00:12<00:03, 262.90it/s]
Adding requests:  80%|███████▉  | 3270/4096 [00:12<00:03, 249.28it/s]
Adding requests:  80%|████████  | 3296/4096 [00:12<00:03, 247.59it/s]
Adding requests:  81%|████████  | 3324/4096 [00:13<00:03, 254.11it/s]
Adding requests:  82%|████████▏ | 3351/4096 [00:13<00:02, 256.31it/s]
Adding requests:  82%|████████▏ | 3377/4096 [00:13<00:02, 250.49it/s]
Adding requests:  83%|████████▎ | 3404/4096 [00:13<00:02, 253.46it/s]
Adding requests:  84%|████████▍ | 3432/4096 [00:13<00:02, 260.16it/s]
Adding requests:  84%|████████▍ | 3459/4096 [00:13<00:02, 262.50it/s]
Adding requests:  85%|████████▌ | 3486/4096 [00:13<00:02, 248.41it/s]
Adding requests:  86%|████████▌ | 3515/4096 [00:13<00:02, 258.28it/s]
Adding requests:  87%|████████▋ | 3545/4096 [00:13<00:02, 269.64it/s]
Adding requests:  87%|████████▋ | 3573/4096 [00:14<00:01, 265.62it/s]
Adding requests:  88%|████████▊ | 3600/4096 [00:14<00:01, 251.97it/s]
Adding requests:  89%|████████▊ | 3628/4096 [00:14<00:01, 257.73it/s]
Adding requests:  89%|████████▉ | 3655/4096 [00:14<00:01, 260.54it/s]
Adding requests:  90%|████████▉ | 3682/4096 [00:14<00:01, 252.99it/s]
Adding requests:  91%|█████████ | 3708/4096 [00:14<00:01, 233.99it/s]
Adding requests:  91%|█████████ | 3736/4096 [00:14<00:01, 245.77it/s]
Adding requests:  92%|█████████▏| 3762/4096 [00:14<00:01, 248.06it/s]
Adding requests:  92%|█████████▏| 3788/4096 [00:14<00:01, 243.27it/s]
Adding requests:  93%|█████████▎| 3813/4096 [00:15<00:01, 229.51it/s]
Adding requests:  94%|█████████▍| 3841/4096 [00:15<00:01, 242.50it/s]
Adding requests:  94%|█████████▍| 3869/4096 [00:15<00:00, 252.69it/s]
Adding requests:  95%|█████████▌| 3895/4096 [00:15<00:00, 244.86it/s]
Adding requests:  96%|█████████▌| 3920/4096 [00:15<00:00, 232.62it/s]
Adding requests:  96%|█████████▋| 3946/4096 [00:15<00:00, 240.01it/s]
Adding requests:  97%|█████████▋| 3973/4096 [00:15<00:00, 246.05it/s]
Adding requests:  98%|█████████▊| 3998/4096 [00:15<00:00, 245.83it/s]
Adding requests:  98%|█████████▊| 4023/4096 [00:15<00:00, 240.51it/s]
Adding requests:  99%|█████████▉| 4050/4096 [00:16<00:00, 247.21it/s]
Adding requests: 100%|█████████▉| 4078/4096 [00:16<00:00, 254.31it/s]
Adding requests: 100%|██████████| 4096/4096 [00:16<00:00, 253.04it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 66/4096 [00:02<02:33, 26.23it/s, est. speed input: 26859.03 toks/s, output: 26.23 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:08<06:40,  9.99it/s, est. speed input: 11695.17 toks/s, output: 11.42 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:14<08:47,  7.51it/s, est. speed input: 9089.07 toks/s, output: 8.88 toks/s] 
Processed prompts:   4%|▍         | 162/4096 [00:19<09:01,  7.26it/s, est. speed input: 8580.55 toks/s, output: 8.38 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:25<10:04,  6.46it/s, est. speed input: 7821.51 toks/s, output: 7.64 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:31<10:42,  6.03it/s, est. speed input: 7355.15 toks/s, output: 7.18 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:37<11:05,  5.77it/s, est. speed input: 7038.56 toks/s, output: 6.87 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:43<11:19,  5.60it/s, est. speed input: 6810.50 toks/s, output: 6.65 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:48<10:36,  5.93it/s, est. speed input: 6827.98 toks/s, output: 6.67 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:54<10:54,  5.71it/s, est. speed input: 6668.96 toks/s, output: 6.51 toks/s]
Processed prompts:   9%|▉         | 386/4096 [01:00<11:05,  5.57it/s, est. speed input: 6541.73 toks/s, output: 6.39 toks/s]
Processed prompts:  10%|█         | 418/4096 [01:06<11:11,  5.48it/s, est. speed input: 6437.74 toks/s, output: 6.29 toks/s]
Processed prompts:  11%|█         | 450/4096 [01:12<11:13,  5.42it/s, est. speed input: 6351.12 toks/s, output: 6.20 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [01:17<10:25,  5.78it/s, est. speed input: 6389.75 toks/s, output: 6.24 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [01:23<10:37,  5.62it/s, est. speed input: 6317.90 toks/s, output: 6.17 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [01:29<10:44,  5.51it/s, est. speed input: 6255.61 toks/s, output: 6.11 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [01:35<10:47,  5.44it/s, est. speed input: 6201.26 toks/s, output: 6.06 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [01:41<10:47,  5.39it/s, est. speed input: 6153.38 toks/s, output: 6.01 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [01:47<10:45,  5.35it/s, est. speed input: 6110.95 toks/s, output: 5.97 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [01:52<09:58,  5.72it/s, est. speed input: 6147.33 toks/s, output: 6.00 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [01:58<10:07,  5.58it/s, est. speed input: 6109.09 toks/s, output: 5.97 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [02:04<10:12,  5.48it/s, est. speed input: 6074.45 toks/s, output: 5.93 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [02:10<10:07,  5.48it/s, est. speed input: 6052.77 toks/s, output: 5.91 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [02:16<10:08,  5.41it/s, est. speed input: 6023.71 toks/s, output: 5.88 toks/s]
Processed prompts:  20%|██        | 834/4096 [02:21<09:25,  5.77it/s, est. speed input: 6055.77 toks/s, output: 5.91 toks/s]
Processed prompts:  21%|██        | 866/4096 [02:27<09:35,  5.61it/s, est. speed input: 6028.91 toks/s, output: 5.89 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [02:33<09:40,  5.51it/s, est. speed input: 6003.89 toks/s, output: 5.86 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [02:39<09:42,  5.43it/s, est. speed input: 5980.65 toks/s, output: 5.84 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [02:45<09:42,  5.38it/s, est. speed input: 5959.10 toks/s, output: 5.82 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [02:49<08:59,  5.75it/s, est. speed input: 5987.76 toks/s, output: 5.85 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [02:56<09:08,  5.60it/s, est. speed input: 5967.89 toks/s, output: 5.83 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [03:02<09:12,  5.50it/s, est. speed input: 5949.16 toks/s, output: 5.81 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [03:08<09:14,  5.42it/s, est. speed input: 5930.50 toks/s, output: 5.79 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [03:14<09:13,  5.37it/s, est. speed input: 5913.69 toks/s, output: 5.78 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [03:20<09:10,  5.34it/s, est. speed input: 5898.08 toks/s, output: 5.76 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [03:25<08:28,  5.72it/s, est. speed input: 5923.69 toks/s, output: 5.78 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [03:31<08:35,  5.58it/s, est. speed input: 5908.60 toks/s, output: 5.77 toks/s]
Processed prompts:  31%|███       | 1250/4096 [03:37<08:38,  5.48it/s, est. speed input: 5894.41 toks/s, output: 5.76 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [03:43<08:39,  5.42it/s, est. speed input: 5881.00 toks/s, output: 5.74 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [03:49<08:37,  5.37it/s, est. speed input: 5868.28 toks/s, output: 5.73 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [03:53<07:59,  5.74it/s, est. speed input: 5890.72 toks/s, output: 5.75 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [04:00<08:06,  5.59it/s, est. speed input: 5878.30 toks/s, output: 5.74 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [04:06<08:09,  5.49it/s, est. speed input: 5866.53 toks/s, output: 5.73 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [04:12<08:09,  5.42it/s, est. speed input: 5855.28 toks/s, output: 5.72 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [04:18<08:07,  5.38it/s, est. speed input: 5844.59 toks/s, output: 5.71 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [04:24<08:04,  5.35it/s, est. speed input: 5834.39 toks/s, output: 5.70 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [04:29<07:27,  5.72it/s, est. speed input: 5854.53 toks/s, output: 5.72 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [04:35<07:32,  5.58it/s, est. speed input: 5844.50 toks/s, output: 5.71 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [04:40<07:30,  5.54it/s, est. speed input: 5839.25 toks/s, output: 5.70 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [04:47<07:31,  5.46it/s, est. speed input: 5829.95 toks/s, output: 5.69 toks/s]
Processed prompts:  41%|████      | 1666/4096 [04:53<07:29,  5.40it/s, est. speed input: 5821.07 toks/s, output: 5.68 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [04:57<06:56,  5.76it/s, est. speed input: 5839.43 toks/s, output: 5.70 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [05:03<07:02,  5.61it/s, est. speed input: 5830.66 toks/s, output: 5.69 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [05:09<07:04,  5.50it/s, est. speed input: 5822.16 toks/s, output: 5.69 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [05:15<07:03,  5.43it/s, est. speed input: 5814.06 toks/s, output: 5.68 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [05:22<07:01,  5.38it/s, est. speed input: 5806.28 toks/s, output: 5.67 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [05:26<06:29,  5.75it/s, est. speed input: 5823.22 toks/s, output: 5.69 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [05:32<06:34,  5.60it/s, est. speed input: 5815.53 toks/s, output: 5.68 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [05:38<06:35,  5.50it/s, est. speed input: 5808.08 toks/s, output: 5.67 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [05:44<06:34,  5.43it/s, est. speed input: 5800.88 toks/s, output: 5.66 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [05:50<06:32,  5.38it/s, est. speed input: 5793.93 toks/s, output: 5.66 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [05:57<06:28,  5.35it/s, est. speed input: 5787.23 toks/s, output: 5.65 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [06:01<05:57,  5.72it/s, est. speed input: 5802.91 toks/s, output: 5.67 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [06:07<06:00,  5.58it/s, est. speed input: 5796.31 toks/s, output: 5.66 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [06:13<06:01,  5.48it/s, est. speed input: 5789.81 toks/s, output: 5.65 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [06:19<06:00,  5.42it/s, est. speed input: 5783.41 toks/s, output: 5.65 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [06:25<05:53,  5.43it/s, est. speed input: 5780.45 toks/s, output: 5.64 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [06:30<05:25,  5.79it/s, est. speed input: 5795.16 toks/s, output: 5.66 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [06:36<05:29,  5.63it/s, est. speed input: 5789.44 toks/s, output: 5.65 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [06:42<05:30,  5.51it/s, est. speed input: 5783.26 toks/s, output: 5.65 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [06:48<05:29,  5.43it/s, est. speed input: 5777.19 toks/s, output: 5.64 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [06:54<05:26,  5.38it/s, est. speed input: 5771.45 toks/s, output: 5.64 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [06:59<04:59,  5.76it/s, est. speed input: 5785.57 toks/s, output: 5.65 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [07:05<05:02,  5.60it/s, est. speed input: 5780.09 toks/s, output: 5.64 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [07:11<05:02,  5.50it/s, est. speed input: 5774.75 toks/s, output: 5.64 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [07:17<05:00,  5.43it/s, est. speed input: 5769.55 toks/s, output: 5.63 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [07:23<04:56,  5.38it/s, est. speed input: 5764.60 toks/s, output: 5.63 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [07:29<04:52,  5.35it/s, est. speed input: 5759.71 toks/s, output: 5.62 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [07:34<04:28,  5.72it/s, est. speed input: 5772.23 toks/s, output: 5.64 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [07:40<04:29,  5.57it/s, est. speed input: 5767.22 toks/s, output: 5.63 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [07:46<04:28,  5.48it/s, est. speed input: 5762.39 toks/s, output: 5.63 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [07:52<04:25,  5.42it/s, est. speed input: 5757.72 toks/s, output: 5.62 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [07:58<04:21,  5.37it/s, est. speed input: 5753.16 toks/s, output: 5.62 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [08:03<03:59,  5.74it/s, est. speed input: 5765.33 toks/s, output: 5.63 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [08:09<03:59,  5.59it/s, est. speed input: 5760.79 toks/s, output: 5.63 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [08:15<03:58,  5.49it/s, est. speed input: 5756.36 toks/s, output: 5.62 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [08:21<03:55,  5.43it/s, est. speed input: 5752.04 toks/s, output: 5.62 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [08:27<03:51,  5.38it/s, est. speed input: 5747.82 toks/s, output: 5.61 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [08:32<03:31,  5.75it/s, est. speed input: 5759.25 toks/s, output: 5.62 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [08:38<03:31,  5.60it/s, est. speed input: 5755.04 toks/s, output: 5.62 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [08:44<03:29,  5.49it/s, est. speed input: 5750.92 toks/s, output: 5.62 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [08:50<03:26,  5.43it/s, est. speed input: 5746.89 toks/s, output: 5.61 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [08:56<03:21,  5.38it/s, est. speed input: 5742.95 toks/s, output: 5.61 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [09:02<03:17,  5.35it/s, est. speed input: 5739.13 toks/s, output: 5.60 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [09:07<02:58,  5.72it/s, est. speed input: 5749.88 toks/s, output: 5.62 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [09:13<02:57,  5.58it/s, est. speed input: 5746.04 toks/s, output: 5.61 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [09:19<02:54,  5.48it/s, est. speed input: 5742.27 toks/s, output: 5.61 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [09:25<02:50,  5.42it/s, est. speed input: 5738.59 toks/s, output: 5.60 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [09:31<02:46,  5.37it/s, est. speed input: 5735.01 toks/s, output: 5.60 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [09:36<02:30,  5.74it/s, est. speed input: 5745.23 toks/s, output: 5.61 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [09:42<02:28,  5.59it/s, est. speed input: 5741.63 toks/s, output: 5.61 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [09:48<02:25,  5.49it/s, est. speed input: 5738.09 toks/s, output: 5.60 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [09:54<02:21,  5.42it/s, est. speed input: 5734.61 toks/s, output: 5.60 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [10:00<02:16,  5.38it/s, est. speed input: 5731.21 toks/s, output: 5.60 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [10:06<02:11,  5.34it/s, est. speed input: 5727.87 toks/s, output: 5.59 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [10:11<01:57,  5.72it/s, est. speed input: 5737.67 toks/s, output: 5.60 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [10:17<01:54,  5.58it/s, est. speed input: 5734.34 toks/s, output: 5.60 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [10:23<01:50,  5.48it/s, est. speed input: 5731.02 toks/s, output: 5.60 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [10:29<01:46,  5.41it/s, est. speed input: 5727.70 toks/s, output: 5.59 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [10:35<01:40,  5.37it/s, est. speed input: 5724.53 toks/s, output: 5.59 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [10:40<01:28,  5.74it/s, est. speed input: 5734.02 toks/s, output: 5.60 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [10:46<01:25,  5.60it/s, est. speed input: 5731.13 toks/s, output: 5.60 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [10:52<01:21,  5.49it/s, est. speed input: 5727.72 toks/s, output: 5.59 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [10:58<01:15,  5.47it/s, est. speed input: 5726.30 toks/s, output: 5.59 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [11:04<01:10,  5.41it/s, est. speed input: 5723.21 toks/s, output: 5.59 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [11:09<01:00,  5.78it/s, est. speed input: 5732.45 toks/s, output: 5.60 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [11:15<00:56,  5.62it/s, est. speed input: 5729.47 toks/s, output: 5.60 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [11:21<00:51,  5.51it/s, est. speed input: 5726.53 toks/s, output: 5.59 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [11:27<00:46,  5.44it/s, est. speed input: 5723.65 toks/s, output: 5.59 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [11:33<00:41,  5.39it/s, est. speed input: 5720.88 toks/s, output: 5.59 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [11:39<00:35,  5.41it/s, est. speed input: 5719.80 toks/s, output: 5.59 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [11:43<00:27,  5.77it/s, est. speed input: 5728.19 toks/s, output: 5.59 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [11:50<00:22,  5.61it/s, est. speed input: 5725.33 toks/s, output: 5.59 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [11:56<00:17,  5.50it/s, est. speed input: 5722.54 toks/s, output: 5.59 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [12:02<00:11,  5.43it/s, est. speed input: 5719.80 toks/s, output: 5.59 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [12:08<00:05,  5.44it/s, est. speed input: 5718.87 toks/s, output: 5.58 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [12:08<00:00,  5.44it/s, est. speed input: 5761.06 toks/s, output: 5.63 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [12:08<00:00,  5.63it/s, est. speed input: 5761.06 toks/s, output: 5.63 toks/s]
[rank0]:[W126 03:12:01.548362107 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 03:12:04
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 03:13:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 03:13:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=104255) WARNING 01-26 03:13:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=104255) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=104255) WARNING 01-26 03:13:37 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     def forward(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     raise e
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "<eval_with_key>.58", line 339, in forward
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     submod_6 = self.submod_6(getitem_13, s72, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, getitem_14, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_13 = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = getitem_14 = l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 177, in __call__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     self._maybe_compile_for_range_entry(range_entry, args)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 144, in _maybe_compile_for_range_entry
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     range_entry.runnable = self.vllm_backend.compiler_manager.compile(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/backends.py", line 244, in compile
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     compiled_graph, handle = self.compiler.compile(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                              ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 233, in compile
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     compiled_graph = standalone_compile(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                      ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/__init__.py", line 422, in standalone_compile
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return standalone_compile(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 252, in standalone_compile
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     compiled_fn = compile_fx(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                   ^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 2413, in compile_fx
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return compile_fx(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 2681, in compile_fx
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return aot_autograd(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/backends/common.py", line 117, in __call__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1096, in aot_module_simplified
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     aot_state = create_aot_state(
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                 ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 522, in create_aot_state
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     stack.enter_context(preserve_rng_state())
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/lib/python3.12/contextlib.py", line 526, in enter_context
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     result = _enter(cm)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]              ^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/lib/python3.12/contextlib.py", line 137, in __enter__
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return next(self.gen)
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 2220, in preserve_rng_state
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     cuda_rng_state = torch.clone(torch.cuda.get_rng_state())
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/random.py", line 43, in get_rng_state
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]     return default_generator.get_state()
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=104255) ERROR 01-26 03:13:44 [core.py:866] 

STDERR:
[2026-01-26 03:13:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:13:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:13:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:13:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:13:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:13:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:13:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:13:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 03:13:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 03:13:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:13:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 03:13:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 03:13:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 03:13:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 03:13:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 03:13:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 03:13:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 03:13:24.586040678 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=104255) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=104255) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.03it/s]
(EngineCore_DP0 pid=104255) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.26s/it]
(EngineCore_DP0 pid=104255) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.21s/it]
(EngineCore_DP0 pid=104255) 
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=104255) [2026-01-26 03:13:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=104255) Process EngineCore_DP0:
(EngineCore_DP0 pid=104255) Traceback (most recent call last):
(EngineCore_DP0 pid=104255)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=104255)     self.run()
(EngineCore_DP0 pid=104255)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=104255)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=104255)     raise e
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=104255)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=104255)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=104255)     super().__init__(
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=104255)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=104255)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=104255)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=104255)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=104255)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=104255)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=104255)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=104255)     return func(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=104255)     return func(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=104255)     self.model_runner.profile_run()
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=104255)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=104255)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=104255)     return func(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=104255)     outputs = self.model(
(EngineCore_DP0 pid=104255)               ^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=104255)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=104255)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=104255)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=104255)     hidden_states = self.model(
(EngineCore_DP0 pid=104255)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=104255)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=104255)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=104255)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=104255)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=104255)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=104255)     def forward(
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=104255)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=104255)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=104255)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=104255)     raise e
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=104255)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=104255)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=104255)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "<eval_with_key>.58", line 339, in forward
(EngineCore_DP0 pid=104255)     submod_6 = self.submod_6(getitem_13, s72, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, getitem_14, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_13 = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = getitem_14 = l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=104255)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=104255)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 177, in __call__
(EngineCore_DP0 pid=104255)     self._maybe_compile_for_range_entry(range_entry, args)
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 144, in _maybe_compile_for_range_entry
(EngineCore_DP0 pid=104255)     range_entry.runnable = self.vllm_backend.compiler_manager.compile(
(EngineCore_DP0 pid=104255)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/compilation/backends.py", line 244, in compile
(EngineCore_DP0 pid=104255)     compiled_graph, handle = self.compiler.compile(
(EngineCore_DP0 pid=104255)                              ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/root/vllmbench/vllm/compilation/compiler_interface.py", line 233, in compile
(EngineCore_DP0 pid=104255)     compiled_graph = standalone_compile(
(EngineCore_DP0 pid=104255)                      ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/__init__.py", line 422, in standalone_compile
(EngineCore_DP0 pid=104255)     return standalone_compile(
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 252, in standalone_compile
(EngineCore_DP0 pid=104255)     compiled_fn = compile_fx(
(EngineCore_DP0 pid=104255)                   ^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 2413, in compile_fx
(EngineCore_DP0 pid=104255)     return compile_fx(
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 2681, in compile_fx
(EngineCore_DP0 pid=104255)     return aot_autograd(
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/backends/common.py", line 117, in __call__
(EngineCore_DP0 pid=104255)     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
(EngineCore_DP0 pid=104255)          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1096, in aot_module_simplified
(EngineCore_DP0 pid=104255)     aot_state = create_aot_state(
(EngineCore_DP0 pid=104255)                 ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 522, in create_aot_state
(EngineCore_DP0 pid=104255)     stack.enter_context(preserve_rng_state())
(EngineCore_DP0 pid=104255)   File "/usr/lib/python3.12/contextlib.py", line 526, in enter_context
(EngineCore_DP0 pid=104255)     result = _enter(cm)
(EngineCore_DP0 pid=104255)              ^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/lib/python3.12/contextlib.py", line 137, in __enter__
(EngineCore_DP0 pid=104255)     return next(self.gen)
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 2220, in preserve_rng_state
(EngineCore_DP0 pid=104255)     cuda_rng_state = torch.clone(torch.cuda.get_rng_state())
(EngineCore_DP0 pid=104255)                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/random.py", line 43, in get_rng_state
(EngineCore_DP0 pid=104255)     return default_generator.get_state()
(EngineCore_DP0 pid=104255)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=104255) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=104255) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=104255) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=104255) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=104255) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=104255) 
[rank0]:[W126 03:13:45.171259920 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 06:26:53
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 06:27:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 06:27:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=287741) WARNING 01-26 06:27:09 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=287741) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=287741) WARNING 01-26 06:27:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=287741) ERROR 01-26 06:28:01 [core.py:866] ValueError: To serve at least one request with the models's max seq len (513), (0.10 GiB KV cache is needed, which is larger than the available KV cache memory (0.01 GiB). Based on the available memory, the estimated maximum model length is 48. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 06:27:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:27:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:27:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:27:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:27:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:27:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:27:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:27:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 06:27:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:27:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:27:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:27:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:27:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:27:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:27:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:27:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:27:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=287741) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=287741) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:17,  5.82s/it]
(EngineCore_DP0 pid=287741) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:11<00:11,  5.59s/it]
(EngineCore_DP0 pid=287741) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:12<00:03,  3.59s/it]
(EngineCore_DP0 pid=287741) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  3.77s/it]
(EngineCore_DP0 pid=287741) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.13s/it]
(EngineCore_DP0 pid=287741) 
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=287741) [2026-01-26 06:27:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=287741) Process EngineCore_DP0:
(EngineCore_DP0 pid=287741) Traceback (most recent call last):
(EngineCore_DP0 pid=287741)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=287741)     self.run()
(EngineCore_DP0 pid=287741)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=287741)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=287741)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=287741)     raise e
(EngineCore_DP0 pid=287741)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=287741)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=287741)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287741)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=287741)     super().__init__(
(EngineCore_DP0 pid=287741)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=287741)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=287741)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287741)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=287741)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=287741)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287741)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=287741)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=287741)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=287741)     raise ValueError(
(EngineCore_DP0 pid=287741) ValueError: To serve at least one request with the models's max seq len (513), (0.10 GiB KV cache is needed, which is larger than the available KV cache memory (0.01 GiB). Based on the available memory, the estimated maximum model length is 48. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 06:28:02.038987693 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=1024 ==========
Time: 2026-01-26 06:28:06
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 06:28:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 06:28:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=289008) WARNING 01-26 06:28:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=289008) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=289008) WARNING 01-26 06:28:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=289008) ERROR 01-26 06:29:18 [core.py:866] ValueError: To serve at least one request with the models's max seq len (1025), (0.19 GiB KV cache is needed, which is larger than the available KV cache memory (0.00 GiB).  Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 06:28:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:28:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:28:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:28:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:28:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:28:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:28:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:28:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 06:28:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:28:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:28:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:28:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:28:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:28:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:28:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:28:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:28:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=289008) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=289008) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.94s/it]
(EngineCore_DP0 pid=289008) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:10<00:10,  5.23s/it]
(EngineCore_DP0 pid=289008) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.41s/it]
(EngineCore_DP0 pid=289008) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:17<00:00,  4.33s/it]
(EngineCore_DP0 pid=289008) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:17<00:00,  4.34s/it]
(EngineCore_DP0 pid=289008) 
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=289008) [2026-01-26 06:28:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=289008) Process EngineCore_DP0:
(EngineCore_DP0 pid=289008) Traceback (most recent call last):
(EngineCore_DP0 pid=289008)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=289008)     self.run()
(EngineCore_DP0 pid=289008)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=289008)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=289008)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=289008)     raise e
(EngineCore_DP0 pid=289008)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=289008)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=289008)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=289008)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=289008)     super().__init__(
(EngineCore_DP0 pid=289008)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=289008)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=289008)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=289008)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=289008)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=289008)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=289008)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=289008)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=289008)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=289008)     raise ValueError(
(EngineCore_DP0 pid=289008) ValueError: To serve at least one request with the models's max seq len (1025), (0.19 GiB KV cache is needed, which is larger than the available KV cache memory (0.00 GiB).  Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 06:29:19.115892385 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=1024

========== M=2048 ==========
Time: 2026-01-26 06:29:24
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 06:29:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 06:29:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=290348) WARNING 01-26 06:29:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=290348) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=290348) WARNING 01-26 06:30:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=290348) ERROR 01-26 06:30:26 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 06:29:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:29:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:29:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:29:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:29:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:29:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:29:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:29:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 06:29:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:29:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:29:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:29:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:29:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:29:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:29:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:29:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:29:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=290348) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=290348) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.47s/it]
(EngineCore_DP0 pid=290348) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.45s/it]
(EngineCore_DP0 pid=290348) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.03it/s]
(EngineCore_DP0 pid=290348) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
(EngineCore_DP0 pid=290348) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.19s/it]
(EngineCore_DP0 pid=290348) 
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=290348) [2026-01-26 06:29:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=290348) Process EngineCore_DP0:
(EngineCore_DP0 pid=290348) Traceback (most recent call last):
(EngineCore_DP0 pid=290348)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=290348)     self.run()
(EngineCore_DP0 pid=290348)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=290348)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=290348)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=290348)     raise e
(EngineCore_DP0 pid=290348)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=290348)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=290348)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290348)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=290348)     super().__init__(
(EngineCore_DP0 pid=290348)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=290348)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=290348)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290348)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=290348)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=290348)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290348)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=290348)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=290348)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=290348)     raise ValueError(
(EngineCore_DP0 pid=290348) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 06:30:27.726503867 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=2048

========== M=4096 ==========
Time: 2026-01-26 06:30:31
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 06:30:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 06:30:42 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=291483) WARNING 01-26 06:30:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=291483) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=291483) WARNING 01-26 06:31:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=291483) ERROR 01-26 06:31:38 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 06:30:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:30:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:30:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:30:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:30:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:30:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:30:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:30:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 06:30:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:30:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:30:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:30:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:30:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:30:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:30:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:30:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:30:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=291483) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=291483) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.46s/it]
(EngineCore_DP0 pid=291483) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.44s/it]
(EngineCore_DP0 pid=291483) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.03it/s]
(EngineCore_DP0 pid=291483) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
(EngineCore_DP0 pid=291483) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.18s/it]
(EngineCore_DP0 pid=291483) 
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=291483) [2026-01-26 06:30:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=291483) Process EngineCore_DP0:
(EngineCore_DP0 pid=291483) Traceback (most recent call last):
(EngineCore_DP0 pid=291483)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=291483)     self.run()
(EngineCore_DP0 pid=291483)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=291483)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=291483)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=291483)     raise e
(EngineCore_DP0 pid=291483)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=291483)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=291483)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291483)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=291483)     super().__init__(
(EngineCore_DP0 pid=291483)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=291483)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=291483)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291483)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=291483)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=291483)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291483)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=291483)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=291483)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=291483)     raise ValueError(
(EngineCore_DP0 pid=291483) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 06:31:39.091520753 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=4096

========== M=8192 ==========
Time: 2026-01-26 06:31:43
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 06:31:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 06:32:00 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=292784) WARNING 01-26 06:32:06 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=292784) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=292784) WARNING 01-26 06:32:29 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=292784) ERROR 01-26 06:33:10 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 06:31:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:31:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:31:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:31:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:31:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:31:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:31:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:31:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:31:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:31:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:31:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:31:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:31:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:31:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 06:32:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:32:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:32:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:32:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:32:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:32:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:32:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:32:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:32:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:32:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:32:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:32:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:32:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:32:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=292784) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=292784) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.47s/it]
(EngineCore_DP0 pid=292784) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.45s/it]
(EngineCore_DP0 pid=292784) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.02it/s]
(EngineCore_DP0 pid=292784) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.19s/it]
(EngineCore_DP0 pid=292784) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.21s/it]
(EngineCore_DP0 pid=292784) 
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=292784) [2026-01-26 06:32:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=292784) Process EngineCore_DP0:
(EngineCore_DP0 pid=292784) Traceback (most recent call last):
(EngineCore_DP0 pid=292784)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=292784)     self.run()
(EngineCore_DP0 pid=292784)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=292784)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=292784)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=292784)     raise e
(EngineCore_DP0 pid=292784)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=292784)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=292784)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=292784)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=292784)     super().__init__(
(EngineCore_DP0 pid=292784)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=292784)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=292784)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=292784)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=292784)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=292784)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=292784)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=292784)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=292784)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=292784)     raise ValueError(
(EngineCore_DP0 pid=292784) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 06:33:12.246742730 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=8192

========== M=16384 ==========
Time: 2026-01-26 06:33:16
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 06:33:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 06:33:39 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=294387) WARNING 01-26 06:33:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=294387) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=294387) WARNING 01-26 06:34:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=294387) ERROR 01-26 06:36:40 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 06:33:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:33:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:33:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:33:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:33:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:33:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:33:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:33:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 06:33:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:33:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:33:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:33:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:33:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:33:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:33:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:33:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:33:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=294387) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=294387) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.48s/it]
(EngineCore_DP0 pid=294387) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.45s/it]
(EngineCore_DP0 pid=294387) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.02it/s]
(EngineCore_DP0 pid=294387) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
(EngineCore_DP0 pid=294387) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.19s/it]
(EngineCore_DP0 pid=294387) 
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=294387) [2026-01-26 06:33:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=294387) [rank0]:W0126 06:34:21.707000 294387 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=294387) [rank0]:W0126 06:34:21.790000 294387 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=294387) [rank0]:W0126 06:34:23.204000 294387 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=294387) [rank0]:W0126 06:34:23.321000 294387 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=294387) Process EngineCore_DP0:
(EngineCore_DP0 pid=294387) Traceback (most recent call last):
(EngineCore_DP0 pid=294387)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=294387)     self.run()
(EngineCore_DP0 pid=294387)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=294387)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=294387)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=294387)     raise e
(EngineCore_DP0 pid=294387)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=294387)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=294387)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294387)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=294387)     super().__init__(
(EngineCore_DP0 pid=294387)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=294387)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=294387)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294387)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=294387)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=294387)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294387)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=294387)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=294387)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=294387)     raise ValueError(
(EngineCore_DP0 pid=294387) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 06:36:42.658986350 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=32768 ==========
Time: 2026-01-26 06:36:44
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 06:37:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 06:37:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=297747) WARNING 01-26 06:37:31 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=297747) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=297747) WARNING 01-26 06:37:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=297747) ERROR 01-26 06:42:31 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 06:37:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:37:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:37:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:37:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:37:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:37:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:37:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:37:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 06:37:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:37:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:37:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:37:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:37:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:37:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:37:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:37:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:37:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=297747) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=297747) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.54s/it]
(EngineCore_DP0 pid=297747) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.50s/it]
(EngineCore_DP0 pid=297747) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
(EngineCore_DP0 pid=297747) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.20s/it]
(EngineCore_DP0 pid=297747) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.24s/it]
(EngineCore_DP0 pid=297747) 
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=297747) [2026-01-26 06:37:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=297747) [rank0]:W0126 06:38:08.556000 297747 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=297747) [rank0]:W0126 06:38:08.636000 297747 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=297747) [rank0]:W0126 06:38:10.323000 297747 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=297747) [rank0]:W0126 06:38:10.453000 297747 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=297747) Process EngineCore_DP0:
(EngineCore_DP0 pid=297747) Traceback (most recent call last):
(EngineCore_DP0 pid=297747)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=297747)     self.run()
(EngineCore_DP0 pid=297747)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=297747)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=297747)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=297747)     raise e
(EngineCore_DP0 pid=297747)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=297747)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=297747)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297747)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=297747)     super().__init__(
(EngineCore_DP0 pid=297747)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=297747)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=297747)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297747)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=297747)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=297747)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297747)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=297747)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=297747)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=297747)     raise ValueError(
(EngineCore_DP0 pid=297747) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 06:42:32.445153161 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=65536 ==========
Time: 2026-01-26 06:42:36
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 06:43:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 06:43:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=303769) WARNING 01-26 06:44:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=303769) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=303769) WARNING 01-26 06:44:29 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=303769) ERROR 01-26 06:53:45 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 06:43:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:43:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:43:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:43:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:43:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:43:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:43:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:43:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 06:43:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 06:43:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:43:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 06:43:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 06:43:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 06:43:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 06:43:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 06:43:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 06:43:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 06:44:01.716345622 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=303769) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=303769) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:06,  2.06s/it]
(EngineCore_DP0 pid=303769) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.99s/it]
(EngineCore_DP0 pid=303769) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.32s/it]
(EngineCore_DP0 pid=303769) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.51s/it]
(EngineCore_DP0 pid=303769) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.58s/it]
(EngineCore_DP0 pid=303769) 
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=303769) [2026-01-26 06:44:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=303769) [rank0]:W0126 06:44:41.765000 303769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=303769) [rank0]:W0126 06:44:41.847000 303769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=303769) [rank0]:W0126 06:44:46.265000 303769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=303769) [rank0]:W0126 06:44:46.381000 303769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=303769) Process EngineCore_DP0:
(EngineCore_DP0 pid=303769) Traceback (most recent call last):
(EngineCore_DP0 pid=303769)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=303769)     self.run()
(EngineCore_DP0 pid=303769)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=303769)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=303769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=303769)     raise e
(EngineCore_DP0 pid=303769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=303769)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=303769)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=303769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=303769)     super().__init__(
(EngineCore_DP0 pid=303769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=303769)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=303769)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=303769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=303769)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=303769)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=303769)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=303769)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=303769)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=303769)     raise ValueError(
(EngineCore_DP0 pid=303769) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 06:53:47.031009147 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 20:51:45
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:51:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:51:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1057214) ERROR 01-26 20:52:03 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:51:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:51:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:51:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:51:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:51:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:51:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:51:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:51:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:51:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:51:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:51:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:51:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:51:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:51:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:52:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:52:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:52:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:52:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:52:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:52:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:52:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:52:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1057214) Process EngineCore_DP0:
(EngineCore_DP0 pid=1057214) Traceback (most recent call last):
(EngineCore_DP0 pid=1057214)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1057214)     self.run()
(EngineCore_DP0 pid=1057214)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1057214)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1057214)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1057214)     raise e
(EngineCore_DP0 pid=1057214)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1057214)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1057214)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1057214)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1057214)     super().__init__(
(EngineCore_DP0 pid=1057214)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1057214)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1057214)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1057214)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1057214)     self._init_executor()
(EngineCore_DP0 pid=1057214)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1057214)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1057214)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1057214)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1057214)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1057214)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1057214)     raise ValueError(
(EngineCore_DP0 pid=1057214) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:52:04.273470770 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=512 ==========
Time: 2026-01-26 20:52:26
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:52:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:52:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1058019) ERROR 01-26 20:52:46 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:52:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:52:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:52:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:52:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:52:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:52:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:52:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:52:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:52:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:52:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:52:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:52:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:52:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:52:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:52:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:52:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:52:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1058019) Process EngineCore_DP0:
(EngineCore_DP0 pid=1058019) Traceback (most recent call last):
(EngineCore_DP0 pid=1058019)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1058019)     self.run()
(EngineCore_DP0 pid=1058019)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1058019)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1058019)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1058019)     raise e
(EngineCore_DP0 pid=1058019)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1058019)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1058019)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058019)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1058019)     super().__init__(
(EngineCore_DP0 pid=1058019)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1058019)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1058019)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058019)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1058019)     self._init_executor()
(EngineCore_DP0 pid=1058019)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1058019)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1058019)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1058019)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1058019)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058019)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1058019)     raise ValueError(
(EngineCore_DP0 pid=1058019) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:52:46.367473476 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=1024 ==========
Time: 2026-01-26 20:53:09
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:53:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:53:20 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1058809) ERROR 01-26 20:53:29 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:53:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:53:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:53:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:53:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:53:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:53:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:53:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:53:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:53:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:53:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:53:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:53:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:53:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:53:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:53:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:53:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:53:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1058809) Process EngineCore_DP0:
(EngineCore_DP0 pid=1058809) Traceback (most recent call last):
(EngineCore_DP0 pid=1058809)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1058809)     self.run()
(EngineCore_DP0 pid=1058809)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1058809)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1058809)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1058809)     raise e
(EngineCore_DP0 pid=1058809)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1058809)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1058809)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058809)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1058809)     super().__init__(
(EngineCore_DP0 pid=1058809)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1058809)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1058809)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058809)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1058809)     self._init_executor()
(EngineCore_DP0 pid=1058809)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1058809)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1058809)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1058809)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1058809)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1058809)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1058809)     raise ValueError(
(EngineCore_DP0 pid=1058809) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:53:30.530249851 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=1024

========== M=1024 ==========
Time: 2026-01-26 20:53:53
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:54:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:54:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1059642) ERROR 01-26 20:54:13 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:54:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:54:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:54:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:54:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:54:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:54:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:54:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:54:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:54:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:54:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:54:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:54:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:54:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:54:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:54:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:54:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1059642) Process EngineCore_DP0:
(EngineCore_DP0 pid=1059642) Traceback (most recent call last):
(EngineCore_DP0 pid=1059642)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1059642)     self.run()
(EngineCore_DP0 pid=1059642)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1059642)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1059642)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1059642)     raise e
(EngineCore_DP0 pid=1059642)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1059642)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1059642)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1059642)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1059642)     super().__init__(
(EngineCore_DP0 pid=1059642)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1059642)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1059642)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1059642)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1059642)     self._init_executor()
(EngineCore_DP0 pid=1059642)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1059642)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1059642)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1059642)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1059642)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1059642)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1059642)     raise ValueError(
(EngineCore_DP0 pid=1059642) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:54:14.043421906 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=1024

========== M=2048 ==========
Time: 2026-01-26 20:54:36
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:54:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:54:47 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1060429) ERROR 01-26 20:54:56 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:54:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:54:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:54:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:54:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:54:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:54:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:54:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:54:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:54:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:54:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:54:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:54:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:54:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:54:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:54:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:54:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:54:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1060429) Process EngineCore_DP0:
(EngineCore_DP0 pid=1060429) Traceback (most recent call last):
(EngineCore_DP0 pid=1060429)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1060429)     self.run()
(EngineCore_DP0 pid=1060429)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1060429)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1060429)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1060429)     raise e
(EngineCore_DP0 pid=1060429)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1060429)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1060429)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1060429)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1060429)     super().__init__(
(EngineCore_DP0 pid=1060429)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1060429)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1060429)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1060429)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1060429)     self._init_executor()
(EngineCore_DP0 pid=1060429)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1060429)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1060429)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1060429)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1060429)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1060429)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1060429)     raise ValueError(
(EngineCore_DP0 pid=1060429) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:54:57.582763444 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=2048

========== M=2048 ==========
Time: 2026-01-26 20:55:22
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:55:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:55:31 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1061275) ERROR 01-26 20:55:42 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:55:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:55:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:55:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:55:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:55:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:55:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:55:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:55:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:55:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:55:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:55:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:55:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:55:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:55:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:55:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:55:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:55:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1061275) Process EngineCore_DP0:
(EngineCore_DP0 pid=1061275) Traceback (most recent call last):
(EngineCore_DP0 pid=1061275)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1061275)     self.run()
(EngineCore_DP0 pid=1061275)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1061275)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1061275)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1061275)     raise e
(EngineCore_DP0 pid=1061275)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1061275)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1061275)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1061275)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1061275)     super().__init__(
(EngineCore_DP0 pid=1061275)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1061275)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1061275)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1061275)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1061275)     self._init_executor()
(EngineCore_DP0 pid=1061275)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1061275)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1061275)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1061275)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1061275)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1061275)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1061275)     raise ValueError(
(EngineCore_DP0 pid=1061275) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:55:43.767196133 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=2048

========== M=4096 ==========
Time: 2026-01-26 20:56:06
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:56:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:56:19 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1062121) ERROR 01-26 20:56:26 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:56:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:56:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:56:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:56:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:56:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:56:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:56:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:56:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:56:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:56:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:56:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:56:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:56:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:56:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:56:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:56:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:56:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1062121) Process EngineCore_DP0:
(EngineCore_DP0 pid=1062121) Traceback (most recent call last):
(EngineCore_DP0 pid=1062121)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1062121)     self.run()
(EngineCore_DP0 pid=1062121)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1062121)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1062121)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1062121)     raise e
(EngineCore_DP0 pid=1062121)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1062121)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1062121)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062121)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1062121)     super().__init__(
(EngineCore_DP0 pid=1062121)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1062121)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1062121)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062121)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1062121)     self._init_executor()
(EngineCore_DP0 pid=1062121)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1062121)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1062121)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1062121)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1062121)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062121)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1062121)     raise ValueError(
(EngineCore_DP0 pid=1062121) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:56:27.134061252 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=4096

========== M=4096 ==========
Time: 2026-01-26 20:56:51
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:57:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:57:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1062958) ERROR 01-26 20:57:12 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:57:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:57:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:57:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:57:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:57:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:57:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:57:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:57:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:57:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:57:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:57:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:57:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:57:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:57:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:57:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:57:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1062958) Process EngineCore_DP0:
(EngineCore_DP0 pid=1062958) Traceback (most recent call last):
(EngineCore_DP0 pid=1062958)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1062958)     self.run()
(EngineCore_DP0 pid=1062958)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1062958)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1062958)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1062958)     raise e
(EngineCore_DP0 pid=1062958)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1062958)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1062958)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062958)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1062958)     super().__init__(
(EngineCore_DP0 pid=1062958)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1062958)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1062958)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062958)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1062958)     self._init_executor()
(EngineCore_DP0 pid=1062958)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1062958)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1062958)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1062958)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1062958)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1062958)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1062958)     raise ValueError(
(EngineCore_DP0 pid=1062958) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:57:13.365423764 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=4096

========== M=8192 ==========
Time: 2026-01-26 20:57:35
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:57:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:57:53 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1063828) ERROR 01-26 20:58:02 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:57:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:57:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:57:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:57:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:57:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:57:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:57:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:57:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:57:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:57:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:57:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:57:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:57:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:57:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:57:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:57:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:57:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1063828) Process EngineCore_DP0:
(EngineCore_DP0 pid=1063828) Traceback (most recent call last):
(EngineCore_DP0 pid=1063828)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1063828)     self.run()
(EngineCore_DP0 pid=1063828)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1063828)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1063828)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1063828)     raise e
(EngineCore_DP0 pid=1063828)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1063828)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1063828)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1063828)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1063828)     super().__init__(
(EngineCore_DP0 pid=1063828)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1063828)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1063828)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1063828)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1063828)     self._init_executor()
(EngineCore_DP0 pid=1063828)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1063828)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1063828)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1063828)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1063828)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1063828)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1063828)     raise ValueError(
(EngineCore_DP0 pid=1063828) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:58:03.525338268 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=8192

========== M=8192 ==========
Time: 2026-01-26 20:58:27
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:58:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:58:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1064750) ERROR 01-26 20:58:55 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:58:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:58:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:58:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:58:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:58:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:58:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:58:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:58:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:58:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:58:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:58:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:58:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:58:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:58:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:58:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:58:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:58:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1064750) Process EngineCore_DP0:
(EngineCore_DP0 pid=1064750) Traceback (most recent call last):
(EngineCore_DP0 pid=1064750)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1064750)     self.run()
(EngineCore_DP0 pid=1064750)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1064750)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1064750)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1064750)     raise e
(EngineCore_DP0 pid=1064750)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1064750)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1064750)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1064750)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1064750)     super().__init__(
(EngineCore_DP0 pid=1064750)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1064750)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1064750)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1064750)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1064750)     self._init_executor()
(EngineCore_DP0 pid=1064750)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1064750)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1064750)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1064750)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1064750)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1064750)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1064750)     raise ValueError(
(EngineCore_DP0 pid=1064750) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:58:56.543001934 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=8192

========== M=16384 ==========
Time: 2026-01-26 20:59:18
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 20:59:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 20:59:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1065748) ERROR 01-26 20:59:51 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 20:59:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:59:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:59:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:59:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:59:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:59:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:59:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:59:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 20:59:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 20:59:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:59:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 20:59:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 20:59:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 20:59:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 20:59:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 20:59:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 20:59:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1065748) Process EngineCore_DP0:
(EngineCore_DP0 pid=1065748) Traceback (most recent call last):
(EngineCore_DP0 pid=1065748)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1065748)     self.run()
(EngineCore_DP0 pid=1065748)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1065748)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1065748)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1065748)     raise e
(EngineCore_DP0 pid=1065748)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1065748)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1065748)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1065748)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1065748)     super().__init__(
(EngineCore_DP0 pid=1065748)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1065748)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1065748)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1065748)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1065748)     self._init_executor()
(EngineCore_DP0 pid=1065748)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1065748)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1065748)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1065748)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1065748)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1065748)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1065748)     raise ValueError(
(EngineCore_DP0 pid=1065748) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 20:59:52.062952998 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=16384 ==========
Time: 2026-01-26 21:00:15
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:00:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:00:39 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1066742) ERROR 01-26 21:00:48 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:00:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:00:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:00:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:00:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:00:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:00:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:00:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:00:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:00:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:00:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:00:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:00:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:00:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:00:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:00:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:00:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:00:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1066742) Process EngineCore_DP0:
(EngineCore_DP0 pid=1066742) Traceback (most recent call last):
(EngineCore_DP0 pid=1066742)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1066742)     self.run()
(EngineCore_DP0 pid=1066742)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1066742)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1066742)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1066742)     raise e
(EngineCore_DP0 pid=1066742)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1066742)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1066742)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1066742)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1066742)     super().__init__(
(EngineCore_DP0 pid=1066742)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1066742)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1066742)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1066742)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1066742)     self._init_executor()
(EngineCore_DP0 pid=1066742)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1066742)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1066742)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1066742)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1066742)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1066742)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1066742)     raise ValueError(
(EngineCore_DP0 pid=1066742) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:00:49.870893858 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=32768 ==========
Time: 2026-01-26 21:01:12
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:01:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:01:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1067907) ERROR 01-26 21:02:01 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:01:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:01:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:01:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:01:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:01:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:01:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:01:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:01:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:01:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:01:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:01:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:01:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:01:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:01:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:01:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:01:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:01:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1067907) Process EngineCore_DP0:
(EngineCore_DP0 pid=1067907) Traceback (most recent call last):
(EngineCore_DP0 pid=1067907)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1067907)     self.run()
(EngineCore_DP0 pid=1067907)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1067907)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1067907)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1067907)     raise e
(EngineCore_DP0 pid=1067907)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1067907)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1067907)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1067907)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1067907)     super().__init__(
(EngineCore_DP0 pid=1067907)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1067907)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1067907)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1067907)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1067907)     self._init_executor()
(EngineCore_DP0 pid=1067907)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1067907)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1067907)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1067907)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1067907)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1067907)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1067907)     raise ValueError(
(EngineCore_DP0 pid=1067907) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:02:01.428256619 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=32768 ==========
Time: 2026-01-26 21:02:25
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:03:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:03:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1069116) ERROR 01-26 21:03:12 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:03:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:03:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:03:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:03:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:03:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:03:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:03:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:03:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:03:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:03:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:03:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:03:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:03:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:03:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:03:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:03:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:03:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1069116) Process EngineCore_DP0:
(EngineCore_DP0 pid=1069116) Traceback (most recent call last):
(EngineCore_DP0 pid=1069116)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1069116)     self.run()
(EngineCore_DP0 pid=1069116)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1069116)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1069116)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1069116)     raise e
(EngineCore_DP0 pid=1069116)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1069116)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1069116)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1069116)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1069116)     super().__init__(
(EngineCore_DP0 pid=1069116)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1069116)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1069116)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1069116)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1069116)     self._init_executor()
(EngineCore_DP0 pid=1069116)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1069116)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1069116)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1069116)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1069116)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1069116)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1069116)     raise ValueError(
(EngineCore_DP0 pid=1069116) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:03:12.912408773 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=65536 ==========
Time: 2026-01-26 21:03:37
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:04:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:04:42 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1070713) ERROR 01-26 21:04:51 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:04:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:04:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:04:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:04:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:04:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:04:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:04:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:04:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:04:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:04:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:04:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:04:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:04:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:04:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:04:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:04:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:04:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1070713) Process EngineCore_DP0:
(EngineCore_DP0 pid=1070713) Traceback (most recent call last):
(EngineCore_DP0 pid=1070713)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1070713)     self.run()
(EngineCore_DP0 pid=1070713)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1070713)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1070713)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1070713)     raise e
(EngineCore_DP0 pid=1070713)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1070713)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1070713)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1070713)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1070713)     super().__init__(
(EngineCore_DP0 pid=1070713)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1070713)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1070713)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1070713)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1070713)     self._init_executor()
(EngineCore_DP0 pid=1070713)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1070713)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1070713)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1070713)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1070713)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1070713)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1070713)     raise ValueError(
(EngineCore_DP0 pid=1070713) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:04:52.390012555 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=65536 ==========
Time: 2026-01-26 21:05:17
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:06:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:06:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1072295) ERROR 01-26 21:06:33 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:06:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:06:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:06:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:06:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:06:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:06:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:06:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:06:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:06:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:06:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:06:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 21:06:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 21:06:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:06:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:06:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:06:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:06:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1072295) Process EngineCore_DP0:
(EngineCore_DP0 pid=1072295) Traceback (most recent call last):
(EngineCore_DP0 pid=1072295)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1072295)     self.run()
(EngineCore_DP0 pid=1072295)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1072295)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1072295)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1072295)     raise e
(EngineCore_DP0 pid=1072295)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1072295)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1072295)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1072295)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1072295)     super().__init__(
(EngineCore_DP0 pid=1072295)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1072295)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1072295)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1072295)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1072295)     self._init_executor()
(EngineCore_DP0 pid=1072295)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1072295)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1072295)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1072295)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1072295)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1072295)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1072295)     raise ValueError(
(EngineCore_DP0 pid=1072295) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:06:34.727136638 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-28 08:20:35
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:20:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:20:59 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=852374) WARNING 01-28 08:22:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=852374) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=852374) WARNING 01-28 08:24:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.69 requests/s, 8046.43 total tokens/s, 15.69 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-28 08:20:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:20:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:20:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:20:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:20:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:20:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:20:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:20:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:20:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:20:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:20:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:20:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:20:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:20:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:21:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:21:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:21:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:21:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:21:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:21:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:21:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:21:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:21:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:21:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:21:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:21:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:21:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:21:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=852374) [2026-01-28 08:22:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=852374) [2026-01-28 08:22:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=852374) [2026-01-28 08:22:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=852374) [2026-01-28 08:22:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=852374) [2026-01-28 08:22:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=852374) [2026-01-28 08:22:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=852374) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=852374) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.38s/it]
(EngineCore_DP0 pid=852374) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.38s/it]
(EngineCore_DP0 pid=852374) 
(EngineCore_DP0 pid=852374) [2026-01-28 08:23:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=852374) [2026-01-28 08:23:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=852374) [2026-01-28 08:23:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=852374) [2026-01-28 08:23:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=852374) [2026-01-28 08:23:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=852374) [2026-01-28 08:23:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=852374) [2026-01-28 08:23:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=852374) [2026-01-28 08:23:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=852374) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:03<00:00,  1.91s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]
(EngineCore_DP0 pid=852374) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 631.86it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 646.47it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:43,  2.92it/s, est. speed input: 1492.92 toks/s, output: 2.92 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:17,  7.12it/s, est. speed input: 3186.85 toks/s, output: 6.22 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:12,  9.87it/s, est. speed input: 4191.95 toks/s, output: 8.19 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:10, 11.40it/s, est. speed input: 4783.84 toks/s, output: 9.34 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:09, 13.01it/s, est. speed input: 5314.60 toks/s, output: 10.38 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:08, 14.15it/s, est. speed input: 5715.52 toks/s, output: 11.16 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:07, 14.99it/s, est. speed input: 6034.42 toks/s, output: 11.79 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 15.53it/s, est. speed input: 6284.70 toks/s, output: 12.27 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 15.93it/s, est. speed input: 6493.42 toks/s, output: 12.68 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 16.27it/s, est. speed input: 6673.66 toks/s, output: 13.03 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 16.52it/s, est. speed input: 6828.69 toks/s, output: 13.34 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 16.67it/s, est. speed input: 6959.84 toks/s, output: 13.59 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 16.77it/s, est. speed input: 7073.49 toks/s, output: 13.82 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 16.82it/s, est. speed input: 7171.24 toks/s, output: 14.01 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:05, 16.84it/s, est. speed input: 7256.68 toks/s, output: 14.17 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:05, 16.87it/s, est. speed input: 7334.15 toks/s, output: 14.32 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 16.93it/s, est. speed input: 7405.92 toks/s, output: 14.46 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.96it/s, est. speed input: 7470.17 toks/s, output: 14.59 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.96it/s, est. speed input: 7527.07 toks/s, output: 14.70 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.90it/s, est. speed input: 7574.60 toks/s, output: 14.79 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.88it/s, est. speed input: 7619.50 toks/s, output: 14.88 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.84it/s, est. speed input: 7659.31 toks/s, output: 14.96 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 16.81it/s, est. speed input: 7695.93 toks/s, output: 15.03 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:04, 16.77it/s, est. speed input: 7728.23 toks/s, output: 15.09 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:04, 16.75it/s, est. speed input: 7758.56 toks/s, output: 15.15 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 16.72it/s, est. speed input: 7786.42 toks/s, output: 15.21 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 16.76it/s, est. speed input: 7815.02 toks/s, output: 15.26 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.77it/s, est. speed input: 7841.24 toks/s, output: 15.31 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.78it/s, est. speed input: 7865.31 toks/s, output: 15.36 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.76it/s, est. speed input: 7887.25 toks/s, output: 15.40 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 16.75it/s, est. speed input: 7907.82 toks/s, output: 15.44 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:03, 16.77it/s, est. speed input: 7928.36 toks/s, output: 15.48 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:03, 16.78it/s, est. speed input: 7947.66 toks/s, output: 15.52 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.78it/s, est. speed input: 7965.33 toks/s, output: 15.56 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.81it/s, est. speed input: 7983.36 toks/s, output: 15.59 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.79it/s, est. speed input: 7999.02 toks/s, output: 15.62 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.79it/s, est. speed input: 8014.09 toks/s, output: 15.65 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.75it/s, est. speed input: 8026.99 toks/s, output: 15.68 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.74it/s, est. speed input: 8039.93 toks/s, output: 15.70 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:05<00:02, 16.73it/s, est. speed input: 8052.15 toks/s, output: 15.73 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:02, 16.75it/s, est. speed input: 8064.78 toks/s, output: 15.75 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 16.75it/s, est. speed input: 8076.40 toks/s, output: 15.77 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.71it/s, est. speed input: 8086.20 toks/s, output: 15.79 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.74it/s, est. speed input: 8097.51 toks/s, output: 15.82 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.74it/s, est. speed input: 8107.51 toks/s, output: 15.83 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.76it/s, est. speed input: 8117.90 toks/s, output: 15.86 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.80it/s, est. speed input: 8128.58 toks/s, output: 15.88 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.75it/s, est. speed input: 8136.25 toks/s, output: 15.89 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:01, 16.74it/s, est. speed input: 8144.64 toks/s, output: 15.91 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.77it/s, est. speed input: 8153.87 toks/s, output: 15.93 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.81it/s, est. speed input: 8163.14 toks/s, output: 15.94 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.86it/s, est. speed input: 8172.77 toks/s, output: 15.96 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.74it/s, est. speed input: 8177.53 toks/s, output: 15.97 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.76it/s, est. speed input: 8185.16 toks/s, output: 15.99 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.81it/s, est. speed input: 8193.63 toks/s, output: 16.00 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.28it/s, est. speed input: 8185.39 toks/s, output: 15.99 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:00, 16.37it/s, est. speed input: 8190.49 toks/s, output: 16.00 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 16.50it/s, est. speed input: 8197.57 toks/s, output: 16.01 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 16.57it/s, est. speed input: 8203.58 toks/s, output: 16.02 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 16.56it/s, est. speed input: 8207.75 toks/s, output: 16.03 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 16.58it/s, est. speed input: 8212.80 toks/s, output: 16.04 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 16.64it/s, est. speed input: 8218.59 toks/s, output: 16.05 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.66it/s, est. speed input: 8223.71 toks/s, output: 16.06 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.69it/s, est. speed input: 8229.23 toks/s, output: 16.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.69it/s, est. speed input: 8232.05 toks/s, output: 16.08 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.08it/s, est. speed input: 8232.05 toks/s, output: 16.08 toks/s]
[rank0]:[W128 08:25:44.223420941 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-28 08:26:44
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:27:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:27:47 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=855631) WARNING 01-28 08:29:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=855631) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=855631) WARNING 01-28 08:31:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.70 requests/s, 16095.59 total tokens/s, 15.70 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-28 08:27:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:27:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:27:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:27:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:27:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:27:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:27:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:27:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:27:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:28:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:28:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:28:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:28:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:28:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:28:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:28:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:28:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:28:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W128 08:29:23.049198075 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=855631) [2026-01-28 08:29:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=855631) [2026-01-28 08:29:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=855631) [2026-01-28 08:29:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=855631) [2026-01-28 08:29:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=855631) [2026-01-28 08:29:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=855631) [2026-01-28 08:29:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=855631) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=855631) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=855631) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=855631) 
(EngineCore_DP0 pid=855631) [2026-01-28 08:30:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=855631) [2026-01-28 08:30:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=855631) [2026-01-28 08:30:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=855631) [2026-01-28 08:30:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=855631) [2026-01-28 08:30:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=855631) [2026-01-28 08:30:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=855631) [2026-01-28 08:30:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=855631) [2026-01-28 08:30:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=855631) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:04<00:04,  4.23s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:04<00:00,  1.99s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:04<00:00,  2.32s/it]
(EngineCore_DP0 pid=855631) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  29%|██▉       | 37/128 [00:00<00:00, 364.08it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 370.67it/s]
Adding requests:  88%|████████▊ | 113/128 [00:00<00:00, 373.35it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 373.75it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:08, 14.32it/s, est. speed input: 14665.72 toks/s, output: 14.32 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 15.80it/s, est. speed input: 15932.44 toks/s, output: 15.56 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:07, 16.35it/s, est. speed input: 16416.85 toks/s, output: 16.03 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:07, 16.64it/s, est. speed input: 16674.02 toks/s, output: 16.28 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:07, 16.77it/s, est. speed input: 16820.92 toks/s, output: 16.43 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:06, 16.78it/s, est. speed input: 16882.27 toks/s, output: 16.49 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 16.75it/s, est. speed input: 16913.19 toks/s, output: 16.52 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 16.74it/s, est. speed input: 16935.72 toks/s, output: 16.54 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:06, 16.72it/s, est. speed input: 16953.62 toks/s, output: 16.56 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 16.68it/s, est. speed input: 16956.88 toks/s, output: 16.56 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 16.70it/s, est. speed input: 16972.54 toks/s, output: 16.57 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 16.76it/s, est. speed input: 17000.40 toks/s, output: 16.60 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:06, 16.78it/s, est. speed input: 17018.35 toks/s, output: 16.62 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 16.84it/s, est. speed input: 17044.35 toks/s, output: 16.64 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 16.86it/s, est. speed input: 17060.55 toks/s, output: 16.66 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 16.84it/s, est. speed input: 17068.96 toks/s, output: 16.67 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:02<00:05, 16.84it/s, est. speed input: 17080.47 toks/s, output: 16.68 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:05, 16.74it/s, est. speed input: 17070.06 toks/s, output: 16.67 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 16.71it/s, est. speed input: 17068.50 toks/s, output: 16.67 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 16.63it/s, est. speed input: 17057.61 toks/s, output: 16.66 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 16.60it/s, est. speed input: 17050.56 toks/s, output: 16.65 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:05, 16.60it/s, est. speed input: 17049.08 toks/s, output: 16.65 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 16.63it/s, est. speed input: 17050.34 toks/s, output: 16.65 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 16.64it/s, est. speed input: 17051.71 toks/s, output: 16.65 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:03<00:04, 16.66it/s, est. speed input: 17054.26 toks/s, output: 16.65 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:04, 16.60it/s, est. speed input: 17045.52 toks/s, output: 16.65 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 16.43it/s, est. speed input: 17021.81 toks/s, output: 16.62 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 16.39it/s, est. speed input: 17010.19 toks/s, output: 16.61 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 16.34it/s, est. speed input: 16996.28 toks/s, output: 16.60 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:04, 16.31it/s, est. speed input: 16983.21 toks/s, output: 16.59 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:04, 16.29it/s, est. speed input: 16972.55 toks/s, output: 16.57 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 16.24it/s, est. speed input: 16957.31 toks/s, output: 16.56 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 16.27it/s, est. speed input: 16950.32 toks/s, output: 16.55 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:03, 16.29it/s, est. speed input: 16943.73 toks/s, output: 16.55 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 16.29it/s, est. speed input: 16935.90 toks/s, output: 16.54 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 16.30it/s, est. speed input: 16930.26 toks/s, output: 16.53 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 16.24it/s, est. speed input: 16917.96 toks/s, output: 16.52 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 16.26it/s, est. speed input: 16912.07 toks/s, output: 16.52 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:03, 16.25it/s, est. speed input: 16904.49 toks/s, output: 16.51 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 16.24it/s, est. speed input: 16897.10 toks/s, output: 16.50 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 16.24it/s, est. speed input: 16890.13 toks/s, output: 16.49 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:05<00:02, 16.20it/s, est. speed input: 16880.63 toks/s, output: 16.48 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 16.10it/s, est. speed input: 16865.44 toks/s, output: 16.47 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 16.12it/s, est. speed input: 16857.98 toks/s, output: 16.46 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 16.15it/s, est. speed input: 16852.78 toks/s, output: 16.46 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 16.21it/s, est. speed input: 16850.44 toks/s, output: 16.46 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:02, 16.25it/s, est. speed input: 16847.49 toks/s, output: 16.45 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 16.25it/s, est. speed input: 16843.60 toks/s, output: 16.45 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 16.26it/s, est. speed input: 16840.07 toks/s, output: 16.45 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:06<00:01, 16.25it/s, est. speed input: 16835.79 toks/s, output: 16.44 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:06<00:01, 16.27it/s, est. speed input: 16833.23 toks/s, output: 16.44 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 16.25it/s, est. speed input: 16828.46 toks/s, output: 16.43 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 16.24it/s, est. speed input: 16824.00 toks/s, output: 16.43 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 16.22it/s, est. speed input: 16819.43 toks/s, output: 16.43 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 16.21it/s, est. speed input: 16814.46 toks/s, output: 16.42 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:00, 16.22it/s, est. speed input: 16811.45 toks/s, output: 16.42 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 16.24it/s, est. speed input: 16809.13 toks/s, output: 16.42 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:07<00:00, 16.26it/s, est. speed input: 16807.24 toks/s, output: 16.41 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:07<00:00, 16.24it/s, est. speed input: 16803.25 toks/s, output: 16.41 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:07<00:00, 16.21it/s, est. speed input: 16798.90 toks/s, output: 16.41 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 16.24it/s, est. speed input: 16796.87 toks/s, output: 16.40 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 16.21it/s, est. speed input: 16792.86 toks/s, output: 16.40 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 16.21it/s, est. speed input: 16789.38 toks/s, output: 16.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.25it/s, est. speed input: 16788.94 toks/s, output: 16.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.25it/s, est. speed input: 16788.94 toks/s, output: 16.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.39it/s, est. speed input: 16788.94 toks/s, output: 16.40 toks/s]
[rank0]:[W128 08:33:00.728465342 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-28 08:33:23
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:33:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:33:49 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=858926) WARNING 01-28 08:35:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=858926) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=858926) WARNING 01-28 08:37:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.97 requests/s, 31746.12 total tokens/s, 30.97 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-28 08:33:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:33:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:33:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:33:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:33:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:33:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:33:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:33:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:34:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:34:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:34:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:34:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:34:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:34:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:34:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:34:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:34:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W128 08:35:24.537648186 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=858926) [2026-01-28 08:35:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=858926) [2026-01-28 08:35:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=858926) [2026-01-28 08:35:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=858926) [2026-01-28 08:35:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=858926) [2026-01-28 08:35:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=858926) [2026-01-28 08:35:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=858926) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=858926) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.08s/it]
(EngineCore_DP0 pid=858926) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.08s/it]
(EngineCore_DP0 pid=858926) 
(EngineCore_DP0 pid=858926) [2026-01-28 08:36:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=858926) [2026-01-28 08:36:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=858926) [2026-01-28 08:36:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=858926) [2026-01-28 08:36:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=858926) [2026-01-28 08:36:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=858926) [2026-01-28 08:36:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=858926) [2026-01-28 08:36:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=858926) [2026-01-28 08:36:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=858926) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:03<00:06,  3.07s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:07<00:03,  3.85s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:18<00:00,  7.29s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:18<00:00,  6.28s/it]
(EngineCore_DP0 pid=858926) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:02<00:02,  2.23s/it]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:02<00:00,  1.50s/it]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 36/256 [00:00<00:00, 359.29it/s]
Adding requests:  29%|██▉       | 75/256 [00:00<00:00, 374.53it/s]
Adding requests:  44%|████▍     | 113/256 [00:00<00:00, 364.35it/s]
Adding requests:  59%|█████▊    | 150/256 [00:00<00:00, 359.05it/s]
Adding requests:  73%|███████▎  | 186/256 [00:00<00:00, 352.86it/s]
Adding requests:  87%|████████▋ | 222/256 [00:00<00:00, 352.39it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 357.33it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:02, 108.95it/s, est. speed input: 111595.89 toks/s, output: 108.96 toks/s]
Processed prompts:  10%|▉         | 25/256 [00:00<00:04, 51.08it/s, est. speed input: 57427.36 toks/s, output: 56.08 toks/s]   
Processed prompts:  12%|█▎        | 32/256 [00:00<00:05, 40.03it/s, est. speed input: 46863.25 toks/s, output: 45.76 toks/s]
Processed prompts:  14%|█▍        | 37/256 [00:00<00:05, 39.81it/s, est. speed input: 45818.02 toks/s, output: 44.74 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:01<00:06, 35.05it/s, est. speed input: 42268.51 toks/s, output: 41.28 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:01<00:06, 34.17it/s, est. speed input: 41158.50 toks/s, output: 40.19 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:01<00:06, 33.52it/s, est. speed input: 40293.35 toks/s, output: 39.35 toks/s]
Processed prompts:  21%|██        | 54/256 [00:01<00:06, 33.00it/s, est. speed input: 39577.20 toks/s, output: 38.65 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:01<00:06, 32.79it/s, est. speed input: 39041.41 toks/s, output: 38.13 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:05, 32.62it/s, est. speed input: 38581.71 toks/s, output: 37.68 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:05, 32.51it/s, est. speed input: 38191.05 toks/s, output: 37.30 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:05, 32.35it/s, est. speed input: 37829.63 toks/s, output: 36.94 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:02<00:05, 32.33it/s, est. speed input: 37538.17 toks/s, output: 36.66 toks/s]
Processed prompts:  30%|███       | 78/256 [00:03<00:20,  8.79it/s, est. speed input: 24456.69 toks/s, output: 23.88 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:03<00:15, 11.19it/s, est. speed input: 24757.67 toks/s, output: 24.18 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:03<00:12, 13.89it/s, est. speed input: 25048.53 toks/s, output: 24.46 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:03<00:09, 16.72it/s, est. speed input: 25315.95 toks/s, output: 24.72 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:03<00:08, 19.50it/s, est. speed input: 25565.09 toks/s, output: 24.97 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:03<00:07, 22.11it/s, est. speed input: 25800.36 toks/s, output: 25.20 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:04<00:06, 24.34it/s, est. speed input: 26015.02 toks/s, output: 25.41 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:04<00:05, 26.27it/s, est. speed input: 26224.69 toks/s, output: 25.61 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:04<00:05, 27.73it/s, est. speed input: 26413.32 toks/s, output: 25.79 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:04<00:04, 28.88it/s, est. speed input: 26593.28 toks/s, output: 25.97 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:04<00:04, 29.75it/s, est. speed input: 26764.40 toks/s, output: 26.14 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:04<00:04, 30.35it/s, est. speed input: 26922.84 toks/s, output: 26.29 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:04<00:04, 30.83it/s, est. speed input: 27076.46 toks/s, output: 26.44 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:04<00:04, 31.22it/s, est. speed input: 27225.34 toks/s, output: 26.59 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:05<00:03, 31.50it/s, est. speed input: 27367.13 toks/s, output: 26.73 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:05<00:03, 31.70it/s, est. speed input: 27502.10 toks/s, output: 26.86 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:05<00:03, 31.84it/s, est. speed input: 27630.84 toks/s, output: 26.98 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:05<00:03, 32.15it/s, est. speed input: 27768.16 toks/s, output: 27.12 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:05<00:03, 32.44it/s, est. speed input: 27902.84 toks/s, output: 27.25 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:05<00:03, 32.78it/s, est. speed input: 28040.41 toks/s, output: 27.38 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:05<00:02, 33.02it/s, est. speed input: 28172.34 toks/s, output: 27.51 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:05<00:02, 33.20it/s, est. speed input: 28299.23 toks/s, output: 27.64 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:05<00:02, 33.31it/s, est. speed input: 28420.36 toks/s, output: 27.75 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:06<00:02, 33.37it/s, est. speed input: 28535.66 toks/s, output: 27.87 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:06<00:02, 33.50it/s, est. speed input: 28651.63 toks/s, output: 27.98 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:06<00:02, 33.46it/s, est. speed input: 28756.02 toks/s, output: 28.08 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:06<00:02, 33.54it/s, est. speed input: 28861.95 toks/s, output: 28.19 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:06<00:02, 33.53it/s, est. speed input: 28961.34 toks/s, output: 28.28 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:06<00:01, 33.42it/s, est. speed input: 29051.02 toks/s, output: 28.37 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:06<00:01, 33.39it/s, est. speed input: 29140.14 toks/s, output: 28.46 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:06<00:01, 33.44it/s, est. speed input: 29229.73 toks/s, output: 28.54 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:07<00:01, 33.36it/s, est. speed input: 29310.86 toks/s, output: 28.62 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:07<00:01, 33.47it/s, est. speed input: 29397.32 toks/s, output: 28.71 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:07<00:01, 33.43it/s, est. speed input: 29475.14 toks/s, output: 28.78 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:07<00:01, 33.42it/s, est. speed input: 29551.68 toks/s, output: 28.86 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:07<00:01, 33.31it/s, est. speed input: 29620.54 toks/s, output: 28.93 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:07<00:01, 33.34it/s, est. speed input: 29692.20 toks/s, output: 29.00 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:07<00:00, 33.44it/s, est. speed input: 29765.39 toks/s, output: 29.07 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:07<00:00, 33.47it/s, est. speed input: 29834.51 toks/s, output: 29.14 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:08<00:00, 33.39it/s, est. speed input: 29897.45 toks/s, output: 29.20 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:08<00:00, 33.45it/s, est. speed input: 29963.06 toks/s, output: 29.26 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:08<00:00, 33.48it/s, est. speed input: 30026.81 toks/s, output: 29.32 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:08<00:00, 33.45it/s, est. speed input: 30086.15 toks/s, output: 29.38 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:08<00:00, 33.49it/s, est. speed input: 30146.48 toks/s, output: 29.44 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:08<00:00, 33.50it/s, est. speed input: 30204.54 toks/s, output: 29.50 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 33.50it/s, est. speed input: 30234.95 toks/s, output: 29.53 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 29.53it/s, est. speed input: 30234.95 toks/s, output: 29.53 toks/s]
[rank0]:[W128 08:39:17.403332974 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-28 08:40:47
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:41:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:41:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=862520) WARNING 01-28 08:43:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=862520) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=862520) WARNING 01-28 08:45:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.46 requests/s, 51725.11 total tokens/s, 50.46 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-28 08:41:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:41:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:41:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:41:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:41:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:41:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:41:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:41:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:41:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:42:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:42:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:42:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:42:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:42:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:42:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:42:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:42:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:42:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:42:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:42:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:42:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:42:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:42:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W128 08:43:43.661742150 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=862520) [2026-01-28 08:43:52] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=862520) [2026-01-28 08:43:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=862520) [2026-01-28 08:43:52] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=862520) [2026-01-28 08:43:52] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=862520) [2026-01-28 08:43:52] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=862520) [2026-01-28 08:43:52] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=862520) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=862520) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]
(EngineCore_DP0 pid=862520) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]
(EngineCore_DP0 pid=862520) 
(EngineCore_DP0 pid=862520) [2026-01-28 08:43:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=862520) [2026-01-28 08:44:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=862520) [2026-01-28 08:44:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=862520) [2026-01-28 08:44:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=862520) [2026-01-28 08:44:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=862520) [2026-01-28 08:44:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=862520) [2026-01-28 08:44:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=862520) [2026-01-28 08:44:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=862520) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:05<00:16,  5.60s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:06<00:05,  2.87s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:11<00:03,  3.81s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:20<00:00,  6.05s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:20<00:00,  5.24s/it]
(EngineCore_DP0 pid=862520) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:05<00:11,  5.96s/it]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:06<00:02,  2.56s/it]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:06<00:00,  1.52s/it]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:06<00:00,  2.14s/it]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 36/512 [00:00<00:01, 354.95it/s]
Adding requests:  15%|█▍        | 75/512 [00:00<00:01, 370.71it/s]
Adding requests:  22%|██▏       | 114/512 [00:00<00:01, 375.24it/s]
Adding requests:  30%|██▉       | 152/512 [00:00<00:00, 373.25it/s]
Adding requests:  37%|███▋      | 190/512 [00:00<00:00, 370.89it/s]
Adding requests:  45%|████▍     | 228/512 [00:00<00:00, 369.53it/s]
Adding requests:  52%|█████▏    | 266/512 [00:00<00:00, 371.91it/s]
Adding requests:  59%|█████▉    | 304/512 [00:00<00:00, 365.13it/s]
Adding requests:  67%|██████▋   | 341/512 [00:00<00:00, 366.14it/s]
Adding requests:  74%|███████▍  | 378/512 [00:01<00:00, 363.75it/s]
Adding requests:  81%|████████  | 415/512 [00:01<00:00, 357.99it/s]
Adding requests:  88%|████████▊ | 453/512 [00:01<00:00, 361.86it/s]
Adding requests:  96%|█████████▌| 491/512 [00:01<00:00, 365.98it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 366.99it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:00<00:01, 357.88it/s, est. speed input: 366528.85 toks/s, output: 357.90 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:00<00:04, 99.13it/s, est. speed input: 117653.66 toks/s, output: 114.89 toks/s] 
Processed prompts:  23%|██▎       | 116/512 [00:01<00:04, 85.25it/s, est. speed input: 102812.92 toks/s, output: 100.40 toks/s]
Processed prompts:  25%|██▌       | 129/512 [00:01<00:04, 77.65it/s, est. speed input: 95588.29 toks/s, output: 93.35 toks/s]  
Processed prompts:  27%|██▋       | 139/512 [00:01<00:05, 68.28it/s, est. speed input: 88507.53 toks/s, output: 86.43 toks/s]
Processed prompts:  29%|██▊       | 147/512 [00:01<00:05, 64.99it/s, est. speed input: 85533.28 toks/s, output: 83.53 toks/s]
Processed prompts:  30%|███       | 154/512 [00:01<00:05, 60.68it/s, est. speed input: 82523.08 toks/s, output: 80.59 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:02<00:05, 58.78it/s, est. speed input: 80458.77 toks/s, output: 78.57 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:05, 57.20it/s, est. speed input: 78647.36 toks/s, output: 76.80 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:02<00:05, 55.99it/s, est. speed input: 77070.56 toks/s, output: 75.26 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:02<00:05, 55.15it/s, est. speed input: 75701.54 toks/s, output: 73.93 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:02<00:05, 54.48it/s, est. speed input: 74473.43 toks/s, output: 72.73 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:02<00:05, 53.98it/s, est. speed input: 73374.15 toks/s, output: 71.65 toks/s]
Processed prompts:  41%|████      | 210/512 [00:02<00:05, 53.30it/s, est. speed input: 72309.53 toks/s, output: 70.61 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:03<00:05, 52.60it/s, est. speed input: 71301.79 toks/s, output: 69.63 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:03<00:05, 52.15it/s, est. speed input: 70397.54 toks/s, output: 68.75 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:03<00:05, 51.86it/s, est. speed input: 69580.99 toks/s, output: 67.95 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:03<00:05, 51.67it/s, est. speed input: 68838.13 toks/s, output: 67.22 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:03<00:05, 51.56it/s, est. speed input: 68161.66 toks/s, output: 66.56 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:03<00:04, 51.56it/s, est. speed input: 67552.43 toks/s, output: 65.97 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:04<00:04, 51.71it/s, est. speed input: 67014.31 toks/s, output: 65.44 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:04<00:04, 51.80it/s, est. speed input: 66513.06 toks/s, output: 64.95 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:04<00:04, 51.89it/s, est. speed input: 66050.99 toks/s, output: 64.50 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:04<00:04, 51.98it/s, est. speed input: 65623.93 toks/s, output: 64.09 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:04<00:04, 51.99it/s, est. speed input: 65217.09 toks/s, output: 63.69 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:04<00:03, 52.02it/s, est. speed input: 64840.30 toks/s, output: 63.32 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:04<00:03, 51.97it/s, est. speed input: 64476.68 toks/s, output: 62.97 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:05<00:03, 51.94it/s, est. speed input: 64136.24 toks/s, output: 62.63 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:05<00:03, 51.98it/s, est. speed input: 63822.42 toks/s, output: 62.33 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:05<00:03, 51.90it/s, est. speed input: 63514.31 toks/s, output: 62.03 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:05<00:03, 51.94it/s, est. speed input: 63233.31 toks/s, output: 61.75 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:05<00:03, 51.91it/s, est. speed input: 62961.51 toks/s, output: 61.49 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:05<00:02, 51.85it/s, est. speed input: 62699.13 toks/s, output: 61.23 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:06<00:02, 51.93it/s, est. speed input: 62463.26 toks/s, output: 61.00 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:06<00:02, 51.91it/s, est. speed input: 62231.49 toks/s, output: 60.77 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:06<00:02, 51.96it/s, est. speed input: 62016.30 toks/s, output: 60.56 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:06<00:02, 51.93it/s, est. speed input: 61806.88 toks/s, output: 60.36 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:06<00:02, 51.96it/s, est. speed input: 61609.96 toks/s, output: 60.17 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:06<00:01, 51.96it/s, est. speed input: 61420.39 toks/s, output: 59.98 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:06<00:01, 51.94it/s, est. speed input: 61237.88 toks/s, output: 59.80 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:07<00:01, 51.92it/s, est. speed input: 61062.55 toks/s, output: 59.63 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:07<00:01, 51.94it/s, est. speed input: 60897.41 toks/s, output: 59.47 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:07<00:01, 51.96it/s, est. speed input: 60739.25 toks/s, output: 59.32 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:07<00:01, 51.92it/s, est. speed input: 60583.69 toks/s, output: 59.16 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:07<00:01, 51.95it/s, est. speed input: 60439.13 toks/s, output: 59.02 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:07<00:00, 51.96it/s, est. speed input: 60299.04 toks/s, output: 58.89 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:08<00:00, 51.90it/s, est. speed input: 60159.41 toks/s, output: 58.75 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:08<00:00, 51.91it/s, est. speed input: 60028.42 toks/s, output: 58.62 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:08<00:00, 51.91it/s, est. speed input: 59901.65 toks/s, output: 58.50 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:08<00:00, 51.94it/s, est. speed input: 59782.21 toks/s, output: 58.38 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:08<00:00, 51.92it/s, est. speed input: 59663.64 toks/s, output: 58.27 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 51.92it/s, est. speed input: 59924.69 toks/s, output: 58.52 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 58.52it/s, est. speed input: 59924.69 toks/s, output: 58.52 toks/s]
[rank0]:[W128 08:47:09.336183663 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-28 08:47:34
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:48:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:48:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=865663) WARNING 01-28 08:50:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=865663) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=865663) WARNING 01-28 08:51:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.63 requests/s, 51895.20 total tokens/s, 50.63 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-28 08:47:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:48:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:48:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:48:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:48:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:48:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:48:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:48:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:48:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:48:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:48:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:48:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:48:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:48:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:48:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:48:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:48:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=865663) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=865663) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.75s/it]
(EngineCore_DP0 pid=865663) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.75s/it]
(EngineCore_DP0 pid=865663) 
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=865663) [2026-01-28 08:50:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=865663) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:02<00:09,  2.35s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:06<00:09,  3.24s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:08<00:05,  2.68s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:12<00:03,  3.32s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:16<00:00,  3.44s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:16<00:00,  3.24s/it]
(EngineCore_DP0 pid=865663) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:01,  2.77it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:01<00:01,  1.10it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:01<00:00,  1.59it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:02<00:00,  1.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:02<00:00,  1.73it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 35/1024 [00:00<00:02, 343.78it/s]
Adding requests:   7%|▋         | 72/1024 [00:00<00:02, 358.02it/s]
Adding requests:  11%|█         | 110/1024 [00:00<00:02, 363.41it/s]
Adding requests:  14%|█▍        | 147/1024 [00:00<00:02, 361.83it/s]
Adding requests:  18%|█▊        | 184/1024 [00:00<00:02, 361.39it/s]
Adding requests:  22%|██▏       | 221/1024 [00:00<00:02, 362.83it/s]
Adding requests:  25%|██▌       | 258/1024 [00:00<00:02, 361.66it/s]
Adding requests:  29%|██▉       | 295/1024 [00:00<00:02, 350.88it/s]
Adding requests:  32%|███▏      | 331/1024 [00:00<00:01, 350.82it/s]
Adding requests:  36%|███▌      | 368/1024 [00:01<00:01, 356.29it/s]
Adding requests:  39%|███▉      | 404/1024 [00:01<00:01, 356.76it/s]
Adding requests:  43%|████▎     | 440/1024 [00:01<00:01, 355.40it/s]
Adding requests:  47%|████▋     | 477/1024 [00:01<00:01, 359.21it/s]
Adding requests:  50%|█████     | 513/1024 [00:01<00:01, 356.58it/s]
Adding requests:  54%|█████▎    | 549/1024 [00:01<00:01, 348.53it/s]
Adding requests:  57%|█████▋    | 586/1024 [00:01<00:01, 354.15it/s]
Adding requests:  61%|██████    | 623/1024 [00:01<00:01, 357.44it/s]
Adding requests:  64%|██████▍   | 659/1024 [00:01<00:01, 353.32it/s]
Adding requests:  68%|██████▊   | 698/1024 [00:01<00:00, 362.74it/s]
Adding requests:  72%|███████▏  | 736/1024 [00:02<00:00, 366.59it/s]
Adding requests:  75%|███████▌  | 773/1024 [00:02<00:00, 364.30it/s]
Adding requests:  79%|███████▉  | 810/1024 [00:02<00:00, 362.90it/s]
Adding requests:  83%|████████▎ | 847/1024 [00:02<00:00, 359.16it/s]
Adding requests:  87%|████████▋ | 886/1024 [00:02<00:00, 365.83it/s]
Adding requests:  90%|█████████ | 925/1024 [00:02<00:00, 369.86it/s]
Adding requests:  94%|█████████▍| 963/1024 [00:02<00:00, 372.74it/s]
Adding requests:  98%|█████████▊| 1001/1024 [00:02<00:00, 373.08it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 361.35it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:00<00:02, 433.71it/s, est. speed input: 444180.34 toks/s, output: 433.73 toks/s]
Processed prompts:  13%|█▎        | 134/1024 [00:00<00:07, 116.45it/s, est. speed input: 139860.35 toks/s, output: 136.58 toks/s]
Processed prompts:  15%|█▌        | 156/1024 [00:01<00:09, 87.57it/s, est. speed input: 110587.49 toks/s, output: 108.00 toks/s] 
Processed prompts:  17%|█▋        | 170/1024 [00:01<00:11, 75.50it/s, est. speed input: 99262.79 toks/s, output: 96.94 toks/s]  
Processed prompts:  18%|█▊        | 181/1024 [00:01<00:11, 74.79it/s, est. speed input: 97136.93 toks/s, output: 94.86 toks/s]
Processed prompts:  19%|█▊        | 191/1024 [00:02<00:11, 72.83it/s, est. speed input: 94826.04 toks/s, output: 92.60 toks/s]
Processed prompts:  20%|█▉        | 200/1024 [00:02<00:11, 69.60it/s, est. speed input: 92351.87 toks/s, output: 90.19 toks/s]
Processed prompts:  20%|██        | 208/1024 [00:02<00:12, 65.32it/s, est. speed input: 89766.35 toks/s, output: 87.66 toks/s]
Processed prompts:  21%|██        | 215/1024 [00:02<00:13, 60.24it/s, est. speed input: 87110.04 toks/s, output: 85.07 toks/s]
Processed prompts:  22%|██▏       | 222/1024 [00:02<00:14, 56.26it/s, est. speed input: 84757.84 toks/s, output: 82.77 toks/s]
Processed prompts:  22%|██▏       | 228/1024 [00:02<00:15, 51.47it/s, est. speed input: 82304.39 toks/s, output: 80.38 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:02<00:16, 47.90it/s, est. speed input: 80105.59 toks/s, output: 78.23 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:03<00:15, 49.02it/s, est. speed input: 78777.36 toks/s, output: 76.93 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:03<00:15, 49.79it/s, est. speed input: 77562.93 toks/s, output: 75.74 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:03<00:15, 50.36it/s, est. speed input: 76462.13 toks/s, output: 74.67 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:03<00:14, 52.26it/s, est. speed input: 75770.23 toks/s, output: 73.99 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:03<00:13, 53.68it/s, est. speed input: 75131.28 toks/s, output: 73.37 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:03<00:13, 54.72it/s, est. speed input: 74537.77 toks/s, output: 72.79 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:04<00:13, 55.49it/s, est. speed input: 73989.79 toks/s, output: 72.26 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:04<00:12, 56.02it/s, est. speed input: 73473.95 toks/s, output: 71.75 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:04<00:12, 56.39it/s, est. speed input: 72991.42 toks/s, output: 71.28 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:04<00:12, 56.65it/s, est. speed input: 72539.24 toks/s, output: 70.84 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:04<00:12, 56.87it/s, est. speed input: 72119.05 toks/s, output: 70.43 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:04<00:12, 56.97it/s, est. speed input: 71717.72 toks/s, output: 70.04 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:04<00:12, 57.09it/s, est. speed input: 71344.32 toks/s, output: 69.67 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:04<00:11, 57.13it/s, est. speed input: 70987.45 toks/s, output: 69.32 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:05<00:11, 57.16it/s, est. speed input: 70649.96 toks/s, output: 68.99 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:05<00:11, 57.20it/s, est. speed input: 70332.69 toks/s, output: 68.68 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:05<00:11, 57.24it/s, est. speed input: 70033.20 toks/s, output: 68.39 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:05<00:11, 57.27it/s, est. speed input: 69748.90 toks/s, output: 68.11 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:05<00:11, 57.28it/s, est. speed input: 69477.00 toks/s, output: 67.85 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:05<00:10, 57.27it/s, est. speed input: 69216.99 toks/s, output: 67.59 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:05<00:10, 57.27it/s, est. speed input: 68968.96 toks/s, output: 67.35 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:06<00:10, 57.22it/s, est. speed input: 68728.57 toks/s, output: 67.12 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:06<00:10, 57.22it/s, est. speed input: 68501.83 toks/s, output: 66.90 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:06<00:10, 57.23it/s, est. speed input: 68286.06 toks/s, output: 66.69 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:06<00:10, 57.21it/s, est. speed input: 68077.37 toks/s, output: 66.48 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:06<00:10, 57.21it/s, est. speed input: 67877.50 toks/s, output: 66.29 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:06<00:10, 57.21it/s, est. speed input: 67686.93 toks/s, output: 66.10 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:06<00:09, 57.21it/s, est. speed input: 67503.75 toks/s, output: 65.92 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:07<00:09, 57.21it/s, est. speed input: 67327.66 toks/s, output: 65.75 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:07<00:09, 57.21it/s, est. speed input: 67158.46 toks/s, output: 65.58 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:07<00:09, 57.23it/s, est. speed input: 66997.12 toks/s, output: 65.43 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:07<00:09, 57.21it/s, est. speed input: 66839.59 toks/s, output: 65.27 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:07<00:09, 57.22it/s, est. speed input: 66689.10 toks/s, output: 65.13 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:07<00:09, 57.22it/s, est. speed input: 66543.46 toks/s, output: 64.98 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:07<00:08, 57.23it/s, est. speed input: 66403.95 toks/s, output: 64.85 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:08<00:08, 57.23it/s, est. speed input: 66268.64 toks/s, output: 64.72 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:08<00:08, 57.20it/s, est. speed input: 66136.13 toks/s, output: 64.59 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:08<00:08, 57.19it/s, est. speed input: 66009.09 toks/s, output: 64.46 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:08<00:08, 57.17it/s, est. speed input: 65884.98 toks/s, output: 64.34 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:08<00:08, 56.19it/s, est. speed input: 65704.03 toks/s, output: 64.16 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:08<00:08, 55.15it/s, est. speed input: 65504.53 toks/s, output: 63.97 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:08<00:08, 54.42it/s, est. speed input: 65310.48 toks/s, output: 63.78 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:09<00:08, 53.91it/s, est. speed input: 65122.21 toks/s, output: 63.60 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:09<00:08, 53.56it/s, est. speed input: 64940.24 toks/s, output: 63.42 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:09<00:08, 53.34it/s, est. speed input: 64765.62 toks/s, output: 63.25 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:09<00:07, 53.15it/s, est. speed input: 64593.59 toks/s, output: 63.08 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:09<00:07, 53.04it/s, est. speed input: 64428.74 toks/s, output: 62.92 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:09<00:07, 52.97it/s, est. speed input: 64269.85 toks/s, output: 62.76 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:09<00:07, 52.94it/s, est. speed input: 64116.78 toks/s, output: 62.61 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:10<00:07, 52.88it/s, est. speed input: 63965.56 toks/s, output: 62.47 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:10<00:07, 52.85it/s, est. speed input: 63819.24 toks/s, output: 62.32 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:10<00:07, 52.80it/s, est. speed input: 63676.05 toks/s, output: 62.18 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:10<00:06, 52.81it/s, est. speed input: 63539.13 toks/s, output: 62.05 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:10<00:06, 52.79it/s, est. speed input: 63404.85 toks/s, output: 61.92 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:10<00:06, 52.79it/s, est. speed input: 63274.92 toks/s, output: 61.79 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:11<00:06, 52.80it/s, est. speed input: 63149.32 toks/s, output: 61.67 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:11<00:06, 52.78it/s, est. speed input: 63025.39 toks/s, output: 61.55 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:11<00:06, 52.77it/s, est. speed input: 62905.16 toks/s, output: 61.43 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:11<00:06, 52.77it/s, est. speed input: 62788.08 toks/s, output: 61.32 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:11<00:05, 52.82it/s, est. speed input: 62676.88 toks/s, output: 61.21 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:11<00:05, 52.83it/s, est. speed input: 62567.60 toks/s, output: 61.10 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:11<00:05, 52.83it/s, est. speed input: 62460.09 toks/s, output: 61.00 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:12<00:05, 52.84it/s, est. speed input: 62356.17 toks/s, output: 60.89 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:12<00:05, 52.85it/s, est. speed input: 62254.77 toks/s, output: 60.80 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:12<00:05, 52.84it/s, est. speed input: 62155.47 toks/s, output: 60.70 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:12<00:04, 52.81it/s, est. speed input: 62057.16 toks/s, output: 60.60 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:12<00:04, 52.80it/s, est. speed input: 61961.46 toks/s, output: 60.51 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:12<00:04, 52.80it/s, est. speed input: 61868.52 toks/s, output: 60.42 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:13<00:04, 52.81it/s, est. speed input: 61778.36 toks/s, output: 60.33 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:13<00:04, 52.81it/s, est. speed input: 61690.02 toks/s, output: 60.24 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:13<00:04, 52.80it/s, est. speed input: 61602.87 toks/s, output: 60.16 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:04, 52.83it/s, est. speed input: 61519.34 toks/s, output: 60.08 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:13<00:03, 52.83it/s, est. speed input: 61437.27 toks/s, output: 60.00 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:13<00:03, 52.84it/s, est. speed input: 61357.07 toks/s, output: 59.92 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:13<00:03, 52.85it/s, est. speed input: 61278.69 toks/s, output: 59.84 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:14<00:03, 52.84it/s, est. speed input: 61201.49 toks/s, output: 59.77 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:14<00:03, 52.84it/s, est. speed input: 61126.03 toks/s, output: 59.69 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:14<00:03, 52.84it/s, est. speed input: 61052.30 toks/s, output: 59.62 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:14<00:02, 52.83it/s, est. speed input: 60979.70 toks/s, output: 59.55 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:14<00:02, 52.81it/s, est. speed input: 60907.73 toks/s, output: 59.48 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:14<00:02, 52.82it/s, est. speed input: 60838.66 toks/s, output: 59.41 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:14<00:02, 52.82it/s, est. speed input: 60770.47 toks/s, output: 59.35 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:15<00:02, 52.80it/s, est. speed input: 60702.80 toks/s, output: 59.28 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:15<00:02, 52.80it/s, est. speed input: 60637.11 toks/s, output: 59.22 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:15<00:02, 52.81it/s, est. speed input: 60573.18 toks/s, output: 59.15 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:15<00:01, 52.80it/s, est. speed input: 60509.79 toks/s, output: 59.09 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:15<00:01, 52.80it/s, est. speed input: 60447.73 toks/s, output: 59.03 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:15<00:01, 52.80it/s, est. speed input: 60387.07 toks/s, output: 58.97 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:16<00:01, 52.78it/s, est. speed input: 60326.48 toks/s, output: 58.91 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:16<00:01, 52.78it/s, est. speed input: 60267.75 toks/s, output: 58.86 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:16<00:01, 52.78it/s, est. speed input: 60210.27 toks/s, output: 58.80 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:16<00:01, 52.77it/s, est. speed input: 60153.31 toks/s, output: 58.74 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:16<00:00, 52.76it/s, est. speed input: 60097.35 toks/s, output: 58.69 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:16<00:00, 54.42it/s, est. speed input: 60097.50 toks/s, output: 58.69 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:16<00:00, 53.89it/s, est. speed input: 60042.43 toks/s, output: 58.64 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:17<00:00, 53.53it/s, est. speed input: 59988.71 toks/s, output: 58.58 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:17<00:00, 53.32it/s, est. speed input: 59937.18 toks/s, output: 58.53 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:17<00:00, 55.09it/s, est. speed input: 59946.40 toks/s, output: 58.54 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 55.09it/s, est. speed input: 60298.88 toks/s, output: 58.89 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 58.89it/s, est. speed input: 60298.88 toks/s, output: 58.89 toks/s]
[rank0]:[W128 08:53:53.809744570 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-28 08:55:33
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:56:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:56:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=870062) WARNING 01-28 08:58:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=870062) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=870062) WARNING 01-28 08:59:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.60 requests/s, 40594.81 total tokens/s, 39.60 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-28 08:56:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:56:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:56:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:56:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:56:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:56:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:56:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:56:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:56:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:56:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:56:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:56:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:56:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:56:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:56:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:57:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:57:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:57:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:57:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:57:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:57:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:57:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:57:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:57:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:57:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:57:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:57:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:57:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=870062) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=870062) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.21it/s]
(EngineCore_DP0 pid=870062) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.21it/s]
(EngineCore_DP0 pid=870062) 
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=870062) [2026-01-28 08:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=870062) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:04<00:29,  4.94s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:08<00:20,  4.09s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:12<00:16,  4.20s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:16<00:12,  4.13s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:21<00:08,  4.29s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:28<00:05,  5.11s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:32<00:00,  4.91s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:32<00:00,  4.65s/it]
(EngineCore_DP0 pid=870062) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:01<00:04,  1.24s/it]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:01<00:02,  1.07it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:02<00:01,  1.45it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:03<00:00,  1.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:04<00:00,  1.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 36/2048 [00:00<00:05, 357.05it/s]
Adding requests:   4%|▎         | 75/2048 [00:00<00:05, 372.73it/s]
Adding requests:   6%|▌         | 113/2048 [00:00<00:05, 375.21it/s]
Adding requests:   7%|▋         | 151/2048 [00:00<00:05, 372.39it/s]
Adding requests:   9%|▉         | 189/2048 [00:00<00:04, 373.05it/s]
Adding requests:  11%|█         | 228/2048 [00:00<00:04, 378.52it/s]
Adding requests:  13%|█▎        | 267/2048 [00:00<00:04, 381.85it/s]
Adding requests:  15%|█▍        | 306/2048 [00:00<00:04, 379.51it/s]
Adding requests:  17%|█▋        | 345/2048 [00:00<00:04, 381.71it/s]
Adding requests:  19%|█▉        | 384/2048 [00:01<00:04, 383.22it/s]
Adding requests:  21%|██        | 423/2048 [00:01<00:04, 383.45it/s]
Adding requests:  23%|██▎       | 462/2048 [00:01<00:04, 382.30it/s]
Adding requests:  24%|██▍       | 501/2048 [00:01<00:04, 383.32it/s]
Adding requests:  26%|██▋       | 540/2048 [00:01<00:03, 379.94it/s]
Adding requests:  28%|██▊       | 580/2048 [00:01<00:03, 385.19it/s]
Adding requests:  30%|███       | 620/2048 [00:01<00:03, 389.20it/s]
Adding requests:  32%|███▏      | 660/2048 [00:01<00:03, 391.80it/s]
Adding requests:  34%|███▍      | 702/2048 [00:01<00:03, 397.46it/s]
Adding requests:  36%|███▌      | 742/2048 [00:01<00:03, 395.66it/s]
Adding requests:  38%|███▊      | 782/2048 [00:02<00:03, 393.23it/s]
Adding requests:  40%|████      | 822/2048 [00:02<00:03, 385.74it/s]
Adding requests:  42%|████▏     | 861/2048 [00:02<00:03, 380.20it/s]
Adding requests:  44%|████▍     | 901/2048 [00:02<00:02, 383.98it/s]
Adding requests:  46%|████▌     | 941/2048 [00:02<00:02, 387.46it/s]
Adding requests:  48%|████▊     | 981/2048 [00:02<00:02, 390.72it/s]
Adding requests:  50%|████▉     | 1022/2048 [00:02<00:02, 394.41it/s]
Adding requests:  52%|█████▏    | 1063/2048 [00:02<00:02, 395.44it/s]
Adding requests:  54%|█████▍    | 1103/2048 [00:02<00:02, 394.10it/s]
Adding requests:  56%|█████▌    | 1143/2048 [00:02<00:02, 388.75it/s]
Adding requests:  58%|█████▊    | 1184/2048 [00:03<00:02, 394.69it/s]
Adding requests:  60%|█████▉    | 1225/2048 [00:03<00:02, 396.89it/s]
Adding requests:  62%|██████▏   | 1265/2048 [00:03<00:01, 393.14it/s]
Adding requests:  64%|██████▎   | 1305/2048 [00:03<00:01, 393.20it/s]
Adding requests:  66%|██████▌   | 1346/2048 [00:03<00:01, 397.07it/s]
Adding requests:  68%|██████▊   | 1386/2048 [00:03<00:01, 397.57it/s]
Adding requests:  70%|██████▉   | 1427/2048 [00:03<00:01, 399.86it/s]
Adding requests:  72%|███████▏  | 1467/2048 [00:03<00:01, 399.62it/s]
Adding requests:  74%|███████▎  | 1509/2048 [00:03<00:01, 403.27it/s]
Adding requests:  76%|███████▌  | 1550/2048 [00:03<00:01, 398.26it/s]
Adding requests:  78%|███████▊  | 1592/2048 [00:04<00:01, 403.13it/s]
Adding requests:  80%|███████▉  | 1633/2048 [00:04<00:01, 404.93it/s]
Adding requests:  82%|████████▏ | 1674/2048 [00:04<00:00, 400.77it/s]
Adding requests:  84%|████████▎ | 1715/2048 [00:04<00:00, 398.52it/s]
Adding requests:  86%|████████▌ | 1755/2048 [00:04<00:00, 396.60it/s]
Adding requests:  88%|████████▊ | 1795/2048 [00:04<00:00, 393.23it/s]
Adding requests:  90%|████████▉ | 1835/2048 [00:04<00:00, 394.42it/s]
Adding requests:  92%|█████████▏| 1875/2048 [00:04<00:00, 393.86it/s]
Adding requests:  94%|█████████▎| 1915/2048 [00:04<00:00, 394.18it/s]
Adding requests:  95%|█████████▌| 1955/2048 [00:05<00:00, 392.02it/s]
Adding requests:  97%|█████████▋| 1995/2048 [00:06<00:00, 102.43it/s]
Adding requests:  99%|█████████▉| 2035/2048 [00:06<00:00, 131.80it/s]
Adding requests: 100%|██████████| 2048/2048 [00:06<00:00, 329.45it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 2/2048 [00:09<2:41:22,  4.73s/it, est. speed input: 216.38 toks/s, output: 0.21 toks/s]
Processed prompts:   1%|          | 18/2048 [00:09<13:26,  2.52it/s, est. speed input: 1889.21 toks/s, output: 1.84 toks/s]
Processed prompts:   2%|▏         | 34/2048 [00:10<06:06,  5.49it/s, est. speed input: 3465.35 toks/s, output: 3.38 toks/s]
Processed prompts:   2%|▏         | 50/2048 [00:10<03:37,  9.20it/s, est. speed input: 4952.95 toks/s, output: 4.84 toks/s]
Processed prompts:   3%|▎         | 66/2048 [00:10<02:25, 13.63it/s, est. speed input: 6359.24 toks/s, output: 6.21 toks/s]
Processed prompts:   4%|▍         | 82/2048 [00:10<01:45, 18.66it/s, est. speed input: 7690.69 toks/s, output: 7.51 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:11<01:21, 24.03it/s, est. speed input: 8953.16 toks/s, output: 8.74 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:11<01:05, 29.44it/s, est. speed input: 10151.72 toks/s, output: 9.91 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:11<00:55, 34.54it/s, est. speed input: 11291.19 toks/s, output: 11.03 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:12<00:48, 39.11it/s, est. speed input: 12376.60 toks/s, output: 12.09 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:12<00:43, 42.92it/s, est. speed input: 13409.47 toks/s, output: 13.10 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:12<00:40, 46.01it/s, est. speed input: 14395.35 toks/s, output: 14.06 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:12<00:38, 48.42it/s, est. speed input: 15336.92 toks/s, output: 14.98 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:13<00:36, 50.26it/s, est. speed input: 16237.83 toks/s, output: 15.86 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:13<00:35, 51.62it/s, est. speed input: 17099.76 toks/s, output: 16.70 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:13<00:34, 52.60it/s, est. speed input: 17925.09 toks/s, output: 17.50 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:14<00:33, 53.32it/s, est. speed input: 18716.86 toks/s, output: 18.28 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:14<00:32, 53.84it/s, est. speed input: 19476.89 toks/s, output: 19.02 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:14<00:32, 54.19it/s, est. speed input: 20206.47 toks/s, output: 19.73 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:14<00:31, 54.45it/s, est. speed input: 20907.79 toks/s, output: 20.42 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:15<00:31, 54.62it/s, est. speed input: 21582.22 toks/s, output: 21.08 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:15<00:31, 54.72it/s, est. speed input: 22231.12 toks/s, output: 21.71 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:15<00:30, 54.80it/s, est. speed input: 22856.28 toks/s, output: 22.32 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:16<00:30, 54.85it/s, est. speed input: 23458.68 toks/s, output: 22.91 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:16<00:30, 54.90it/s, est. speed input: 24040.17 toks/s, output: 23.48 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:16<00:29, 54.97it/s, est. speed input: 24602.37 toks/s, output: 24.03 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:17<00:29, 54.92it/s, est. speed input: 25142.91 toks/s, output: 24.55 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:17<00:29, 54.94it/s, est. speed input: 25666.62 toks/s, output: 25.07 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:17<00:29, 54.95it/s, est. speed input: 26173.03 toks/s, output: 25.56 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:17<00:28, 54.97it/s, est. speed input: 26663.02 toks/s, output: 26.04 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:18<00:28, 54.97it/s, est. speed input: 27137.28 toks/s, output: 26.50 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:18<00:28, 54.97it/s, est. speed input: 27596.46 toks/s, output: 26.95 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:18<00:27, 54.97it/s, est. speed input: 28041.31 toks/s, output: 27.38 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:19<00:27, 54.96it/s, est. speed input: 28472.54 toks/s, output: 27.81 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:19<00:27, 54.98it/s, est. speed input: 28891.23 toks/s, output: 28.21 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:19<00:27, 54.98it/s, est. speed input: 29297.23 toks/s, output: 28.61 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:19<00:26, 54.97it/s, est. speed input: 29691.23 toks/s, output: 29.00 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:20<00:26, 54.98it/s, est. speed input: 30074.28 toks/s, output: 29.37 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:20<00:26, 54.98it/s, est. speed input: 30446.32 toks/s, output: 29.73 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:20<00:25, 54.98it/s, est. speed input: 30807.81 toks/s, output: 30.09 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:21<00:25, 54.96it/s, est. speed input: 31159.07 toks/s, output: 30.43 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:21<00:25, 54.96it/s, est. speed input: 31500.89 toks/s, output: 30.76 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:21<00:25, 54.94it/s, est. speed input: 31833.19 toks/s, output: 31.09 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:21<00:24, 54.94it/s, est. speed input: 32156.91 toks/s, output: 31.40 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:22<00:24, 54.93it/s, est. speed input: 32472.00 toks/s, output: 31.71 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:22<00:24, 54.93it/s, est. speed input: 32779.03 toks/s, output: 32.01 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:22<00:23, 54.94it/s, est. speed input: 33078.37 toks/s, output: 32.30 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:23<00:23, 54.93it/s, est. speed input: 33370.07 toks/s, output: 32.59 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:23<00:23, 54.97it/s, est. speed input: 33655.38 toks/s, output: 32.87 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:23<00:22, 54.90it/s, est. speed input: 33931.39 toks/s, output: 33.14 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:24<00:22, 54.87it/s, est. speed input: 34201.22 toks/s, output: 33.40 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:24<00:22, 54.89it/s, est. speed input: 34465.33 toks/s, output: 33.66 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:24<00:22, 54.90it/s, est. speed input: 34723.28 toks/s, output: 33.91 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:24<00:21, 54.90it/s, est. speed input: 34975.16 toks/s, output: 34.16 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:25<00:21, 54.92it/s, est. speed input: 35221.35 toks/s, output: 34.40 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:25<00:21, 54.91it/s, est. speed input: 35461.65 toks/s, output: 34.63 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:25<00:20, 54.90it/s, est. speed input: 35696.30 toks/s, output: 34.86 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:26<00:20, 54.91it/s, est. speed input: 35925.99 toks/s, output: 35.08 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:26<00:20, 54.91it/s, est. speed input: 36150.70 toks/s, output: 35.30 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:26<00:20, 54.90it/s, est. speed input: 36370.09 toks/s, output: 35.52 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:26<00:19, 54.89it/s, est. speed input: 36584.70 toks/s, output: 35.73 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:27<00:19, 55.79it/s, est. speed input: 36815.95 toks/s, output: 35.95 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:27<00:18, 55.57it/s, est. speed input: 37022.83 toks/s, output: 36.16 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:27<00:18, 55.30it/s, est. speed input: 37222.61 toks/s, output: 36.35 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:28<00:18, 55.20it/s, est. speed input: 37420.23 toks/s, output: 36.54 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:28<00:18, 55.07it/s, est. speed input: 37612.38 toks/s, output: 36.73 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:28<00:18, 55.00it/s, est. speed input: 37801.18 toks/s, output: 36.92 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:28<00:17, 54.96it/s, est. speed input: 37986.37 toks/s, output: 37.10 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:29<00:17, 54.92it/s, est. speed input: 38167.52 toks/s, output: 37.27 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:29<00:17, 54.89it/s, est. speed input: 38345.13 toks/s, output: 37.45 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:29<00:16, 54.88it/s, est. speed input: 38519.33 toks/s, output: 37.62 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:30<00:16, 54.87it/s, est. speed input: 38690.31 toks/s, output: 37.78 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:30<00:16, 55.83it/s, est. speed input: 38879.37 toks/s, output: 37.97 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:30<00:15, 55.53it/s, est. speed input: 39043.80 toks/s, output: 38.13 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:30<00:15, 55.33it/s, est. speed input: 39205.02 toks/s, output: 38.29 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:31<00:15, 55.18it/s, est. speed input: 39363.22 toks/s, output: 38.44 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:31<00:15, 55.13it/s, est. speed input: 39519.58 toks/s, output: 38.59 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:31<00:14, 54.99it/s, est. speed input: 39670.79 toks/s, output: 38.74 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:33<00:29, 27.40it/s, est. speed input: 38646.87 toks/s, output: 37.74 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:33<00:24, 32.23it/s, est. speed input: 38799.55 toks/s, output: 37.89 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:33<00:20, 36.77it/s, est. speed input: 38949.45 toks/s, output: 38.04 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:33<00:18, 40.80it/s, est. speed input: 39096.87 toks/s, output: 38.18 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:34<00:16, 44.18it/s, est. speed input: 39241.69 toks/s, output: 38.32 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:34<00:15, 46.93it/s, est. speed input: 39384.72 toks/s, output: 38.46 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:34<00:14, 48.99it/s, est. speed input: 39523.59 toks/s, output: 38.60 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:35<00:13, 50.59it/s, est. speed input: 39661.14 toks/s, output: 38.73 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:35<00:12, 51.78it/s, est. speed input: 39796.48 toks/s, output: 38.86 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:35<00:12, 52.64it/s, est. speed input: 39929.55 toks/s, output: 38.99 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:36<00:11, 53.25it/s, est. speed input: 40060.20 toks/s, output: 39.12 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:36<00:11, 53.69it/s, est. speed input: 40188.97 toks/s, output: 39.25 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:36<00:11, 54.00it/s, est. speed input: 40315.60 toks/s, output: 39.37 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:36<00:10, 54.22it/s, est. speed input: 40440.20 toks/s, output: 39.49 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:37<00:10, 54.38it/s, est. speed input: 40562.86 toks/s, output: 39.61 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:37<00:10, 54.47it/s, est. speed input: 40683.26 toks/s, output: 39.73 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:37<00:09, 54.55it/s, est. speed input: 40802.17 toks/s, output: 39.85 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:38<00:09, 54.58it/s, est. speed input: 40918.67 toks/s, output: 39.96 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:38<00:09, 54.61it/s, est. speed input: 41033.54 toks/s, output: 40.07 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:38<00:09, 54.62it/s, est. speed input: 41146.60 toks/s, output: 40.18 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:38<00:08, 54.64it/s, est. speed input: 41258.11 toks/s, output: 40.29 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:39<00:08, 54.63it/s, est. speed input: 41367.58 toks/s, output: 40.40 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:39<00:08, 54.63it/s, est. speed input: 41475.46 toks/s, output: 40.50 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:39<00:07, 54.62it/s, est. speed input: 41581.62 toks/s, output: 40.61 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:40<00:07, 54.64it/s, est. speed input: 41686.72 toks/s, output: 40.71 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:40<00:07, 54.65it/s, est. speed input: 41790.22 toks/s, output: 40.81 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:40<00:06, 54.64it/s, est. speed input: 41891.96 toks/s, output: 40.91 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:41<00:06, 54.65it/s, est. speed input: 41992.49 toks/s, output: 41.01 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:41<00:06, 54.63it/s, est. speed input: 42091.20 toks/s, output: 41.10 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:41<00:06, 54.62it/s, est. speed input: 42188.52 toks/s, output: 41.20 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:41<00:05, 54.61it/s, est. speed input: 42284.32 toks/s, output: 41.29 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:42<00:05, 54.61it/s, est. speed input: 42379.05 toks/s, output: 41.39 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:42<00:05, 54.61it/s, est. speed input: 42472.43 toks/s, output: 41.48 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:42<00:04, 54.62it/s, est. speed input: 42564.67 toks/s, output: 41.57 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:43<00:04, 54.61it/s, est. speed input: 42655.45 toks/s, output: 41.66 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:43<00:04, 54.57it/s, est. speed input: 42744.37 toks/s, output: 41.74 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:43<00:04, 54.57it/s, est. speed input: 42832.62 toks/s, output: 41.83 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:43<00:03, 54.43it/s, est. speed input: 42917.23 toks/s, output: 41.91 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:44<00:03, 54.47it/s, est. speed input: 43003.02 toks/s, output: 42.00 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:44<00:03, 55.44it/s, est. speed input: 43103.81 toks/s, output: 42.09 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:44<00:02, 55.17it/s, est. speed input: 43187.27 toks/s, output: 42.18 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:45<00:02, 54.99it/s, est. speed input: 43269.92 toks/s, output: 42.26 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:45<00:02, 54.85it/s, est. speed input: 43351.12 toks/s, output: 42.34 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:45<00:02, 54.75it/s, est. speed input: 43431.22 toks/s, output: 42.41 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:45<00:01, 54.69it/s, est. speed input: 43510.56 toks/s, output: 42.49 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:46<00:01, 54.65it/s, est. speed input: 43588.89 toks/s, output: 42.57 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:46<00:01, 54.62it/s, est. speed input: 43666.08 toks/s, output: 42.64 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:46<00:00, 54.61it/s, est. speed input: 43742.54 toks/s, output: 42.72 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:47<00:00, 54.59it/s, est. speed input: 43817.98 toks/s, output: 42.79 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:47<00:00, 55.68it/s, est. speed input: 43910.21 toks/s, output: 42.88 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:47<00:00, 55.68it/s, est. speed input: 44212.12 toks/s, output: 43.18 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:47<00:00, 43.18it/s, est. speed input: 44212.12 toks/s, output: 43.18 toks/s]
[rank0]:[W128 09:02:38.959149476 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-28 09:02:52
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:03:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:03:47 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=873778) WARNING 01-28 09:05:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=873778) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=873778) WARNING 01-28 09:06:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.86 requests/s, 48028.94 total tokens/s, 46.86 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-28 09:03:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:03:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:03:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:03:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:03:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:03:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:03:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:03:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:03:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:03:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:03:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:03:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:03:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:03:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:04:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:04:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:04:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:04:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:04:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:04:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:04:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:04:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:04:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:04:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:04:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:04:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:04:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:04:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:18] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=873778) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=873778) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.18it/s]
(EngineCore_DP0 pid=873778) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.18it/s]
(EngineCore_DP0 pid=873778) 
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=873778) [2026-01-28 09:05:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=873778) [rank0]:W0128 09:07:06.986000 873778 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=873778) [rank0]:W0128 09:07:07.094000 873778 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=873778) [rank0]:W0128 09:07:17.942000 873778 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=873778) [rank0]:W0128 09:07:18.110000 873778 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=873778) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:01<00:13,  1.32s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:04<00:23,  2.57s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:09<00:28,  3.56s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:13<00:26,  3.72s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:14<00:17,  2.83s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:15<00:10,  2.14s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:17<00:08,  2.14s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:21<00:07,  2.66s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:28<00:07,  3.94s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:29<00:03,  3.03s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:30<00:00,  2.41s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:30<00:00,  2.74s/it]
(EngineCore_DP0 pid=873778) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:01<00:07,  1.21s/it]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:02<00:06,  1.35s/it]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:02<00:03,  1.21it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:03<00:02,  1.06it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:05<00:02,  1.22s/it]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:06<00:00,  1.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:08<00:00,  1.38s/it]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:08<00:00,  1.20s/it]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 34/4096 [00:00<00:12, 337.44it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:11, 357.87it/s]
Adding requests:   3%|▎         | 110/4096 [00:00<00:10, 364.77it/s]
Adding requests:   4%|▎         | 147/4096 [00:00<00:10, 365.97it/s]
Adding requests:   4%|▍         | 184/4096 [00:00<00:10, 366.52it/s]
Adding requests:   5%|▌         | 221/4096 [00:00<00:10, 365.20it/s]
Adding requests:   6%|▋         | 259/4096 [00:00<00:10, 368.96it/s]
Adding requests:   7%|▋         | 296/4096 [00:00<00:10, 365.39it/s]
Adding requests:   8%|▊         | 333/4096 [00:00<00:10, 362.70it/s]
Adding requests:   9%|▉         | 371/4096 [00:01<00:10, 366.39it/s]
Adding requests:  10%|█         | 410/4096 [00:01<00:09, 370.87it/s]
Adding requests:  11%|█         | 448/4096 [00:01<00:09, 370.93it/s]
Adding requests:  12%|█▏        | 486/4096 [00:01<00:09, 373.01it/s]
Adding requests:  13%|█▎        | 524/4096 [00:01<00:09, 365.55it/s]
Adding requests:  14%|█▍        | 564/4096 [00:01<00:09, 373.17it/s]
Adding requests:  15%|█▍        | 602/4096 [00:01<00:09, 371.08it/s]
Adding requests:  16%|█▌        | 641/4096 [00:01<00:09, 375.37it/s]
Adding requests:  17%|█▋        | 680/4096 [00:01<00:09, 377.70it/s]
Adding requests:  18%|█▊        | 718/4096 [00:01<00:08, 378.18it/s]
Adding requests:  18%|█▊        | 756/4096 [00:02<00:08, 375.72it/s]
Adding requests:  19%|█▉        | 794/4096 [00:02<00:08, 376.96it/s]
Adding requests:  20%|██        | 832/4096 [00:02<00:08, 369.00it/s]
Adding requests:  21%|██        | 870/4096 [00:02<00:08, 371.68it/s]
Adding requests:  22%|██▏       | 909/4096 [00:02<00:08, 376.81it/s]
Adding requests:  23%|██▎       | 948/4096 [00:02<00:08, 379.98it/s]
Adding requests:  24%|██▍       | 987/4096 [00:02<00:08, 381.91it/s]
Adding requests:  25%|██▌       | 1026/4096 [00:02<00:08, 379.30it/s]
Adding requests:  26%|██▌       | 1065/4096 [00:02<00:07, 381.16it/s]
Adding requests:  27%|██▋       | 1104/4096 [00:02<00:08, 366.66it/s]
Adding requests:  28%|██▊       | 1141/4096 [00:03<00:08, 367.47it/s]
Adding requests:  29%|██▉       | 1182/4096 [00:03<00:07, 377.82it/s]
Adding requests:  30%|██▉       | 1223/4096 [00:03<00:07, 386.88it/s]
Adding requests:  31%|███       | 1262/4096 [00:03<00:07, 384.60it/s]
Adding requests:  32%|███▏      | 1301/4096 [00:03<00:07, 381.45it/s]
Adding requests:  33%|███▎      | 1341/4096 [00:03<00:07, 386.87it/s]
Adding requests:  34%|███▎      | 1381/4096 [00:03<00:06, 390.12it/s]
Adding requests:  35%|███▍      | 1421/4096 [00:03<00:06, 391.21it/s]
Adding requests:  36%|███▌      | 1461/4096 [00:03<00:06, 391.08it/s]
Adding requests:  37%|███▋      | 1501/4096 [00:03<00:06, 391.98it/s]
Adding requests:  38%|███▊      | 1541/4096 [00:04<00:06, 392.90it/s]
Adding requests:  39%|███▊      | 1582/4096 [00:04<00:06, 395.54it/s]
Adding requests:  40%|███▉      | 1623/4096 [00:04<00:06, 397.36it/s]
Adding requests:  41%|████      | 1663/4096 [00:04<00:06, 395.85it/s]
Adding requests:  42%|████▏     | 1703/4096 [00:04<00:06, 395.54it/s]
Adding requests:  43%|████▎     | 1743/4096 [00:05<00:22, 102.89it/s]
Adding requests:  44%|████▎     | 1782/4096 [00:05<00:17, 130.95it/s]
Adding requests:  44%|████▍     | 1822/4096 [00:05<00:13, 163.95it/s]
Adding requests:  45%|████▌     | 1860/4096 [00:05<00:11, 195.99it/s]
Adding requests:  46%|████▋     | 1900/4096 [00:05<00:09, 231.55it/s]
Adding requests:  47%|████▋     | 1939/4096 [00:06<00:08, 263.32it/s]
Adding requests:  48%|████▊     | 1979/4096 [00:06<00:07, 292.23it/s]
Adding requests:  49%|████▉     | 2019/4096 [00:06<00:06, 316.70it/s]
Adding requests:  50%|█████     | 2059/4096 [00:06<00:06, 336.89it/s]
Adding requests:  51%|█████     | 2099/4096 [00:06<00:05, 351.80it/s]
Adding requests:  52%|█████▏    | 2138/4096 [00:06<00:05, 358.42it/s]
Adding requests:  53%|█████▎    | 2177/4096 [00:06<00:05, 364.14it/s]
Adding requests:  54%|█████▍    | 2216/4096 [00:06<00:05, 370.49it/s]
Adding requests:  55%|█████▌    | 2255/4096 [00:06<00:04, 372.82it/s]
Adding requests:  56%|█████▌    | 2294/4096 [00:06<00:04, 376.71it/s]
Adding requests:  57%|█████▋    | 2333/4096 [00:07<00:04, 367.55it/s]
Adding requests:  58%|█████▊    | 2371/4096 [00:07<00:04, 369.58it/s]
Adding requests:  59%|█████▉    | 2411/4096 [00:07<00:04, 376.30it/s]
Adding requests:  60%|█████▉    | 2451/4096 [00:07<00:04, 380.16it/s]
Adding requests:  61%|██████    | 2490/4096 [00:07<00:04, 382.50it/s]
Adding requests:  62%|██████▏   | 2529/4096 [00:07<00:04, 379.57it/s]
Adding requests:  63%|██████▎   | 2568/4096 [00:07<00:04, 381.23it/s]
Adding requests:  64%|██████▎   | 2607/4096 [00:07<00:03, 378.25it/s]
Adding requests:  65%|██████▍   | 2645/4096 [00:07<00:03, 377.99it/s]
Adding requests:  66%|██████▌   | 2684/4096 [00:08<00:03, 378.86it/s]
Adding requests:  66%|██████▋   | 2723/4096 [00:08<00:03, 381.06it/s]
Adding requests:  67%|██████▋   | 2762/4096 [00:08<00:03, 380.08it/s]
Adding requests:  68%|██████▊   | 2801/4096 [00:08<00:03, 375.98it/s]
Adding requests:  69%|██████▉   | 2840/4096 [00:08<00:03, 379.26it/s]
Adding requests:  70%|███████   | 2880/4096 [00:08<00:03, 382.81it/s]
Adding requests:  71%|███████▏  | 2919/4096 [00:08<00:03, 381.50it/s]
Adding requests:  72%|███████▏  | 2958/4096 [00:08<00:03, 379.14it/s]
Adding requests:  73%|███████▎  | 2997/4096 [00:08<00:02, 380.95it/s]
Adding requests:  74%|███████▍  | 3036/4096 [00:08<00:02, 377.17it/s]
Adding requests:  75%|███████▌  | 3074/4096 [00:09<00:02, 374.75it/s]
Adding requests:  76%|███████▌  | 3114/4096 [00:09<00:02, 380.31it/s]
Adding requests:  77%|███████▋  | 3153/4096 [00:09<00:02, 378.68it/s]
Adding requests:  78%|███████▊  | 3192/4096 [00:09<00:02, 380.86it/s]
Adding requests:  79%|███████▉  | 3231/4096 [00:09<00:02, 383.50it/s]
Adding requests:  80%|███████▉  | 3271/4096 [00:09<00:02, 386.87it/s]
Adding requests:  81%|████████  | 3311/4096 [00:09<00:02, 389.02it/s]
Adding requests:  82%|████████▏ | 3350/4096 [00:09<00:01, 388.95it/s]
Adding requests:  83%|████████▎ | 3389/4096 [00:09<00:01, 385.06it/s]
Adding requests:  84%|████████▎ | 3428/4096 [00:09<00:01, 381.82it/s]
Adding requests:  85%|████████▍ | 3467/4096 [00:10<00:01, 378.08it/s]
Adding requests:  86%|████████▌ | 3505/4096 [00:10<00:01, 377.80it/s]
Adding requests:  87%|████████▋ | 3544/4096 [00:10<00:01, 381.16it/s]
Adding requests:  87%|████████▋ | 3583/4096 [00:10<00:01, 380.75it/s]
Adding requests:  88%|████████▊ | 3622/4096 [00:10<00:01, 380.92it/s]
Adding requests:  89%|████████▉ | 3661/4096 [00:10<00:01, 380.06it/s]
Adding requests:  90%|█████████ | 3701/4096 [00:10<00:01, 385.43it/s]
Adding requests:  91%|█████████▏| 3740/4096 [00:10<00:00, 370.15it/s]
Adding requests:  92%|█████████▏| 3782/4096 [00:10<00:00, 381.50it/s]
Adding requests:  93%|█████████▎| 3821/4096 [00:11<00:00, 381.12it/s]
Adding requests:  94%|█████████▍| 3862/4096 [00:11<00:00, 387.94it/s]
Adding requests:  95%|█████████▌| 3901/4096 [00:11<00:00, 384.55it/s]
Adding requests:  96%|█████████▌| 3941/4096 [00:11<00:00, 385.98it/s]
Adding requests:  97%|█████████▋| 3980/4096 [00:11<00:00, 386.57it/s]
Adding requests:  98%|█████████▊| 4021/4096 [00:11<00:00, 392.17it/s]
Adding requests:  99%|█████████▉| 4061/4096 [00:11<00:00, 386.17it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 349.83it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 2/4096 [00:04<2:41:48,  2.37s/it, est. speed input: 431.83 toks/s, output: 0.42 toks/s]
Processed prompts:   1%|          | 34/4096 [00:05<07:53,  8.59it/s, est. speed input: 6553.60 toks/s, output: 6.40 toks/s]
Processed prompts:   2%|▏         | 66/4096 [00:05<03:58, 16.86it/s, est. speed input: 11506.62 toks/s, output: 11.24 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:06<02:41, 24.72it/s, est. speed input: 15589.54 toks/s, output: 15.22 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:06<02:04, 31.77it/s, est. speed input: 19017.87 toks/s, output: 18.57 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:07<01:44, 37.75it/s, est. speed input: 21934.79 toks/s, output: 21.42 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:08<01:31, 42.60it/s, est. speed input: 24447.02 toks/s, output: 23.87 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:08<01:23, 46.39it/s, est. speed input: 26632.94 toks/s, output: 26.01 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:09<01:17, 49.27it/s, est. speed input: 28554.19 toks/s, output: 27.88 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:09<01:14, 51.39it/s, est. speed input: 30252.35 toks/s, output: 29.54 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:10<01:11, 52.94it/s, est. speed input: 31767.05 toks/s, output: 31.02 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:10<01:09, 54.05it/s, est. speed input: 33124.13 toks/s, output: 32.35 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:11<01:07, 54.85it/s, est. speed input: 34349.49 toks/s, output: 33.54 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:12<01:06, 55.47it/s, est. speed input: 35465.14 toks/s, output: 34.63 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:12<01:05, 55.79it/s, est. speed input: 36470.54 toks/s, output: 35.62 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:13<01:04, 56.13it/s, est. speed input: 37399.85 toks/s, output: 36.52 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:13<01:03, 56.27it/s, est. speed input: 38244.46 toks/s, output: 37.35 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:14<01:02, 56.42it/s, est. speed input: 39027.03 toks/s, output: 38.11 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:14<01:02, 56.51it/s, est. speed input: 39748.78 toks/s, output: 38.82 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:15<01:01, 56.57it/s, est. speed input: 40418.33 toks/s, output: 39.47 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:16<01:01, 56.61it/s, est. speed input: 41039.92 toks/s, output: 40.08 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:16<01:00, 56.64it/s, est. speed input: 41619.48 toks/s, output: 40.64 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:17<00:59, 56.66it/s, est. speed input: 42160.26 toks/s, output: 41.17 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:17<00:59, 56.67it/s, est. speed input: 42667.00 toks/s, output: 41.67 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:18<00:58, 56.68it/s, est. speed input: 43142.36 toks/s, output: 42.13 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:18<00:58, 56.69it/s, est. speed input: 43589.71 toks/s, output: 42.57 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:19<00:57, 56.75it/s, est. speed input: 44014.70 toks/s, output: 42.98 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:19<00:56, 56.68it/s, est. speed input: 44407.95 toks/s, output: 43.37 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:20<00:56, 56.68it/s, est. speed input: 44782.97 toks/s, output: 43.73 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:21<00:55, 56.69it/s, est. speed input: 45138.31 toks/s, output: 44.08 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:21<00:55, 56.68it/s, est. speed input: 45474.17 toks/s, output: 44.41 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:22<00:54, 56.67it/s, est. speed input: 45792.70 toks/s, output: 44.72 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:22<00:54, 56.66it/s, est. speed input: 46094.92 toks/s, output: 45.01 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:23<00:53, 56.69it/s, est. speed input: 46385.42 toks/s, output: 45.30 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:23<00:53, 56.59it/s, est. speed input: 46653.86 toks/s, output: 45.56 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:24<00:52, 56.58it/s, est. speed input: 46913.96 toks/s, output: 45.81 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:25<00:51, 57.06it/s, est. speed input: 47192.80 toks/s, output: 46.09 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:25<00:51, 56.91it/s, est. speed input: 47429.77 toks/s, output: 46.32 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:27<01:17, 36.93it/s, est. speed input: 45884.16 toks/s, output: 44.81 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:27<01:09, 41.22it/s, est. speed input: 46129.24 toks/s, output: 45.05 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:28<01:02, 44.87it/s, est. speed input: 46365.15 toks/s, output: 45.28 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:28<00:58, 47.83it/s, est. speed input: 46590.56 toks/s, output: 45.50 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:29<00:54, 50.13it/s, est. speed input: 46807.22 toks/s, output: 45.71 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:30<00:52, 51.89it/s, est. speed input: 47015.74 toks/s, output: 45.91 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:30<00:50, 53.18it/s, est. speed input: 47215.88 toks/s, output: 46.11 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:31<00:49, 54.13it/s, est. speed input: 47409.29 toks/s, output: 46.30 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:31<00:47, 54.81it/s, est. speed input: 47595.22 toks/s, output: 46.48 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:32<00:46, 55.31it/s, est. speed input: 47775.21 toks/s, output: 46.66 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:32<00:45, 55.64it/s, est. speed input: 47948.35 toks/s, output: 46.82 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:33<00:45, 55.88it/s, est. speed input: 48115.43 toks/s, output: 46.99 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:33<00:44, 56.05it/s, est. speed input: 48276.87 toks/s, output: 47.15 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:34<00:43, 56.15it/s, est. speed input: 48432.65 toks/s, output: 47.30 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:35<00:43, 56.24it/s, est. speed input: 48583.62 toks/s, output: 47.44 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:35<00:42, 56.31it/s, est. speed input: 48730.59 toks/s, output: 47.59 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:36<00:42, 56.30it/s, est. speed input: 48870.31 toks/s, output: 47.72 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:36<00:41, 56.32it/s, est. speed input: 49006.72 toks/s, output: 47.86 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:37<00:40, 56.33it/s, est. speed input: 49138.79 toks/s, output: 47.99 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:37<00:40, 56.33it/s, est. speed input: 49266.82 toks/s, output: 48.11 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:38<00:39, 56.85it/s, est. speed input: 49412.93 toks/s, output: 48.25 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:39<00:38, 56.70it/s, est. speed input: 49533.56 toks/s, output: 48.37 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:39<00:38, 56.60it/s, est. speed input: 49650.65 toks/s, output: 48.49 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:40<00:37, 56.52it/s, est. speed input: 49764.15 toks/s, output: 48.60 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:40<00:37, 56.46it/s, est. speed input: 49874.35 toks/s, output: 48.71 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:41<00:36, 56.41it/s, est. speed input: 49981.29 toks/s, output: 48.81 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:41<00:36, 56.38it/s, est. speed input: 50085.37 toks/s, output: 48.91 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:42<00:35, 56.29it/s, est. speed input: 50183.88 toks/s, output: 49.01 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:43<00:35, 56.28it/s, est. speed input: 50281.84 toks/s, output: 49.10 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:43<00:34, 56.29it/s, est. speed input: 50378.30 toks/s, output: 49.20 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:44<00:34, 56.30it/s, est. speed input: 50471.94 toks/s, output: 49.29 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:44<00:33, 56.29it/s, est. speed input: 50562.67 toks/s, output: 49.38 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:45<00:32, 56.33it/s, est. speed input: 50652.86 toks/s, output: 49.47 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:45<00:32, 56.75it/s, est. speed input: 50755.42 toks/s, output: 49.57 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:46<00:31, 56.67it/s, est. speed input: 50841.72 toks/s, output: 49.65 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:47<00:31, 56.49it/s, est. speed input: 50921.76 toks/s, output: 49.73 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:47<00:30, 56.44it/s, est. speed input: 51002.24 toks/s, output: 49.81 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:48<00:30, 56.39it/s, est. speed input: 51080.68 toks/s, output: 49.88 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:48<00:29, 56.37it/s, est. speed input: 51157.46 toks/s, output: 49.96 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:49<00:28, 56.34it/s, est. speed input: 51232.04 toks/s, output: 50.03 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:49<00:28, 56.83it/s, est. speed input: 51322.32 toks/s, output: 50.12 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:50<00:27, 56.66it/s, est. speed input: 51393.43 toks/s, output: 50.19 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:50<00:27, 56.55it/s, est. speed input: 51463.14 toks/s, output: 50.26 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:51<00:26, 56.47it/s, est. speed input: 51531.34 toks/s, output: 50.32 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:52<00:26, 56.40it/s, est. speed input: 51597.36 toks/s, output: 50.39 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:52<00:25, 56.35it/s, est. speed input: 51662.03 toks/s, output: 50.45 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:53<00:24, 56.37it/s, est. speed input: 51727.07 toks/s, output: 50.51 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:53<00:24, 56.29it/s, est. speed input: 51787.58 toks/s, output: 50.57 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:54<00:23, 56.26it/s, est. speed input: 51847.79 toks/s, output: 50.63 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:54<00:23, 56.19it/s, est. speed input: 51905.39 toks/s, output: 50.69 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:55<00:22, 56.14it/s, est. speed input: 51961.45 toks/s, output: 50.74 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:56<00:22, 56.17it/s, est. speed input: 52018.47 toks/s, output: 50.80 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:56<00:21, 56.22it/s, est. speed input: 52075.40 toks/s, output: 50.85 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:57<00:21, 56.16it/s, est. speed input: 52128.15 toks/s, output: 50.91 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:57<00:20, 56.18it/s, est. speed input: 52181.68 toks/s, output: 50.96 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:58<00:19, 56.18it/s, est. speed input: 52233.93 toks/s, output: 51.01 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:58<00:19, 56.18it/s, est. speed input: 52285.14 toks/s, output: 51.06 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:00<00:28, 36.57it/s, est. speed input: 51455.04 toks/s, output: 50.25 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:01<00:25, 40.85it/s, est. speed input: 51512.04 toks/s, output: 50.30 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:01<00:22, 44.50it/s, est. speed input: 51567.87 toks/s, output: 50.36 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:02<00:20, 47.47it/s, est. speed input: 51622.57 toks/s, output: 50.41 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:02<00:18, 49.79it/s, est. speed input: 51676.37 toks/s, output: 50.47 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:03<00:17, 51.56it/s, est. speed input: 51729.39 toks/s, output: 50.52 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:03<00:16, 52.87it/s, est. speed input: 51781.15 toks/s, output: 50.57 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:04<00:15, 53.83it/s, est. speed input: 51832.12 toks/s, output: 50.62 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:05<00:14, 54.52it/s, est. speed input: 51882.28 toks/s, output: 50.67 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:05<00:13, 55.02it/s, est. speed input: 51931.38 toks/s, output: 50.71 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:06<00:13, 55.38it/s, est. speed input: 51979.97 toks/s, output: 50.76 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:06<00:12, 55.63it/s, est. speed input: 52027.59 toks/s, output: 50.81 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:07<00:12, 55.81it/s, est. speed input: 52074.59 toks/s, output: 50.85 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:07<00:11, 55.93it/s, est. speed input: 52120.56 toks/s, output: 50.90 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:08<00:10, 56.02it/s, est. speed input: 52165.89 toks/s, output: 50.94 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:09<00:10, 56.07it/s, est. speed input: 52210.28 toks/s, output: 50.99 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:09<00:09, 56.11it/s, est. speed input: 52253.79 toks/s, output: 51.03 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:10<00:09, 56.14it/s, est. speed input: 52296.89 toks/s, output: 51.07 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:10<00:08, 56.16it/s, est. speed input: 52339.31 toks/s, output: 51.11 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:11<00:07, 56.18it/s, est. speed input: 52380.93 toks/s, output: 51.15 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:11<00:07, 56.20it/s, est. speed input: 52422.20 toks/s, output: 51.19 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:12<00:06, 56.72it/s, est. speed input: 52475.03 toks/s, output: 51.25 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:13<00:06, 56.56it/s, est. speed input: 52514.63 toks/s, output: 51.28 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:13<00:05, 56.47it/s, est. speed input: 52553.96 toks/s, output: 51.32 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:14<00:05, 56.39it/s, est. speed input: 52592.34 toks/s, output: 51.36 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:14<00:04, 56.34it/s, est. speed input: 52630.21 toks/s, output: 51.40 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:15<00:03, 56.30it/s, est. speed input: 52667.56 toks/s, output: 51.43 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:15<00:03, 56.29it/s, est. speed input: 52704.60 toks/s, output: 51.47 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:16<00:02, 56.27it/s, est. speed input: 52740.95 toks/s, output: 51.50 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:17<00:02, 56.27it/s, est. speed input: 52776.81 toks/s, output: 51.54 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:17<00:01, 56.25it/s, est. speed input: 52811.83 toks/s, output: 51.57 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:18<00:01, 56.75it/s, est. speed input: 52858.06 toks/s, output: 51.62 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:18<00:00, 57.27it/s, est. speed input: 52907.21 toks/s, output: 51.67 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:18<00:00, 57.27it/s, est. speed input: 53297.23 toks/s, output: 52.05 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:18<00:00, 52.05it/s, est. speed input: 53297.23 toks/s, output: 52.05 toks/s]
[rank0]:[W128 09:10:10.425682399 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-28 11:34:26
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:34:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:34:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=23050) WARNING 01-28 11:34:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=23050) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=23050) WARNING 01-28 11:34:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.94 requests/s, 8688.70 total tokens/s, 16.94 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-28 11:34:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:34:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:34:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:34:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:34:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:34:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:34:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:34:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:34:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:34:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:34:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:34:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:34:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:34:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:34:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:34:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=23050) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=23050) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.09s/it]
(EngineCore_DP0 pid=23050) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.09s/it]
(EngineCore_DP0 pid=23050) 
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=23050) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.68it/s]
(EngineCore_DP0 pid=23050) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  52%|█████▏    | 67/128 [00:00<00:00, 669.51it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 690.65it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 38.28it/s, est. speed input: 19605.95 toks/s, output: 38.29 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:05, 21.75it/s, est. speed input: 11907.22 toks/s, output: 23.25 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:06, 19.42it/s, est. speed input: 10731.95 toks/s, output: 20.96 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 18.37it/s, est. speed input: 10180.19 toks/s, output: 19.88 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 18.03it/s, est. speed input: 9976.21 toks/s, output: 19.48 toks/s] 
Processed prompts:  14%|█▍        | 18/128 [00:00<00:06, 17.74it/s, est. speed input: 9817.51 toks/s, output: 19.17 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 17.55it/s, est. speed input: 9696.09 toks/s, output: 18.94 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 17.42it/s, est. speed input: 9602.86 toks/s, output: 18.76 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:05, 17.37it/s, est. speed input: 9533.13 toks/s, output: 18.62 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 17.35it/s, est. speed input: 9477.31 toks/s, output: 18.51 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 17.31it/s, est. speed input: 9427.46 toks/s, output: 18.41 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 17.26it/s, est. speed input: 9381.28 toks/s, output: 18.32 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 17.26it/s, est. speed input: 9344.67 toks/s, output: 18.25 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:05, 17.18it/s, est. speed input: 9304.65 toks/s, output: 18.17 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:05, 17.11it/s, est. speed input: 9266.94 toks/s, output: 18.10 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 17.13it/s, est. speed input: 9240.79 toks/s, output: 18.05 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 17.16it/s, est. speed input: 9218.73 toks/s, output: 18.01 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 17.18it/s, est. speed input: 9199.51 toks/s, output: 17.97 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:04, 17.07it/s, est. speed input: 9170.38 toks/s, output: 17.91 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 17.06it/s, est. speed input: 9150.26 toks/s, output: 17.87 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 17.10it/s, est. speed input: 9134.93 toks/s, output: 17.84 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 17.09it/s, est. speed input: 9118.51 toks/s, output: 17.81 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:04, 17.05it/s, est. speed input: 9100.73 toks/s, output: 17.77 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 17.03it/s, est. speed input: 9084.94 toks/s, output: 17.74 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 17.03it/s, est. speed input: 9071.66 toks/s, output: 17.72 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 17.03it/s, est. speed input: 9059.11 toks/s, output: 17.69 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:03, 17.04it/s, est. speed input: 9047.93 toks/s, output: 17.67 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 17.04it/s, est. speed input: 9036.88 toks/s, output: 17.65 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 17.07it/s, est. speed input: 9028.55 toks/s, output: 17.63 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 17.11it/s, est. speed input: 9021.87 toks/s, output: 17.62 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 17.08it/s, est. speed input: 9012.38 toks/s, output: 17.60 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:03, 17.03it/s, est. speed input: 9001.66 toks/s, output: 17.58 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 17.07it/s, est. speed input: 8995.87 toks/s, output: 17.57 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 17.15it/s, est. speed input: 8992.47 toks/s, output: 17.56 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 17.21it/s, est. speed input: 8989.56 toks/s, output: 17.56 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:02, 17.23it/s, est. speed input: 8985.94 toks/s, output: 17.55 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 17.14it/s, est. speed input: 8977.71 toks/s, output: 17.53 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 17.13it/s, est. speed input: 8972.12 toks/s, output: 17.52 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:04<00:02, 17.15it/s, est. speed input: 8968.04 toks/s, output: 17.52 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:04<00:02, 17.07it/s, est. speed input: 8960.22 toks/s, output: 17.50 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 17.08it/s, est. speed input: 8955.40 toks/s, output: 17.49 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 17.12it/s, est. speed input: 8952.27 toks/s, output: 17.48 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 17.14it/s, est. speed input: 8948.93 toks/s, output: 17.48 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:01, 17.20it/s, est. speed input: 8947.32 toks/s, output: 17.48 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 17.23it/s, est. speed input: 8945.36 toks/s, output: 17.47 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 17.24it/s, est. speed input: 8943.16 toks/s, output: 17.47 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:05<00:01, 17.24it/s, est. speed input: 8940.99 toks/s, output: 17.46 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:05<00:01, 17.05it/s, est. speed input: 8931.91 toks/s, output: 17.45 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:05<00:01, 16.94it/s, est. speed input: 8924.12 toks/s, output: 17.43 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 16.91it/s, est. speed input: 8918.15 toks/s, output: 17.42 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 16.99it/s, est. speed input: 8915.89 toks/s, output: 17.41 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 17.06it/s, est. speed input: 8914.23 toks/s, output: 17.41 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:00, 17.07it/s, est. speed input: 8911.18 toks/s, output: 17.40 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 17.02it/s, est. speed input: 8906.67 toks/s, output: 17.40 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:06<00:00, 17.04it/s, est. speed input: 8903.72 toks/s, output: 17.39 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:06<00:00, 17.08it/s, est. speed input: 8901.96 toks/s, output: 17.39 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:06<00:00, 17.04it/s, est. speed input: 8898.03 toks/s, output: 17.38 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 17.04it/s, est. speed input: 8895.09 toks/s, output: 17.37 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 17.09it/s, est. speed input: 8893.67 toks/s, output: 17.37 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 17.14it/s, est. speed input: 8892.84 toks/s, output: 17.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.16it/s, est. speed input: 8891.59 toks/s, output: 17.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.16it/s, est. speed input: 8891.59 toks/s, output: 17.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.37it/s, est. speed input: 8891.59 toks/s, output: 17.37 toks/s]
[rank0]:[W128 11:35:13.852533854 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-28 11:35:16
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:35:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:35:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=23588) WARNING 01-28 11:35:33 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=23588) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=23588) WARNING 01-28 11:35:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.27 requests/s, 16681.55 total tokens/s, 16.27 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-28 11:35:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:35:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:35:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:35:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:35:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:35:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:35:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:35:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:35:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:35:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:35:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:35:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:35:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:35:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:35:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:35:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=23588) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=23588) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=23588) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=23588) 
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=23588) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.98it/s]
(EngineCore_DP0 pid=23588) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  29%|██▉       | 37/128 [00:00<00:00, 369.80it/s]
Adding requests:  59%|█████▉    | 76/128 [00:00<00:00, 380.59it/s]
Adding requests:  90%|████████▉ | 115/128 [00:00<00:00, 379.92it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 379.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 42.97it/s, est. speed input: 44007.80 toks/s, output: 42.97 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:05, 22.51it/s, est. speed input: 24826.01 toks/s, output: 24.24 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 20.23it/s, est. speed input: 22537.62 toks/s, output: 22.01 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 18.91it/s, est. speed input: 21252.40 toks/s, output: 20.75 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:05, 18.18it/s, est. speed input: 20488.35 toks/s, output: 20.01 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 17.81it/s, est. speed input: 20104.49 toks/s, output: 19.63 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 17.55it/s, est. speed input: 19813.57 toks/s, output: 19.35 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.29it/s, est. speed input: 19555.20 toks/s, output: 19.10 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.13it/s, est. speed input: 19350.41 toks/s, output: 18.90 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.03it/s, est. speed input: 19183.89 toks/s, output: 18.73 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.91it/s, est. speed input: 19026.46 toks/s, output: 18.58 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 16.79it/s, est. speed input: 18882.82 toks/s, output: 18.44 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 16.70it/s, est. speed input: 18757.16 toks/s, output: 18.32 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.66it/s, est. speed input: 18648.74 toks/s, output: 18.21 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.50it/s, est. speed input: 18527.71 toks/s, output: 18.09 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:04<00:30,  2.81it/s, est. speed input: 9847.63 toks/s, output: 9.62 toks/s]  
Processed prompts:  34%|███▎      | 43/128 [00:04<00:22,  3.73it/s, est. speed input: 10042.40 toks/s, output: 9.81 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:04<00:17,  4.86it/s, est. speed input: 10228.79 toks/s, output: 9.99 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:04<00:13,  6.16it/s, est. speed input: 10404.84 toks/s, output: 10.16 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:04<00:10,  7.58it/s, est. speed input: 10570.68 toks/s, output: 10.32 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:04<00:08,  9.05it/s, est. speed input: 10728.83 toks/s, output: 10.48 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:04<00:07, 10.49it/s, est. speed input: 10881.61 toks/s, output: 10.63 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:05<00:06, 11.78it/s, est. speed input: 11024.88 toks/s, output: 10.77 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:05<00:05, 12.93it/s, est. speed input: 11164.58 toks/s, output: 10.90 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:05<00:04, 13.83it/s, est. speed input: 11294.63 toks/s, output: 11.03 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:05<00:04, 14.56it/s, est. speed input: 11420.61 toks/s, output: 11.15 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:05<00:04, 15.16it/s, est. speed input: 11543.39 toks/s, output: 11.27 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:05<00:04, 15.54it/s, est. speed input: 11656.81 toks/s, output: 11.38 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:05<00:03, 15.82it/s, est. speed input: 11766.22 toks/s, output: 11.49 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:05<00:03, 16.09it/s, est. speed input: 11874.34 toks/s, output: 11.60 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:06<00:03, 16.28it/s, est. speed input: 11978.11 toks/s, output: 11.70 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:06<00:03, 16.42it/s, est. speed input: 12077.70 toks/s, output: 11.79 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:06<00:03, 16.50it/s, est. speed input: 12173.14 toks/s, output: 11.89 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:06<00:03, 16.55it/s, est. speed input: 12264.27 toks/s, output: 11.98 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:06<00:02, 16.57it/s, est. speed input: 12351.85 toks/s, output: 12.06 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:06<00:02, 16.52it/s, est. speed input: 12433.13 toks/s, output: 12.14 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:06<00:02, 16.50it/s, est. speed input: 12511.85 toks/s, output: 12.22 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:06<00:02, 16.50it/s, est. speed input: 12588.67 toks/s, output: 12.29 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:07<00:02, 16.47it/s, est. speed input: 12661.52 toks/s, output: 12.36 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:07<00:02, 16.54it/s, est. speed input: 12736.02 toks/s, output: 12.44 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:07<00:02, 16.57it/s, est. speed input: 12807.18 toks/s, output: 12.51 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:07<00:02, 16.50it/s, est. speed input: 12871.91 toks/s, output: 12.57 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:07<00:02, 16.49it/s, est. speed input: 12936.26 toks/s, output: 12.63 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:07<00:01, 16.42it/s, est. speed input: 12996.26 toks/s, output: 12.69 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:07<00:01, 16.41it/s, est. speed input: 13055.50 toks/s, output: 12.75 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:07<00:01, 16.46it/s, est. speed input: 13115.70 toks/s, output: 12.81 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:08<00:01, 16.54it/s, est. speed input: 13175.56 toks/s, output: 12.87 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:08<00:01, 16.49it/s, est. speed input: 13229.60 toks/s, output: 12.92 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:08<00:01, 16.55it/s, est. speed input: 13285.86 toks/s, output: 12.97 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:08<00:01, 16.61it/s, est. speed input: 13340.79 toks/s, output: 13.03 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:08<00:01, 16.66it/s, est. speed input: 13394.97 toks/s, output: 13.08 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:08<00:00, 16.68it/s, est. speed input: 13446.67 toks/s, output: 13.13 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:08<00:00, 16.65it/s, est. speed input: 13495.72 toks/s, output: 13.18 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:08<00:00, 16.56it/s, est. speed input: 13540.41 toks/s, output: 13.22 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:08<00:00, 16.62it/s, est. speed input: 13588.88 toks/s, output: 13.27 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:09<00:00, 16.62it/s, est. speed input: 13634.17 toks/s, output: 13.31 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:09<00:00, 16.64it/s, est. speed input: 13678.95 toks/s, output: 13.36 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:09<00:00, 16.60it/s, est. speed input: 13720.87 toks/s, output: 13.40 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:09<00:00, 16.54it/s, est. speed input: 13760.66 toks/s, output: 13.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 16.54it/s, est. speed input: 13779.21 toks/s, output: 13.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 13.46it/s, est. speed input: 13779.21 toks/s, output: 13.46 toks/s]
[rank0]:[W128 11:36:04.371275209 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-28 11:36:06
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:36:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:36:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=24114) WARNING 01-28 11:36:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=24114) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=24114) WARNING 01-28 11:36:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.60 requests/s, 35462.85 total tokens/s, 34.60 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-28 11:36:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:36:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:36:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:36:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:36:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:36:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:36:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:36:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:36:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:36:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:36:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:36:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:36:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:36:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:36:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:36:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=24114) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=24114) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=24114) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=24114) 
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=24114) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.23it/s]
(EngineCore_DP0 pid=24114) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.36it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 37/256 [00:00<00:00, 362.54it/s]
Adding requests:  30%|███       | 77/256 [00:00<00:00, 382.28it/s]
Adding requests:  45%|████▌     | 116/256 [00:00<00:00, 372.27it/s]
Adding requests:  60%|██████    | 154/256 [00:00<00:00, 370.29it/s]
Adding requests:  75%|███████▌  | 192/256 [00:00<00:00, 368.35it/s]
Adding requests:  90%|████████▉ | 230/256 [00:00<00:00, 371.72it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 373.70it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 147.82it/s, est. speed input: 151399.70 toks/s, output: 147.83 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:00<00:03, 59.37it/s, est. speed input: 67743.54 toks/s, output: 66.16 toks/s]   
Processed prompts:  17%|█▋        | 44/256 [00:00<00:04, 47.00it/s, est. speed input: 55587.21 toks/s, output: 54.28 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:04, 43.96it/s, est. speed input: 52398.12 toks/s, output: 51.17 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:01<00:04, 41.81it/s, est. speed input: 50195.73 toks/s, output: 49.02 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:01<00:04, 42.56it/s, est. speed input: 49864.69 toks/s, output: 48.70 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:04, 38.75it/s, est. speed input: 47676.17 toks/s, output: 46.56 toks/s]
Processed prompts:  28%|██▊       | 71/256 [00:01<00:04, 40.08it/s, est. speed input: 47518.82 toks/s, output: 46.40 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:04, 36.72it/s, est. speed input: 45857.17 toks/s, output: 44.78 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:04, 36.51it/s, est. speed input: 45293.57 toks/s, output: 44.23 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:04, 36.34it/s, est. speed input: 44793.19 toks/s, output: 43.74 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:02<00:04, 36.32it/s, est. speed input: 44376.37 toks/s, output: 43.34 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:02<00:04, 36.12it/s, est. speed input: 43961.61 toks/s, output: 42.93 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:04, 35.92it/s, est. speed input: 43576.08 toks/s, output: 42.55 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:04, 35.75it/s, est. speed input: 43222.17 toks/s, output: 42.21 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:04, 35.67it/s, est. speed input: 42909.21 toks/s, output: 41.90 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:04, 35.73it/s, est. speed input: 42644.26 toks/s, output: 41.64 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:04, 35.72it/s, est. speed input: 42391.36 toks/s, output: 41.40 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:03, 35.84it/s, est. speed input: 42179.70 toks/s, output: 41.19 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:03, 35.95it/s, est. speed input: 41987.14 toks/s, output: 41.00 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:03<00:03, 36.08it/s, est. speed input: 41815.95 toks/s, output: 40.84 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:03<00:03, 36.18it/s, est. speed input: 41657.64 toks/s, output: 40.68 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:03<00:03, 36.23it/s, est. speed input: 41508.17 toks/s, output: 40.54 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 36.22it/s, est. speed input: 41362.09 toks/s, output: 40.39 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:03<00:03, 36.14it/s, est. speed input: 41216.97 toks/s, output: 40.25 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:03<00:03, 35.72it/s, est. speed input: 41037.01 toks/s, output: 40.07 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:03, 35.74it/s, est. speed input: 40905.13 toks/s, output: 39.95 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:03<00:02, 35.85it/s, est. speed input: 40790.34 toks/s, output: 39.83 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:03<00:02, 35.50it/s, est. speed input: 40636.92 toks/s, output: 39.68 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:04<00:02, 35.63it/s, est. speed input: 40530.95 toks/s, output: 39.58 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:04<00:02, 35.82it/s, est. speed input: 40440.58 toks/s, output: 39.49 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:04<00:02, 35.91it/s, est. speed input: 40351.11 toks/s, output: 39.41 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:02, 35.85it/s, est. speed input: 40254.38 toks/s, output: 39.31 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:04<00:02, 35.89it/s, est. speed input: 40170.38 toks/s, output: 39.23 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:04<00:02, 35.95it/s, est. speed input: 40093.09 toks/s, output: 39.15 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:04<00:01, 36.02it/s, est. speed input: 40020.92 toks/s, output: 39.08 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:04<00:01, 36.08it/s, est. speed input: 39953.83 toks/s, output: 39.02 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:04<00:01, 36.20it/s, est. speed input: 39896.11 toks/s, output: 38.96 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:05<00:01, 36.26it/s, est. speed input: 39839.40 toks/s, output: 38.91 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:05<00:01, 36.05it/s, est. speed input: 39764.88 toks/s, output: 38.83 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:05<00:01, 35.99it/s, est. speed input: 39699.68 toks/s, output: 38.77 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 35.93it/s, est. speed input: 39636.12 toks/s, output: 38.71 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 35.84it/s, est. speed input: 39571.62 toks/s, output: 38.64 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 35.76it/s, est. speed input: 39508.49 toks/s, output: 38.58 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:05<00:01, 35.78it/s, est. speed input: 39453.63 toks/s, output: 38.53 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:05<00:00, 35.87it/s, est. speed input: 39405.56 toks/s, output: 38.48 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:05<00:00, 35.75it/s, est. speed input: 39347.36 toks/s, output: 38.43 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:06<00:00, 35.79it/s, est. speed input: 39299.29 toks/s, output: 38.38 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:06<00:00, 35.88it/s, est. speed input: 39257.35 toks/s, output: 38.34 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:06<00:00, 36.02it/s, est. speed input: 39221.72 toks/s, output: 38.30 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:06<00:00, 36.06it/s, est. speed input: 39183.11 toks/s, output: 38.26 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:06<00:00, 36.09it/s, est. speed input: 39146.32 toks/s, output: 38.23 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:06<00:00, 36.00it/s, est. speed input: 39104.41 toks/s, output: 38.19 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 35.76it/s, est. speed input: 39052.54 toks/s, output: 38.14 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 35.76it/s, est. speed input: 39052.54 toks/s, output: 38.14 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 38.14it/s, est. speed input: 39052.54 toks/s, output: 38.14 toks/s]
[rank0]:[W128 11:36:53.021953597 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-28 11:36:55
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:37:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:37:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=24646) WARNING 01-28 11:37:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=24646) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=24646) WARNING 01-28 11:37:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 52.02 requests/s, 53319.95 total tokens/s, 52.02 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-28 11:37:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:37:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:37:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:37:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:37:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:37:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:37:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:37:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:37:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:37:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:37:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:37:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:37:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:37:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:37:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:37:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=24646) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=24646) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=24646) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=24646) 
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=24646) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.05it/s]
(EngineCore_DP0 pid=24646) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.36it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 36/512 [00:00<00:01, 356.14it/s]
Adding requests:  15%|█▍        | 75/512 [00:00<00:01, 372.50it/s]
Adding requests:  22%|██▏       | 113/512 [00:00<00:01, 369.90it/s]
Adding requests:  29%|██▉       | 150/512 [00:00<00:00, 369.04it/s]
Adding requests:  37%|███▋      | 187/512 [00:00<00:00, 368.39it/s]
Adding requests:  44%|████▍     | 224/512 [00:00<00:00, 368.51it/s]
Adding requests:  51%|█████     | 262/512 [00:00<00:00, 369.41it/s]
Adding requests:  58%|█████▊    | 299/512 [00:00<00:00, 368.50it/s]
Adding requests:  66%|██████▌   | 336/512 [00:00<00:00, 368.87it/s]
Adding requests:  73%|███████▎  | 374/512 [00:01<00:00, 372.15it/s]
Adding requests:  80%|████████  | 412/512 [00:01<00:00, 373.30it/s]
Adding requests:  88%|████████▊ | 450/512 [00:01<00:00, 374.04it/s]
Adding requests:  95%|█████████▌| 488/512 [00:01<00:00, 370.00it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 368.90it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:00<00:00, 520.83it/s, est. speed input: 533430.89 toks/s, output: 520.86 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:01<00:04, 93.42it/s, est. speed input: 110795.31 toks/s, output: 108.20 toks/s]
Processed prompts:  28%|██▊       | 144/512 [00:01<00:04, 79.20it/s, est. speed input: 95194.49 toks/s, output: 92.96 toks/s]  
Processed prompts:  31%|███▏      | 160/512 [00:01<00:04, 72.48it/s, est. speed input: 88702.15 toks/s, output: 86.62 toks/s]
Processed prompts:  34%|███▎      | 172/512 [00:02<00:04, 68.25it/s, est. speed input: 85000.32 toks/s, output: 83.01 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:02<00:05, 62.61it/s, est. speed input: 81173.10 toks/s, output: 79.27 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:02<00:05, 60.96it/s, est. speed input: 79575.43 toks/s, output: 77.71 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:02<00:05, 59.43it/s, est. speed input: 78159.76 toks/s, output: 76.33 toks/s]
Processed prompts:  40%|████      | 206/512 [00:02<00:05, 58.12it/s, est. speed input: 76902.85 toks/s, output: 75.10 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:02<00:05, 56.98it/s, est. speed input: 75762.25 toks/s, output: 73.99 toks/s]
Processed prompts:  43%|████▎     | 220/512 [00:05<00:24, 11.72it/s, est. speed input: 44761.39 toks/s, output: 43.71 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:05<00:20, 13.91it/s, est. speed input: 44652.75 toks/s, output: 43.61 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:05<00:15, 17.83it/s, est. speed input: 44941.33 toks/s, output: 43.89 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:05<00:12, 22.26it/s, est. speed input: 45211.04 toks/s, output: 44.15 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:09, 26.97it/s, est. speed input: 45469.21 toks/s, output: 44.40 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:05<00:08, 31.68it/s, est. speed input: 45712.91 toks/s, output: 44.64 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:05<00:06, 36.10it/s, est. speed input: 45944.29 toks/s, output: 44.87 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:06<00:05, 39.99it/s, est. speed input: 46160.24 toks/s, output: 45.08 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:05, 43.21it/s, est. speed input: 46361.60 toks/s, output: 45.27 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:04, 45.81it/s, est. speed input: 46555.19 toks/s, output: 45.46 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:06<00:04, 47.89it/s, est. speed input: 46745.36 toks/s, output: 45.65 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:06<00:04, 49.49it/s, est. speed input: 46928.57 toks/s, output: 45.83 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:06<00:03, 50.66it/s, est. speed input: 47102.90 toks/s, output: 46.00 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:06<00:03, 51.51it/s, est. speed input: 47269.84 toks/s, output: 46.16 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:07<00:03, 52.12it/s, est. speed input: 47428.96 toks/s, output: 46.32 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:07<00:03, 52.56it/s, est. speed input: 47582.76 toks/s, output: 46.47 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:07<00:03, 52.78it/s, est. speed input: 47724.14 toks/s, output: 46.61 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:07<00:02, 53.04it/s, est. speed input: 47866.08 toks/s, output: 46.74 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:07<00:02, 53.21it/s, est. speed input: 48001.92 toks/s, output: 46.88 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:07<00:02, 53.31it/s, est. speed input: 48131.81 toks/s, output: 47.00 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:08<00:02, 53.41it/s, est. speed input: 48258.29 toks/s, output: 47.13 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:08<00:02, 53.31it/s, est. speed input: 48371.49 toks/s, output: 47.24 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:08<00:02, 53.32it/s, est. speed input: 48483.28 toks/s, output: 47.35 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:08<00:02, 53.38it/s, est. speed input: 48594.94 toks/s, output: 47.46 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:08<00:01, 53.43it/s, est. speed input: 48702.95 toks/s, output: 47.56 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:08<00:01, 53.47it/s, est. speed input: 48807.67 toks/s, output: 47.66 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:08<00:01, 53.50it/s, est. speed input: 48908.84 toks/s, output: 47.76 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:09<00:01, 53.38it/s, est. speed input: 48999.94 toks/s, output: 47.85 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:09<00:01, 53.37it/s, est. speed input: 49091.11 toks/s, output: 47.94 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:09<00:01, 53.40it/s, est. speed input: 49181.56 toks/s, output: 48.03 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:09<00:01, 53.43it/s, est. speed input: 49269.68 toks/s, output: 48.11 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:09<00:00, 52.87it/s, est. speed input: 49326.85 toks/s, output: 48.17 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:09<00:00, 52.38it/s, est. speed input: 49377.25 toks/s, output: 48.22 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:09<00:00, 52.01it/s, est. speed input: 49424.73 toks/s, output: 48.27 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:10<00:00, 51.79it/s, est. speed input: 49472.15 toks/s, output: 48.31 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:10<00:00, 51.53it/s, est. speed input: 49513.22 toks/s, output: 48.35 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:10<00:00, 51.42it/s, est. speed input: 49556.14 toks/s, output: 48.39 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 51.42it/s, est. speed input: 49842.94 toks/s, output: 48.67 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 48.67it/s, est. speed input: 49842.94 toks/s, output: 48.67 toks/s]
[rank0]:[W128 11:37:48.125875636 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-28 11:37:50
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:38:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:38:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=25211) WARNING 01-28 11:38:09 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=25211) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=25211) WARNING 01-28 11:38:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.22 requests/s, 55572.99 total tokens/s, 54.22 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-28 11:38:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:38:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:38:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:38:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:38:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:38:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:38:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:38:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:38:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:38:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:38:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:38:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:38:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:38:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:38:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:38:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=25211) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=25211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.38it/s]
(EngineCore_DP0 pid=25211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.38it/s]
(EngineCore_DP0 pid=25211) 
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=25211) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.58it/s]
(EngineCore_DP0 pid=25211) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  9.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.34it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.11it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 378.43it/s]
Adding requests:   8%|▊         | 79/1024 [00:00<00:02, 394.30it/s]
Adding requests:  12%|█▏        | 119/1024 [00:00<00:02, 395.00it/s]
Adding requests:  16%|█▌        | 159/1024 [00:00<00:02, 390.72it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 389.37it/s]
Adding requests:  23%|██▎       | 239/1024 [00:00<00:02, 390.64it/s]
Adding requests:  27%|██▋       | 280/1024 [00:00<00:01, 394.48it/s]
Adding requests:  31%|███▏      | 321/1024 [00:00<00:01, 396.63it/s]
Adding requests:  35%|███▌      | 362/1024 [00:00<00:01, 400.44it/s]
Adding requests:  39%|███▉      | 404/1024 [00:01<00:01, 404.62it/s]
Adding requests:  43%|████▎     | 445/1024 [00:01<00:01, 404.42it/s]
Adding requests:  48%|████▊     | 487/1024 [00:01<00:01, 404.72it/s]
Adding requests:  52%|█████▏    | 528/1024 [00:01<00:01, 393.80it/s]
Adding requests:  56%|█████▌    | 571/1024 [00:01<00:01, 402.49it/s]
Adding requests:  60%|█████▉    | 612/1024 [00:01<00:01, 402.45it/s]
Adding requests:  64%|██████▍   | 653/1024 [00:01<00:00, 393.68it/s]
Adding requests:  68%|██████▊   | 696/1024 [00:01<00:00, 402.37it/s]
Adding requests:  72%|███████▏  | 739/1024 [00:01<00:00, 408.15it/s]
Adding requests:  76%|███████▌  | 780/1024 [00:01<00:00, 405.37it/s]
Adding requests:  80%|████████  | 821/1024 [00:02<00:00, 395.47it/s]
Adding requests:  84%|████████▍ | 863/1024 [00:02<00:00, 400.16it/s]
Adding requests:  88%|████████▊ | 906/1024 [00:02<00:00, 408.11it/s]
Adding requests:  92%|█████████▏| 947/1024 [00:02<00:00, 407.03it/s]
Adding requests:  96%|█████████▋| 988/1024 [00:02<00:00, 407.15it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 400.91it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:00<00:00, 988.96it/s, est. speed input: 1012960.31 toks/s, output: 989.08 toks/s]
Processed prompts:  22%|██▏       | 229/1024 [00:01<00:07, 105.40it/s, est. speed input: 127302.96 toks/s, output: 124.32 toks/s] 
Processed prompts:  27%|██▋       | 274/1024 [00:02<00:08, 84.08it/s, est. speed input: 104019.77 toks/s, output: 101.58 toks/s] 
Processed prompts:  29%|██▉       | 301/1024 [00:03<00:09, 79.35it/s, est. speed input: 98602.92 toks/s, output: 96.29 toks/s]  
Processed prompts:  31%|███▏      | 320/1024 [00:03<00:09, 77.07it/s, est. speed input: 96036.93 toks/s, output: 93.79 toks/s]
Processed prompts:  33%|███▎      | 335/1024 [00:03<00:09, 72.14it/s, est. speed input: 92775.10 toks/s, output: 90.60 toks/s]
Processed prompts:  34%|███▍      | 347/1024 [00:03<00:10, 65.41it/s, est. speed input: 89201.22 toks/s, output: 87.11 toks/s]
Processed prompts:  35%|███▍      | 356/1024 [00:04<00:10, 65.09it/s, est. speed input: 88351.18 toks/s, output: 86.28 toks/s]
Processed prompts:  36%|███▌      | 365/1024 [00:04<00:10, 64.76it/s, est. speed input: 87555.80 toks/s, output: 85.50 toks/s]
Processed prompts:  36%|███▋      | 373/1024 [00:04<00:10, 63.13it/s, est. speed input: 86585.65 toks/s, output: 84.56 toks/s]
Processed prompts:  37%|███▋      | 380/1024 [00:04<00:10, 60.12it/s, est. speed input: 85444.39 toks/s, output: 83.44 toks/s]
Processed prompts:  38%|███▊      | 387/1024 [00:04<00:11, 57.52it/s, est. speed input: 84372.92 toks/s, output: 82.40 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:04<00:11, 55.40it/s, est. speed input: 83365.48 toks/s, output: 81.41 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:04<00:11, 55.57it/s, est. speed input: 82620.75 toks/s, output: 80.68 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:05<00:11, 55.69it/s, est. speed input: 81917.37 toks/s, output: 80.00 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:05<00:10, 55.73it/s, est. speed input: 81244.38 toks/s, output: 79.34 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:05<00:10, 55.78it/s, est. speed input: 80610.20 toks/s, output: 78.72 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:05<00:10, 55.82it/s, est. speed input: 80008.39 toks/s, output: 78.13 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:05<00:10, 55.83it/s, est. speed input: 79435.26 toks/s, output: 77.57 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:05<00:10, 55.85it/s, est. speed input: 78891.01 toks/s, output: 77.04 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:05<00:10, 55.88it/s, est. speed input: 78375.36 toks/s, output: 76.54 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:06<00:09, 55.91it/s, est. speed input: 77883.70 toks/s, output: 76.06 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:06<00:09, 55.96it/s, est. speed input: 77418.94 toks/s, output: 75.60 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:06<00:09, 55.97it/s, est. speed input: 76971.09 toks/s, output: 75.17 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:06<00:09, 56.03it/s, est. speed input: 76548.03 toks/s, output: 74.75 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:06<00:09, 56.02it/s, est. speed input: 76138.28 toks/s, output: 74.35 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:06<00:09, 56.00it/s, est. speed input: 75744.81 toks/s, output: 73.97 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:06<00:09, 55.98it/s, est. speed input: 75366.55 toks/s, output: 73.60 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:07<00:08, 55.97it/s, est. speed input: 75003.34 toks/s, output: 73.25 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:08, 55.95it/s, est. speed input: 74653.91 toks/s, output: 72.90 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:07<00:08, 55.95it/s, est. speed input: 74319.25 toks/s, output: 72.58 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:07<00:08, 55.96it/s, est. speed input: 73997.64 toks/s, output: 72.26 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:07<00:08, 55.98it/s, est. speed input: 73689.30 toks/s, output: 71.96 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:07<00:08, 55.98it/s, est. speed input: 73390.90 toks/s, output: 71.67 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:07<00:08, 55.95it/s, est. speed input: 73100.86 toks/s, output: 71.39 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:08<00:07, 55.95it/s, est. speed input: 72823.06 toks/s, output: 71.12 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:08<00:07, 55.92it/s, est. speed input: 72552.00 toks/s, output: 70.85 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:08<00:07, 55.90it/s, est. speed input: 72290.35 toks/s, output: 70.60 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:08<00:07, 55.89it/s, est. speed input: 72037.92 toks/s, output: 70.35 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:08<00:07, 55.93it/s, est. speed input: 71797.05 toks/s, output: 70.11 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:08<00:07, 55.93it/s, est. speed input: 71562.45 toks/s, output: 69.89 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:08<00:07, 55.94it/s, est. speed input: 71335.48 toks/s, output: 69.66 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:09<00:06, 55.94it/s, est. speed input: 71115.13 toks/s, output: 69.45 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:09<00:06, 55.92it/s, est. speed input: 70900.41 toks/s, output: 69.24 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:09<00:06, 54.55it/s, est. speed input: 70603.65 toks/s, output: 68.95 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:09<00:06, 53.51it/s, est. speed input: 70308.14 toks/s, output: 68.66 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:09<00:06, 52.77it/s, est. speed input: 70019.73 toks/s, output: 68.38 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:09<00:06, 52.27it/s, est. speed input: 69740.45 toks/s, output: 68.11 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:10<00:06, 51.94it/s, est. speed input: 69471.03 toks/s, output: 67.84 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:10<00:06, 51.72it/s, est. speed input: 69210.44 toks/s, output: 67.59 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:10<00:06, 51.58it/s, est. speed input: 68958.36 toks/s, output: 67.34 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:10<00:06, 51.71it/s, est. speed input: 68728.99 toks/s, output: 67.12 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:10<00:05, 52.86it/s, est. speed input: 68572.60 toks/s, output: 66.97 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:10<00:05, 53.71it/s, est. speed input: 68420.78 toks/s, output: 66.82 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:10<00:05, 54.30it/s, est. speed input: 68272.12 toks/s, output: 66.67 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:11<00:05, 54.72it/s, est. speed input: 68126.89 toks/s, output: 66.53 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:11<00:05, 54.98it/s, est. speed input: 67983.35 toks/s, output: 66.39 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:11<00:04, 55.15it/s, est. speed input: 67842.90 toks/s, output: 66.25 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:11<00:04, 55.27it/s, est. speed input: 67706.09 toks/s, output: 66.12 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:11<00:04, 55.35it/s, est. speed input: 67572.32 toks/s, output: 65.99 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:11<00:04, 55.45it/s, est. speed input: 67443.74 toks/s, output: 65.86 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:11<00:04, 55.53it/s, est. speed input: 67318.97 toks/s, output: 65.74 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:12<00:04, 55.58it/s, est. speed input: 67196.77 toks/s, output: 65.62 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:12<00:03, 55.56it/s, est. speed input: 67075.24 toks/s, output: 65.50 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:12<00:03, 55.62it/s, est. speed input: 66959.40 toks/s, output: 65.39 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:12<00:03, 55.64it/s, est. speed input: 66845.68 toks/s, output: 65.28 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:12<00:03, 55.66it/s, est. speed input: 66734.61 toks/s, output: 65.17 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:12<00:03, 55.67it/s, est. speed input: 66625.68 toks/s, output: 65.06 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:12<00:03, 55.67it/s, est. speed input: 66519.25 toks/s, output: 64.96 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:13<00:03, 55.67it/s, est. speed input: 66415.06 toks/s, output: 64.86 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:15<00:17,  9.41it/s, est. speed input: 56305.15 toks/s, output: 54.99 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:15<00:12, 12.53it/s, est. speed input: 56311.73 toks/s, output: 54.99 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:15<00:09, 16.28it/s, est. speed input: 56300.50 toks/s, output: 54.98 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:16<00:06, 20.46it/s, est. speed input: 56260.31 toks/s, output: 54.94 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:16<00:05, 24.93it/s, est. speed input: 56220.22 toks/s, output: 54.90 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:16<00:04, 29.44it/s, est. speed input: 56181.34 toks/s, output: 54.86 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:16<00:03, 33.71it/s, est. speed input: 56142.90 toks/s, output: 54.83 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:16<00:02, 37.52it/s, est. speed input: 56105.78 toks/s, output: 54.79 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:16<00:02, 40.75it/s, est. speed input: 56069.55 toks/s, output: 54.76 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:16<00:02, 43.37it/s, est. speed input: 56034.42 toks/s, output: 54.72 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:17<00:01, 45.40it/s, est. speed input: 55999.31 toks/s, output: 54.69 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:17<00:01, 46.95it/s, est. speed input: 55965.23 toks/s, output: 54.65 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:17<00:01, 48.09it/s, est. speed input: 55931.52 toks/s, output: 54.62 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:17<00:01, 48.94it/s, est. speed input: 55898.97 toks/s, output: 54.59 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:17<00:01, 49.57it/s, est. speed input: 55867.33 toks/s, output: 54.56 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:17<00:00, 49.95it/s, est. speed input: 55834.31 toks/s, output: 54.53 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:18<00:00, 51.85it/s, est. speed input: 55853.11 toks/s, output: 54.54 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:18<00:00, 51.58it/s, est. speed input: 55821.55 toks/s, output: 54.51 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:18<00:00, 51.40it/s, est. speed input: 55790.65 toks/s, output: 54.48 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:18<00:00, 51.27it/s, est. speed input: 55760.37 toks/s, output: 54.45 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:18<00:00, 53.14it/s, est. speed input: 55787.59 toks/s, output: 54.48 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:18<00:00, 53.14it/s, est. speed input: 56115.33 toks/s, output: 54.80 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:18<00:00, 54.80it/s, est. speed input: 56115.33 toks/s, output: 54.80 toks/s]
[rank0]:[W128 11:38:55.729972614 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-28 11:38:57
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:39:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:39:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=25888) WARNING 01-28 11:39:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=25888) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=25888) WARNING 01-28 11:39:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.74 requests/s, 57135.06 total tokens/s, 55.74 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-28 11:39:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:39:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:39:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:39:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:39:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:39:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:39:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:39:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:39:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:39:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:39:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:39:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:39:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:39:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:39:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:39:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=25888) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=25888) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.31it/s]
(EngineCore_DP0 pid=25888) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.31it/s]
(EngineCore_DP0 pid=25888) 
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=25888) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.29it/s]
(EngineCore_DP0 pid=25888) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.38it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.20it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.76it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 34/2048 [00:00<00:06, 334.49it/s]
Adding requests:   3%|▎         | 71/2048 [00:00<00:05, 350.71it/s]
Adding requests:   5%|▌         | 108/2048 [00:00<00:05, 359.36it/s]
Adding requests:   7%|▋         | 145/2048 [00:00<00:05, 359.50it/s]
Adding requests:   9%|▉         | 181/2048 [00:00<00:05, 357.53it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:05, 355.69it/s]
Adding requests:  12%|█▎        | 256/2048 [00:00<00:04, 364.11it/s]
Adding requests:  14%|█▍        | 293/2048 [00:00<00:04, 364.07it/s]
Adding requests:  16%|█▌        | 330/2048 [00:00<00:04, 365.37it/s]
Adding requests:  18%|█▊        | 370/2048 [00:01<00:04, 375.31it/s]
Adding requests:  20%|██        | 410/2048 [00:01<00:04, 382.35it/s]
Adding requests:  22%|██▏       | 450/2048 [00:01<00:04, 384.88it/s]
Adding requests:  24%|██▍       | 490/2048 [00:01<00:04, 388.07it/s]
Adding requests:  26%|██▌       | 529/2048 [00:01<00:03, 381.28it/s]
Adding requests:  28%|██▊       | 569/2048 [00:01<00:03, 384.73it/s]
Adding requests:  30%|██▉       | 608/2048 [00:01<00:03, 384.85it/s]
Adding requests:  32%|███▏      | 649/2048 [00:01<00:03, 390.93it/s]
Adding requests:  34%|███▎      | 689/2048 [00:01<00:03, 392.06it/s]
Adding requests:  36%|███▌      | 730/2048 [00:01<00:03, 396.32it/s]
Adding requests:  38%|███▊      | 770/2048 [00:02<00:03, 389.83it/s]
Adding requests:  40%|███▉      | 810/2048 [00:02<00:03, 384.43it/s]
Adding requests:  41%|████▏     | 849/2048 [00:02<00:03, 375.71it/s]
Adding requests:  43%|████▎     | 890/2048 [00:02<00:03, 385.44it/s]
Adding requests:  45%|████▌     | 931/2048 [00:02<00:02, 392.49it/s]
Adding requests:  47%|████▋     | 971/2048 [00:02<00:02, 391.35it/s]
Adding requests:  49%|████▉     | 1011/2048 [00:02<00:02, 390.72it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:02<00:02, 388.76it/s]
Adding requests:  53%|█████▎    | 1090/2048 [00:02<00:02, 384.72it/s]
Adding requests:  55%|█████▌    | 1130/2048 [00:02<00:02, 386.51it/s]
Adding requests:  57%|█████▋    | 1173/2048 [00:03<00:02, 396.94it/s]
Adding requests:  59%|█████▉    | 1216/2048 [00:03<00:02, 404.79it/s]
Adding requests:  61%|██████▏   | 1257/2048 [00:03<00:01, 400.28it/s]
Adding requests:  63%|██████▎   | 1298/2048 [00:03<00:01, 402.78it/s]
Adding requests:  65%|██████▌   | 1339/2048 [00:03<00:01, 404.58it/s]
Adding requests:  67%|██████▋   | 1380/2048 [00:03<00:01, 401.45it/s]
Adding requests:  69%|██████▉   | 1421/2048 [00:03<00:01, 399.31it/s]
Adding requests:  71%|███████▏  | 1462/2048 [00:03<00:01, 401.34it/s]
Adding requests:  73%|███████▎  | 1503/2048 [00:03<00:01, 403.06it/s]
Adding requests:  75%|███████▌  | 1544/2048 [00:03<00:01, 402.03it/s]
Adding requests:  77%|███████▋  | 1585/2048 [00:04<00:01, 404.13it/s]
Adding requests:  79%|███████▉  | 1628/2048 [00:04<00:01, 409.53it/s]
Adding requests:  81%|████████▏ | 1669/2048 [00:04<00:00, 402.93it/s]
Adding requests:  83%|████████▎ | 1710/2048 [00:04<00:00, 401.88it/s]
Adding requests:  85%|████████▌ | 1751/2048 [00:04<00:00, 403.99it/s]
Adding requests:  88%|████████▊ | 1792/2048 [00:04<00:00, 399.47it/s]
Adding requests:  89%|████████▉ | 1832/2048 [00:04<00:00, 396.62it/s]
Adding requests:  91%|█████████▏| 1873/2048 [00:04<00:00, 397.59it/s]
Adding requests:  93%|█████████▎| 1913/2048 [00:04<00:00, 390.72it/s]
Adding requests:  95%|█████████▌| 1954/2048 [00:05<00:00, 395.38it/s]
Adding requests:  97%|█████████▋| 1995/2048 [00:05<00:00, 399.17it/s]
Adding requests:  99%|█████████▉| 2035/2048 [00:05<00:00, 389.54it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 389.09it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:00<00:00, 2441.40it/s, est. speed input: 2500538.73 toks/s, output: 2441.57 toks/s]
Processed prompts:  25%|██▌       | 519/2048 [00:04<00:15, 100.30it/s, est. speed input: 121097.02 toks/s, output: 118.26 toks/s]   
Processed prompts:  30%|███       | 623/2048 [00:06<00:16, 86.17it/s, est. speed input: 104657.01 toks/s, output: 102.20 toks/s] 
Processed prompts:  33%|███▎      | 683/2048 [00:07<00:17, 77.63it/s, est. speed input: 96671.63 toks/s, output: 94.41 toks/s]  
Processed prompts:  35%|███▌      | 721/2048 [00:09<00:25, 51.06it/s, est. speed input: 76858.73 toks/s, output: 75.06 toks/s]
Processed prompts:  36%|███▋      | 746/2048 [00:10<00:25, 50.13it/s, est. speed input: 75070.12 toks/s, output: 73.31 toks/s]
Processed prompts:  37%|███▋      | 764/2048 [00:10<00:25, 51.23it/s, est. speed input: 74786.77 toks/s, output: 73.03 toks/s]
Processed prompts:  38%|███▊      | 779/2048 [00:10<00:24, 51.39it/s, est. speed input: 74233.89 toks/s, output: 72.49 toks/s]
Processed prompts:  39%|███▊      | 791/2048 [00:11<00:25, 50.15it/s, est. speed input: 73430.63 toks/s, output: 71.71 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:11<00:25, 48.31it/s, est. speed input: 72576.60 toks/s, output: 70.88 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:11<00:24, 49.76it/s, est. speed input: 72205.19 toks/s, output: 70.51 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:11<00:24, 50.49it/s, est. speed input: 71752.90 toks/s, output: 70.07 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:12<00:23, 51.10it/s, est. speed input: 71322.00 toks/s, output: 69.65 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:12<00:22, 51.59it/s, est. speed input: 70911.27 toks/s, output: 69.25 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:12<00:22, 51.96it/s, est. speed input: 70518.22 toks/s, output: 68.87 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:13<00:22, 52.24it/s, est. speed input: 70143.16 toks/s, output: 68.50 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:13<00:21, 52.48it/s, est. speed input: 69788.38 toks/s, output: 68.15 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:13<00:20, 53.51it/s, est. speed input: 69534.72 toks/s, output: 67.90 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:13<00:20, 54.26it/s, est. speed input: 69289.66 toks/s, output: 67.67 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:14<00:19, 54.82it/s, est. speed input: 69055.37 toks/s, output: 67.44 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:14<00:19, 56.11it/s, est. speed input: 68903.72 toks/s, output: 67.29 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:14<00:18, 56.14it/s, est. speed input: 68685.80 toks/s, output: 67.08 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:15<00:18, 56.17it/s, est. speed input: 68477.12 toks/s, output: 66.87 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:15<00:18, 56.16it/s, est. speed input: 68273.53 toks/s, output: 66.67 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:15<00:17, 56.17it/s, est. speed input: 68078.09 toks/s, output: 66.48 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:15<00:17, 56.18it/s, est. speed input: 67890.29 toks/s, output: 66.30 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:16<00:17, 56.20it/s, est. speed input: 67709.88 toks/s, output: 66.12 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:16<00:17, 56.19it/s, est. speed input: 67534.35 toks/s, output: 65.95 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:16<00:16, 56.19it/s, est. speed input: 67365.06 toks/s, output: 65.79 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:17<00:16, 56.19it/s, est. speed input: 67201.32 toks/s, output: 65.63 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:17<00:16, 56.17it/s, est. speed input: 67042.06 toks/s, output: 65.47 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:17<00:15, 57.15it/s, est. speed input: 66949.95 toks/s, output: 65.38 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:17<00:15, 56.87it/s, est. speed input: 66801.02 toks/s, output: 65.24 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:18<00:15, 56.67it/s, est. speed input: 66656.50 toks/s, output: 65.09 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:18<00:14, 56.51it/s, est. speed input: 66515.63 toks/s, output: 64.96 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:18<00:14, 56.42it/s, est. speed input: 66379.65 toks/s, output: 64.82 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:19<00:14, 56.34it/s, est. speed input: 66247.41 toks/s, output: 64.69 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:19<00:14, 56.31it/s, est. speed input: 66119.79 toks/s, output: 64.57 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:19<00:13, 56.27it/s, est. speed input: 65995.07 toks/s, output: 64.45 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:19<00:13, 56.24it/s, est. speed input: 65874.11 toks/s, output: 64.33 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:20<00:13, 56.23it/s, est. speed input: 65757.14 toks/s, output: 64.22 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:20<00:13, 56.20it/s, est. speed input: 65642.08 toks/s, output: 64.10 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:20<00:12, 56.18it/s, est. speed input: 65530.19 toks/s, output: 63.99 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:21<00:12, 56.19it/s, est. speed input: 65422.27 toks/s, output: 63.89 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:21<00:12, 56.18it/s, est. speed input: 65316.82 toks/s, output: 63.79 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:21<00:11, 56.26it/s, est. speed input: 65218.54 toks/s, output: 63.69 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:21<00:11, 56.36it/s, est. speed input: 65124.88 toks/s, output: 63.60 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:22<00:11, 56.42it/s, est. speed input: 65032.95 toks/s, output: 63.51 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:22<00:11, 56.47it/s, est. speed input: 64943.89 toks/s, output: 63.42 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:22<00:10, 56.49it/s, est. speed input: 64856.32 toks/s, output: 63.34 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:23<00:10, 56.51it/s, est. speed input: 64771.20 toks/s, output: 63.25 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:23<00:10, 56.54it/s, est. speed input: 64688.69 toks/s, output: 63.17 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:23<00:09, 56.54it/s, est. speed input: 64607.43 toks/s, output: 63.09 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:23<00:09, 56.54it/s, est. speed input: 64528.09 toks/s, output: 63.02 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:24<00:09, 56.55it/s, est. speed input: 64450.87 toks/s, output: 62.94 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:24<00:09, 56.58it/s, est. speed input: 64376.24 toks/s, output: 62.87 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:24<00:08, 56.56it/s, est. speed input: 64302.00 toks/s, output: 62.79 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:25<00:08, 56.57it/s, est. speed input: 64230.04 toks/s, output: 62.72 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:25<00:08, 56.58it/s, est. speed input: 64160.39 toks/s, output: 62.66 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:25<00:07, 56.59it/s, est. speed input: 64092.00 toks/s, output: 62.59 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:25<00:07, 56.63it/s, est. speed input: 64026.58 toks/s, output: 62.53 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:26<00:07, 56.57it/s, est. speed input: 63958.89 toks/s, output: 62.46 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:26<00:07, 56.58it/s, est. speed input: 63894.67 toks/s, output: 62.40 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:26<00:06, 56.56it/s, est. speed input: 63830.83 toks/s, output: 62.33 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:27<00:06, 56.54it/s, est. speed input: 63768.31 toks/s, output: 62.27 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:27<00:06, 56.56it/s, est. speed input: 63708.23 toks/s, output: 62.21 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:27<00:05, 56.57it/s, est. speed input: 63649.03 toks/s, output: 62.16 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:27<00:05, 56.57it/s, est. speed input: 63591.13 toks/s, output: 62.10 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:28<00:05, 56.56it/s, est. speed input: 63533.70 toks/s, output: 62.04 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:28<00:05, 56.54it/s, est. speed input: 63477.25 toks/s, output: 61.99 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:28<00:04, 56.55it/s, est. speed input: 63422.56 toks/s, output: 61.94 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:28<00:04, 56.55it/s, est. speed input: 63368.79 toks/s, output: 61.88 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:29<00:04, 56.56it/s, est. speed input: 63316.13 toks/s, output: 61.83 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:29<00:03, 56.39it/s, est. speed input: 63258.34 toks/s, output: 61.78 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:29<00:03, 56.10it/s, est. speed input: 63195.47 toks/s, output: 61.71 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:30<00:03, 55.90it/s, est. speed input: 63134.20 toks/s, output: 61.65 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:30<00:03, 56.72it/s, est. speed input: 63107.28 toks/s, output: 61.63 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:30<00:02, 56.31it/s, est. speed input: 63047.07 toks/s, output: 61.57 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:30<00:02, 56.04it/s, est. speed input: 62988.46 toks/s, output: 61.51 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:31<00:02, 55.87it/s, est. speed input: 62931.36 toks/s, output: 61.46 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:31<00:01, 55.73it/s, est. speed input: 62874.75 toks/s, output: 61.40 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:31<00:01, 55.64it/s, est. speed input: 62819.38 toks/s, output: 61.35 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:32<00:01, 55.57it/s, est. speed input: 62764.53 toks/s, output: 61.29 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:32<00:01, 55.51it/s, est. speed input: 62710.63 toks/s, output: 61.24 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:32<00:00, 55.48it/s, est. speed input: 62657.90 toks/s, output: 61.19 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:33<00:00, 55.46it/s, est. speed input: 62606.30 toks/s, output: 61.14 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:33<00:00, 56.54it/s, est. speed input: 62590.49 toks/s, output: 61.12 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:33<00:00, 56.54it/s, est. speed input: 63020.62 toks/s, output: 61.54 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:33<00:00, 61.54it/s, est. speed input: 63020.62 toks/s, output: 61.54 toks/s]
[rank0]:[W128 11:40:25.859146389 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-28 11:40:27
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/json/BitNet-2B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:40:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:40:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=26753) WARNING 01-28 11:41:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=26753) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=26753) WARNING 01-28 11:41:17 [backends.py:609] Failed to read file <frozen os>
Throughput: 57.59 requests/s, 59026.51 total tokens/s, 57.59 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-28 11:40:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:40:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:40:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:40:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:40:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:40:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:40:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:40:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:41:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:41:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:41:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:41:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:41:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:41:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:41:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:41:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=26753) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=26753) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.47it/s]
(EngineCore_DP0 pid=26753) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.47it/s]
(EngineCore_DP0 pid=26753) 
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=26753) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.55it/s]
(EngineCore_DP0 pid=26753) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.92it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 36/4096 [00:00<00:11, 351.76it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:11, 350.16it/s]
Adding requests:   3%|▎         | 110/4096 [00:00<00:11, 361.25it/s]
Adding requests:   4%|▎         | 147/4096 [00:00<00:10, 363.42it/s]
Adding requests:   5%|▍         | 185/4096 [00:00<00:10, 368.56it/s]
Adding requests:   5%|▌         | 224/4096 [00:00<00:10, 373.43it/s]
Adding requests:   6%|▋         | 264/4096 [00:00<00:10, 379.31it/s]
Adding requests:   7%|▋         | 302/4096 [00:00<00:10, 378.62it/s]
Adding requests:   8%|▊         | 340/4096 [00:00<00:09, 377.76it/s]
Adding requests:   9%|▉         | 380/4096 [00:01<00:09, 384.40it/s]
Adding requests:  10%|█         | 420/4096 [00:01<00:09, 386.90it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:09, 387.20it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:10, 344.21it/s]
Adding requests:  13%|█▎        | 534/4096 [00:01<00:10, 343.65it/s]
Adding requests:  14%|█▍        | 574/4096 [00:01<00:09, 358.79it/s]
Adding requests:  15%|█▍        | 612/4096 [00:01<00:09, 364.17it/s]
Adding requests:  16%|█▌        | 653/4096 [00:01<00:09, 375.24it/s]
Adding requests:  17%|█▋        | 694/4096 [00:01<00:08, 385.33it/s]
Adding requests:  18%|█▊        | 733/4096 [00:01<00:08, 385.84it/s]
Adding requests:  19%|█▉        | 772/4096 [00:02<00:08, 381.45it/s]
Adding requests:  20%|█▉        | 811/4096 [00:02<00:08, 377.78it/s]
Adding requests:  21%|██        | 849/4096 [00:02<00:08, 375.05it/s]
Adding requests:  22%|██▏       | 890/4096 [00:02<00:08, 383.55it/s]
Adding requests:  23%|██▎       | 930/4096 [00:02<00:08, 387.38it/s]
Adding requests:  24%|██▎       | 969/4096 [00:02<00:08, 388.06it/s]
Adding requests:  25%|██▍       | 1009/4096 [00:02<00:07, 390.81it/s]
Adding requests:  26%|██▌       | 1049/4096 [00:02<00:07, 391.27it/s]
Adding requests:  27%|██▋       | 1089/4096 [00:02<00:08, 375.84it/s]
Adding requests:  28%|██▊       | 1128/4096 [00:03<00:07, 378.09it/s]
Adding requests:  29%|██▊       | 1168/4096 [00:03<00:07, 382.99it/s]
Adding requests:  30%|██▉       | 1210/4096 [00:03<00:07, 390.67it/s]
Adding requests:  31%|███       | 1250/4096 [00:03<00:07, 391.44it/s]
Adding requests:  31%|███▏      | 1290/4096 [00:03<00:07, 389.80it/s]
Adding requests:  32%|███▏      | 1331/4096 [00:03<00:06, 395.41it/s]
Adding requests:  33%|███▎      | 1371/4096 [00:03<00:06, 395.59it/s]
Adding requests:  34%|███▍      | 1412/4096 [00:03<00:06, 398.12it/s]
Adding requests:  35%|███▌      | 1452/4096 [00:03<00:06, 396.51it/s]
Adding requests:  36%|███▋      | 1494/4096 [00:03<00:06, 400.75it/s]
Adding requests:  37%|███▋      | 1535/4096 [00:04<00:06, 397.62it/s]
Adding requests:  38%|███▊      | 1575/4096 [00:04<00:06, 396.23it/s]
Adding requests:  39%|███▉      | 1616/4096 [00:04<00:06, 397.69it/s]
Adding requests:  40%|████      | 1656/4096 [00:04<00:06, 396.10it/s]
Adding requests:  41%|████▏     | 1696/4096 [00:04<00:06, 393.47it/s]
Adding requests:  42%|████▏     | 1736/4096 [00:04<00:06, 389.96it/s]
Adding requests:  43%|████▎     | 1776/4096 [00:04<00:06, 386.51it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:04<00:05, 389.20it/s]
Adding requests:  45%|████▌     | 1855/4096 [00:04<00:05, 381.77it/s]
Adding requests:  46%|████▌     | 1894/4096 [00:04<00:05, 378.65it/s]
Adding requests:  47%|████▋     | 1932/4096 [00:05<00:05, 372.07it/s]
Adding requests:  48%|████▊     | 1970/4096 [00:05<00:05, 365.40it/s]
Adding requests:  49%|████▉     | 2009/4096 [00:05<00:05, 371.77it/s]
Adding requests:  50%|████▉     | 2047/4096 [00:05<00:05, 371.35it/s]
Adding requests:  51%|█████     | 2085/4096 [00:05<00:05, 373.69it/s]
Adding requests:  52%|█████▏    | 2123/4096 [00:05<00:05, 367.89it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:05<00:05, 361.00it/s]
Adding requests:  54%|█████▎    | 2197/4096 [00:05<00:05, 361.46it/s]
Adding requests:  55%|█████▍    | 2235/4096 [00:05<00:05, 365.60it/s]
Adding requests:  55%|█████▌    | 2273/4096 [00:05<00:04, 369.04it/s]
Adding requests:  56%|█████▋    | 2310/4096 [00:06<00:04, 359.17it/s]
Adding requests:  57%|█████▋    | 2348/4096 [00:06<00:04, 362.15it/s]
Adding requests:  58%|█████▊    | 2386/4096 [00:06<00:04, 366.68it/s]
Adding requests:  59%|█████▉    | 2423/4096 [00:06<00:04, 367.54it/s]
Adding requests:  60%|██████    | 2461/4096 [00:06<00:04, 368.76it/s]
Adding requests:  61%|██████    | 2500/4096 [00:06<00:04, 371.31it/s]
Adding requests:  62%|██████▏   | 2539/4096 [00:06<00:04, 374.35it/s]
Adding requests:  63%|██████▎   | 2579/4096 [00:06<00:03, 381.53it/s]
Adding requests:  64%|██████▍   | 2618/4096 [00:06<00:03, 381.67it/s]
Adding requests:  65%|██████▍   | 2657/4096 [00:07<00:03, 379.11it/s]
Adding requests:  66%|██████▌   | 2695/4096 [00:07<00:03, 375.52it/s]
Adding requests:  67%|██████▋   | 2733/4096 [00:07<00:03, 375.06it/s]
Adding requests:  68%|██████▊   | 2772/4096 [00:07<00:03, 376.04it/s]
Adding requests:  69%|██████▊   | 2810/4096 [00:07<00:03, 372.55it/s]
Adding requests:  70%|██████▉   | 2848/4096 [00:07<00:03, 371.75it/s]
Adding requests:  70%|███████   | 2886/4096 [00:07<00:03, 371.68it/s]
Adding requests:  71%|███████▏  | 2924/4096 [00:07<00:03, 367.90it/s]
Adding requests:  72%|███████▏  | 2961/4096 [00:07<00:03, 363.96it/s]
Adding requests:  73%|███████▎  | 3000/4096 [00:07<00:02, 368.92it/s]
Adding requests:  74%|███████▍  | 3039/4096 [00:08<00:02, 374.90it/s]
Adding requests:  75%|███████▌  | 3077/4096 [00:08<00:02, 370.29it/s]
Adding requests:  76%|███████▌  | 3119/4096 [00:08<00:02, 382.29it/s]
Adding requests:  77%|███████▋  | 3159/4096 [00:08<00:02, 385.68it/s]
Adding requests:  78%|███████▊  | 3198/4096 [00:08<00:02, 381.60it/s]
Adding requests:  79%|███████▉  | 3238/4096 [00:08<00:02, 386.00it/s]
Adding requests:  80%|████████  | 3277/4096 [00:08<00:02, 385.24it/s]
Adding requests:  81%|████████  | 3316/4096 [00:08<00:02, 384.39it/s]
Adding requests:  82%|████████▏ | 3357/4096 [00:08<00:01, 389.73it/s]
Adding requests:  83%|████████▎ | 3396/4096 [00:08<00:01, 386.99it/s]
Adding requests:  84%|████████▍ | 3437/4096 [00:09<00:01, 392.63it/s]
Adding requests:  85%|████████▍ | 3477/4096 [00:09<00:01, 386.16it/s]
Adding requests:  86%|████████▌ | 3517/4096 [00:09<00:01, 388.61it/s]
Adding requests:  87%|████████▋ | 3557/4096 [00:09<00:01, 391.20it/s]
Adding requests:  88%|████████▊ | 3597/4096 [00:09<00:01, 392.57it/s]
Adding requests:  89%|████████▉ | 3637/4096 [00:09<00:01, 376.76it/s]
Adding requests:  90%|████████▉ | 3677/4096 [00:09<00:01, 381.58it/s]
Adding requests:  91%|█████████ | 3717/4096 [00:09<00:00, 386.63it/s]
Adding requests:  92%|█████████▏| 3757/4096 [00:09<00:00, 390.45it/s]
Adding requests:  93%|█████████▎| 3800/4096 [00:10<00:00, 399.13it/s]
Adding requests:  94%|█████████▍| 3842/4096 [00:10<00:00, 402.33it/s]
Adding requests:  95%|█████████▍| 3883/4096 [00:10<00:00, 400.56it/s]
Adding requests:  96%|█████████▌| 3924/4096 [00:10<00:00, 401.46it/s]
Adding requests:  97%|█████████▋| 3965/4096 [00:10<00:00, 400.98it/s]
Adding requests:  98%|█████████▊| 4006/4096 [00:10<00:00, 396.66it/s]
Adding requests:  99%|█████████▉| 4046/4096 [00:10<00:00, 392.37it/s]
Adding requests: 100%|█████████▉| 4087/4096 [00:10<00:00, 395.22it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 380.89it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:00<00:03, 1017.63it/s, est. speed input: 1042107.04 toks/s, output: 1017.64 toks/s]
Processed prompts:  17%|█▋        | 712/4096 [00:04<00:25, 133.78it/s, est. speed input: 176356.15 toks/s, output: 172.22 toks/s]   
Processed prompts:  18%|█▊        | 756/4096 [00:04<00:26, 124.76it/s, est. speed input: 165167.18 toks/s, output: 161.30 toks/s]
Processed prompts:  19%|█▉        | 784/4096 [00:05<00:29, 110.59it/s, est. speed input: 153270.58 toks/s, output: 149.68 toks/s]
Processed prompts:  20%|█▉        | 803/4096 [00:05<00:34, 94.23it/s, est. speed input: 142036.64 toks/s, output: 138.71 toks/s] 
Processed prompts:  20%|██        | 834/4096 [00:06<00:38, 84.97it/s, est. speed input: 134499.93 toks/s, output: 131.35 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:06<00:41, 77.90it/s, est. speed input: 128336.93 toks/s, output: 125.33 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:07<00:44, 72.37it/s, est. speed input: 123095.20 toks/s, output: 120.21 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:08<00:46, 68.21it/s, est. speed input: 118606.82 toks/s, output: 115.83 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:08<00:48, 65.01it/s, est. speed input: 114657.80 toks/s, output: 111.97 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:09<00:49, 62.74it/s, est. speed input: 111216.48 toks/s, output: 108.61 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:09<00:50, 61.09it/s, est. speed input: 108170.26 toks/s, output: 105.63 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:10<00:50, 59.90it/s, est. speed input: 105453.87 toks/s, output: 102.98 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:10<00:50, 59.06it/s, est. speed input: 103020.23 toks/s, output: 100.61 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:11<00:50, 58.46it/s, est. speed input: 100822.56 toks/s, output: 98.46 toks/s] 
Processed prompts:  28%|██▊       | 1154/4096 [00:11<00:50, 58.53it/s, est. speed input: 98965.25 toks/s, output: 96.65 toks/s] 
Processed prompts:  29%|██▉       | 1186/4096 [00:12<00:50, 58.09it/s, est. speed input: 97146.66 toks/s, output: 94.87 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:13<00:49, 57.78it/s, est. speed input: 95482.94 toks/s, output: 93.24 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:13<00:49, 57.54it/s, est. speed input: 93953.60 toks/s, output: 91.75 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:14<00:48, 57.71it/s, est. speed input: 92615.49 toks/s, output: 90.44 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:14<00:47, 57.98it/s, est. speed input: 91407.07 toks/s, output: 89.26 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:15<00:47, 58.18it/s, est. speed input: 90286.31 toks/s, output: 88.17 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:15<00:46, 58.32it/s, est. speed input: 89243.36 toks/s, output: 87.15 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:16<00:45, 58.42it/s, est. speed input: 88269.46 toks/s, output: 86.20 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:16<00:45, 58.48it/s, est. speed input: 87357.22 toks/s, output: 85.31 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:17<00:44, 58.52it/s, est. speed input: 86502.85 toks/s, output: 84.48 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:17<00:44, 58.55it/s, est. speed input: 85699.65 toks/s, output: 83.69 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:18<00:43, 58.58it/s, est. speed input: 84945.10 toks/s, output: 82.95 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:19<00:43, 58.61it/s, est. speed input: 84234.84 toks/s, output: 82.26 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:19<00:42, 58.63it/s, est. speed input: 83563.75 toks/s, output: 81.61 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:20<00:42, 58.57it/s, est. speed input: 82919.95 toks/s, output: 80.98 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:20<00:41, 58.53it/s, est. speed input: 82310.13 toks/s, output: 80.38 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:21<00:40, 58.59it/s, est. speed input: 81741.78 toks/s, output: 79.83 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:21<00:40, 58.61it/s, est. speed input: 81199.61 toks/s, output: 79.30 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:22<00:40, 58.22it/s, est. speed input: 80638.14 toks/s, output: 78.75 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:22<00:39, 57.95it/s, est. speed input: 80105.01 toks/s, output: 78.23 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:23<00:39, 57.76it/s, est. speed input: 79595.98 toks/s, output: 77.73 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:24<00:38, 58.15it/s, est. speed input: 79165.72 toks/s, output: 77.31 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:24<00:38, 57.88it/s, est. speed input: 78699.49 toks/s, output: 76.85 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:25<00:37, 57.72it/s, est. speed input: 78255.69 toks/s, output: 76.42 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:25<00:37, 57.59it/s, est. speed input: 77830.55 toks/s, output: 76.01 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:26<00:36, 57.51it/s, est. speed input: 77424.06 toks/s, output: 75.61 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:26<00:36, 57.46it/s, est. speed input: 77034.50 toks/s, output: 75.23 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:27<00:35, 57.42it/s, est. speed input: 76661.02 toks/s, output: 74.86 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:27<00:35, 57.39it/s, est. speed input: 76302.43 toks/s, output: 74.51 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:28<00:34, 57.39it/s, est. speed input: 75958.81 toks/s, output: 74.18 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:29<00:33, 57.38it/s, est. speed input: 75628.27 toks/s, output: 73.86 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:29<00:33, 57.37it/s, est. speed input: 75309.92 toks/s, output: 73.54 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:30<00:32, 57.47it/s, est. speed input: 75011.91 toks/s, output: 73.25 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:30<00:32, 57.59it/s, est. speed input: 74728.24 toks/s, output: 72.98 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:31<00:31, 58.19it/s, est. speed input: 74493.44 toks/s, output: 72.75 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:31<00:30, 58.09it/s, est. speed input: 74228.66 toks/s, output: 72.49 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:32<00:30, 58.03it/s, est. speed input: 73973.43 toks/s, output: 72.24 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:32<00:29, 57.99it/s, est. speed input: 73726.89 toks/s, output: 72.00 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:33<00:29, 57.97it/s, est. speed input: 73488.97 toks/s, output: 71.77 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:34<00:28, 57.94it/s, est. speed input: 73257.57 toks/s, output: 71.54 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:34<00:28, 57.92it/s, est. speed input: 73033.62 toks/s, output: 71.32 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:35<00:27, 58.42it/s, est. speed input: 72850.47 toks/s, output: 71.14 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:37<00:54, 28.59it/s, est. speed input: 68965.65 toks/s, output: 67.35 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:38<00:45, 33.72it/s, est. speed input: 68827.77 toks/s, output: 67.21 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:38<00:38, 38.52it/s, est. speed input: 68687.72 toks/s, output: 67.08 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:39<00:34, 42.81it/s, est. speed input: 68554.23 toks/s, output: 66.95 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:39<00:30, 46.40it/s, est. speed input: 68422.21 toks/s, output: 66.82 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:40<00:28, 49.14it/s, est. speed input: 68282.19 toks/s, output: 66.68 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:40<00:26, 51.26it/s, est. speed input: 68145.63 toks/s, output: 66.55 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:41<00:25, 52.85it/s, est. speed input: 68012.73 toks/s, output: 66.42 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:42<00:24, 54.02it/s, est. speed input: 67883.65 toks/s, output: 66.29 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:42<00:23, 54.88it/s, est. speed input: 67758.01 toks/s, output: 66.17 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:43<00:22, 55.50it/s, est. speed input: 67635.69 toks/s, output: 66.05 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:43<00:21, 55.93it/s, est. speed input: 67516.28 toks/s, output: 65.93 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:44<00:21, 56.24it/s, est. speed input: 67399.97 toks/s, output: 65.82 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:44<00:20, 56.44it/s, est. speed input: 67285.65 toks/s, output: 65.71 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:45<00:19, 56.60it/s, est. speed input: 67174.99 toks/s, output: 65.60 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:45<00:19, 56.71it/s, est. speed input: 67067.05 toks/s, output: 65.50 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:46<00:18, 56.79it/s, est. speed input: 66961.46 toks/s, output: 65.39 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:47<00:17, 56.85it/s, est. speed input: 66859.07 toks/s, output: 65.29 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:47<00:17, 56.89it/s, est. speed input: 66758.88 toks/s, output: 65.19 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:48<00:16, 57.32it/s, est. speed input: 66678.97 toks/s, output: 65.12 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:48<00:16, 57.69it/s, est. speed input: 66603.97 toks/s, output: 65.04 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:49<00:15, 58.01it/s, est. speed input: 66532.73 toks/s, output: 64.97 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:49<00:14, 58.12it/s, est. speed input: 66458.42 toks/s, output: 64.90 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:50<00:14, 58.25it/s, est. speed input: 66388.04 toks/s, output: 64.83 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:50<00:13, 58.34it/s, est. speed input: 66319.11 toks/s, output: 64.76 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:51<00:13, 58.42it/s, est. speed input: 66252.06 toks/s, output: 64.70 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:52<00:12, 58.48it/s, est. speed input: 66186.63 toks/s, output: 64.64 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:52<00:12, 58.49it/s, est. speed input: 66121.41 toks/s, output: 64.57 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:53<00:11, 58.51it/s, est. speed input: 66058.27 toks/s, output: 64.51 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:53<00:10, 58.52it/s, est. speed input: 65995.91 toks/s, output: 64.45 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:54<00:10, 58.53it/s, est. speed input: 65934.92 toks/s, output: 64.39 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:54<00:09, 58.54it/s, est. speed input: 65875.31 toks/s, output: 64.33 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:55<00:09, 58.57it/s, est. speed input: 65818.06 toks/s, output: 64.28 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:55<00:08, 58.47it/s, est. speed input: 65757.18 toks/s, output: 64.22 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:56<00:08, 58.12it/s, est. speed input: 65687.22 toks/s, output: 64.15 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:56<00:07, 57.90it/s, est. speed input: 65619.46 toks/s, output: 64.08 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:57<00:07, 57.72it/s, est. speed input: 65552.19 toks/s, output: 64.02 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:58<00:06, 58.12it/s, est. speed input: 65504.83 toks/s, output: 63.97 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:58<00:06, 57.88it/s, est. speed input: 65440.02 toks/s, output: 63.91 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:59<00:05, 57.72it/s, est. speed input: 65376.70 toks/s, output: 63.84 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:59<00:04, 57.62it/s, est. speed input: 65314.90 toks/s, output: 63.78 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:00<00:04, 57.53it/s, est. speed input: 65253.70 toks/s, output: 63.72 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:00<00:03, 57.48it/s, est. speed input: 65193.91 toks/s, output: 63.67 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:01<00:03, 57.44it/s, est. speed input: 65135.21 toks/s, output: 63.61 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:01<00:02, 57.40it/s, est. speed input: 65077.20 toks/s, output: 63.55 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:02<00:02, 57.39it/s, est. speed input: 65020.71 toks/s, output: 63.50 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:03<00:01, 57.38it/s, est. speed input: 64965.05 toks/s, output: 63.44 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:03<00:01, 57.90it/s, est. speed input: 64927.53 toks/s, output: 63.41 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:04<00:00, 58.52it/s, est. speed input: 64898.90 toks/s, output: 63.38 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:04<00:00, 58.52it/s, est. speed input: 65377.24 toks/s, output: 63.84 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:04<00:00, 63.84it/s, est. speed input: 65377.24 toks/s, output: 63.84 toks/s]
[rank0]:[W128 11:42:45.605607369 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

