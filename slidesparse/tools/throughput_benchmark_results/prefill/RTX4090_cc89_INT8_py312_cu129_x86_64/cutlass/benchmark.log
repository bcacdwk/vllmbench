
========== M=16 ==========
Time: 2026-01-25 16:53:03
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:53:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 16:53:20 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=149871) WARNING 01-25 16:53:36 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
Throughput: 77.56 requests/s, 1318.48 total tokens/s, 77.56 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 16:53:10] INFO font_manager.py:1639: generated new fontManager
[2026-01-25 16:53:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:53:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:53:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:53:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:53:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:53:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:53:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:53:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:53:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:53:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:53:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:53:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:53:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:53:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:53:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:53:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:53:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:53:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:53:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:53:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:53:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:53:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:53:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:53:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:53:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:53:19] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX4090_cc89_py312_cu129_x86_64
[2026-01-25 16:53:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 16:53:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:53:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:53:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:53:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:53:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:53:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:53:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:53:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:53:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:53:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:53:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:53:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:53:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:53:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:53:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:53:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:53:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:53:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:53:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:53:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:53:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:53:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:53:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:53:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:53:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:53:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:53:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:53:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:53:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:53:28] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX4090_cc89_py312_cu129_x86_64
[2026-01-25 16:53:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 16:53:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:53:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:53:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:53:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W125 16:53:35.928999250 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=149871) [2026-01-25 16:53:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=149871) [2026-01-25 16:53:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=149871) [2026-01-25 16:53:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=149871) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=149871) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.26s/it]
(EngineCore_DP0 pid=149871) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.26s/it]
(EngineCore_DP0 pid=149871) 
(EngineCore_DP0 pid=149871) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.12it/s]
(EngineCore_DP0 pid=149871) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 19.23it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2915.24it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 85.21it/s, est. speed input: 1363.57 toks/s, output: 85.21 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:01, 84.83it/s, est. speed input: 1358.27 toks/s, output: 84.89 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 85.44it/s, est. speed input: 1365.10 toks/s, output: 85.31 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 85.81it/s, est. speed input: 1369.29 toks/s, output: 85.58 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:00, 84.94it/s, est. speed input: 1362.34 toks/s, output: 85.14 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:00, 84.94it/s, est. speed input: 1361.75 toks/s, output: 85.11 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:00<00:00, 82.70it/s, est. speed input: 1345.41 toks/s, output: 84.09 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:00<00:00, 80.48it/s, est. speed input: 1328.00 toks/s, output: 83.00 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:00<00:00, 78.60it/s, est. speed input: 1311.95 toks/s, output: 82.00 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 77.51it/s, est. speed input: 1300.86 toks/s, output: 81.30 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 77.91it/s, est. speed input: 1297.60 toks/s, output: 81.10 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 78.13it/s, est. speed input: 1294.56 toks/s, output: 80.91 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 77.63it/s, est. speed input: 1289.26 toks/s, output: 80.58 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 77.04it/s, est. speed input: 1283.73 toks/s, output: 80.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.04it/s, est. speed input: 1276.14 toks/s, output: 79.76 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.75it/s, est. speed input: 1276.14 toks/s, output: 79.76 toks/s]
[rank0]:[W125 16:54:27.272154046 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 16:54:30
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:54:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 16:54:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=151376) WARNING 01-25 16:54:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
Throughput: 76.55 requests/s, 9874.77 total tokens/s, 76.55 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 16:54:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:54:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:54:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:54:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:54:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:54:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:54:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:54:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:54:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:54:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:54:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:54:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:54:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:54:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:54:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:54:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:54:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:54:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:54:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:54:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:54:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:54:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:54:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:54:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:54:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:54:37] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX4090_cc89_py312_cu129_x86_64
[2026-01-25 16:54:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 16:54:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:54:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:54:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:54:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:54:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:54:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:54:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:54:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:54:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:54:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:54:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:54:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:54:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:54:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:54:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:54:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:54:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:54:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:54:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:54:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:54:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:54:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:54:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:54:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:54:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:54:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:54:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:54:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:54:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:54:45] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX4090_cc89_py312_cu129_x86_64
[2026-01-25 16:54:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 16:54:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:54:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:54:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:54:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=151376) [2026-01-25 16:54:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=151376) [2026-01-25 16:54:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=151376) [2026-01-25 16:54:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=151376) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=151376) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.80it/s]
(EngineCore_DP0 pid=151376) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.80it/s]
(EngineCore_DP0 pid=151376) 
(EngineCore_DP0 pid=151376) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 40.42it/s]
(EngineCore_DP0 pid=151376) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 22.37it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1280.43it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:01, 115.25it/s, est. speed input: 14754.27 toks/s, output: 115.25 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:01, 89.80it/s, est. speed input: 11889.52 toks/s, output: 92.88 toks/s]  
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 84.20it/s, est. speed input: 11225.41 toks/s, output: 87.69 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 81.75it/s, est. speed input: 10923.49 toks/s, output: 85.34 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:00, 79.84it/s, est. speed input: 10702.38 toks/s, output: 83.61 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 79.17it/s, est. speed input: 10585.53 toks/s, output: 82.70 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:00<00:00, 78.55it/s, est. speed input: 10497.22 toks/s, output: 82.00 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:00<00:00, 78.59it/s, est. speed input: 10451.48 toks/s, output: 81.65 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 77.80it/s, est. speed input: 10378.20 toks/s, output: 81.08 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:01<00:00, 79.24it/s, est. speed input: 10394.73 toks/s, output: 81.21 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 79.75it/s, est. speed input: 10390.85 toks/s, output: 81.18 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 80.81it/s, est. speed input: 10411.84 toks/s, output: 81.34 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 81.68it/s, est. speed input: 10433.75 toks/s, output: 81.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 81.68it/s, est. speed input: 10432.00 toks/s, output: 81.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 81.49it/s, est. speed input: 10432.00 toks/s, output: 81.50 toks/s]
[rank0]:[W125 16:55:10.855282188 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 16:55:13
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:55:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 16:55:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=152295) WARNING 01-25 16:55:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
Throughput: 76.77 requests/s, 19728.93 total tokens/s, 76.77 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 16:55:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:55:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:55:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:55:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:55:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:55:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:55:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:55:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:55:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:55:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:55:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:55:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:55:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:55:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:55:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:55:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:55:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:55:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:55:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:55:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:55:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:55:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:55:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:55:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:55:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:55:21] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX4090_cc89_py312_cu129_x86_64
[2026-01-25 16:55:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 16:55:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:55:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:55:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:55:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:55:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:55:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 16:55:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:55:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 16:55:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:55:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 16:55:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 16:55:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 16:55:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 16:55:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:55:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:55:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:55:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 16:55:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:55:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:55:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:55:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 16:55:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:55:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 16:55:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:55:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 16:55:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:55:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 16:55:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:55:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 16:55:30] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX4090_cc89_py312_cu129_x86_64
[2026-01-25 16:55:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 16:55:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:55:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:55:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:55:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=152295) [2026-01-25 16:55:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=152295) [2026-01-25 16:55:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=152295) [2026-01-25 16:55:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=152295) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=152295) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.86it/s]
(EngineCore_DP0 pid=152295) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.86it/s]
(EngineCore_DP0 pid=152295) 
(EngineCore_DP0 pid=152295) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 40.77it/s]
(EngineCore_DP0 pid=152295) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 22.79it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  69%|██████▉   | 88/128 [00:00<00:00, 874.90it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 901.49it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:01, 107.81it/s, est. speed input: 27605.34 toks/s, output: 107.82 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:01, 89.44it/s, est. speed input: 23525.62 toks/s, output: 91.89 toks/s]  
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 84.78it/s, est. speed input: 22434.20 toks/s, output: 87.63 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 82.69it/s, est. speed input: 21927.97 toks/s, output: 85.65 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:00, 82.11it/s, est. speed input: 21707.36 toks/s, output: 84.79 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:00<00:00, 80.35it/s, est. speed input: 21377.72 toks/s, output: 83.51 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:00<00:00, 80.74it/s, est. speed input: 21312.86 toks/s, output: 83.25 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:00<00:00, 81.48it/s, est. speed input: 21310.62 toks/s, output: 83.24 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 82.45it/s, est. speed input: 21349.12 toks/s, output: 83.39 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 83.21it/s, est. speed input: 21386.27 toks/s, output: 83.54 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 83.43it/s, est. speed input: 21394.88 toks/s, output: 83.57 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:01<00:00, 83.66it/s, est. speed input: 21407.32 toks/s, output: 83.62 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:01<00:00, 84.20it/s, est. speed input: 21441.62 toks/s, output: 83.76 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 84.20it/s, est. speed input: 21503.37 toks/s, output: 84.00 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 83.99it/s, est. speed input: 21503.37 toks/s, output: 84.00 toks/s]
[rank0]:[W125 16:55:53.892985690 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

