
========== M=16 ==========
Time: 2026-01-26 13:19:34
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:19:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=479400) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=479400) WARNING 01-26 13:19:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=479400) WARNING 01-26 13:20:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 44.21 requests/s, 751.54 total tokens/s, 44.21 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-26 13:19:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:19:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:19:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:19:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:19:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:19:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:19:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:19:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:19:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:19:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:19:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:19:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:19:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:19:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:19:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:19:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:19:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:19:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:19:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:19:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:19:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:19:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:19:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:19:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:19:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:19:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:19:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:19:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=479400) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=479400) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.02it/s]
(EngineCore_DP0 pid=479400) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.02it/s]
(EngineCore_DP0 pid=479400) 
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=479400) [2026-01-26 13:19:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=479400) 2026-01-26 13:20:00,198 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=479400) 2026-01-26 13:20:00,222 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=479400) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.91it/s]
(EngineCore_DP0 pid=479400) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 19.59it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4651.78it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 50.63it/s, est. speed input: 810.09 toks/s, output: 50.63 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 46.65it/s, est. speed input: 755.33 toks/s, output: 47.21 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 45.76it/s, est. speed input: 742.28 toks/s, output: 46.39 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 45.26it/s, est. speed input: 734.88 toks/s, output: 45.93 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 44.99it/s, est. speed input: 730.52 toks/s, output: 45.66 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 44.84it/s, est. speed input: 727.65 toks/s, output: 45.48 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 44.66it/s, est. speed input: 724.99 toks/s, output: 45.31 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 44.57it/s, est. speed input: 723.20 toks/s, output: 45.20 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 44.50it/s, est. speed input: 721.71 toks/s, output: 45.11 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 44.52it/s, est. speed input: 720.84 toks/s, output: 45.05 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 44.52it/s, est. speed input: 720.11 toks/s, output: 45.01 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 44.51it/s, est. speed input: 719.41 toks/s, output: 44.96 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 44.33it/s, est. speed input: 718.13 toks/s, output: 44.88 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 44.14it/s, est. speed input: 716.79 toks/s, output: 44.80 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 44.21it/s, est. speed input: 716.35 toks/s, output: 44.77 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 44.26it/s, est. speed input: 715.96 toks/s, output: 44.75 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 44.36it/s, est. speed input: 715.82 toks/s, output: 44.74 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 44.36it/s, est. speed input: 715.48 toks/s, output: 44.72 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 44.32it/s, est. speed input: 715.08 toks/s, output: 44.69 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 44.35it/s, est. speed input: 714.87 toks/s, output: 44.68 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 44.44it/s, est. speed input: 714.85 toks/s, output: 44.68 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 44.48it/s, est. speed input: 714.76 toks/s, output: 44.67 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 44.49it/s, est. speed input: 714.67 toks/s, output: 44.67 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 44.40it/s, est. speed input: 714.34 toks/s, output: 44.65 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 44.49it/s, est. speed input: 714.38 toks/s, output: 44.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.49it/s, est. speed input: 714.37 toks/s, output: 44.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.65it/s, est. speed input: 714.37 toks/s, output: 44.65 toks/s]
[rank0]:[W126 13:20:05.787933323 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 13:20:07
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:20:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=480465) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=480465) WARNING 01-26 13:20:26 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=480465) WARNING 01-26 13:20:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.47 requests/s, 5349.40 total tokens/s, 41.47 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-26 13:20:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:20:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:20:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:20:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:20:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:20:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:20:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:20:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:20:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:20:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:20:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:20:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:20:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:20:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:20:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:20:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:20:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:20:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=480465) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=480465) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]
(EngineCore_DP0 pid=480465) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]
(EngineCore_DP0 pid=480465) 
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=480465) [2026-01-26 13:20:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=480465) 2026-01-26 13:20:33,905 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=480465) 2026-01-26 13:20:33,927 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=480465) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.89it/s]
(EngineCore_DP0 pid=480465) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.68it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1968.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:06, 19.35it/s, est. speed input: 2477.06 toks/s, output: 19.35 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 34.04it/s, est. speed input: 4090.91 toks/s, output: 31.96 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 38.44it/s, est. speed input: 4598.60 toks/s, output: 35.93 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 40.38it/s, est. speed input: 4840.48 toks/s, output: 37.82 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 41.45it/s, est. speed input: 4984.64 toks/s, output: 38.94 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 42.04it/s, est. speed input: 5076.48 toks/s, output: 39.66 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 42.41it/s, est. speed input: 5141.58 toks/s, output: 40.17 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 42.76it/s, est. speed input: 5195.49 toks/s, output: 40.59 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 42.93it/s, est. speed input: 5234.57 toks/s, output: 40.89 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 43.14it/s, est. speed input: 5269.48 toks/s, output: 41.17 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 43.35it/s, est. speed input: 5300.38 toks/s, output: 41.41 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 43.22it/s, est. speed input: 5316.92 toks/s, output: 41.54 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 43.15it/s, est. speed input: 5331.36 toks/s, output: 41.65 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 43.15it/s, est. speed input: 5345.36 toks/s, output: 41.76 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 43.02it/s, est. speed input: 5353.69 toks/s, output: 41.83 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 43.04it/s, est. speed input: 5363.75 toks/s, output: 41.90 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 43.02it/s, est. speed input: 5371.93 toks/s, output: 41.97 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:00, 43.07it/s, est. speed input: 5380.76 toks/s, output: 42.04 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 43.08it/s, est. speed input: 5387.98 toks/s, output: 42.09 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 43.14it/s, est. speed input: 5395.59 toks/s, output: 42.15 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 43.18it/s, est. speed input: 5402.51 toks/s, output: 42.21 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 43.24it/s, est. speed input: 5409.31 toks/s, output: 42.26 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 43.15it/s, est. speed input: 5413.13 toks/s, output: 42.29 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 43.06it/s, est. speed input: 5416.11 toks/s, output: 42.31 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 43.02it/s, est. speed input: 5419.35 toks/s, output: 42.34 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 43.07it/s, est. speed input: 5423.46 toks/s, output: 42.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 43.07it/s, est. speed input: 5424.13 toks/s, output: 42.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 42.37it/s, est. speed input: 5424.13 toks/s, output: 42.38 toks/s]
[rank0]:[W126 13:20:38.192815247 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 13:20:41
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:20:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=481503) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=481503) WARNING 01-26 13:21:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=481503) WARNING 01-26 13:21:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.38 requests/s, 10378.15 total tokens/s, 40.38 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-26 13:20:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:20:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:20:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:20:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:20:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:20:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:20:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:20:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:20:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:20:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:20:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:20:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:20:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:20:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:20:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:20:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:20:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:20:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:20:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=481503) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=481503) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=481503) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=481503) 
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=481503) [2026-01-26 13:20:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=481503) 2026-01-26 13:21:07,714 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=481503) 2026-01-26 13:21:07,737 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=481503) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.35it/s]
(EngineCore_DP0 pid=481503) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.20it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  96%|█████████▌| 123/128 [00:00<00:00, 1222.33it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1223.47it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:21,  6.03it/s, est. speed input: 1543.70 toks/s, output: 6.03 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 24.70it/s, est. speed input: 5474.89 toks/s, output: 21.39 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 32.64it/s, est. speed input: 7139.18 toks/s, output: 27.89 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 36.88it/s, est. speed input: 8065.41 toks/s, output: 31.51 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 39.34it/s, est. speed input: 8649.86 toks/s, output: 33.79 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 40.85it/s, est. speed input: 9050.57 toks/s, output: 35.35 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 41.84it/s, est. speed input: 9345.13 toks/s, output: 36.50 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 42.42it/s, est. speed input: 9563.11 toks/s, output: 37.36 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 42.83it/s, est. speed input: 9736.38 toks/s, output: 38.03 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:01, 43.13it/s, est. speed input: 9877.93 toks/s, output: 38.59 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 43.41it/s, est. speed input: 9999.45 toks/s, output: 39.06 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 43.63it/s, est. speed input: 10103.18 toks/s, output: 39.47 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 43.66it/s, est. speed input: 10184.64 toks/s, output: 39.78 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 43.65it/s, est. speed input: 10252.91 toks/s, output: 40.05 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.70it/s, est. speed input: 10315.67 toks/s, output: 40.30 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 43.72it/s, est. speed input: 10369.39 toks/s, output: 40.51 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 43.44it/s, est. speed input: 10403.84 toks/s, output: 40.64 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:00, 43.59it/s, est. speed input: 10449.68 toks/s, output: 40.82 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 43.77it/s, est. speed input: 10493.81 toks/s, output: 40.99 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 43.52it/s, est. speed input: 10518.63 toks/s, output: 41.09 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 43.76it/s, est. speed input: 10557.02 toks/s, output: 41.24 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 43.93it/s, est. speed input: 10591.88 toks/s, output: 41.37 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 43.95it/s, est. speed input: 10620.42 toks/s, output: 41.49 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 43.77it/s, est. speed input: 10640.18 toks/s, output: 41.56 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 43.81it/s, est. speed input: 10663.61 toks/s, output: 41.65 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 43.84it/s, est. speed input: 10685.34 toks/s, output: 41.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 43.84it/s, est. speed input: 10694.24 toks/s, output: 41.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.77it/s, est. speed input: 10694.24 toks/s, output: 41.77 toks/s]
[rank0]:[W126 13:21:12.088596644 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 14:08:47
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:08:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=558891) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=558891) WARNING 01-26 14:09:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=558891) WARNING 01-26 14:09:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 43.00 requests/s, 22058.07 total tokens/s, 43.00 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 14:08:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:08:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:08:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:08:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:08:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:08:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:09:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:09:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:09:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:09:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:09:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:09:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=558891) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=558891) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=558891) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=558891) 
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=558891) 2026-01-26 14:09:13,655 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=558891) 2026-01-26 14:09:13,678 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=558891) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 18.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 18.87it/s]
(EngineCore_DP0 pid=558891) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 18.28it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  54%|█████▍    | 69/128 [00:00<00:00, 684.99it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 292.84it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:00, 192.65it/s, est. speed input: 98643.56 toks/s, output: 192.65 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 65.74it/s, est. speed input: 37548.45 toks/s, output: 73.34 toks/s]  
Processed prompts:  41%|████▏     | 53/128 [00:00<00:01, 56.87it/s, est. speed input: 32892.41 toks/s, output: 64.24 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 52.90it/s, est. speed input: 30946.40 toks/s, output: 60.44 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 50.43it/s, est. speed input: 29770.11 toks/s, output: 58.14 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 48.73it/s, est. speed input: 28983.36 toks/s, output: 56.61 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 47.41it/s, est. speed input: 28353.01 toks/s, output: 55.38 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 46.54it/s, est. speed input: 27916.15 toks/s, output: 54.52 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 45.87it/s, est. speed input: 27544.69 toks/s, output: 53.80 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 45.35it/s, est. speed input: 27222.50 toks/s, output: 53.17 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 44.90it/s, est. speed input: 26931.33 toks/s, output: 52.60 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 44.61it/s, est. speed input: 26678.40 toks/s, output: 52.11 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 44.41it/s, est. speed input: 26453.87 toks/s, output: 51.67 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 44.25it/s, est. speed input: 26250.24 toks/s, output: 51.27 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 44.16it/s, est. speed input: 26069.01 toks/s, output: 50.92 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 44.14it/s, est. speed input: 25908.79 toks/s, output: 50.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.14it/s, est. speed input: 25820.21 toks/s, output: 50.43 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 50.43it/s, est. speed input: 25820.21 toks/s, output: 50.43 toks/s]
[rank0]:[W126 14:09:18.879843764 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 14:09:20
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:09:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=559907) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=559907) WARNING 01-26 14:09:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=559907) WARNING 01-26 14:09:47 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.60 requests/s, 42636.83 total tokens/s, 41.60 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 14:09:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:09:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:09:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:09:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:09:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:09:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:09:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:09:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:09:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:09:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:09:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:09:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=559907) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=559907) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=559907) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]
(EngineCore_DP0 pid=559907) 
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=559907) 2026-01-26 14:09:47,367 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=559907) 2026-01-26 14:09:47,389 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=559907) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 17.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 17.15it/s]
(EngineCore_DP0 pid=559907) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.26it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|███       | 39/128 [00:00<00:00, 384.89it/s]
Adding requests:  66%|██████▌   | 84/128 [00:00<00:00, 422.73it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 425.85it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 100.38it/s, est. speed input: 102799.15 toks/s, output: 100.38 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 57.16it/s, est. speed input: 62580.32 toks/s, output: 61.11 toks/s]   
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 51.71it/s, est. speed input: 57160.16 toks/s, output: 55.82 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 49.03it/s, est. speed input: 54571.94 toks/s, output: 53.29 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 47.42it/s, est. speed input: 52941.47 toks/s, output: 51.70 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 46.52it/s, est. speed input: 51978.24 toks/s, output: 50.76 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 45.80it/s, est. speed input: 51206.14 toks/s, output: 50.01 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 45.31it/s, est. speed input: 50601.97 toks/s, output: 49.42 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 44.93it/s, est. speed input: 50095.27 toks/s, output: 48.92 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 44.66it/s, est. speed input: 49675.35 toks/s, output: 48.51 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 44.50it/s, est. speed input: 49329.54 toks/s, output: 48.17 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 44.33it/s, est. speed input: 49018.07 toks/s, output: 47.87 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 44.22it/s, est. speed input: 48751.28 toks/s, output: 47.61 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 44.14it/s, est. speed input: 48517.17 toks/s, output: 47.38 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 43.79it/s, est. speed input: 48245.60 toks/s, output: 47.11 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 43.77it/s, est. speed input: 48050.49 toks/s, output: 46.92 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 43.69it/s, est. speed input: 47864.39 toks/s, output: 46.74 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 43.68it/s, est. speed input: 47705.50 toks/s, output: 46.59 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 43.71it/s, est. speed input: 47567.72 toks/s, output: 46.45 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 43.83it/s, est. speed input: 47459.34 toks/s, output: 46.35 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 43.87it/s, est. speed input: 47353.89 toks/s, output: 46.24 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 43.88it/s, est. speed input: 47253.30 toks/s, output: 46.15 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.88it/s, est. speed input: 47225.58 toks/s, output: 46.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.12it/s, est. speed input: 47225.58 toks/s, output: 46.12 toks/s]
[rank0]:[W126 14:09:52.671524193 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 14:09:54
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:10:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=560965) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=560965) WARNING 01-26 14:10:15 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=560965) WARNING 01-26 14:10:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 78.11 requests/s, 80059.59 total tokens/s, 78.11 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 14:10:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:10:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:10:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:10:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:10:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:10:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:10:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:10:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:10:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:10:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:10:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:10:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=560965) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=560965) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.68it/s]
(EngineCore_DP0 pid=560965) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.68it/s]
(EngineCore_DP0 pid=560965) 
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=560965) 2026-01-26 14:10:21,899 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=560965) 2026-01-26 14:10:21,921 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=560965) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.50it/s]
(EngineCore_DP0 pid=560965) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 22.48it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:47,  5.36it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:01, 171.16it/s]
Adding requests:  33%|███▎      | 85/256 [00:00<00:00, 277.26it/s]
Adding requests:  50%|█████     | 129/256 [00:00<00:00, 331.59it/s]
Adding requests:  67%|██████▋   | 172/256 [00:00<00:00, 362.92it/s]
Adding requests:  84%|████████▍ | 215/256 [00:00<00:00, 383.86it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 327.49it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:00, 496.90it/s, est. speed input: 508871.53 toks/s, output: 496.91 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:00<00:01, 130.73it/s, est. speed input: 150510.86 toks/s, output: 146.98 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:00<00:01, 112.92it/s, est. speed input: 131333.98 toks/s, output: 128.26 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:01<00:01, 105.24it/s, est. speed input: 123689.39 toks/s, output: 120.79 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:01<00:00, 101.69it/s, est. speed input: 119918.53 toks/s, output: 117.11 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:00, 96.23it/s, est. speed input: 115878.96 toks/s, output: 113.16 toks/s] 
Processed prompts:  71%|███████▏  | 183/256 [00:01<00:00, 95.93it/s, est. speed input: 114542.88 toks/s, output: 111.86 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:01<00:00, 91.25it/s, est. speed input: 111805.77 toks/s, output: 109.18 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:01<00:00, 90.11it/s, est. speed input: 110377.35 toks/s, output: 107.79 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:02<00:00, 88.99it/s, est. speed input: 109065.32 toks/s, output: 106.51 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:02<00:00, 88.12it/s, est. speed input: 107896.28 toks/s, output: 105.37 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:02<00:00, 87.77it/s, est. speed input: 106921.53 toks/s, output: 104.42 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 87.40it/s, est. speed input: 106019.23 toks/s, output: 103.53 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 87.24it/s, est. speed input: 105221.42 toks/s, output: 102.76 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 87.24it/s, est. speed input: 105085.38 toks/s, output: 102.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.62it/s, est. speed input: 105085.38 toks/s, output: 102.62 toks/s]
[rank0]:[W126 14:10:27.519999788 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 14:10:29
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:10:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=562014) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=562014) WARNING 01-26 14:10:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=562014) WARNING 01-26 14:10:57 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 161.47 requests/s, 165506.87 total tokens/s, 161.47 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 14:10:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:10:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:10:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:10:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:10:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:10:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:10:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:10:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:10:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:10:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:10:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:10:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=562014) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=562014) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.02it/s]
(EngineCore_DP0 pid=562014) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.02it/s]
(EngineCore_DP0 pid=562014) 
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=562014) 2026-01-26 14:10:57,483 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=562014) 2026-01-26 14:10:57,503 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=562014) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.13it/s]
(EngineCore_DP0 pid=562014) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.10it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 26/512 [00:00<00:01, 256.53it/s]
Adding requests:  13%|█▎        | 68/512 [00:00<00:01, 351.91it/s]
Adding requests:  22%|██▏       | 114/512 [00:00<00:01, 397.96it/s]
Adding requests:  31%|███       | 157/512 [00:00<00:00, 410.18it/s]
Adding requests:  39%|███▉      | 201/512 [00:00<00:00, 420.62it/s]
Adding requests:  48%|████▊     | 248/512 [00:00<00:00, 436.72it/s]
Adding requests:  57%|█████▋    | 293/512 [00:00<00:00, 441.03it/s]
Adding requests:  66%|██████▌   | 338/512 [00:00<00:00, 442.49it/s]
Adding requests:  75%|███████▌  | 384/512 [00:00<00:00, 446.87it/s]
Adding requests:  84%|████████▍ | 432/512 [00:01<00:00, 454.36it/s]
Adding requests:  93%|█████████▎| 478/512 [00:01<00:00, 453.84it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 431.82it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:00<00:00, 1794.35it/s, est. speed input: 1837535.01 toks/s, output: 1794.38 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:01<00:00, 278.68it/s, est. speed input: 328063.15 toks/s, output: 320.37 toks/s]   
Processed prompts:  89%|████████▊ | 454/512 [00:01<00:00, 236.42it/s, est. speed input: 281628.37 toks/s, output: 275.03 toks/s]
Processed prompts:  99%|█████████▉| 508/512 [00:01<00:00, 222.50it/s, est. speed input: 266821.14 toks/s, output: 260.57 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 222.50it/s, est. speed input: 264260.99 toks/s, output: 258.07 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 258.06it/s, est. speed input: 264260.99 toks/s, output: 258.07 toks/s]
[rank0]:[W126 14:11:03.567098554 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 14:11:05
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:11:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=563103) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=563103) WARNING 01-26 14:11:29 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=563103) WARNING 01-26 14:11:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 308.26 requests/s, 315971.14 total tokens/s, 308.26 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 14:11:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:11:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:11:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:11:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:11:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:11:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:11:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:11:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:11:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:11:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:11:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:11:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=563103) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=563103) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.38it/s]
(EngineCore_DP0 pid=563103) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.38it/s]
(EngineCore_DP0 pid=563103) 
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=563103) 2026-01-26 14:11:35,885 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=563103) 2026-01-26 14:11:35,912 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=563103) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 22.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 18.87it/s]
(EngineCore_DP0 pid=563103) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.13it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 378.69it/s]
Adding requests:   8%|▊         | 83/1024 [00:00<00:02, 418.18it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 431.81it/s]
Adding requests:  17%|█▋        | 173/1024 [00:00<00:01, 435.65it/s]
Adding requests:  21%|██▏       | 219/1024 [00:00<00:01, 441.74it/s]
Adding requests:  26%|██▌       | 267/1024 [00:00<00:01, 452.89it/s]
Adding requests:  31%|███       | 313/1024 [00:00<00:01, 449.31it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 452.60it/s]
Adding requests:  40%|███▉      | 408/1024 [00:00<00:01, 458.25it/s]
Adding requests:  44%|████▍     | 455/1024 [00:01<00:01, 461.20it/s]
Adding requests:  49%|████▉     | 502/1024 [00:01<00:01, 460.21it/s]
Adding requests:  54%|█████▎    | 549/1024 [00:01<00:01, 455.94it/s]
Adding requests:  58%|█████▊    | 597/1024 [00:01<00:00, 461.98it/s]
Adding requests:  63%|██████▎   | 645/1024 [00:01<00:00, 466.66it/s]
Adding requests:  68%|██████▊   | 694/1024 [00:01<00:00, 472.97it/s]
Adding requests:  72%|███████▏  | 742/1024 [00:01<00:00, 471.82it/s]
Adding requests:  77%|███████▋  | 790/1024 [00:01<00:00, 469.75it/s]
Adding requests:  82%|████████▏ | 837/1024 [00:01<00:00, 460.00it/s]
Adding requests:  87%|████████▋ | 886/1024 [00:01<00:00, 467.78it/s]
Adding requests:  91%|█████████▏| 935/1024 [00:02<00:00, 470.53it/s]
Adding requests:  96%|█████████▌| 983/1024 [00:02<00:00, 472.95it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.97it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:00<00:00, 6687.88it/s, est. speed input: 6848906.43 toks/s, output: 6688.03 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 6687.88it/s, est. speed input: 958173.60 toks/s, output: 935.71 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 935.61it/s, est. speed input: 958173.60 toks/s, output: 935.71 toks/s] 
[rank0]:[W126 14:11:41.210065475 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 14:11:43
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:11:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=564266) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=564266) WARNING 01-26 14:12:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=564266) WARNING 01-26 14:12:19 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 449.22 requests/s, 460450.70 total tokens/s, 449.22 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 14:11:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:11:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:11:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:11:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:11:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:11:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:12:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:12:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:12:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:12:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:12:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:12:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:12:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:12:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=564266) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=564266) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.40it/s]
(EngineCore_DP0 pid=564266) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.40it/s]
(EngineCore_DP0 pid=564266) 
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=564266) 2026-01-26 14:12:19,129 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=564266) 2026-01-26 14:12:19,152 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=564266) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 17.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 19.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 18.45it/s]
(EngineCore_DP0 pid=564266) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 23.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 23.65it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 381.63it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 419.81it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 431.58it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 436.43it/s]
Adding requests:  11%|█         | 221/2048 [00:00<00:04, 445.28it/s]
Adding requests:  13%|█▎        | 269/2048 [00:00<00:03, 455.92it/s]
Adding requests:  15%|█▌        | 315/2048 [00:00<00:03, 452.69it/s]
Adding requests:  18%|█▊        | 362/2048 [00:00<00:03, 457.49it/s]
Adding requests:  20%|█▉        | 409/2048 [00:00<00:03, 461.10it/s]
Adding requests:  22%|██▏       | 456/2048 [00:01<00:03, 463.52it/s]
Adding requests:  25%|██▍       | 503/2048 [00:01<00:03, 462.21it/s]
Adding requests:  27%|██▋       | 550/2048 [00:01<00:03, 456.58it/s]
Adding requests:  29%|██▉       | 596/2048 [00:01<00:03, 450.78it/s]
Adding requests:  31%|███▏      | 642/2048 [00:01<00:03, 448.61it/s]
Adding requests:  34%|███▎      | 688/2048 [00:01<00:03, 449.56it/s]
Adding requests:  36%|███▌      | 734/2048 [00:01<00:02, 450.98it/s]
Adding requests:  38%|███▊      | 780/2048 [00:01<00:02, 441.97it/s]
Adding requests:  40%|████      | 825/2048 [00:01<00:02, 434.00it/s]
Adding requests:  42%|████▏     | 869/2048 [00:01<00:02, 434.61it/s]
Adding requests:  45%|████▍     | 914/2048 [00:02<00:02, 438.36it/s]
Adding requests:  47%|████▋     | 959/2048 [00:02<00:02, 439.68it/s]
Adding requests:  49%|████▉     | 1004/2048 [00:02<00:02, 440.24it/s]
Adding requests:  51%|█████     | 1049/2048 [00:02<00:02, 442.29it/s]
Adding requests:  53%|█████▎    | 1094/2048 [00:02<00:02, 437.05it/s]
Adding requests:  56%|█████▌    | 1138/2048 [00:02<00:02, 428.48it/s]
Adding requests:  58%|█████▊    | 1186/2048 [00:02<00:01, 439.17it/s]
Adding requests:  60%|██████    | 1233/2048 [00:02<00:01, 445.04it/s]
Adding requests:  62%|██████▏   | 1278/2048 [00:02<00:01, 437.97it/s]
Adding requests:  65%|██████▍   | 1325/2048 [00:02<00:01, 445.23it/s]
Adding requests:  67%|██████▋   | 1371/2048 [00:03<00:01, 448.60it/s]
Adding requests:  69%|██████▉   | 1420/2048 [00:03<00:01, 458.87it/s]
Adding requests:  72%|███████▏  | 1469/2048 [00:03<00:01, 466.25it/s]
Adding requests:  74%|███████▍  | 1519/2048 [00:03<00:01, 473.29it/s]
Adding requests:  77%|███████▋  | 1567/2048 [00:03<00:01, 467.85it/s]
Adding requests:  79%|███████▉  | 1614/2048 [00:03<00:00, 467.56it/s]
Adding requests:  81%|████████  | 1662/2048 [00:03<00:00, 470.43it/s]
Adding requests:  83%|████████▎ | 1710/2048 [00:03<00:00, 471.66it/s]
Adding requests:  86%|████████▌ | 1758/2048 [00:03<00:00, 473.80it/s]
Adding requests:  88%|████████▊ | 1806/2048 [00:03<00:00, 472.92it/s]
Adding requests:  91%|█████████ | 1855/2048 [00:04<00:00, 476.33it/s]
Adding requests:  93%|█████████▎| 1903/2048 [00:04<00:00, 475.52it/s]
Adding requests:  95%|█████████▌| 1951/2048 [00:04<00:00, 475.94it/s]
Adding requests:  98%|█████████▊| 2000/2048 [00:04<00:00, 478.46it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 478.45it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 454.88it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 37081.36it/s, est. speed input: 37982136.24 toks/s, output: 37088.57 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 37010.74it/s, est. speed input: 37982136.24 toks/s, output: 37088.57 toks/s]
[rank0]:[W126 14:12:26.522390291 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 14:12:28
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:12:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=565544) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=565544) WARNING 01-26 14:13:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=565544) WARNING 01-26 14:13:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 458.78 requests/s, 470245.84 total tokens/s, 458.78 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 14:12:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:12:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:12:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:12:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:12:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:12:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:12:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:12:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:13:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:13:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:13:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:13:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:13:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:13:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:13:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:13:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=565544) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=565544) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.38it/s]
(EngineCore_DP0 pid=565544) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.38it/s]
(EngineCore_DP0 pid=565544) 
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=565544) [rank0]:W0126 14:13:09.683000 565544 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=565544) [rank0]:W0126 14:13:09.752000 565544 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=565544) [rank0]:W0126 14:13:10.557000 565544 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=565544) [rank0]:W0126 14:13:10.655000 565544 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=565544) 2026-01-26 14:13:13,103 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=565544) 2026-01-26 14:13:13,139 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=565544) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 22.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.30it/s]
(EngineCore_DP0 pid=565544) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 23.40it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 21.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 21.44it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 39/4096 [00:00<00:10, 380.56it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 415.36it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 429.51it/s]
Adding requests:   4%|▍         | 174/4096 [00:00<00:09, 434.34it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:08, 441.26it/s]
Adding requests:   7%|▋         | 268/4096 [00:00<00:08, 452.75it/s]
Adding requests:   8%|▊         | 314/4096 [00:00<00:08, 450.39it/s]
Adding requests:   9%|▉         | 360/4096 [00:00<00:08, 452.61it/s]
Adding requests:  10%|▉         | 407/4096 [00:00<00:08, 457.05it/s]
Adding requests:  11%|█         | 453/4096 [00:01<00:07, 457.56it/s]
Adding requests:  12%|█▏        | 499/4096 [00:01<00:07, 453.85it/s]
Adding requests:  13%|█▎        | 545/4096 [00:01<00:07, 447.01it/s]
Adding requests:  15%|█▍        | 594/4096 [00:01<00:07, 457.65it/s]
Adding requests:  16%|█▌        | 641/4096 [00:01<00:07, 459.68it/s]
Adding requests:  17%|█▋        | 689/4096 [00:01<00:07, 465.15it/s]
Adding requests:  18%|█▊        | 738/4096 [00:01<00:07, 470.11it/s]
Adding requests:  19%|█▉        | 786/4096 [00:01<00:07, 466.13it/s]
Adding requests:  20%|██        | 833/4096 [00:01<00:07, 450.32it/s]
Adding requests:  22%|██▏       | 881/4096 [00:01<00:07, 456.27it/s]
Adding requests:  23%|██▎       | 929/4096 [00:02<00:06, 463.16it/s]
Adding requests:  24%|██▍       | 977/4096 [00:02<00:06, 465.72it/s]
Adding requests:  25%|██▌       | 1025/4096 [00:02<00:06, 468.38it/s]
Adding requests:  26%|██▌       | 1072/4096 [00:02<00:06, 462.57it/s]
Adding requests:  27%|██▋       | 1119/4096 [00:02<00:06, 461.87it/s]
Adding requests:  29%|██▊       | 1168/4096 [00:02<00:06, 468.52it/s]
Adding requests:  30%|██▉       | 1217/4096 [00:02<00:06, 474.80it/s]
Adding requests:  31%|███       | 1265/4096 [00:02<00:06, 467.77it/s]
Adding requests:  32%|███▏      | 1312/4096 [00:02<00:05, 468.15it/s]
Adding requests:  33%|███▎      | 1361/4096 [00:02<00:05, 472.75it/s]
Adding requests:  34%|███▍      | 1411/4096 [00:03<00:05, 477.98it/s]
Adding requests:  36%|███▌      | 1459/4096 [00:03<00:05, 475.56it/s]
Adding requests:  37%|███▋      | 1507/4096 [00:03<00:05, 476.79it/s]
Adding requests:  38%|███▊      | 1555/4096 [00:03<00:05, 474.88it/s]
Adding requests:  39%|███▉      | 1605/4096 [00:03<00:05, 480.62it/s]
Adding requests:  40%|████      | 1654/4096 [00:03<00:05, 476.94it/s]
Adding requests:  42%|████▏     | 1702/4096 [00:03<00:05, 473.66it/s]
Adding requests:  43%|████▎     | 1750/4096 [00:03<00:04, 473.16it/s]
Adding requests:  44%|████▍     | 1798/4096 [00:03<00:05, 452.72it/s]
Adding requests:  45%|████▌     | 1844/4096 [00:04<00:05, 446.38it/s]
Adding requests:  46%|████▌     | 1891/4096 [00:04<00:04, 450.24it/s]
Adding requests:  47%|████▋     | 1938/4096 [00:04<00:04, 455.05it/s]
Adding requests:  48%|████▊     | 1985/4096 [00:04<00:04, 458.37it/s]
Adding requests:  50%|████▉     | 2033/4096 [00:04<00:04, 463.46it/s]
Adding requests:  51%|█████     | 2081/4096 [00:04<00:04, 468.03it/s]
Adding requests:  52%|█████▏    | 2128/4096 [00:04<00:04, 463.52it/s]
Adding requests:  53%|█████▎    | 2175/4096 [00:04<00:04, 460.96it/s]
Adding requests:  54%|█████▍    | 2222/4096 [00:04<00:04, 461.39it/s]
Adding requests:  55%|█████▌    | 2269/4096 [00:04<00:03, 463.25it/s]
Adding requests:  57%|█████▋    | 2318/4096 [00:05<00:03, 468.60it/s]
Adding requests:  58%|█████▊    | 2365/4096 [00:05<00:03, 465.53it/s]
Adding requests:  59%|█████▉    | 2412/4096 [00:05<00:03, 465.51it/s]
Adding requests:  60%|██████    | 2460/4096 [00:05<00:03, 468.95it/s]
Adding requests:  61%|██████    | 2507/4096 [00:05<00:03, 465.94it/s]
Adding requests:  62%|██████▏   | 2556/4096 [00:05<00:03, 470.77it/s]
Adding requests:  64%|██████▎   | 2604/4096 [00:05<00:03, 468.03it/s]
Adding requests:  65%|██████▍   | 2652/4096 [00:05<00:03, 471.45it/s]
Adding requests:  66%|██████▌   | 2700/4096 [00:05<00:02, 467.56it/s]
Adding requests:  67%|██████▋   | 2747/4096 [00:05<00:02, 465.92it/s]
Adding requests:  68%|██████▊   | 2794/4096 [00:06<00:02, 463.17it/s]
Adding requests:  69%|██████▉   | 2841/4096 [00:06<00:02, 464.00it/s]
Adding requests:  71%|███████   | 2889/4096 [00:06<00:02, 467.62it/s]
Adding requests:  72%|███████▏  | 2936/4096 [00:06<00:02, 462.29it/s]
Adding requests:  73%|███████▎  | 2983/4096 [00:06<00:02, 454.66it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:06<00:02, 456.99it/s]
Adding requests:  75%|███████▌  | 3076/4096 [00:06<00:02, 446.99it/s]
Adding requests:  76%|███████▋  | 3124/4096 [00:06<00:02, 454.92it/s]
Adding requests:  77%|███████▋  | 3170/4096 [00:06<00:02, 453.02it/s]
Adding requests:  79%|███████▊  | 3218/4096 [00:06<00:01, 456.77it/s]
Adding requests:  80%|███████▉  | 3266/4096 [00:07<00:01, 460.87it/s]
Adding requests:  81%|████████  | 3313/4096 [00:07<00:01, 462.71it/s]
Adding requests:  82%|████████▏ | 3361/4096 [00:07<00:01, 465.20it/s]
Adding requests:  83%|████████▎ | 3408/4096 [00:07<00:01, 464.84it/s]
Adding requests:  84%|████████▍ | 3455/4096 [00:07<00:01, 461.87it/s]
Adding requests:  85%|████████▌ | 3502/4096 [00:07<00:01, 460.11it/s]
Adding requests:  87%|████████▋ | 3549/4096 [00:07<00:01, 462.17it/s]
Adding requests:  88%|████████▊ | 3596/4096 [00:07<00:01, 462.32it/s]
Adding requests:  89%|████████▉ | 3643/4096 [00:07<00:00, 459.32it/s]
Adding requests:  90%|█████████ | 3690/4096 [00:07<00:00, 461.88it/s]
Adding requests:  91%|█████████ | 3737/4096 [00:08<00:00, 461.32it/s]
Adding requests:  92%|█████████▏| 3786/4096 [00:08<00:00, 468.59it/s]
Adding requests:  94%|█████████▎| 3834/4096 [00:08<00:00, 469.24it/s]
Adding requests:  95%|█████████▍| 3882/4096 [00:08<00:00, 470.77it/s]
Adding requests:  96%|█████████▌| 3930/4096 [00:08<00:00, 471.23it/s]
Adding requests:  97%|█████████▋| 3978/4096 [00:08<00:00, 469.88it/s]
Adding requests:  98%|█████████▊| 4025/4096 [00:08<00:00, 467.94it/s]
Adding requests:  99%|█████████▉| 4072/4096 [00:08<00:00, 463.56it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 462.29it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62210.29it/s, est. speed input: 63717872.63 toks/s, output: 62220.43 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62111.55it/s, est. speed input: 63717872.63 toks/s, output: 62220.43 toks/s]
[rank0]:[W126 14:13:24.205375091 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 14:13:26
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:14:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=567149) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=567149) WARNING 01-26 14:14:23 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=567149) WARNING 01-26 14:14:29 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 464.33 requests/s, 475937.09 total tokens/s, 464.33 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 14:14:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:14:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:14:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:14:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:14:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:14:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:14:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:14:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:14:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:14:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:14:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:14:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:14:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:14:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:14:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:14:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=567149) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=567149) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.40it/s]
(EngineCore_DP0 pid=567149) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.40it/s]
(EngineCore_DP0 pid=567149) 
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=567149) [rank0]:W0126 14:14:26.516000 567149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=567149) [rank0]:W0126 14:14:26.585000 567149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=567149) [rank0]:W0126 14:14:27.384000 567149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=567149) [rank0]:W0126 14:14:27.481000 567149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=567149) 2026-01-26 14:14:29,794 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=567149) 2026-01-26 14:14:29,818 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=567149) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 18.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 20.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 21.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 21.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 21.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 22.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 21.09it/s]
(EngineCore_DP0 pid=567149) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 18.65it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 19.20it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 19.52it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 21.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.13it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 38/8192 [00:00<00:21, 375.18it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:19, 410.22it/s]
Adding requests:   2%|▏         | 126/8192 [00:00<00:19, 422.20it/s]
Adding requests:   2%|▏         | 170/8192 [00:00<00:18, 427.56it/s]
Adding requests:   3%|▎         | 215/8192 [00:00<00:18, 433.10it/s]
Adding requests:   3%|▎         | 262/8192 [00:00<00:17, 444.71it/s]
Adding requests:   4%|▎         | 307/8192 [00:00<00:17, 444.59it/s]
Adding requests:   4%|▍         | 354/8192 [00:00<00:17, 450.32it/s]
Adding requests:   5%|▍         | 400/8192 [00:00<00:17, 452.99it/s]
Adding requests:   5%|▌         | 446/8192 [00:01<00:17, 455.01it/s]
Adding requests:   6%|▌         | 492/8192 [00:01<00:16, 453.08it/s]
Adding requests:   7%|▋         | 538/8192 [00:01<00:17, 441.60it/s]
Adding requests:   7%|▋         | 587/8192 [00:01<00:16, 454.75it/s]
Adding requests:   8%|▊         | 634/8192 [00:01<00:16, 457.83it/s]
Adding requests:   8%|▊         | 683/8192 [00:01<00:16, 465.39it/s]
Adding requests:   9%|▉         | 731/8192 [00:01<00:15, 468.67it/s]
Adding requests:   9%|▉         | 778/8192 [00:01<00:15, 463.80it/s]
Adding requests:  10%|█         | 825/8192 [00:01<00:16, 456.26it/s]
Adding requests:  11%|█         | 873/8192 [00:01<00:15, 460.38it/s]
Adding requests:  11%|█         | 921/8192 [00:02<00:15, 465.86it/s]
Adding requests:  12%|█▏        | 969/8192 [00:02<00:15, 466.49it/s]
Adding requests:  12%|█▏        | 1018/8192 [00:02<00:15, 470.14it/s]
Adding requests:  13%|█▎        | 1066/8192 [00:02<00:15, 469.38it/s]
Adding requests:  14%|█▎        | 1113/8192 [00:02<00:15, 465.63it/s]
Adding requests:  14%|█▍        | 1160/8192 [00:02<00:15, 466.22it/s]
Adding requests:  15%|█▍        | 1210/8192 [00:02<00:14, 475.09it/s]
Adding requests:  15%|█▌        | 1258/8192 [00:02<00:14, 468.17it/s]
Adding requests:  16%|█▌        | 1306/8192 [00:02<00:14, 469.00it/s]
Adding requests:  17%|█▋        | 1355/8192 [00:02<00:14, 473.09it/s]
Adding requests:  17%|█▋        | 1404/8192 [00:03<00:14, 476.89it/s]
Adding requests:  18%|█▊        | 1452/8192 [00:03<00:14, 473.93it/s]
Adding requests:  18%|█▊        | 1500/8192 [00:03<00:14, 468.62it/s]
Adding requests:  19%|█▉        | 1548/8192 [00:03<00:14, 470.24it/s]
Adding requests:  19%|█▉        | 1597/8192 [00:03<00:13, 476.00it/s]
Adding requests:  20%|██        | 1646/8192 [00:03<00:13, 478.08it/s]
Adding requests:  21%|██        | 1694/8192 [00:03<00:13, 472.40it/s]
Adding requests:  21%|██▏       | 1743/8192 [00:03<00:13, 475.33it/s]
Adding requests:  22%|██▏       | 1791/8192 [00:03<00:13, 472.98it/s]
Adding requests:  22%|██▏       | 1839/8192 [00:03<00:13, 472.84it/s]
Adding requests:  23%|██▎       | 1887/8192 [00:04<00:13, 474.07it/s]
Adding requests:  24%|██▎       | 1935/8192 [00:04<00:13, 473.31it/s]
Adding requests:  24%|██▍       | 1983/8192 [00:04<00:13, 473.84it/s]
Adding requests:  25%|██▍       | 2032/8192 [00:04<00:12, 476.14it/s]
Adding requests:  25%|██▌       | 2081/8192 [00:04<00:12, 479.12it/s]
Adding requests:  26%|██▌       | 2129/8192 [00:04<00:12, 472.93it/s]
Adding requests:  27%|██▋       | 2177/8192 [00:04<00:12, 467.68it/s]
Adding requests:  27%|██▋       | 2225/8192 [00:04<00:12, 469.78it/s]
Adding requests:  28%|██▊       | 2273/8192 [00:04<00:12, 470.11it/s]
Adding requests:  28%|██▊       | 2321/8192 [00:05<00:12, 472.74it/s]
Adding requests:  29%|██▉       | 2369/8192 [00:05<00:12, 471.23it/s]
Adding requests:  30%|██▉       | 2417/8192 [00:05<00:12, 471.29it/s]
Adding requests:  30%|███       | 2465/8192 [00:05<00:12, 473.35it/s]
Adding requests:  31%|███       | 2513/8192 [00:05<00:12, 466.21it/s]
Adding requests:  31%|███▏      | 2562/8192 [00:05<00:11, 470.87it/s]
Adding requests:  32%|███▏      | 2610/8192 [00:05<00:12, 460.54it/s]
Adding requests:  32%|███▏      | 2659/8192 [00:05<00:11, 466.98it/s]
Adding requests:  33%|███▎      | 2706/8192 [00:05<00:11, 462.92it/s]
Adding requests:  34%|███▎      | 2754/8192 [00:05<00:11, 464.64it/s]
Adding requests:  34%|███▍      | 2801/8192 [00:06<00:11, 463.27it/s]
Adding requests:  35%|███▍      | 2849/8192 [00:06<00:11, 466.83it/s]
Adding requests:  35%|███▌      | 2896/8192 [00:06<00:11, 467.76it/s]
Adding requests:  36%|███▌      | 2943/8192 [00:06<00:11, 464.99it/s]
Adding requests:  37%|███▋      | 2991/8192 [00:06<00:11, 468.61it/s]
Adding requests:  37%|███▋      | 3038/8192 [00:06<00:11, 467.45it/s]
Adding requests:  38%|███▊      | 3085/8192 [00:06<00:10, 465.37it/s]
Adding requests:  38%|███▊      | 3133/8192 [00:06<00:10, 469.48it/s]
Adding requests:  39%|███▉      | 3181/8192 [00:06<00:10, 470.65it/s]
Adding requests:  39%|███▉      | 3229/8192 [00:06<00:10, 469.23it/s]
Adding requests:  40%|████      | 3277/8192 [00:07<00:10, 470.80it/s]
Adding requests:  41%|████      | 3325/8192 [00:07<00:10, 470.72it/s]
Adding requests:  41%|████      | 3374/8192 [00:07<00:10, 475.93it/s]
Adding requests:  42%|████▏     | 3423/8192 [00:07<00:09, 479.16it/s]
Adding requests:  42%|████▏     | 3471/8192 [00:07<00:10, 466.51it/s]
Adding requests:  43%|████▎     | 3519/8192 [00:07<00:09, 469.67it/s]
Adding requests:  44%|████▎     | 3567/8192 [00:07<00:09, 466.59it/s]
Adding requests:  44%|████▍     | 3615/8192 [00:07<00:09, 466.82it/s]
Adding requests:  45%|████▍     | 3662/8192 [00:07<00:09, 465.42it/s]
Adding requests:  45%|████▌     | 3709/8192 [00:07<00:09, 457.04it/s]
Adding requests:  46%|████▌     | 3755/8192 [00:08<00:09, 444.25it/s]
Adding requests:  46%|████▋     | 3804/8192 [00:08<00:09, 455.65it/s]
Adding requests:  47%|████▋     | 3852/8192 [00:08<00:09, 461.61it/s]
Adding requests:  48%|████▊     | 3899/8192 [00:08<00:09, 463.20it/s]
Adding requests:  48%|████▊     | 3946/8192 [00:08<00:09, 464.51it/s]
Adding requests:  49%|████▊     | 3993/8192 [00:08<00:09, 462.26it/s]
Adding requests:  49%|████▉     | 4040/8192 [00:08<00:08, 462.89it/s]
Adding requests:  50%|████▉     | 4087/8192 [00:08<00:08, 464.83it/s]
Adding requests:  50%|█████     | 4135/8192 [00:08<00:08, 468.98it/s]
Adding requests:  51%|█████     | 4183/8192 [00:08<00:08, 469.25it/s]
Adding requests:  52%|█████▏    | 4230/8192 [00:09<00:08, 468.94it/s]
Adding requests:  52%|█████▏    | 4277/8192 [00:09<00:08, 468.39it/s]
Adding requests:  53%|█████▎    | 4326/8192 [00:09<00:08, 472.46it/s]
Adding requests:  53%|█████▎    | 4375/8192 [00:09<00:08, 476.42it/s]
Adding requests:  54%|█████▍    | 4423/8192 [00:09<00:07, 473.82it/s]
Adding requests:  55%|█████▍    | 4472/8192 [00:09<00:07, 477.34it/s]
Adding requests:  55%|█████▌    | 4520/8192 [00:09<00:07, 465.90it/s]
Adding requests:  56%|█████▌    | 4569/8192 [00:09<00:07, 470.56it/s]
Adding requests:  56%|█████▋    | 4617/8192 [00:09<00:07, 472.49it/s]
Adding requests:  57%|█████▋    | 4665/8192 [00:10<00:07, 472.57it/s]
Adding requests:  58%|█████▊    | 4713/8192 [00:10<00:07, 471.03it/s]
Adding requests:  58%|█████▊    | 4761/8192 [00:10<00:07, 471.36it/s]
Adding requests:  59%|█████▊    | 4809/8192 [00:10<00:07, 470.73it/s]
Adding requests:  59%|█████▉    | 4857/8192 [00:10<00:07, 471.73it/s]
Adding requests:  60%|█████▉    | 4905/8192 [00:10<00:07, 467.20it/s]
Adding requests:  60%|██████    | 4952/8192 [00:10<00:07, 460.10it/s]
Adding requests:  61%|██████    | 4999/8192 [00:10<00:06, 462.17it/s]
Adding requests:  62%|██████▏   | 5047/8192 [00:10<00:06, 467.38it/s]
Adding requests:  62%|██████▏   | 5096/8192 [00:10<00:06, 473.60it/s]
Adding requests:  63%|██████▎   | 5144/8192 [00:11<00:06, 472.09it/s]
Adding requests:  63%|██████▎   | 5192/8192 [00:11<00:06, 471.81it/s]
Adding requests:  64%|██████▍   | 5240/8192 [00:11<00:06, 469.45it/s]
Adding requests:  65%|██████▍   | 5287/8192 [00:11<00:06, 467.58it/s]
Adding requests:  65%|██████▌   | 5335/8192 [00:11<00:06, 471.13it/s]
Adding requests:  66%|██████▌   | 5383/8192 [00:11<00:05, 470.98it/s]
Adding requests:  66%|██████▋   | 5431/8192 [00:11<00:05, 472.62it/s]
Adding requests:  67%|██████▋   | 5479/8192 [00:11<00:05, 467.52it/s]
Adding requests:  67%|██████▋   | 5526/8192 [00:11<00:05, 464.83it/s]
Adding requests:  68%|██████▊   | 5573/8192 [00:11<00:05, 465.40it/s]
Adding requests:  69%|██████▊   | 5620/8192 [00:12<00:05, 466.60it/s]
Adding requests:  69%|██████▉   | 5667/8192 [00:12<00:05, 459.56it/s]
Adding requests:  70%|██████▉   | 5715/8192 [00:12<00:05, 464.61it/s]
Adding requests:  70%|███████   | 5763/8192 [00:12<00:05, 468.39it/s]
Adding requests:  71%|███████   | 5810/8192 [00:12<00:05, 463.84it/s]
Adding requests:  71%|███████▏  | 5857/8192 [00:12<00:05, 464.02it/s]
Adding requests:  72%|███████▏  | 5906/8192 [00:12<00:04, 468.68it/s]
Adding requests:  73%|███████▎  | 5954/8192 [00:12<00:04, 467.06it/s]
Adding requests:  73%|███████▎  | 6003/8192 [00:12<00:04, 470.93it/s]
Adding requests:  74%|███████▍  | 6052/8192 [00:12<00:04, 476.35it/s]
Adding requests:  74%|███████▍  | 6100/8192 [00:13<00:04, 445.44it/s]
Adding requests:  75%|███████▌  | 6148/8192 [00:13<00:04, 453.80it/s]
Adding requests:  76%|███████▌  | 6196/8192 [00:13<00:04, 459.49it/s]
Adding requests:  76%|███████▌  | 6245/8192 [00:13<00:04, 467.38it/s]
Adding requests:  77%|███████▋  | 6294/8192 [00:13<00:04, 472.91it/s]
Adding requests:  77%|███████▋  | 6343/8192 [00:13<00:03, 475.20it/s]
Adding requests:  78%|███████▊  | 6391/8192 [00:13<00:03, 476.20it/s]
Adding requests:  79%|███████▊  | 6441/8192 [00:13<00:03, 480.81it/s]
Adding requests:  79%|███████▉  | 6490/8192 [00:13<00:03, 480.10it/s]
Adding requests:  80%|███████▉  | 6540/8192 [00:14<00:03, 483.84it/s]
Adding requests:  80%|████████  | 6589/8192 [00:14<00:03, 480.51it/s]
Adding requests:  81%|████████  | 6638/8192 [00:14<00:03, 477.51it/s]
Adding requests:  82%|████████▏ | 6686/8192 [00:14<00:03, 475.81it/s]
Adding requests:  82%|████████▏ | 6735/8192 [00:14<00:03, 477.46it/s]
Adding requests:  83%|████████▎ | 6783/8192 [00:14<00:02, 476.08it/s]
Adding requests:  83%|████████▎ | 6832/8192 [00:14<00:02, 478.44it/s]
Adding requests:  84%|████████▍ | 6881/8192 [00:14<00:02, 478.94it/s]
Adding requests:  85%|████████▍ | 6931/8192 [00:14<00:02, 483.14it/s]
Adding requests:  85%|████████▌ | 6980/8192 [00:14<00:02, 478.90it/s]
Adding requests:  86%|████████▌ | 7028/8192 [00:15<00:02, 476.13it/s]
Adding requests:  86%|████████▋ | 7076/8192 [00:15<00:02, 473.91it/s]
Adding requests:  87%|████████▋ | 7125/8192 [00:15<00:02, 477.77it/s]
Adding requests:  88%|████████▊ | 7173/8192 [00:15<00:02, 471.02it/s]
Adding requests:  88%|████████▊ | 7222/8192 [00:15<00:02, 474.40it/s]
Adding requests:  89%|████████▊ | 7270/8192 [00:15<00:01, 467.96it/s]
Adding requests:  89%|████████▉ | 7318/8192 [00:15<00:01, 471.05it/s]
Adding requests:  90%|████████▉ | 7366/8192 [00:15<00:01, 468.29it/s]
Adding requests:  91%|█████████ | 7416/8192 [00:15<00:01, 476.53it/s]
Adding requests:  91%|█████████ | 7464/8192 [00:15<00:01, 477.10it/s]
Adding requests:  92%|█████████▏| 7513/8192 [00:16<00:01, 478.30it/s]
Adding requests:  92%|█████████▏| 7561/8192 [00:16<00:01, 472.78it/s]
Adding requests:  93%|█████████▎| 7609/8192 [00:16<00:01, 469.57it/s]
Adding requests:  93%|█████████▎| 7659/8192 [00:16<00:01, 475.64it/s]
Adding requests:  94%|█████████▍| 7708/8192 [00:16<00:01, 477.13it/s]
Adding requests:  95%|█████████▍| 7756/8192 [00:16<00:00, 473.81it/s]
Adding requests:  95%|█████████▌| 7804/8192 [00:16<00:00, 469.82it/s]
Adding requests:  96%|█████████▌| 7853/8192 [00:16<00:00, 472.91it/s]
Adding requests:  96%|█████████▋| 7901/8192 [00:16<00:00, 471.42it/s]
Adding requests:  97%|█████████▋| 7949/8192 [00:16<00:00, 468.25it/s]
Adding requests:  98%|█████████▊| 7997/8192 [00:17<00:00, 468.80it/s]
Adding requests:  98%|█████████▊| 8044/8192 [00:17<00:00, 465.52it/s]
Adding requests:  99%|█████████▉| 8093/8192 [00:17<00:00, 471.66it/s]
Adding requests:  99%|█████████▉| 8141/8192 [00:17<00:00, 470.75it/s]
Adding requests: 100%|█████████▉| 8190/8192 [00:17<00:00, 475.53it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 468.09it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  71%|███████   | 5807/8192 [00:00<00:00, 58068.26it/s, est. speed input: 59465296.25 toks/s, output: 58069.09 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 58068.26it/s, est. speed input: 60494993.34 toks/s, output: 59075.01 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 59024.47it/s, est. speed input: 60494993.34 toks/s, output: 59075.01 toks/s]
[rank0]:[W126 14:14:51.261244311 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 15:18:31
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:18:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=662492) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=662492) WARNING 01-26 15:18:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=662492) WARNING 01-26 15:19:03 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 34.44 requests/s, 17667.90 total tokens/s, 34.44 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 15:18:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:18:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:18:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:18:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:18:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:18:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:18:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:18:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:18:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:18:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:18:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:18:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:18:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:18:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:18:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:18:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:18:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:18:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:18:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:18:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:18:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:18:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:18:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:18:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:18:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:18:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:18:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:18:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=662492) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=662492) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.22it/s]
(EngineCore_DP0 pid=662492) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.22it/s]
(EngineCore_DP0 pid=662492) 
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=662492) [2026-01-26 15:18:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=662492) 2026-01-26 15:19:03,498 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=662492) 2026-01-26 15:19:03,521 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=662492) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.56it/s]
(EngineCore_DP0 pid=662492) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 13.42it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  56%|█████▋    | 72/128 [00:00<00:00, 714.36it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 743.60it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:13,  9.09it/s, est. speed input: 4657.02 toks/s, output: 9.09 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:04, 25.11it/s, est. speed input: 11626.37 toks/s, output: 22.71 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 30.39it/s, est. speed input: 14005.28 toks/s, output: 27.35 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 33.01it/s, est. speed input: 15234.42 toks/s, output: 29.75 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 34.44it/s, est. speed input: 15970.25 toks/s, output: 31.19 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 35.40it/s, est. speed input: 16482.34 toks/s, output: 32.19 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 35.99it/s, est. speed input: 16847.48 toks/s, output: 32.90 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 36.29it/s, est. speed input: 17103.35 toks/s, output: 33.40 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 36.56it/s, est. speed input: 17315.44 toks/s, output: 33.82 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 36.74it/s, est. speed input: 17484.27 toks/s, output: 34.15 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 36.81it/s, est. speed input: 17615.23 toks/s, output: 34.40 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 36.87it/s, est. speed input: 17726.87 toks/s, output: 34.62 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 36.86it/s, est. speed input: 17814.02 toks/s, output: 34.79 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 36.96it/s, est. speed input: 17901.51 toks/s, output: 34.96 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 37.02it/s, est. speed input: 17975.95 toks/s, output: 35.11 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 37.00it/s, est. speed input: 18034.42 toks/s, output: 35.22 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 37.03it/s, est. speed input: 18090.61 toks/s, output: 35.33 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 37.03it/s, est. speed input: 18139.18 toks/s, output: 35.43 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 37.04it/s, est. speed input: 18182.97 toks/s, output: 35.51 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 37.00it/s, est. speed input: 18219.16 toks/s, output: 35.58 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 36.99it/s, est. speed input: 18252.91 toks/s, output: 35.65 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 36.96it/s, est. speed input: 18281.29 toks/s, output: 35.71 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 36.86it/s, est. speed input: 18302.22 toks/s, output: 35.75 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 36.94it/s, est. speed input: 18331.70 toks/s, output: 35.80 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 37.01it/s, est. speed input: 18359.49 toks/s, output: 35.86 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 36.99it/s, est. speed input: 18380.58 toks/s, output: 35.90 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 37.01it/s, est. speed input: 18402.91 toks/s, output: 35.94 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 36.94it/s, est. speed input: 18418.26 toks/s, output: 35.97 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 37.00it/s, est. speed input: 18438.51 toks/s, output: 36.01 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 36.97it/s, est. speed input: 18453.97 toks/s, output: 36.04 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 36.94it/s, est. speed input: 18467.69 toks/s, output: 36.07 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 36.96it/s, est. speed input: 18482.46 toks/s, output: 36.10 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.96it/s, est. speed input: 18495.28 toks/s, output: 36.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.12it/s, est. speed input: 18495.28 toks/s, output: 36.12 toks/s]
[rank0]:[W126 15:19:09.980347336 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 15:19:11
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:19:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=663697) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=663697) WARNING 01-26 15:19:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=663697) WARNING 01-26 15:19:44 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 34.07 requests/s, 34917.82 total tokens/s, 34.07 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 15:19:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:19:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:19:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:19:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:19:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:19:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:19:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:19:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:19:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:19:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:19:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:19:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:19:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:19:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:19:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:19:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:19:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:19:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=663697) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=663697) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.36it/s]
(EngineCore_DP0 pid=663697) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.36it/s]
(EngineCore_DP0 pid=663697) 
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=663697) [2026-01-26 15:19:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=663697) 2026-01-26 15:19:44,000 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=663697) 2026-01-26 15:19:44,024 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=663697) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.27it/s]
(EngineCore_DP0 pid=663697) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.77it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|██▉       | 38/128 [00:00<00:00, 373.44it/s]
Adding requests:  66%|██████▌   | 84/128 [00:00<00:00, 418.94it/s]
Adding requests:  98%|█████████▊| 126/128 [00:00<00:00, 225.97it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 256.95it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 125.60it/s, est. speed input: 128625.21 toks/s, output: 125.61 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 52.94it/s, est. speed input: 59764.11 toks/s, output: 58.36 toks/s]   
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 45.84it/s, est. speed input: 52487.89 toks/s, output: 51.26 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:02, 42.83it/s, est. speed input: 49508.83 toks/s, output: 48.35 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 40.99it/s, est. speed input: 47763.04 toks/s, output: 46.64 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 39.57it/s, est. speed input: 46426.94 toks/s, output: 45.34 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 38.56it/s, est. speed input: 45396.02 toks/s, output: 44.33 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 37.96it/s, est. speed input: 44727.37 toks/s, output: 43.68 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 37.44it/s, est. speed input: 44143.06 toks/s, output: 43.11 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 36.97it/s, est. speed input: 43618.64 toks/s, output: 42.60 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 36.73it/s, est. speed input: 43194.19 toks/s, output: 42.18 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 36.51it/s, est. speed input: 42809.75 toks/s, output: 41.81 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 36.39it/s, est. speed input: 42477.11 toks/s, output: 41.48 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 36.33it/s, est. speed input: 42187.09 toks/s, output: 41.20 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 36.24it/s, est. speed input: 41916.04 toks/s, output: 40.93 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 36.20it/s, est. speed input: 41675.72 toks/s, output: 40.70 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 36.13it/s, est. speed input: 41451.45 toks/s, output: 40.48 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 36.07it/s, est. speed input: 41243.80 toks/s, output: 40.28 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 36.05it/s, est. speed input: 41058.94 toks/s, output: 40.10 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 36.04it/s, est. speed input: 40889.37 toks/s, output: 39.93 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 36.01it/s, est. speed input: 40728.77 toks/s, output: 39.77 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 36.00it/s, est. speed input: 40582.37 toks/s, output: 39.63 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.97it/s, est. speed input: 40443.62 toks/s, output: 39.50 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.97it/s, est. speed input: 40317.12 toks/s, output: 39.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.97it/s, est. speed input: 40228.86 toks/s, output: 39.29 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.28it/s, est. speed input: 40228.86 toks/s, output: 39.29 toks/s]
[rank0]:[W126 15:19:49.800191584 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 15:19:51
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:19:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=664826) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=664826) WARNING 01-26 15:20:14 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=664826) WARNING 01-26 15:20:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 67.18 requests/s, 68862.03 total tokens/s, 67.18 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 15:19:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:19:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:19:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:19:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:19:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:19:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:19:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:19:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:19:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:19:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:20:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:20:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:20:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:20:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:20:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:20:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:20:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:20:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:20:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:07] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=664826) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=664826) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.31it/s]
(EngineCore_DP0 pid=664826) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.30it/s]
(EngineCore_DP0 pid=664826) 
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=664826) [2026-01-26 15:20:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=664826) 2026-01-26 15:20:23,951 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=664826) 2026-01-26 15:20:23,974 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=664826) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.63it/s]
(EngineCore_DP0 pid=664826) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.80it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:48,  5.29it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:01, 170.06it/s]
Adding requests:  32%|███▏      | 82/256 [00:00<00:00, 264.24it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 317.24it/s]
Adding requests:  65%|██████▍   | 166/256 [00:00<00:00, 350.36it/s]
Adding requests:  81%|████████▏ | 208/256 [00:00<00:00, 371.77it/s]
Adding requests:  99%|█████████▉| 254/256 [00:00<00:00, 397.14it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 319.58it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:00<00:00, 387.33it/s, est. speed input: 396693.92 toks/s, output: 387.35 toks/s]
Processed prompts:  32%|███▏      | 81/256 [00:00<00:01, 115.90it/s, est. speed input: 133205.33 toks/s, output: 130.08 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:00<00:01, 96.67it/s, est. speed input: 113269.85 toks/s, output: 110.61 toks/s]
Processed prompts:  46%|████▌     | 117/256 [00:01<00:01, 91.88it/s, est. speed input: 107801.98 toks/s, output: 105.27 toks/s]
Processed prompts:  50%|█████     | 129/256 [00:01<00:01, 87.31it/s, est. speed input: 103686.31 toks/s, output: 101.26 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:01<00:01, 82.15it/s, est. speed input: 99818.53 toks/s, output: 97.48 toks/s]  
Processed prompts:  59%|█████▊    | 150/256 [00:01<00:01, 80.10it/s, est. speed input: 97705.48 toks/s, output: 95.42 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:01<00:01, 80.68it/s, est. speed input: 96870.85 toks/s, output: 94.60 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:01<00:01, 76.67it/s, est. speed input: 94699.52 toks/s, output: 92.48 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:01<00:01, 76.06it/s, est. speed input: 93645.41 toks/s, output: 91.45 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:02<00:00, 75.56it/s, est. speed input: 92702.70 toks/s, output: 90.53 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:02<00:00, 75.08it/s, est. speed input: 91832.16 toks/s, output: 89.68 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:02<00:00, 74.58it/s, est. speed input: 91018.14 toks/s, output: 88.88 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:02<00:00, 74.37it/s, est. speed input: 90309.21 toks/s, output: 88.19 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:02<00:00, 74.18it/s, est. speed input: 89657.08 toks/s, output: 87.56 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:02<00:00, 74.11it/s, est. speed input: 89071.83 toks/s, output: 86.98 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:02<00:00, 74.04it/s, est. speed input: 88530.09 toks/s, output: 86.45 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:02<00:00, 73.86it/s, est. speed input: 88009.40 toks/s, output: 85.95 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 73.90it/s, est. speed input: 87553.52 toks/s, output: 85.50 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 74.03it/s, est. speed input: 87144.89 toks/s, output: 85.10 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 74.03it/s, est. speed input: 87144.89 toks/s, output: 85.10 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 85.10it/s, est. speed input: 87144.89 toks/s, output: 85.10 toks/s]
[rank0]:[W126 15:20:29.913031385 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 15:20:31
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:20:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=665947) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=665947) WARNING 01-26 15:20:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=665947) WARNING 01-26 15:21:05 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 137.06 requests/s, 140485.82 total tokens/s, 137.06 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 15:20:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:20:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:20:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:20:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:20:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:20:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:20:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:20:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:20:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:20:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:20:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:20:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:20:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:20:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:20:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:20:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:20:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:20:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:20:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=665947) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=665947) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=665947) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=665947) 
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=665947) [2026-01-26 15:20:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=665947) 2026-01-26 15:21:05,305 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=665947) 2026-01-26 15:21:05,328 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=665947) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.45it/s]
(EngineCore_DP0 pid=665947) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.97it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 37/512 [00:00<00:01, 367.38it/s]
Adding requests:  15%|█▍        | 75/512 [00:00<00:01, 372.09it/s]
Adding requests:  23%|██▎       | 118/512 [00:00<00:00, 395.06it/s]
Adding requests:  31%|███▏      | 161/512 [00:00<00:00, 408.30it/s]
Adding requests:  40%|███▉      | 204/512 [00:00<00:00, 414.42it/s]
Adding requests:  49%|████▉     | 250/512 [00:00<00:00, 427.53it/s]
Adding requests:  57%|█████▋    | 294/512 [00:00<00:00, 431.28it/s]
Adding requests:  66%|██████▌   | 338/512 [00:00<00:00, 369.76it/s]
Adding requests:  75%|███████▍  | 382/512 [00:00<00:00, 387.16it/s]
Adding requests:  83%|████████▎ | 427/512 [00:01<00:00, 402.47it/s]
Adding requests:  92%|█████████▏| 472/512 [00:01<00:00, 415.48it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 407.15it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:00<00:00, 1495.33it/s, est. speed input: 1531314.30 toks/s, output: 1495.35 toks/s]
Processed prompts:  62%|██████▎   | 320/512 [00:01<00:00, 245.68it/s, est. speed input: 290234.50 toks/s, output: 283.43 toks/s]   
Processed prompts:  76%|███████▋  | 391/512 [00:01<00:00, 203.91it/s, est. speed input: 245001.08 toks/s, output: 239.26 toks/s]
Processed prompts:  85%|████████▌ | 436/512 [00:01<00:00, 190.25it/s, est. speed input: 230770.85 toks/s, output: 225.36 toks/s]
Processed prompts:  92%|█████████▏| 469/512 [00:02<00:00, 179.73it/s, est. speed input: 221614.78 toks/s, output: 216.42 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [00:02<00:00, 171.88it/s, est. speed input: 215373.90 toks/s, output: 210.33 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 171.88it/s, est. speed input: 211671.52 toks/s, output: 206.71 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 206.70it/s, est. speed input: 211671.52 toks/s, output: 206.71 toks/s]
[rank0]:[W126 15:21:11.605574603 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 15:21:13
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:21:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=667117) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=667117) WARNING 01-26 15:21:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=667117) WARNING 01-26 15:21:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 215.28 requests/s, 220661.99 total tokens/s, 215.28 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 15:21:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:21:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:21:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:21:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:21:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:21:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:21:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:21:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:21:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:21:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:21:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:21:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:21:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:21:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:21:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:21:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:21:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:21:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:21:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:21:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:21:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:21:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:21:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:21:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:21:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:21:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:21:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:21:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:32] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:32] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:32] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:32] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:32] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=667117) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=667117) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=667117) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=667117) 
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=667117) [2026-01-26 15:21:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=667117) 2026-01-26 15:21:50,079 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=667117) 2026-01-26 15:21:50,103 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=667117) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.06it/s]
(EngineCore_DP0 pid=667117) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.32it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 39/1024 [00:00<00:02, 380.39it/s]
Adding requests:   8%|▊         | 84/1024 [00:00<00:02, 419.87it/s]
Adding requests:  13%|█▎        | 129/1024 [00:00<00:02, 433.33it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 437.69it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 443.99it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 455.21it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 451.92it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 454.04it/s]
Adding requests:  40%|███▉      | 406/1024 [00:00<00:01, 455.71it/s]
Adding requests:  44%|████▍     | 453/1024 [00:01<00:01, 458.73it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 458.81it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 455.14it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 463.34it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 465.87it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 471.98it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 472.77it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 470.11it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 459.65it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:01<00:00, 465.26it/s]
Adding requests:  91%|█████████ | 933/1024 [00:02<00:00, 471.43it/s]
Adding requests:  96%|█████████▌| 981/1024 [00:02<00:00, 470.40it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.83it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:00<00:00, 3912.70it/s, est. speed input: 4006842.60 toks/s, output: 3912.76 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:01<00:00, 394.43it/s, est. speed input: 475093.59 toks/s, output: 463.96 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 394.43it/s, est. speed input: 414722.17 toks/s, output: 405.00 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 404.98it/s, est. speed input: 414722.17 toks/s, output: 405.00 toks/s]
[rank0]:[W126 15:21:57.654430322 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 15:21:59
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:22:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=668360) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=668360) WARNING 01-26 15:22:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=668360) WARNING 01-26 15:22:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 237.47 requests/s, 243408.05 total tokens/s, 237.47 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 15:22:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:22:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:22:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:22:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:22:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:22:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:22:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:22:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:22:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:22:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:22:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:22:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:22:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:22:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:22:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:22:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:22:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:22:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:22:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:22:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:22:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:22:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:22:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:22:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:22:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:22:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:22:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:22:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=668360) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=668360) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=668360) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=668360) 
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=668360) [2026-01-26 15:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=668360) 2026-01-26 15:22:40,350 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=668360) 2026-01-26 15:22:40,374 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=668360) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 17.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 16.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.19it/s]
(EngineCore_DP0 pid=668360) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 15.92it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 17.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.35it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 382.35it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 418.70it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 431.59it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 435.69it/s]
Adding requests:  11%|█         | 221/2048 [00:00<00:04, 444.65it/s]
Adding requests:  13%|█▎        | 269/2048 [00:00<00:03, 455.06it/s]
Adding requests:  15%|█▌        | 315/2048 [00:00<00:03, 451.96it/s]
Adding requests:  18%|█▊        | 362/2048 [00:00<00:03, 456.23it/s]
Adding requests:  20%|██        | 410/2048 [00:00<00:03, 460.51it/s]
Adding requests:  22%|██▏       | 457/2048 [00:01<00:03, 462.44it/s]
Adding requests:  25%|██▍       | 504/2048 [00:01<00:03, 462.32it/s]
Adding requests:  27%|██▋       | 551/2048 [00:01<00:03, 456.52it/s]
Adding requests:  29%|██▉       | 599/2048 [00:01<00:03, 462.76it/s]
Adding requests:  32%|███▏      | 646/2048 [00:01<00:03, 462.36it/s]
Adding requests:  34%|███▍      | 695/2048 [00:01<00:02, 469.74it/s]
Adding requests:  36%|███▌      | 742/2048 [00:01<00:02, 468.88it/s]
Adding requests:  39%|███▊      | 789/2048 [00:01<00:02, 468.49it/s]
Adding requests:  41%|████      | 836/2048 [00:01<00:02, 458.08it/s]
Adding requests:  43%|████▎     | 884/2048 [00:01<00:02, 464.10it/s]
Adding requests:  46%|████▌     | 933/2048 [00:02<00:02, 470.60it/s]
Adding requests:  48%|████▊     | 981/2048 [00:02<00:02, 470.03it/s]
Adding requests:  50%|█████     | 1030/2048 [00:02<00:02, 474.20it/s]
Adding requests:  53%|█████▎    | 1078/2048 [00:02<00:02, 468.10it/s]
Adding requests:  55%|█████▍    | 1125/2048 [00:02<00:01, 466.17it/s]
Adding requests:  57%|█████▋    | 1173/2048 [00:02<00:01, 466.71it/s]
Adding requests:  60%|█████▉    | 1223/2048 [00:02<00:01, 474.49it/s]
Adding requests:  62%|██████▏   | 1271/2048 [00:02<00:01, 469.25it/s]
Adding requests:  64%|██████▍   | 1320/2048 [00:02<00:01, 472.41it/s]
Adding requests:  67%|██████▋   | 1368/2048 [00:02<00:01, 474.38it/s]
Adding requests:  69%|██████▉   | 1417/2048 [00:03<00:01, 475.73it/s]
Adding requests:  72%|███████▏  | 1466/2048 [00:03<00:01, 477.47it/s]
Adding requests:  74%|███████▍  | 1515/2048 [00:03<00:01, 480.63it/s]
Adding requests:  76%|███████▋  | 1564/2048 [00:03<00:01, 479.56it/s]
Adding requests:  79%|███████▉  | 1613/2048 [00:03<00:00, 481.99it/s]
Adding requests:  81%|████████  | 1662/2048 [00:03<00:00, 470.89it/s]
Adding requests:  83%|████████▎ | 1710/2048 [00:03<00:00, 461.28it/s]
Adding requests:  86%|████████▌ | 1758/2048 [00:03<00:00, 464.68it/s]
Adding requests:  88%|████████▊ | 1805/2048 [00:03<00:00, 464.34it/s]
Adding requests:  90%|█████████ | 1853/2048 [00:03<00:00, 467.76it/s]
Adding requests:  93%|█████████▎| 1901/2048 [00:04<00:00, 470.26it/s]
Adding requests:  95%|█████████▌| 1949/2048 [00:04<00:00, 450.32it/s]
Adding requests:  97%|█████████▋| 1996/2048 [00:04<00:00, 453.96it/s]
Adding requests: 100%|█████████▉| 2044/2048 [00:04<00:00, 460.71it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 463.61it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:00<00:00, 6497.14it/s, est. speed input: 6653384.62 toks/s, output: 6497.23 toks/s]
Processed prompts:  83%|████████▎ | 1708/2048 [00:02<00:00, 502.88it/s, est. speed input: 621509.19 toks/s, output: 606.94 toks/s]   
Processed prompts:  97%|█████████▋| 1990/2048 [00:04<00:00, 401.93it/s, est. speed input: 509199.38 toks/s, output: 497.26 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:04<00:00, 401.93it/s, est. speed input: 498704.13 toks/s, output: 487.02 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:04<00:00, 487.00it/s, est. speed input: 498704.13 toks/s, output: 487.02 toks/s]
[rank0]:[W126 15:22:51.818409258 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 15:22:53
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:23:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=669792) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=669792) WARNING 01-26 15:23:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=669792) WARNING 01-26 15:23:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 246.69 requests/s, 252854.43 total tokens/s, 246.69 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 15:23:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:23:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:23:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:23:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:23:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:23:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:23:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:23:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:23:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:23:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:23:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:23:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:23:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:23:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:23:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:23:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:23:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:23:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:23:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:23:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:23:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:23:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:23:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:23:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:23:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:23:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:23:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:23:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=669792) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=669792) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]
(EngineCore_DP0 pid=669792) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]
(EngineCore_DP0 pid=669792) 
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=669792) [2026-01-26 15:23:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=669792) [rank0]:W0126 15:23:39.057000 669792 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=669792) [rank0]:W0126 15:23:39.141000 669792 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=669792) [rank0]:W0126 15:23:40.052000 669792 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=669792) [rank0]:W0126 15:23:40.174000 669792 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=669792) 2026-01-26 15:23:43,557 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=669792) 2026-01-26 15:23:43,582 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=669792) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 16.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 16.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.00it/s]
(EngineCore_DP0 pid=669792) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.75it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.88it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.99it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 39/4096 [00:00<00:10, 384.46it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 421.65it/s]
Adding requests:   3%|▎         | 127/4096 [00:00<00:09, 408.52it/s]
Adding requests:   4%|▍         | 172/4096 [00:00<00:09, 421.09it/s]
Adding requests:   5%|▌         | 218/4096 [00:00<00:08, 432.79it/s]
Adding requests:   7%|▋         | 267/4096 [00:00<00:08, 448.67it/s]
Adding requests:   8%|▊         | 312/4096 [00:00<00:08, 449.00it/s]
Adding requests:   9%|▉         | 359/4096 [00:00<00:08, 454.20it/s]
Adding requests:  10%|▉         | 407/4096 [00:00<00:08, 460.45it/s]
Adding requests:  11%|█         | 454/4096 [00:01<00:07, 462.90it/s]
Adding requests:  12%|█▏        | 501/4096 [00:01<00:07, 462.72it/s]
Adding requests:  13%|█▎        | 548/4096 [00:01<00:07, 458.22it/s]
Adding requests:  15%|█▍        | 595/4096 [00:01<00:07, 459.13it/s]
Adding requests:  16%|█▌        | 643/4096 [00:01<00:07, 464.19it/s]
Adding requests:  17%|█▋        | 692/4096 [00:01<00:07, 471.79it/s]
Adding requests:  18%|█▊        | 740/4096 [00:01<00:07, 473.32it/s]
Adding requests:  19%|█▉        | 788/4096 [00:01<00:07, 471.84it/s]
Adding requests:  20%|██        | 836/4096 [00:01<00:07, 461.46it/s]
Adding requests:  22%|██▏       | 885/4096 [00:01<00:06, 468.13it/s]
Adding requests:  23%|██▎       | 934/4096 [00:02<00:06, 473.17it/s]
Adding requests:  24%|██▍       | 982/4096 [00:02<00:06, 463.84it/s]
Adding requests:  25%|██▌       | 1031/4096 [00:02<00:06, 468.72it/s]
Adding requests:  26%|██▋       | 1078/4096 [00:02<00:06, 448.57it/s]
Adding requests:  27%|██▋       | 1124/4096 [00:02<00:06, 448.57it/s]
Adding requests:  29%|██▊       | 1174/4096 [00:02<00:06, 460.50it/s]
Adding requests:  30%|██▉       | 1224/4096 [00:02<00:06, 469.45it/s]
Adding requests:  31%|███       | 1272/4096 [00:02<00:06, 462.23it/s]
Adding requests:  32%|███▏      | 1320/4096 [00:02<00:05, 467.08it/s]
Adding requests:  33%|███▎      | 1369/4096 [00:02<00:05, 472.36it/s]
Adding requests:  35%|███▍      | 1417/4096 [00:03<00:05, 457.89it/s]
Adding requests:  36%|███▌      | 1466/4096 [00:03<00:05, 464.51it/s]
Adding requests:  37%|███▋      | 1515/4096 [00:03<00:05, 470.29it/s]
Adding requests:  38%|███▊      | 1564/4096 [00:03<00:05, 474.04it/s]
Adding requests:  39%|███▉      | 1613/4096 [00:03<00:05, 478.28it/s]
Adding requests:  41%|████      | 1662/4096 [00:03<00:05, 478.66it/s]
Adding requests:  42%|████▏     | 1710/4096 [00:03<00:04, 478.43it/s]
Adding requests:  43%|████▎     | 1758/4096 [00:03<00:04, 477.77it/s]
Adding requests:  44%|████▍     | 1807/4096 [00:03<00:04, 479.05it/s]
Adding requests:  45%|████▌     | 1855/4096 [00:04<00:04, 478.99it/s]
Adding requests:  46%|████▋     | 1903/4096 [00:04<00:04, 478.31it/s]
Adding requests:  48%|████▊     | 1951/4096 [00:04<00:04, 477.63it/s]
Adding requests:  49%|████▉     | 2000/4096 [00:04<00:04, 479.93it/s]
Adding requests:  50%|█████     | 2048/4096 [00:04<00:04, 467.99it/s]
Adding requests:  51%|█████     | 2097/4096 [00:04<00:04, 473.41it/s]
Adding requests:  52%|█████▏    | 2145/4096 [00:04<00:04, 470.96it/s]
Adding requests:  54%|█████▎    | 2193/4096 [00:04<00:04, 468.44it/s]
Adding requests:  55%|█████▍    | 2243/4096 [00:04<00:03, 474.51it/s]
Adding requests:  56%|█████▌    | 2291/4096 [00:04<00:03, 474.22it/s]
Adding requests:  57%|█████▋    | 2339/4096 [00:05<00:03, 474.65it/s]
Adding requests:  58%|█████▊    | 2388/4096 [00:05<00:03, 476.65it/s]
Adding requests:  59%|█████▉    | 2437/4096 [00:05<00:03, 477.82it/s]
Adding requests:  61%|██████    | 2485/4096 [00:05<00:03, 474.83it/s]
Adding requests:  62%|██████▏   | 2533/4096 [00:05<00:03, 476.00it/s]
Adding requests:  63%|██████▎   | 2582/4096 [00:05<00:03, 479.18it/s]
Adding requests:  64%|██████▍   | 2630/4096 [00:05<00:03, 478.60it/s]
Adding requests:  65%|██████▌   | 2678/4096 [00:05<00:02, 478.29it/s]
Adding requests:  67%|██████▋   | 2726/4096 [00:05<00:02, 478.26it/s]
Adding requests:  68%|██████▊   | 2774/4096 [00:05<00:02, 478.70it/s]
Adding requests:  69%|██████▉   | 2822/4096 [00:06<00:02, 471.55it/s]
Adding requests:  70%|███████   | 2871/4096 [00:06<00:02, 475.98it/s]
Adding requests:  71%|███████▏  | 2919/4096 [00:06<00:02, 475.49it/s]
Adding requests:  72%|███████▏  | 2967/4096 [00:06<00:02, 473.93it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:06<00:02, 475.51it/s]
Adding requests:  75%|███████▍  | 3063/4096 [00:06<00:02, 474.94it/s]
Adding requests:  76%|███████▌  | 3111/4096 [00:06<00:02, 475.12it/s]
Adding requests:  77%|███████▋  | 3160/4096 [00:06<00:01, 476.54it/s]
Adding requests:  78%|███████▊  | 3208/4096 [00:06<00:01, 475.11it/s]
Adding requests:  80%|███████▉  | 3258/4096 [00:06<00:01, 478.44it/s]
Adding requests:  81%|████████  | 3307/4096 [00:07<00:01, 480.56it/s]
Adding requests:  82%|████████▏ | 3356/4096 [00:07<00:01, 467.93it/s]
Adding requests:  83%|████████▎ | 3404/4096 [00:07<00:01, 469.38it/s]
Adding requests:  84%|████████▍ | 3451/4096 [00:07<00:01, 469.25it/s]
Adding requests:  85%|████████▌ | 3498/4096 [00:07<00:01, 467.36it/s]
Adding requests:  87%|████████▋ | 3546/4096 [00:07<00:01, 470.51it/s]
Adding requests:  88%|████████▊ | 3594/4096 [00:07<00:01, 471.80it/s]
Adding requests:  89%|████████▉ | 3642/4096 [00:07<00:00, 467.73it/s]
Adding requests:  90%|█████████ | 3691/4096 [00:07<00:00, 472.99it/s]
Adding requests:  91%|█████████▏| 3739/4096 [00:07<00:00, 472.63it/s]
Adding requests:  93%|█████████▎| 3789/4096 [00:08<00:00, 479.47it/s]
Adding requests:  94%|█████████▎| 3837/4096 [00:08<00:00, 477.90it/s]
Adding requests:  95%|█████████▍| 3886/4096 [00:08<00:00, 480.71it/s]
Adding requests:  96%|█████████▌| 3935/4096 [00:08<00:00, 479.26it/s]
Adding requests:  97%|█████████▋| 3983/4096 [00:08<00:00, 460.38it/s]
Adding requests:  98%|█████████▊| 4030/4096 [00:08<00:00, 462.78it/s]
Adding requests: 100%|█████████▉| 4077/4096 [00:08<00:00, 462.47it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 468.46it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  53%|█████▎    | 2164/4096 [00:00<00:00, 11115.16it/s, est. speed input: 11382426.81 toks/s, output: 11115.30 toks/s]
Processed prompts:  80%|███████▉  | 3276/4096 [00:04<00:01, 582.97it/s, est. speed input: 734976.23 toks/s, output: 717.75 toks/s]      
Processed prompts:  92%|█████████▏| 3749/4096 [00:06<00:00, 454.49it/s, est. speed input: 590913.55 toks/s, output: 577.06 toks/s]
Processed prompts:  98%|█████████▊| 4018/4096 [00:07<00:00, 411.06it/s, est. speed input: 546601.85 toks/s, output: 533.79 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:07<00:00, 411.06it/s, est. speed input: 533745.35 toks/s, output: 521.24 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:07<00:00, 521.23it/s, est. speed input: 533745.35 toks/s, output: 521.24 toks/s]
[rank0]:[W126 15:24:03.508899939 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 15:24:05
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:24:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=671576) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=671576) WARNING 01-26 15:25:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=671576) WARNING 01-26 15:25:14 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 248.55 requests/s, 254763.15 total tokens/s, 248.55 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 15:24:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:24:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:24:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:24:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:24:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:24:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:24:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:24:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:24:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:24:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:24:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:24:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:24:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:24:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:24:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:24:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:24:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:24:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:24:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:24:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:24:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:24:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:24:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:24:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:24:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:24:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:24:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:24:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=671576) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=671576) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]
(EngineCore_DP0 pid=671576) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]
(EngineCore_DP0 pid=671576) 
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=671576) [2026-01-26 15:24:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=671576) [rank0]:W0126 15:25:09.839000 671576 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=671576) [rank0]:W0126 15:25:09.927000 671576 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=671576) [rank0]:W0126 15:25:10.834000 671576 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=671576) [rank0]:W0126 15:25:10.955000 671576 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=671576) 2026-01-26 15:25:14,365 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=671576) 2026-01-26 15:25:14,392 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=671576) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:05,  3.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  6.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01, 10.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00, 10.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00, 12.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 13.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 14.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 15.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 11.06it/s]
(EngineCore_DP0 pid=671576) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 17.65it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.97it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 18.02it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 17.26it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 17.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 17.34it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 38/8192 [00:00<00:21, 375.01it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:19, 409.23it/s]
Adding requests:   2%|▏         | 126/8192 [00:00<00:19, 422.36it/s]
Adding requests:   2%|▏         | 170/8192 [00:00<00:18, 427.81it/s]
Adding requests:   3%|▎         | 215/8192 [00:00<00:18, 433.48it/s]
Adding requests:   3%|▎         | 262/8192 [00:00<00:17, 445.52it/s]
Adding requests:   4%|▎         | 307/8192 [00:00<00:17, 445.51it/s]
Adding requests:   4%|▍         | 354/8192 [00:00<00:17, 451.81it/s]
Adding requests:   5%|▍         | 401/8192 [00:00<00:17, 454.86it/s]
Adding requests:   5%|▌         | 448/8192 [00:01<00:16, 457.61it/s]
Adding requests:   6%|▌         | 494/8192 [00:01<00:16, 455.21it/s]
Adding requests:   7%|▋         | 540/8192 [00:01<00:17, 447.90it/s]
Adding requests:   7%|▋         | 586/8192 [00:01<00:16, 449.20it/s]
Adding requests:   8%|▊         | 631/8192 [00:01<00:16, 446.43it/s]
Adding requests:   8%|▊         | 676/8192 [00:01<00:16, 445.66it/s]
Adding requests:   9%|▉         | 721/8192 [00:01<00:16, 445.93it/s]
Adding requests:   9%|▉         | 766/8192 [00:01<00:16, 442.25it/s]
Adding requests:  10%|▉         | 811/8192 [00:01<00:17, 414.44it/s]
Adding requests:  10%|█         | 855/8192 [00:01<00:17, 421.46it/s]
Adding requests:  11%|█         | 903/8192 [00:02<00:16, 438.02it/s]
Adding requests:  12%|█▏        | 949/8192 [00:02<00:16, 441.34it/s]
Adding requests:  12%|█▏        | 997/8192 [00:02<00:15, 450.00it/s]
Adding requests:  13%|█▎        | 1044/8192 [00:02<00:15, 455.51it/s]
Adding requests:  13%|█▎        | 1090/8192 [00:02<00:15, 456.25it/s]
Adding requests:  14%|█▍        | 1136/8192 [00:02<00:15, 453.86it/s]
Adding requests:  14%|█▍        | 1186/8192 [00:02<00:15, 465.22it/s]
Adding requests:  15%|█▌        | 1234/8192 [00:02<00:14, 469.48it/s]
Adding requests:  16%|█▌        | 1281/8192 [00:02<00:14, 465.52it/s]
Adding requests:  16%|█▌        | 1329/8192 [00:02<00:14, 468.41it/s]
Adding requests:  17%|█▋        | 1377/8192 [00:03<00:14, 471.81it/s]
Adding requests:  17%|█▋        | 1425/8192 [00:03<00:14, 472.76it/s]
Adding requests:  18%|█▊        | 1474/8192 [00:03<00:14, 475.61it/s]
Adding requests:  19%|█▊        | 1522/8192 [00:03<00:14, 475.16it/s]
Adding requests:  19%|█▉        | 1570/8192 [00:03<00:14, 466.41it/s]
Adding requests:  20%|█▉        | 1619/8192 [00:03<00:13, 472.71it/s]
Adding requests:  20%|██        | 1667/8192 [00:03<00:13, 471.83it/s]
Adding requests:  21%|██        | 1716/8192 [00:03<00:13, 475.01it/s]
Adding requests:  22%|██▏       | 1764/8192 [00:03<00:13, 473.70it/s]
Adding requests:  22%|██▏       | 1812/8192 [00:03<00:13, 460.52it/s]
Adding requests:  23%|██▎       | 1860/8192 [00:04<00:13, 461.50it/s]
Adding requests:  23%|██▎       | 1908/8192 [00:04<00:13, 464.50it/s]
Adding requests:  24%|██▍       | 1955/8192 [00:04<00:13, 466.06it/s]
Adding requests:  24%|██▍       | 2004/8192 [00:04<00:13, 472.08it/s]
Adding requests:  25%|██▌       | 2052/8192 [00:04<00:12, 472.64it/s]
Adding requests:  26%|██▌       | 2101/8192 [00:04<00:12, 476.52it/s]
Adding requests:  26%|██▌       | 2149/8192 [00:04<00:12, 470.48it/s]
Adding requests:  27%|██▋       | 2197/8192 [00:04<00:12, 467.94it/s]
Adding requests:  27%|██▋       | 2245/8192 [00:04<00:12, 471.45it/s]
Adding requests:  28%|██▊       | 2293/8192 [00:05<00:12, 471.56it/s]
Adding requests:  29%|██▊       | 2341/8192 [00:05<00:12, 472.37it/s]
Adding requests:  29%|██▉       | 2389/8192 [00:05<00:12, 472.86it/s]
Adding requests:  30%|██▉       | 2437/8192 [00:05<00:12, 473.79it/s]
Adding requests:  30%|███       | 2485/8192 [00:05<00:12, 473.45it/s]
Adding requests:  31%|███       | 2533/8192 [00:05<00:11, 473.50it/s]
Adding requests:  32%|███▏      | 2582/8192 [00:05<00:11, 475.72it/s]
Adding requests:  32%|███▏      | 2630/8192 [00:05<00:11, 474.57it/s]
Adding requests:  33%|███▎      | 2678/8192 [00:05<00:11, 474.38it/s]
Adding requests:  33%|███▎      | 2726/8192 [00:05<00:11, 474.02it/s]
Adding requests:  34%|███▍      | 2774/8192 [00:06<00:11, 461.71it/s]
Adding requests:  34%|███▍      | 2821/8192 [00:06<00:11, 458.52it/s]
Adding requests:  35%|███▍      | 2867/8192 [00:06<00:11, 449.89it/s]
Adding requests:  36%|███▌      | 2914/8192 [00:06<00:11, 455.22it/s]
Adding requests:  36%|███▌      | 2960/8192 [00:06<00:11, 456.27it/s]
Adding requests:  37%|███▋      | 3007/8192 [00:06<00:11, 458.42it/s]
Adding requests:  37%|███▋      | 3055/8192 [00:06<00:11, 462.52it/s]
Adding requests:  38%|███▊      | 3102/8192 [00:06<00:11, 461.46it/s]
Adding requests:  38%|███▊      | 3149/8192 [00:06<00:10, 462.50it/s]
Adding requests:  39%|███▉      | 3196/8192 [00:06<00:10, 464.28it/s]
Adding requests:  40%|███▉      | 3244/8192 [00:07<00:10, 467.82it/s]
Adding requests:  40%|████      | 3292/8192 [00:07<00:10, 470.41it/s]
Adding requests:  41%|████      | 3340/8192 [00:07<00:10, 460.93it/s]
Adding requests:  41%|████▏     | 3388/8192 [00:07<00:10, 464.88it/s]
Adding requests:  42%|████▏     | 3436/8192 [00:07<00:10, 468.32it/s]
Adding requests:  43%|████▎     | 3483/8192 [00:07<00:10, 459.91it/s]
Adding requests:  43%|████▎     | 3530/8192 [00:07<00:10, 449.94it/s]
Adding requests:  44%|████▎     | 3578/8192 [00:07<00:10, 457.54it/s]
Adding requests:  44%|████▍     | 3624/8192 [00:07<00:09, 457.34it/s]
Adding requests:  45%|████▍     | 3671/8192 [00:07<00:09, 461.05it/s]
Adding requests:  45%|████▌     | 3718/8192 [00:08<00:09, 461.92it/s]
Adding requests:  46%|████▌     | 3766/8192 [00:08<00:09, 467.20it/s]
Adding requests:  47%|████▋     | 3815/8192 [00:08<00:09, 473.15it/s]
Adding requests:  47%|████▋     | 3864/8192 [00:08<00:09, 476.08it/s]
Adding requests:  48%|████▊     | 3912/8192 [00:08<00:09, 475.16it/s]
Adding requests:  48%|████▊     | 3960/8192 [00:08<00:08, 475.34it/s]
Adding requests:  49%|████▉     | 4008/8192 [00:08<00:08, 474.17it/s]
Adding requests:  50%|████▉     | 4056/8192 [00:08<00:09, 451.66it/s]
Adding requests:  50%|█████     | 4104/8192 [00:08<00:08, 459.26it/s]
Adding requests:  51%|█████     | 4151/8192 [00:09<00:08, 461.80it/s]
Adding requests:  51%|█████▏    | 4201/8192 [00:09<00:08, 469.87it/s]
Adding requests:  52%|█████▏    | 4249/8192 [00:09<00:08, 471.68it/s]
Adding requests:  52%|█████▏    | 4297/8192 [00:09<00:08, 469.52it/s]
Adding requests:  53%|█████▎    | 4347/8192 [00:09<00:08, 476.37it/s]
Adding requests:  54%|█████▎    | 4396/8192 [00:09<00:07, 478.07it/s]
Adding requests:  54%|█████▍    | 4444/8192 [00:09<00:07, 477.93it/s]
Adding requests:  55%|█████▍    | 4492/8192 [00:09<00:07, 471.59it/s]
Adding requests:  55%|█████▌    | 4540/8192 [00:09<00:07, 470.13it/s]
Adding requests:  56%|█████▌    | 4588/8192 [00:09<00:07, 470.55it/s]
Adding requests:  57%|█████▋    | 4636/8192 [00:10<00:07, 473.08it/s]
Adding requests:  57%|█████▋    | 4684/8192 [00:10<00:07, 470.03it/s]
Adding requests:  58%|█████▊    | 4733/8192 [00:10<00:07, 474.38it/s]
Adding requests:  58%|█████▊    | 4781/8192 [00:10<00:07, 471.88it/s]
Adding requests:  59%|█████▉    | 4829/8192 [00:10<00:07, 472.32it/s]
Adding requests:  60%|█████▉    | 4877/8192 [00:10<00:07, 470.59it/s]
Adding requests:  60%|██████    | 4925/8192 [00:10<00:06, 471.07it/s]
Adding requests:  61%|██████    | 4973/8192 [00:10<00:06, 472.00it/s]
Adding requests:  61%|██████▏   | 5021/8192 [00:10<00:06, 471.51it/s]
Adding requests:  62%|██████▏   | 5069/8192 [00:10<00:06, 472.90it/s]
Adding requests:  62%|██████▏   | 5118/8192 [00:11<00:06, 476.51it/s]
Adding requests:  63%|██████▎   | 5166/8192 [00:11<00:06, 475.55it/s]
Adding requests:  64%|██████▎   | 5214/8192 [00:11<00:06, 474.38it/s]
Adding requests:  64%|██████▍   | 5262/8192 [00:11<00:06, 469.82it/s]
Adding requests:  65%|██████▍   | 5309/8192 [00:11<00:06, 460.70it/s]
Adding requests:  65%|██████▌   | 5357/8192 [00:11<00:06, 465.36it/s]
Adding requests:  66%|██████▌   | 5404/8192 [00:11<00:05, 466.41it/s]
Adding requests:  67%|██████▋   | 5451/8192 [00:11<00:05, 464.03it/s]
Adding requests:  67%|██████▋   | 5498/8192 [00:11<00:05, 461.92it/s]
Adding requests:  68%|██████▊   | 5545/8192 [00:11<00:05, 461.87it/s]
Adding requests:  68%|██████▊   | 5592/8192 [00:12<00:05, 462.91it/s]
Adding requests:  69%|██████▉   | 5639/8192 [00:12<00:05, 463.50it/s]
Adding requests:  69%|██████▉   | 5686/8192 [00:12<00:05, 461.56it/s]
Adding requests:  70%|██████▉   | 5734/8192 [00:12<00:05, 464.49it/s]
Adding requests:  71%|███████   | 5782/8192 [00:12<00:05, 467.11it/s]
Adding requests:  71%|███████   | 5829/8192 [00:12<00:05, 463.43it/s]
Adding requests:  72%|███████▏  | 5877/8192 [00:12<00:04, 466.50it/s]
Adding requests:  72%|███████▏  | 5925/8192 [00:12<00:04, 469.18it/s]
Adding requests:  73%|███████▎  | 5972/8192 [00:12<00:04, 468.90it/s]
Adding requests:  73%|███████▎  | 6020/8192 [00:12<00:04, 472.13it/s]
Adding requests:  74%|███████▍  | 6069/8192 [00:13<00:04, 475.31it/s]
Adding requests:  75%|███████▍  | 6117/8192 [00:13<00:04, 472.93it/s]
Adding requests:  75%|███████▌  | 6165/8192 [00:13<00:04, 469.43it/s]
Adding requests:  76%|███████▌  | 6215/8192 [00:13<00:04, 477.39it/s]
Adding requests:  76%|███████▋  | 6264/8192 [00:13<00:04, 480.16it/s]
Adding requests:  77%|███████▋  | 6313/8192 [00:13<00:03, 481.44it/s]
Adding requests:  78%|███████▊  | 6362/8192 [00:13<00:03, 481.74it/s]
Adding requests:  78%|███████▊  | 6411/8192 [00:13<00:03, 482.21it/s]
Adding requests:  79%|███████▉  | 6461/8192 [00:13<00:03, 484.60it/s]
Adding requests:  79%|███████▉  | 6511/8192 [00:14<00:03, 486.08it/s]
Adding requests:  80%|████████  | 6560/8192 [00:14<00:03, 484.98it/s]
Adding requests:  81%|████████  | 6609/8192 [00:14<00:03, 468.26it/s]
Adding requests:  81%|████████▏ | 6658/8192 [00:14<00:03, 472.79it/s]
Adding requests:  82%|████████▏ | 6706/8192 [00:14<00:03, 472.54it/s]
Adding requests:  82%|████████▏ | 6754/8192 [00:14<00:03, 471.20it/s]
Adding requests:  83%|████████▎ | 6803/8192 [00:14<00:02, 476.60it/s]
Adding requests:  84%|████████▎ | 6851/8192 [00:14<00:02, 467.22it/s]
Adding requests:  84%|████████▍ | 6900/8192 [00:14<00:02, 471.85it/s]
Adding requests:  85%|████████▍ | 6950/8192 [00:14<00:02, 478.05it/s]
Adding requests:  85%|████████▌ | 6998/8192 [00:15<00:02, 463.00it/s]
Adding requests:  86%|████████▌ | 7045/8192 [00:15<00:02, 463.93it/s]
Adding requests:  87%|████████▋ | 7093/8192 [00:15<00:02, 466.98it/s]
Adding requests:  87%|████████▋ | 7141/8192 [00:15<00:02, 468.11it/s]
Adding requests:  88%|████████▊ | 7188/8192 [00:15<00:02, 467.96it/s]
Adding requests:  88%|████████▊ | 7235/8192 [00:15<00:02, 468.20it/s]
Adding requests:  89%|████████▉ | 7284/8192 [00:15<00:01, 474.56it/s]
Adding requests:  90%|████████▉ | 7333/8192 [00:15<00:01, 476.84it/s]
Adding requests:  90%|█████████ | 7381/8192 [00:15<00:01, 475.18it/s]
Adding requests:  91%|█████████ | 7431/8192 [00:15<00:01, 481.44it/s]
Adding requests:  91%|█████████▏| 7480/8192 [00:16<00:01, 480.70it/s]
Adding requests:  92%|█████████▏| 7529/8192 [00:16<00:01, 479.47it/s]
Adding requests:  92%|█████████▏| 7577/8192 [00:16<00:01, 477.58it/s]
Adding requests:  93%|█████████▎| 7625/8192 [00:16<00:01, 473.48it/s]
Adding requests:  94%|█████████▎| 7675/8192 [00:16<00:01, 479.24it/s]
Adding requests:  94%|█████████▍| 7723/8192 [00:16<00:00, 479.08it/s]
Adding requests:  95%|█████████▍| 7771/8192 [00:16<00:00, 474.89it/s]
Adding requests:  95%|█████████▌| 7819/8192 [00:16<00:00, 474.19it/s]
Adding requests:  96%|█████████▌| 7867/8192 [00:16<00:00, 461.58it/s]
Adding requests:  97%|█████████▋| 7914/8192 [00:16<00:00, 460.61it/s]
Adding requests:  97%|█████████▋| 7961/8192 [00:17<00:00, 462.59it/s]
Adding requests:  98%|█████████▊| 8008/8192 [00:17<00:00, 463.17it/s]
Adding requests:  98%|█████████▊| 8055/8192 [00:17<00:00, 463.38it/s]
Adding requests:  99%|█████████▉| 8102/8192 [00:17<00:00, 462.02it/s]
Adding requests:  99%|█████████▉| 8149/8192 [00:17<00:00, 460.63it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 465.84it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  53%|█████▎    | 4340/8192 [00:00<00:00, 21698.44it/s, est. speed input: 22220185.47 toks/s, output: 21698.70 toks/s]
Processed prompts:  79%|███████▉  | 6510/8192 [00:08<00:02, 607.22it/s, est. speed input: 771840.31 toks/s, output: 753.75 toks/s]      
Processed prompts:  91%|█████████ | 7419/8192 [00:12<00:01, 461.78it/s, est. speed input: 607948.56 toks/s, output: 593.70 toks/s]
Processed prompts:  97%|█████████▋| 7924/8192 [00:14<00:00, 412.74it/s, est. speed input: 558055.50 toks/s, output: 544.98 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:15<00:00, 412.74it/s, est. speed input: 545771.23 toks/s, output: 532.98 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:15<00:00, 532.98it/s, est. speed input: 545771.23 toks/s, output: 532.98 toks/s]
[rank0]:[W126 15:25:52.331106544 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 16:35:43
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:35:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=772167) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=772167) WARNING 01-26 16:36:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=772167) WARNING 01-26 16:36:16 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 38.97 requests/s, 19992.58 total tokens/s, 38.97 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 16:35:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:35:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:35:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:35:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:35:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:35:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:35:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:35:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:35:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:35:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:35:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:35:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=772167) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=772167) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.13it/s]
(EngineCore_DP0 pid=772167) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.11s/it]
(EngineCore_DP0 pid=772167) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]
(EngineCore_DP0 pid=772167) 
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=772167) 2026-01-26 16:36:16,292 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=772167) 2026-01-26 16:36:16,315 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=772167) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.33it/s]
(EngineCore_DP0 pid=772167) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.34it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 513.17it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 561.17it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 559.44it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:01, 65.17it/s, est. speed input: 33367.98 toks/s, output: 65.17 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 48.91it/s, est. speed input: 26083.87 toks/s, output: 50.94 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 45.26it/s, est. speed input: 24350.33 toks/s, output: 47.56 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 43.81it/s, est. speed input: 23632.80 toks/s, output: 46.16 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 42.90it/s, est. speed input: 23169.35 toks/s, output: 45.25 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 42.26it/s, est. speed input: 22832.01 toks/s, output: 44.59 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:02, 41.90it/s, est. speed input: 22598.31 toks/s, output: 44.14 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:01, 41.58it/s, est. speed input: 22404.94 toks/s, output: 43.76 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 41.38it/s, est. speed input: 22253.59 toks/s, output: 43.46 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 41.13it/s, est. speed input: 22112.24 toks/s, output: 43.19 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 41.06it/s, est. speed input: 22010.85 toks/s, output: 42.99 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 41.02it/s, est. speed input: 21926.95 toks/s, output: 42.83 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 41.00it/s, est. speed input: 21857.40 toks/s, output: 42.69 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 40.98it/s, est. speed input: 21795.70 toks/s, output: 42.57 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 40.96it/s, est. speed input: 21740.98 toks/s, output: 42.46 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 40.99it/s, est. speed input: 21697.68 toks/s, output: 42.38 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 40.98it/s, est. speed input: 21656.69 toks/s, output: 42.30 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.94it/s, est. speed input: 21616.38 toks/s, output: 42.22 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.95it/s, est. speed input: 21583.65 toks/s, output: 42.16 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.97it/s, est. speed input: 21555.58 toks/s, output: 42.10 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.94it/s, est. speed input: 21526.32 toks/s, output: 42.04 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.93it/s, est. speed input: 21500.96 toks/s, output: 41.99 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 40.97it/s, est. speed input: 21480.67 toks/s, output: 41.95 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 40.99it/s, est. speed input: 21461.74 toks/s, output: 41.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.99it/s, est. speed input: 21454.62 toks/s, output: 41.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.90it/s, est. speed input: 21454.62 toks/s, output: 41.90 toks/s]
[rank0]:[W126 16:36:21.116924268 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 16:36:23
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:36:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=773356) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=773356) WARNING 01-26 16:36:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=773356) WARNING 01-26 16:36:57 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.78 requests/s, 40776.04 total tokens/s, 39.78 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 16:36:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:36:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:36:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:36:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:36:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:36:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:36:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:36:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:36:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:36:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:36:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:36:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:36:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:36:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:36:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:36:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=773356) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=773356) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.21it/s]
(EngineCore_DP0 pid=773356) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.02s/it]
(EngineCore_DP0 pid=773356) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=773356) 
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=773356) 2026-01-26 16:36:57,102 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=773356) 2026-01-26 16:36:57,125 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=773356) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.88it/s]
(EngineCore_DP0 pid=773356) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.33it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 275.68it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 318.83it/s]
Adding requests:  74%|███████▍  | 95/128 [00:00<00:00, 315.81it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 320.10it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 315.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:00, 153.22it/s, est. speed input: 156902.83 toks/s, output: 153.22 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 60.95it/s, est. speed input: 69205.32 toks/s, output: 67.58 toks/s]   
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 52.84it/s, est. speed input: 60900.20 toks/s, output: 59.47 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:01, 49.09it/s, est. speed input: 57243.39 toks/s, output: 55.90 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 46.76it/s, est. speed input: 55048.97 toks/s, output: 53.76 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 45.07it/s, est. speed input: 53421.65 toks/s, output: 52.17 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 43.96it/s, est. speed input: 52329.39 toks/s, output: 51.10 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 43.05it/s, est. speed input: 51408.39 toks/s, output: 50.20 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 42.37it/s, est. speed input: 50637.37 toks/s, output: 49.45 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 41.84it/s, est. speed input: 49970.27 toks/s, output: 48.80 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 41.37it/s, est. speed input: 49371.41 toks/s, output: 48.21 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 41.09it/s, est. speed input: 48864.18 toks/s, output: 47.72 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 40.98it/s, est. speed input: 48439.27 toks/s, output: 47.30 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 40.91it/s, est. speed input: 48063.33 toks/s, output: 46.94 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 40.81it/s, est. speed input: 47716.09 toks/s, output: 46.60 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 40.75it/s, est. speed input: 47406.04 toks/s, output: 46.29 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 40.69it/s, est. speed input: 47124.06 toks/s, output: 46.02 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 40.64it/s, est. speed input: 46866.22 toks/s, output: 45.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.59it/s, est. speed input: 46627.03 toks/s, output: 45.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.59it/s, est. speed input: 46627.03 toks/s, output: 45.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.53it/s, est. speed input: 46627.03 toks/s, output: 45.53 toks/s]
[rank0]:[W126 16:37:02.316692195 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 16:37:04
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:37:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=774468) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=774468) WARNING 01-26 16:37:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=774468) WARNING 01-26 16:37:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 74.99 requests/s, 76860.18 total tokens/s, 74.99 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 16:37:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:37:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:37:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:37:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:37:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:37:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:37:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:37:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:37:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:37:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:37:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:37:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=774468) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=774468) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.37it/s]
(EngineCore_DP0 pid=774468) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.11it/s]
(EngineCore_DP0 pid=774468) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
(EngineCore_DP0 pid=774468) 
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=774468) 2026-01-26 16:37:38,161 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=774468) 2026-01-26 16:37:38,184 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=774468) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.17it/s]
(EngineCore_DP0 pid=774468) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.40it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 27/256 [00:00<00:00, 261.73it/s]
Adding requests:  23%|██▎       | 60/256 [00:00<00:00, 299.98it/s]
Adding requests:  36%|███▌      | 91/256 [00:00<00:00, 300.40it/s]
Adding requests:  48%|████▊     | 122/256 [00:00<00:00, 302.68it/s]
Adding requests:  60%|██████    | 154/256 [00:00<00:00, 308.44it/s]
Adding requests:  74%|███████▍  | 189/256 [00:00<00:00, 321.89it/s]
Adding requests:  88%|████████▊ | 224/256 [00:00<00:00, 327.65it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 315.47it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:00<00:00, 579.47it/s, est. speed input: 593412.60 toks/s, output: 579.48 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:00<00:01, 123.97it/s, est. speed input: 144561.15 toks/s, output: 141.17 toks/s]
Processed prompts:  58%|█████▊    | 149/256 [00:01<00:00, 107.43it/s, est. speed input: 126293.80 toks/s, output: 123.33 toks/s]
Processed prompts:  66%|██████▌   | 169/256 [00:01<00:00, 99.06it/s, est. speed input: 118122.53 toks/s, output: 115.35 toks/s] 
Processed prompts:  72%|███████▏  | 184/256 [00:01<00:00, 92.83it/s, est. speed input: 112872.26 toks/s, output: 110.23 toks/s]
Processed prompts:  77%|███████▋  | 197/256 [00:01<00:00, 91.16it/s, est. speed input: 110675.59 toks/s, output: 108.08 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:01<00:00, 85.38it/s, est. speed input: 107209.97 toks/s, output: 104.70 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:02<00:00, 83.83it/s, est. speed input: 105556.58 toks/s, output: 103.08 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:02<00:00, 84.62it/s, est. speed input: 104837.85 toks/s, output: 102.38 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 80.94it/s, est. speed input: 103046.81 toks/s, output: 100.63 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:02<00:00, 82.47it/s, est. speed input: 102488.87 toks/s, output: 100.09 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 78.93it/s, est. speed input: 100968.72 toks/s, output: 98.60 toks/s] 
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 78.93it/s, est. speed input: 100771.69 toks/s, output: 98.41 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 98.41it/s, est. speed input: 100771.69 toks/s, output: 98.41 toks/s]
[rank0]:[W126 16:37:43.633944794 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 16:37:45
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:37:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=775625) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=775625) WARNING 01-26 16:38:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=775625) WARNING 01-26 16:38:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 115.57 requests/s, 118454.63 total tokens/s, 115.57 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 16:37:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:37:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:37:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:37:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:37:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:37:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:38:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:38:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:38:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:38:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:38:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:38:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=775625) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=775625) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=775625) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.13it/s]
(EngineCore_DP0 pid=775625) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
(EngineCore_DP0 pid=775625) 
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=775625) 2026-01-26 16:38:21,312 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=775625) 2026-01-26 16:38:21,335 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=775625) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 12.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.54it/s]
(EngineCore_DP0 pid=775625) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.80it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 25/512 [00:00<00:01, 248.59it/s]
Adding requests:  11%|█▏        | 58/512 [00:00<00:01, 296.12it/s]
Adding requests:  18%|█▊        | 90/512 [00:00<00:01, 302.48it/s]
Adding requests:  24%|██▍       | 124/512 [00:00<00:01, 314.26it/s]
Adding requests:  31%|███       | 159/512 [00:00<00:01, 324.95it/s]
Adding requests:  38%|███▊      | 195/512 [00:00<00:00, 335.87it/s]
Adding requests:  45%|████▌     | 231/512 [00:00<00:00, 343.46it/s]
Adding requests:  52%|█████▏    | 266/512 [00:00<00:00, 343.63it/s]
Adding requests:  59%|█████▉    | 302/512 [00:00<00:00, 348.12it/s]
Adding requests:  66%|██████▌   | 339/512 [00:01<00:00, 354.10it/s]
Adding requests:  73%|███████▎  | 376/512 [00:01<00:00, 357.64it/s]
Adding requests:  81%|████████  | 414/512 [00:01<00:00, 363.29it/s]
Adding requests:  88%|████████▊ | 451/512 [00:01<00:00, 358.80it/s]
Adding requests:  96%|█████████▌| 491/512 [00:01<00:00, 370.82it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 346.49it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:00<00:00, 1492.16it/s, est. speed input: 1528060.96 toks/s, output: 1492.18 toks/s]
Processed prompts:  64%|██████▍   | 328/512 [00:01<00:00, 203.63it/s, est. speed input: 242629.57 toks/s, output: 236.94 toks/s]   
Processed prompts:  78%|███████▊  | 397/512 [00:01<00:00, 171.43it/s, est. speed input: 206978.96 toks/s, output: 202.13 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:02<00:00, 157.34it/s, est. speed input: 193128.90 toks/s, output: 188.60 toks/s]
Processed prompts:  92%|█████████▏| 471/512 [00:02<00:00, 147.87it/s, est. speed input: 185006.86 toks/s, output: 180.67 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [00:02<00:00, 142.00it/s, est. speed input: 180193.16 toks/s, output: 175.97 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 142.00it/s, est. speed input: 177639.91 toks/s, output: 173.48 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 173.47it/s, est. speed input: 177639.91 toks/s, output: 173.48 toks/s]
[rank0]:[W126 16:38:28.438211260 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 16:38:30
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:38:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=776844) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=776844) WARNING 01-26 16:38:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=776844) WARNING 01-26 16:39:08 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 122.05 requests/s, 125105.08 total tokens/s, 122.05 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 16:38:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:38:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:38:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:38:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:38:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:38:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:38:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:38:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:38:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:38:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:38:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:38:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=776844) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=776844) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=776844) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
(EngineCore_DP0 pid=776844) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
(EngineCore_DP0 pid=776844) 
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=776844) 2026-01-26 16:39:08,818 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=776844) 2026-01-26 16:39:08,842 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=776844) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 13.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.60it/s]
(EngineCore_DP0 pid=776844) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.07it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 279.22it/s]
Adding requests:   6%|▋         | 64/1024 [00:00<00:02, 320.53it/s]
Adding requests:   9%|▉         | 97/1024 [00:00<00:02, 318.24it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 322.78it/s]
Adding requests:  16%|█▌        | 166/1024 [00:00<00:02, 333.31it/s]
Adding requests:  20%|█▉        | 203/1024 [00:00<00:02, 343.87it/s]
Adding requests:  23%|██▎       | 239/1024 [00:00<00:02, 348.81it/s]
Adding requests:  27%|██▋       | 274/1024 [00:00<00:02, 348.04it/s]
Adding requests:  30%|███       | 312/1024 [00:00<00:02, 353.16it/s]
Adding requests:  34%|███▍      | 349/1024 [00:01<00:01, 358.17it/s]
Adding requests:  38%|███▊      | 387/1024 [00:01<00:01, 362.56it/s]
Adding requests:  42%|████▏     | 425/1024 [00:01<00:01, 367.58it/s]
Adding requests:  45%|████▌     | 462/1024 [00:01<00:01, 364.90it/s]
Adding requests:  49%|████▉     | 502/1024 [00:01<00:01, 372.10it/s]
Adding requests:  53%|█████▎    | 542/1024 [00:01<00:01, 378.85it/s]
Adding requests:  57%|█████▋    | 580/1024 [00:01<00:01, 374.36it/s]
Adding requests:  60%|██████    | 618/1024 [00:01<00:01, 366.04it/s]
Adding requests:  64%|██████▍   | 655/1024 [00:01<00:01, 358.98it/s]
Adding requests:  68%|██████▊   | 693/1024 [00:01<00:00, 363.05it/s]
Adding requests:  71%|███████▏  | 730/1024 [00:02<00:00, 356.53it/s]
Adding requests:  75%|███████▍  | 766/1024 [00:02<00:00, 357.19it/s]
Adding requests:  78%|███████▊  | 802/1024 [00:02<00:00, 355.92it/s]
Adding requests:  82%|████████▏ | 840/1024 [00:02<00:00, 362.30it/s]
Adding requests:  86%|████████▌ | 877/1024 [00:02<00:00, 361.90it/s]
Adding requests:  89%|████████▉ | 914/1024 [00:02<00:00, 363.16it/s]
Adding requests:  93%|█████████▎| 951/1024 [00:02<00:00, 359.77it/s]
Adding requests:  96%|█████████▋| 988/1024 [00:02<00:00, 360.89it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 356.06it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:00<00:00, 3249.60it/s, est. speed input: 3327872.50 toks/s, output: 3249.67 toks/s]
Processed prompts:  66%|██████▋   | 679/1024 [00:02<00:01, 214.01it/s, est. speed input: 256647.98 toks/s, output: 250.63 toks/s]   
Processed prompts:  80%|████████  | 821/1024 [00:03<00:01, 178.38it/s, est. speed input: 216702.42 toks/s, output: 211.62 toks/s]
Processed prompts:  88%|████████▊ | 904/1024 [00:04<00:00, 166.48it/s, est. speed input: 204334.92 toks/s, output: 199.55 toks/s]
Processed prompts:  94%|█████████▎| 959/1024 [00:04<00:00, 157.76it/s, est. speed input: 196934.28 toks/s, output: 192.32 toks/s]
Processed prompts:  98%|█████████▊| 999/1024 [00:05<00:00, 152.11it/s, est. speed input: 192578.48 toks/s, output: 188.06 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 152.11it/s, est. speed input: 190227.36 toks/s, output: 185.77 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 185.77it/s, est. speed input: 190227.36 toks/s, output: 185.77 toks/s]
[rank0]:[W126 16:39:19.023682633 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 16:39:21
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:39:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=778215) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=778215) WARNING 01-26 16:39:56 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=778215) WARNING 01-26 16:40:06 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 129.66 requests/s, 132904.88 total tokens/s, 129.66 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 16:39:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:39:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:39:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:39:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:39:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:39:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:39:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:39:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:39:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:39:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:39:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:39:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:39:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:39:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:39:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:39:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=778215) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=778215) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=778215) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
(EngineCore_DP0 pid=778215) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
(EngineCore_DP0 pid=778215) 
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=778215) [rank0]:W0126 16:40:01.981000 778215 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=778215) [rank0]:W0126 16:40:02.063000 778215 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=778215) [rank0]:W0126 16:40:03.034000 778215 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=778215) [rank0]:W0126 16:40:03.165000 778215 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=778215) 2026-01-26 16:40:06,641 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=778215) 2026-01-26 16:40:06,668 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=778215) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 16.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.45it/s]
(EngineCore_DP0 pid=778215) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 14.66it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.10it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 275.09it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 317.78it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 316.11it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:05, 323.58it/s]
Adding requests:   8%|▊         | 164/2048 [00:00<00:05, 330.74it/s]
Adding requests:  10%|▉         | 201/2048 [00:00<00:05, 342.61it/s]
Adding requests:  12%|█▏        | 238/2048 [00:00<00:05, 348.59it/s]
Adding requests:  13%|█▎        | 273/2048 [00:00<00:05, 346.11it/s]
Adding requests:  15%|█▌        | 310/2048 [00:00<00:04, 352.87it/s]
Adding requests:  17%|█▋        | 348/2048 [00:01<00:04, 359.41it/s]
Adding requests:  19%|█▉        | 386/2048 [00:01<00:04, 362.38it/s]
Adding requests:  21%|██        | 423/2048 [00:01<00:04, 361.40it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:04, 357.89it/s]
Adding requests:  24%|██▍       | 500/2048 [00:01<00:04, 368.50it/s]
Adding requests:  26%|██▋       | 538/2048 [00:01<00:04, 371.06it/s]
Adding requests:  28%|██▊       | 576/2048 [00:01<00:03, 372.05it/s]
Adding requests:  30%|██▉       | 614/2048 [00:01<00:04, 348.70it/s]
Adding requests:  32%|███▏      | 650/2048 [00:01<00:04, 345.74it/s]
Adding requests:  34%|███▎      | 687/2048 [00:01<00:03, 352.11it/s]
Adding requests:  35%|███▌      | 723/2048 [00:02<00:03, 350.19it/s]
Adding requests:  37%|███▋      | 759/2048 [00:02<00:03, 349.16it/s]
Adding requests:  39%|███▉      | 795/2048 [00:02<00:03, 349.90it/s]
Adding requests:  41%|████      | 832/2048 [00:02<00:03, 354.08it/s]
Adding requests:  42%|████▏     | 868/2048 [00:02<00:03, 355.23it/s]
Adding requests:  44%|████▍     | 906/2048 [00:02<00:03, 361.77it/s]
Adding requests:  46%|████▌     | 943/2048 [00:02<00:03, 353.28it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:03, 354.60it/s]
Adding requests:  50%|████▉     | 1015/2048 [00:02<00:02, 349.15it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:02<00:02, 351.03it/s]
Adding requests:  53%|█████▎    | 1087/2048 [00:03<00:02, 351.56it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:03<00:02, 353.20it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:03<00:02, 352.01it/s]
Adding requests:  58%|█████▊    | 1195/2048 [00:03<00:02, 354.11it/s]
Adding requests:  60%|██████    | 1233/2048 [00:03<00:02, 361.43it/s]
Adding requests:  62%|██████▏   | 1270/2048 [00:03<00:02, 356.68it/s]
Adding requests:  64%|██████▍   | 1306/2048 [00:03<00:02, 355.75it/s]
Adding requests:  66%|██████▌   | 1342/2048 [00:03<00:01, 355.79it/s]
Adding requests:  67%|██████▋   | 1379/2048 [00:03<00:01, 358.58it/s]
Adding requests:  69%|██████▉   | 1415/2048 [00:04<00:01, 355.25it/s]
Adding requests:  71%|███████   | 1452/2048 [00:04<00:01, 357.09it/s]
Adding requests:  73%|███████▎  | 1490/2048 [00:04<00:01, 361.59it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:04<00:01, 361.79it/s]
Adding requests:  76%|███████▋  | 1564/2048 [00:04<00:01, 357.07it/s]
Adding requests:  78%|███████▊  | 1600/2048 [00:04<00:01, 352.39it/s]
Adding requests:  80%|███████▉  | 1636/2048 [00:04<00:01, 347.87it/s]
Adding requests:  82%|████████▏ | 1671/2048 [00:04<00:01, 333.60it/s]
Adding requests:  83%|████████▎ | 1708/2048 [00:04<00:00, 343.91it/s]
Adding requests:  85%|████████▌ | 1744/2048 [00:04<00:00, 346.39it/s]
Adding requests:  87%|████████▋ | 1782/2048 [00:05<00:00, 353.80it/s]
Adding requests:  89%|████████▉ | 1818/2048 [00:05<00:00, 351.01it/s]
Adding requests:  91%|█████████ | 1855/2048 [00:05<00:00, 356.52it/s]
Adding requests:  92%|█████████▏| 1892/2048 [00:05<00:00, 357.84it/s]
Adding requests:  94%|█████████▍| 1930/2048 [00:05<00:00, 362.14it/s]
Adding requests:  96%|█████████▌| 1967/2048 [00:05<00:00, 363.34it/s]
Adding requests:  98%|█████████▊| 2004/2048 [00:05<00:00, 356.65it/s]
Adding requests: 100%|█████████▉| 2040/2048 [00:05<00:00, 353.58it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 352.63it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 748/2048 [00:00<00:00, 5954.41it/s, est. speed input: 6097702.50 toks/s, output: 5954.53 toks/s]
Processed prompts:  66%|██████▌   | 1344/2048 [00:04<00:02, 241.57it/s, est. speed input: 294552.24 toks/s, output: 287.65 toks/s]  
Processed prompts:  78%|███████▊  | 1597/2048 [00:06<00:02, 199.23it/s, est. speed input: 246592.45 toks/s, output: 240.81 toks/s]
Processed prompts:  85%|████████▌ | 1741/2048 [00:07<00:01, 183.31it/s, est. speed input: 230522.19 toks/s, output: 225.12 toks/s]
Processed prompts:  90%|████████▉ | 1833/2048 [00:08<00:01, 178.30it/s, est. speed input: 224979.36 toks/s, output: 219.71 toks/s]
Processed prompts:  93%|█████████▎| 1898/2048 [00:08<00:00, 171.57it/s, est. speed input: 220116.99 toks/s, output: 214.96 toks/s]
Processed prompts:  95%|█████████▍| 1945/2048 [00:09<00:00, 165.39it/s, est. speed input: 216520.63 toks/s, output: 211.45 toks/s]
Processed prompts:  97%|█████████▋| 1981/2048 [00:09<00:00, 154.47it/s, est. speed input: 212145.29 toks/s, output: 207.17 toks/s]
Processed prompts:  98%|█████████▊| 2009/2048 [00:09<00:00, 159.92it/s, est. speed input: 212424.54 toks/s, output: 207.45 toks/s]
Processed prompts:  99%|█████████▉| 2036/2048 [00:09<00:00, 151.49it/s, est. speed input: 210005.80 toks/s, output: 205.08 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:09<00:00, 151.49it/s, est. speed input: 210021.79 toks/s, output: 205.10 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:09<00:00, 205.10it/s, est. speed input: 210021.79 toks/s, output: 205.10 toks/s]
[rank0]:[W126 16:40:24.207988852 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 16:40:26
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:40:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=779803) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=779803) WARNING 01-26 16:41:13 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=779803) WARNING 01-26 16:41:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 133.25 requests/s, 136582.37 total tokens/s, 133.25 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 16:40:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:40:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:40:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:40:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:40:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:40:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:40:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:40:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:41:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:41:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:41:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:41:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:41:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:41:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:41:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:41:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=779803) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=779803) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=779803) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
(EngineCore_DP0 pid=779803) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
(EngineCore_DP0 pid=779803) 
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=779803) [rank0]:W0126 16:41:18.759000 779803 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=779803) [rank0]:W0126 16:41:18.841000 779803 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=779803) [rank0]:W0126 16:41:19.807000 779803 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=779803) [rank0]:W0126 16:41:19.937000 779803 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=779803) 2026-01-26 16:41:23,487 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=779803) 2026-01-26 16:41:23,513 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=779803) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 13.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 13.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 14.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.53it/s]
(EngineCore_DP0 pid=779803) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.84it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.99it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.22it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 24/4096 [00:00<00:17, 235.86it/s]
Adding requests:   1%|▏         | 57/4096 [00:00<00:14, 285.48it/s]
Adding requests:   2%|▏         | 90/4096 [00:00<00:13, 303.04it/s]
Adding requests:   3%|▎         | 123/4096 [00:00<00:12, 312.17it/s]
Adding requests:   4%|▍         | 157/4096 [00:00<00:12, 319.72it/s]
Adding requests:   5%|▍         | 194/4096 [00:00<00:11, 334.89it/s]
Adding requests:   6%|▌         | 230/4096 [00:00<00:11, 340.54it/s]
Adding requests:   6%|▋         | 265/4096 [00:00<00:11, 338.80it/s]
Adding requests:   7%|▋         | 301/4096 [00:00<00:11, 343.94it/s]
Adding requests:   8%|▊         | 338/4096 [00:01<00:10, 350.21it/s]
Adding requests:   9%|▉         | 375/4096 [00:01<00:10, 353.52it/s]
Adding requests:  10%|█         | 412/4096 [00:01<00:10, 358.37it/s]
Adding requests:  11%|█         | 448/4096 [00:01<00:10, 355.32it/s]
Adding requests:  12%|█▏        | 488/4096 [00:01<00:09, 367.36it/s]
Adding requests:  13%|█▎        | 528/4096 [00:01<00:09, 375.20it/s]
Adding requests:  14%|█▍        | 566/4096 [00:01<00:09, 372.73it/s]
Adding requests:  15%|█▍        | 604/4096 [00:01<00:09, 358.07it/s]
Adding requests:  16%|█▌        | 640/4096 [00:01<00:09, 351.64it/s]
Adding requests:  17%|█▋        | 676/4096 [00:01<00:09, 347.96it/s]
Adding requests:  17%|█▋        | 714/4096 [00:02<00:09, 354.75it/s]
Adding requests:  18%|█▊        | 750/4096 [00:02<00:09, 348.26it/s]
Adding requests:  19%|█▉        | 786/4096 [00:02<00:09, 351.32it/s]
Adding requests:  20%|██        | 822/4096 [00:02<00:09, 351.08it/s]
Adding requests:  21%|██        | 860/4096 [00:02<00:09, 357.60it/s]
Adding requests:  22%|██▏       | 897/4096 [00:02<00:08, 359.65it/s]
Adding requests:  23%|██▎       | 933/4096 [00:02<00:08, 352.41it/s]
Adding requests:  24%|██▎       | 969/4096 [00:02<00:08, 347.80it/s]
Adding requests:  25%|██▍       | 1004/4096 [00:02<00:08, 344.30it/s]
Adding requests:  25%|██▌       | 1041/4096 [00:03<00:08, 348.31it/s]
Adding requests:  26%|██▋       | 1076/4096 [00:03<00:08, 345.51it/s]
Adding requests:  27%|██▋       | 1111/4096 [00:03<00:08, 346.59it/s]
Adding requests:  28%|██▊       | 1148/4096 [00:03<00:08, 350.68it/s]
Adding requests:  29%|██▉       | 1184/4096 [00:03<00:08, 349.26it/s]
Adding requests:  30%|██▉       | 1222/4096 [00:03<00:08, 355.80it/s]
Adding requests:  31%|███       | 1258/4096 [00:03<00:08, 351.76it/s]
Adding requests:  32%|███▏      | 1294/4096 [00:03<00:08, 348.24it/s]
Adding requests:  32%|███▏      | 1331/4096 [00:03<00:07, 351.25it/s]
Adding requests:  33%|███▎      | 1368/4096 [00:03<00:07, 355.46it/s]
Adding requests:  34%|███▍      | 1404/4096 [00:04<00:07, 353.52it/s]
Adding requests:  35%|███▌      | 1440/4096 [00:04<00:07, 354.60it/s]
Adding requests:  36%|███▌      | 1476/4096 [00:04<00:07, 352.35it/s]
Adding requests:  37%|███▋      | 1514/4096 [00:04<00:07, 358.44it/s]
Adding requests:  38%|███▊      | 1550/4096 [00:04<00:07, 355.95it/s]
Adding requests:  39%|███▊      | 1586/4096 [00:04<00:07, 348.52it/s]
Adding requests:  40%|███▉      | 1621/4096 [00:04<00:07, 343.99it/s]
Adding requests:  40%|████      | 1656/4096 [00:04<00:07, 339.87it/s]
Adding requests:  41%|████▏     | 1691/4096 [00:04<00:07, 342.33it/s]
Adding requests:  42%|████▏     | 1728/4096 [00:04<00:06, 347.77it/s]
Adding requests:  43%|████▎     | 1764/4096 [00:05<00:06, 351.32it/s]
Adding requests:  44%|████▍     | 1800/4096 [00:05<00:06, 351.51it/s]
Adding requests:  45%|████▍     | 1836/4096 [00:05<00:06, 349.16it/s]
Adding requests:  46%|████▌     | 1873/4096 [00:05<00:06, 353.59it/s]
Adding requests:  47%|████▋     | 1909/4096 [00:05<00:06, 353.95it/s]
Adding requests:  48%|████▊     | 1947/4096 [00:05<00:05, 360.76it/s]
Adding requests:  48%|████▊     | 1984/4096 [00:05<00:05, 359.68it/s]
Adding requests:  49%|████▉     | 2020/4096 [00:05<00:05, 347.02it/s]
Adding requests:  50%|█████     | 2055/4096 [00:05<00:05, 347.73it/s]
Adding requests:  51%|█████     | 2090/4096 [00:05<00:05, 340.13it/s]
Adding requests:  52%|█████▏    | 2125/4096 [00:06<00:05, 340.93it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:06<00:05, 339.74it/s]
Adding requests:  54%|█████▎    | 2195/4096 [00:06<00:05, 336.38it/s]
Adding requests:  54%|█████▍    | 2230/4096 [00:06<00:05, 338.88it/s]
Adding requests:  55%|█████▌    | 2267/4096 [00:06<00:05, 347.14it/s]
Adding requests:  56%|█████▋    | 2304/4096 [00:06<00:05, 353.15it/s]
Adding requests:  57%|█████▋    | 2341/4096 [00:06<00:04, 356.54it/s]
Adding requests:  58%|█████▊    | 2377/4096 [00:06<00:04, 355.10it/s]
Adding requests:  59%|█████▉    | 2414/4096 [00:06<00:04, 358.95it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:07<00:04, 355.25it/s]
Adding requests:  61%|██████    | 2486/4096 [00:07<00:04, 355.90it/s]
Adding requests:  62%|██████▏   | 2524/4096 [00:07<00:04, 359.81it/s]
Adding requests:  63%|██████▎   | 2564/4096 [00:07<00:04, 369.85it/s]
Adding requests:  64%|██████▎   | 2602/4096 [00:07<00:04, 371.08it/s]
Adding requests:  64%|██████▍   | 2640/4096 [00:07<00:04, 357.16it/s]
Adding requests:  65%|██████▌   | 2676/4096 [00:07<00:03, 357.26it/s]
Adding requests:  66%|██████▌   | 2712/4096 [00:07<00:03, 352.18it/s]
Adding requests:  67%|██████▋   | 2749/4096 [00:07<00:03, 356.01it/s]
Adding requests:  68%|██████▊   | 2787/4096 [00:07<00:03, 362.45it/s]
Adding requests:  69%|██████▉   | 2825/4096 [00:08<00:03, 364.62it/s]
Adding requests:  70%|██████▉   | 2862/4096 [00:08<00:03, 362.41it/s]
Adding requests:  71%|███████   | 2899/4096 [00:08<00:03, 361.43it/s]
Adding requests:  72%|███████▏  | 2937/4096 [00:08<00:03, 365.62it/s]
Adding requests:  73%|███████▎  | 2974/4096 [00:08<00:03, 359.77it/s]
Adding requests:  74%|███████▎  | 3012/4096 [00:08<00:02, 365.68it/s]
Adding requests:  74%|███████▍  | 3049/4096 [00:08<00:02, 364.04it/s]
Adding requests:  75%|███████▌  | 3087/4096 [00:08<00:02, 368.21it/s]
Adding requests:  76%|███████▋  | 3125/4096 [00:08<00:02, 370.89it/s]
Adding requests:  77%|███████▋  | 3163/4096 [00:08<00:02, 366.54it/s]
Adding requests:  78%|███████▊  | 3200/4096 [00:09<00:02, 359.36it/s]
Adding requests:  79%|███████▉  | 3238/4096 [00:09<00:02, 364.43it/s]
Adding requests:  80%|███████▉  | 3275/4096 [00:09<00:02, 358.59it/s]
Adding requests:  81%|████████  | 3311/4096 [00:09<00:02, 347.92it/s]
Adding requests:  82%|████████▏ | 3348/4096 [00:09<00:02, 351.56it/s]
Adding requests:  83%|████████▎ | 3385/4096 [00:09<00:01, 356.17it/s]
Adding requests:  84%|████████▎ | 3421/4096 [00:09<00:01, 348.06it/s]
Adding requests:  84%|████████▍ | 3458/4096 [00:09<00:01, 352.37it/s]
Adding requests:  85%|████████▌ | 3494/4096 [00:09<00:01, 349.84it/s]
Adding requests:  86%|████████▋ | 3533/4096 [00:10<00:01, 360.60it/s]
Adding requests:  87%|████████▋ | 3570/4096 [00:10<00:01, 363.07it/s]
Adding requests:  88%|████████▊ | 3607/4096 [00:10<00:01, 360.59it/s]
Adding requests:  89%|████████▉ | 3645/4096 [00:10<00:01, 363.40it/s]
Adding requests:  90%|████████▉ | 3682/4096 [00:10<00:01, 353.77it/s]
Adding requests:  91%|█████████ | 3719/4096 [00:10<00:01, 358.23it/s]
Adding requests:  92%|█████████▏| 3755/4096 [00:10<00:00, 348.55it/s]
Adding requests:  93%|█████████▎| 3790/4096 [00:10<00:00, 339.64it/s]
Adding requests:  93%|█████████▎| 3825/4096 [00:10<00:00, 337.86it/s]
Adding requests:  94%|█████████▍| 3861/4096 [00:10<00:00, 342.88it/s]
Adding requests:  95%|█████████▌| 3896/4096 [00:11<00:00, 342.08it/s]
Adding requests:  96%|█████████▌| 3931/4096 [00:11<00:00, 338.89it/s]
Adding requests:  97%|█████████▋| 3966/4096 [00:11<00:00, 341.83it/s]
Adding requests:  98%|█████████▊| 4001/4096 [00:11<00:00, 340.06it/s]
Adding requests:  99%|█████████▊| 4037/4096 [00:11<00:00, 345.42it/s]
Adding requests:  99%|█████████▉| 4072/4096 [00:11<00:00, 341.72it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 351.26it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 1535/4096 [00:00<00:00, 8390.85it/s, est. speed input: 8592599.14 toks/s, output: 8390.94 toks/s]
Processed prompts:  58%|█████▊    | 2375/4096 [00:06<00:05, 303.56it/s, est. speed input: 382284.83 toks/s, output: 373.32 toks/s]   
Processed prompts:  67%|██████▋   | 2729/4096 [00:09<00:05, 240.18it/s, est. speed input: 310474.21 toks/s, output: 303.20 toks/s]
Processed prompts:  71%|███████▏  | 2928/4096 [00:10<00:05, 217.70it/s, est. speed input: 287613.79 toks/s, output: 280.87 toks/s]
Processed prompts:  75%|███████▍  | 3054/4096 [00:11<00:05, 202.45it/s, est. speed input: 274750.44 toks/s, output: 268.31 toks/s]
Processed prompts:  77%|███████▋  | 3139/4096 [00:12<00:05, 188.75it/s, est. speed input: 265477.47 toks/s, output: 259.26 toks/s]
Processed prompts:  78%|███████▊  | 3200/4096 [00:12<00:04, 180.61it/s, est. speed input: 260315.12 toks/s, output: 254.21 toks/s]
Processed prompts:  79%|███████▉  | 3245/4096 [00:12<00:04, 181.17it/s, est. speed input: 259027.27 toks/s, output: 252.96 toks/s]
Processed prompts:  80%|████████  | 3282/4096 [00:13<00:04, 178.17it/s, est. speed input: 257158.81 toks/s, output: 251.13 toks/s]
Processed prompts:  81%|████████  | 3312/4096 [00:13<00:04, 170.77it/s, est. speed input: 254787.43 toks/s, output: 248.82 toks/s]
Processed prompts:  81%|████████▏ | 3337/4096 [00:13<00:04, 159.87it/s, est. speed input: 252142.49 toks/s, output: 246.23 toks/s]
Processed prompts:  82%|████████▏ | 3359/4096 [00:13<00:05, 146.84it/s, est. speed input: 249352.28 toks/s, output: 243.51 toks/s]
Processed prompts:  83%|████████▎ | 3391/4096 [00:14<00:04, 143.77it/s, est. speed input: 247392.54 toks/s, output: 241.59 toks/s]
Processed prompts:  84%|████████▎ | 3423/4096 [00:14<00:04, 141.78it/s, est. speed input: 245581.31 toks/s, output: 239.83 toks/s]
Processed prompts:  84%|████████▍ | 3455/4096 [00:14<00:04, 139.53it/s, est. speed input: 243761.29 toks/s, output: 238.05 toks/s]
Processed prompts:  85%|████████▌ | 3487/4096 [00:14<00:04, 137.49it/s, est. speed input: 241971.75 toks/s, output: 236.30 toks/s]
Processed prompts:  86%|████████▌ | 3519/4096 [00:14<00:04, 136.06it/s, est. speed input: 240253.01 toks/s, output: 234.62 toks/s]
Processed prompts:  87%|████████▋ | 3551/4096 [00:15<00:03, 136.77it/s, est. speed input: 238764.30 toks/s, output: 233.17 toks/s]
Processed prompts:  87%|████████▋ | 3583/4096 [00:15<00:03, 135.74it/s, est. speed input: 237174.80 toks/s, output: 231.62 toks/s]
Processed prompts:  88%|████████▊ | 3615/4096 [00:15<00:03, 135.70it/s, est. speed input: 235697.22 toks/s, output: 230.17 toks/s]
Processed prompts:  89%|████████▉ | 3647/4096 [00:15<00:03, 134.77it/s, est. speed input: 234184.31 toks/s, output: 228.70 toks/s]
Processed prompts:  90%|████████▉ | 3679/4096 [00:16<00:03, 136.49it/s, est. speed input: 232919.70 toks/s, output: 227.46 toks/s]
Processed prompts:  91%|█████████ | 3711/4096 [00:16<00:02, 136.04it/s, est. speed input: 231551.76 toks/s, output: 226.12 toks/s]
Processed prompts:  91%|█████████▏| 3743/4096 [00:16<00:02, 134.73it/s, est. speed input: 230141.50 toks/s, output: 224.75 toks/s]
Processed prompts:  92%|█████████▏| 3775/4096 [00:16<00:02, 134.22it/s, est. speed input: 228804.17 toks/s, output: 223.44 toks/s]
Processed prompts:  93%|█████████▎| 3807/4096 [00:17<00:02, 133.57it/s, est. speed input: 227481.41 toks/s, output: 222.15 toks/s]
Processed prompts:  94%|█████████▎| 3839/4096 [00:17<00:01, 133.31it/s, est. speed input: 226210.64 toks/s, output: 220.91 toks/s]
Processed prompts:  95%|█████████▍| 3871/4096 [00:17<00:01, 132.79it/s, est. speed input: 224948.20 toks/s, output: 219.68 toks/s]
Processed prompts:  95%|█████████▌| 3903/4096 [00:17<00:01, 134.37it/s, est. speed input: 223866.86 toks/s, output: 218.62 toks/s]
Processed prompts:  96%|█████████▌| 3935/4096 [00:18<00:01, 136.08it/s, est. speed input: 222853.97 toks/s, output: 217.63 toks/s]
Processed prompts:  97%|█████████▋| 3967/4096 [00:18<00:00, 134.58it/s, est. speed input: 221675.68 toks/s, output: 216.48 toks/s]
Processed prompts:  98%|█████████▊| 3999/4096 [00:18<00:00, 135.08it/s, est. speed input: 220636.77 toks/s, output: 215.47 toks/s]
Processed prompts:  98%|█████████▊| 4031/4096 [00:18<00:00, 136.06it/s, est. speed input: 219665.33 toks/s, output: 214.52 toks/s]
Processed prompts:  99%|█████████▉| 4063/4096 [00:19<00:00, 138.42it/s, est. speed input: 218825.92 toks/s, output: 213.70 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:19<00:00, 138.42it/s, est. speed input: 219875.32 toks/s, output: 214.72 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:19<00:00, 214.72it/s, est. speed input: 219875.32 toks/s, output: 214.72 toks/s]
[rank0]:[W126 16:41:57.599308724 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 16:41:59
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:42:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=781956) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=781956) WARNING 01-26 16:43:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     def forward(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     raise e
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/tmp/torchinductor_root/nw/cnwig33mlgcqa6ziknfxcunkmni5hxs2vdbr3vskyaf4e4mk25xu.py", line 1093, in call
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 6)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered

STDERR:
[2026-01-26 16:42:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:42:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:42:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:42:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:42:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:42:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:42:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:42:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:43:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:43:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:43:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:43:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:43:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:43:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=781956) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=781956) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=781956) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
(EngineCore_DP0 pid=781956) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
(EngineCore_DP0 pid=781956) 
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=781956) [rank0]:W0126 16:43:15.924000 781956 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=781956) [rank0]:W0126 16:43:16.006000 781956 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=781956) [rank0]:W0126 16:43:17.090000 781956 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=781956) [rank0]:W0126 16:43:17.219000 781956 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=781956) Process EngineCore_DP0:
(EngineCore_DP0 pid=781956) Traceback (most recent call last):
(EngineCore_DP0 pid=781956)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=781956)     self.run()
(EngineCore_DP0 pid=781956)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=781956)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=781956)     raise e
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=781956)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=781956)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=781956)     super().__init__(
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=781956)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=781956)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=781956)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=781956)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=781956)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=781956)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=781956)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=781956)     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=781956)     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=781956)     self.model_runner.profile_run()
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=781956)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=781956)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=781956)     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=781956)     outputs = self.model(
(EngineCore_DP0 pid=781956)               ^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=781956)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=781956)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=781956)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=781956)     hidden_states = self.model(
(EngineCore_DP0 pid=781956)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=781956)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=781956)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=781956)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=781956)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=781956)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=781956)     def forward(
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=781956)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=781956)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=781956)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=781956)     raise e
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=781956)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=781956)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=781956)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=781956)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=781956)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=781956)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=781956)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=781956)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=781956)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=781956)     return compiled_fn(full_args)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=781956)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=781956)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=781956)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=781956)                             ^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=781956)     outs = compiled_fn(args)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=781956)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=781956)     return self.current_callable(inputs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=781956)     out = model(new_inputs)
(EngineCore_DP0 pid=781956)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/tmp/torchinductor_root/nw/cnwig33mlgcqa6ziknfxcunkmni5hxs2vdbr3vskyaf4e4mk25xu.py", line 1093, in call
(EngineCore_DP0 pid=781956)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 6)
(EngineCore_DP0 pid=781956)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=781956)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=781956)     return fn(input, L)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=781956)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=781956)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=781956)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=781956)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=781956)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=781956)     self._init_handles()
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=781956)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=781956)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:43:18.315825035 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 18:24:01
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:24:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=906883) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=906883) WARNING 01-26 18:24:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=906883) WARNING 01-26 18:24:45 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 22.34 requests/s, 11462.53 total tokens/s, 22.34 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 18:24:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:24:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:24:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:24:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:24:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:24:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:24:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:24:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:24:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:24:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:24:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:24:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:24:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:24:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:24:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:24:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:24:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:16] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:16] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:16] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:16] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:16] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=906883) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=906883) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.20s/it]
(EngineCore_DP0 pid=906883) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.44it/s]
(EngineCore_DP0 pid=906883) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.17it/s]
(EngineCore_DP0 pid=906883) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.03it/s]
(EngineCore_DP0 pid=906883) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.07it/s]
(EngineCore_DP0 pid=906883) 
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=906883) [2026-01-26 18:24:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=906883) 2026-01-26 18:24:45,508 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=906883) 2026-01-26 18:24:45,549 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=906883) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.84it/s]
(EngineCore_DP0 pid=906883) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.41it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 508.80it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 249.23it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 297.18it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:02, 50.28it/s, est. speed input: 25746.47 toks/s, output: 50.28 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 31.05it/s, est. speed input: 16944.75 toks/s, output: 33.09 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 28.05it/s, est. speed input: 15496.33 toks/s, output: 30.27 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:04, 26.75it/s, est. speed input: 14885.65 toks/s, output: 29.07 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:04, 25.75it/s, est. speed input: 14436.58 toks/s, output: 28.20 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:04, 25.10it/s, est. speed input: 14118.71 toks/s, output: 27.58 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:04, 24.69it/s, est. speed input: 13887.34 toks/s, output: 27.12 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:03, 24.39it/s, est. speed input: 13700.60 toks/s, output: 26.76 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:03, 24.18it/s, est. speed input: 13550.91 toks/s, output: 26.47 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:03, 23.95it/s, est. speed input: 13413.30 toks/s, output: 26.20 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:03, 23.90it/s, est. speed input: 13314.13 toks/s, output: 26.00 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:03, 23.86it/s, est. speed input: 13228.64 toks/s, output: 25.84 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:03, 23.64it/s, est. speed input: 13131.35 toks/s, output: 25.65 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:03, 23.65it/s, est. speed input: 13065.79 toks/s, output: 25.52 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:03, 23.25it/s, est. speed input: 12962.04 toks/s, output: 25.32 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:03, 23.41it/s, est. speed input: 12917.41 toks/s, output: 25.23 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:02, 23.46it/s, est. speed input: 12872.14 toks/s, output: 25.14 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 23.31it/s, est. speed input: 12812.90 toks/s, output: 25.03 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:02, 23.43it/s, est. speed input: 12780.23 toks/s, output: 24.96 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:02, 23.40it/s, est. speed input: 12741.29 toks/s, output: 24.89 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:02, 23.47it/s, est. speed input: 12712.74 toks/s, output: 24.83 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:02, 23.47it/s, est. speed input: 12683.35 toks/s, output: 24.77 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:03<00:02, 23.52it/s, est. speed input: 12659.10 toks/s, output: 24.72 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:03<00:02, 23.51it/s, est. speed input: 12634.13 toks/s, output: 24.68 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:03<00:01, 23.49it/s, est. speed input: 12610.11 toks/s, output: 24.63 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:01, 23.34it/s, est. speed input: 12578.85 toks/s, output: 24.57 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:01, 23.40it/s, est. speed input: 12560.68 toks/s, output: 24.53 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:03<00:01, 23.43it/s, est. speed input: 12542.87 toks/s, output: 24.50 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:03<00:01, 23.32it/s, est. speed input: 12518.13 toks/s, output: 24.45 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:04<00:01, 23.37it/s, est. speed input: 12502.54 toks/s, output: 24.42 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:04<00:01, 23.36it/s, est. speed input: 12485.06 toks/s, output: 24.38 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:04<00:01, 23.39it/s, est. speed input: 12470.85 toks/s, output: 24.36 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:04<00:00, 23.40it/s, est. speed input: 12457.20 toks/s, output: 24.33 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:04<00:00, 23.41it/s, est. speed input: 12444.12 toks/s, output: 24.30 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:04<00:00, 23.43it/s, est. speed input: 12432.35 toks/s, output: 24.28 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:04<00:00, 23.47it/s, est. speed input: 12422.47 toks/s, output: 24.26 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:04<00:00, 23.51it/s, est. speed input: 12413.76 toks/s, output: 24.25 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:05<00:00, 23.25it/s, est. speed input: 12392.54 toks/s, output: 24.20 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:05<00:00, 23.31it/s, est. speed input: 12383.23 toks/s, output: 24.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.32it/s, est. speed input: 12372.74 toks/s, output: 24.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.32it/s, est. speed input: 12372.74 toks/s, output: 24.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.16it/s, est. speed input: 12372.74 toks/s, output: 24.17 toks/s]
[rank0]:[W126 18:24:53.195867169 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 18:24:55
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:25:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=908258) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=908258) WARNING 01-26 18:25:24 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=908258) WARNING 01-26 18:25:39 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 23.13 requests/s, 23703.67 total tokens/s, 23.13 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 18:25:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:25:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:25:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:25:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:25:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:25:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:25:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:25:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:25:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:25:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:25:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:25:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:25:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:25:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:25:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:25:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=908258) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=908258) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.14s/it]
(EngineCore_DP0 pid=908258) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.45it/s]
(EngineCore_DP0 pid=908258) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.22it/s]
(EngineCore_DP0 pid=908258) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.09it/s]
(EngineCore_DP0 pid=908258) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.13it/s]
(EngineCore_DP0 pid=908258) 
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=908258) [2026-01-26 18:25:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=908258) 2026-01-26 18:25:39,422 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=908258) 2026-01-26 18:25:39,461 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=908258) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.81it/s]
(EngineCore_DP0 pid=908258) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.96it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 264.02it/s]
Adding requests:  48%|████▊     | 61/128 [00:00<00:00, 305.39it/s]
Adding requests:  73%|███████▎  | 93/128 [00:00<00:00, 308.19it/s]
Adding requests:  98%|█████████▊| 125/128 [00:00<00:00, 312.78it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 307.05it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 94.01it/s, est. speed input: 96272.44 toks/s, output: 94.01 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 35.00it/s, est. speed input: 39762.61 toks/s, output: 38.83 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:03, 30.23it/s, est. speed input: 34774.12 toks/s, output: 33.96 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:03, 28.36it/s, est. speed input: 32918.60 toks/s, output: 32.15 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:03, 26.99it/s, est. speed input: 31613.78 toks/s, output: 30.87 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:03, 26.15it/s, est. speed input: 30844.99 toks/s, output: 30.12 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:03, 25.48it/s, est. speed input: 30223.13 toks/s, output: 29.51 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:03, 24.80it/s, est. speed input: 29646.31 toks/s, output: 28.95 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:03, 24.48it/s, est. speed input: 29225.60 toks/s, output: 28.54 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:03, 24.12it/s, est. speed input: 28828.05 toks/s, output: 28.15 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:03, 23.92it/s, est. speed input: 28502.17 toks/s, output: 27.83 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:03, 23.77it/s, est. speed input: 28217.33 toks/s, output: 27.56 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:02, 23.74it/s, est. speed input: 27983.84 toks/s, output: 27.33 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 23.51it/s, est. speed input: 27728.87 toks/s, output: 27.08 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:02, 23.54it/s, est. speed input: 27542.67 toks/s, output: 26.90 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:02, 23.56it/s, est. speed input: 27374.60 toks/s, output: 26.73 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:02, 23.40it/s, est. speed input: 27188.86 toks/s, output: 26.55 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:02, 23.50it/s, est. speed input: 27059.23 toks/s, output: 26.42 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:02, 23.45it/s, est. speed input: 26919.92 toks/s, output: 26.29 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:03<00:02, 23.50it/s, est. speed input: 26806.28 toks/s, output: 26.18 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:03<00:01, 23.42it/s, est. speed input: 26684.02 toks/s, output: 26.06 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:01, 23.40it/s, est. speed input: 26576.47 toks/s, output: 25.95 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:01, 23.44it/s, est. speed input: 26484.28 toks/s, output: 25.86 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:03<00:01, 23.20it/s, est. speed input: 26363.25 toks/s, output: 25.75 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:03<00:01, 23.24it/s, est. speed input: 26276.97 toks/s, output: 25.66 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:03<00:01, 22.96it/s, est. speed input: 26158.25 toks/s, output: 25.55 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:01, 23.07it/s, est. speed input: 26084.37 toks/s, output: 25.47 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:04<00:01, 23.23it/s, est. speed input: 26025.01 toks/s, output: 25.42 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:04<00:00, 23.10it/s, est. speed input: 25941.78 toks/s, output: 25.33 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:04<00:00, 23.22it/s, est. speed input: 25886.79 toks/s, output: 25.28 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:04<00:00, 23.31it/s, est. speed input: 25835.41 toks/s, output: 25.23 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:04<00:00, 23.33it/s, est. speed input: 25782.38 toks/s, output: 25.18 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:04<00:00, 23.42it/s, est. speed input: 25739.67 toks/s, output: 25.14 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:04<00:00, 23.47it/s, est. speed input: 25698.98 toks/s, output: 25.10 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 23.48it/s, est. speed input: 25656.99 toks/s, output: 25.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.47it/s, est. speed input: 25615.99 toks/s, output: 25.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.47it/s, est. speed input: 25615.99 toks/s, output: 25.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 25.01it/s, est. speed input: 25615.99 toks/s, output: 25.02 toks/s]
[rank0]:[W126 18:25:47.290271360 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 18:25:49
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:25:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=909550) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=909550) WARNING 01-26 18:26:19 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=909550) WARNING 01-26 18:26:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 44.95 requests/s, 46074.23 total tokens/s, 44.95 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 18:25:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:25:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:25:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:25:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:25:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:25:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:25:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:25:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:25:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:26:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:26:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:26:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:26:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:26:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:26:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:26:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:26:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:04] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=909550) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=909550) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.19s/it]
(EngineCore_DP0 pid=909550) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.45it/s]
(EngineCore_DP0 pid=909550) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.23it/s]
(EngineCore_DP0 pid=909550) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.09it/s]
(EngineCore_DP0 pid=909550) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.12it/s]
(EngineCore_DP0 pid=909550) 
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=909550) [2026-01-26 18:26:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=909550) 2026-01-26 18:26:33,570 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=909550) 2026-01-26 18:26:33,609 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=909550) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  9.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.06it/s]
(EngineCore_DP0 pid=909550) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 10.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 10.97it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:52,  4.86it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:01, 117.36it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 194.54it/s]
Adding requests:  36%|███▌      | 91/256 [00:00<00:00, 226.04it/s]
Adding requests:  48%|████▊     | 123/256 [00:00<00:00, 256.01it/s]
Adding requests:  61%|██████    | 155/256 [00:00<00:00, 276.32it/s]
Adding requests:  74%|███████▍  | 189/256 [00:00<00:00, 293.18it/s]
Adding requests:  87%|████████▋ | 223/256 [00:00<00:00, 306.16it/s]
Adding requests: 100%|█████████▉| 255/256 [00:01<00:00, 309.43it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 250.99it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:00<00:00, 317.09it/s, est. speed input: 324723.76 toks/s, output: 317.10 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:00<00:02, 78.03it/s, est. speed input: 91083.26 toks/s, output: 88.95 toks/s]   
Processed prompts:  34%|███▎      | 86/256 [00:01<00:02, 66.30it/s, est. speed input: 78540.12 toks/s, output: 76.70 toks/s]
Processed prompts:  38%|███▊      | 97/256 [00:01<00:02, 62.79it/s, est. speed input: 74677.50 toks/s, output: 72.93 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:01<00:02, 57.57it/s, est. speed input: 70578.22 toks/s, output: 68.92 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:01<00:02, 55.40it/s, est. speed input: 68523.52 toks/s, output: 66.92 toks/s]
Processed prompts:  47%|████▋     | 121/256 [00:01<00:02, 55.56it/s, est. speed input: 67776.56 toks/s, output: 66.19 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:01<00:02, 51.87it/s, est. speed input: 65733.20 toks/s, output: 64.19 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:02<00:02, 51.00it/s, est. speed input: 64762.16 toks/s, output: 63.24 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:02<00:02, 50.23it/s, est. speed input: 63882.47 toks/s, output: 62.39 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:02<00:02, 49.71it/s, est. speed input: 63122.65 toks/s, output: 61.64 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:02<00:02, 49.26it/s, est. speed input: 62424.40 toks/s, output: 60.96 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:02<00:02, 48.91it/s, est. speed input: 61791.67 toks/s, output: 60.34 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:02<00:01, 48.69it/s, est. speed input: 61222.99 toks/s, output: 59.79 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:02<00:01, 48.55it/s, est. speed input: 60707.74 toks/s, output: 59.28 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:02<00:01, 48.47it/s, est. speed input: 60240.15 toks/s, output: 58.83 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:03<00:01, 48.37it/s, est. speed input: 59802.21 toks/s, output: 58.40 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:03<00:01, 48.25it/s, est. speed input: 59390.14 toks/s, output: 58.00 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:03<00:01, 48.20it/s, est. speed input: 59013.06 toks/s, output: 57.63 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:03<00:01, 48.18it/s, est. speed input: 58665.62 toks/s, output: 57.29 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:03<00:01, 47.02it/s, est. speed input: 58179.13 toks/s, output: 56.82 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:03<00:00, 47.24it/s, est. speed input: 57869.18 toks/s, output: 56.51 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:03<00:00, 46.97it/s, est. speed input: 57522.02 toks/s, output: 56.17 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:04<00:00, 47.31it/s, est. speed input: 57264.80 toks/s, output: 55.92 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:04<00:00, 47.55it/s, est. speed input: 57023.81 toks/s, output: 55.69 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:04<00:00, 47.51it/s, est. speed input: 56772.21 toks/s, output: 55.44 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:04<00:00, 47.61it/s, est. speed input: 56549.80 toks/s, output: 55.22 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:04<00:00, 47.81it/s, est. speed input: 56353.16 toks/s, output: 55.03 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:04<00:00, 47.86it/s, est. speed input: 56157.91 toks/s, output: 54.84 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 47.86it/s, est. speed input: 56086.29 toks/s, output: 54.77 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 54.77it/s, est. speed input: 56086.29 toks/s, output: 54.77 toks/s]
[rank0]:[W126 18:26:41.842573564 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 18:26:43
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:26:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=910867) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=910867) WARNING 01-26 18:27:14 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=910867) WARNING 01-26 18:27:29 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 62.26 requests/s, 63813.85 total tokens/s, 62.26 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 18:26:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:26:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:26:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:26:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:26:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:26:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:26:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:26:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:26:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:26:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:26:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:26:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:26:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:26:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:26:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:26:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:26:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:00] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:00] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:00] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:00] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=910867) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=910867) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.10s/it]
(EngineCore_DP0 pid=910867) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.58it/s]
(EngineCore_DP0 pid=910867) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.28it/s]
(EngineCore_DP0 pid=910867) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.12it/s]
(EngineCore_DP0 pid=910867) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.17it/s]
(EngineCore_DP0 pid=910867) 
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=910867) [2026-01-26 18:27:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=910867) 2026-01-26 18:27:29,837 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=910867) 2026-01-26 18:27:29,875 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=910867) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  9.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.26it/s]
(EngineCore_DP0 pid=910867) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 11.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 11.25it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 27/512 [00:00<00:01, 263.62it/s]
Adding requests:  11%|█         | 54/512 [00:00<00:01, 264.43it/s]
Adding requests:  17%|█▋        | 87/512 [00:00<00:01, 292.98it/s]
Adding requests:  23%|██▎       | 120/512 [00:00<00:01, 306.85it/s]
Adding requests:  30%|██▉       | 153/512 [00:00<00:01, 314.42it/s]
Adding requests:  37%|███▋      | 190/512 [00:00<00:00, 331.78it/s]
Adding requests:  44%|████▍     | 225/512 [00:00<00:00, 336.55it/s]
Adding requests:  51%|█████     | 259/512 [00:00<00:00, 321.84it/s]
Adding requests:  57%|█████▋    | 293/512 [00:00<00:00, 326.54it/s]
Adding requests:  64%|██████▍   | 329/512 [00:01<00:00, 336.53it/s]
Adding requests:  71%|███████▏  | 366/512 [00:01<00:00, 344.39it/s]
Adding requests:  79%|███████▉  | 404/512 [00:01<00:00, 353.16it/s]
Adding requests:  86%|████████▌ | 440/512 [00:01<00:00, 352.32it/s]
Adding requests:  93%|█████████▎| 477/512 [00:01<00:00, 356.76it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 335.69it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:00<00:00, 669.23it/s, est. speed input: 685327.32 toks/s, output: 669.24 toks/s]
Processed prompts:  32%|███▏      | 165/512 [00:01<00:02, 120.59it/s, est. speed input: 144613.23 toks/s, output: 141.22 toks/s]
Processed prompts:  38%|███▊      | 197/512 [00:01<00:03, 97.72it/s, est. speed input: 120075.35 toks/s, output: 117.26 toks/s] 
Processed prompts:  42%|████▏     | 217/512 [00:01<00:03, 88.93it/s, est. speed input: 111396.12 toks/s, output: 108.78 toks/s]
Processed prompts:  45%|████▌     | 232/512 [00:02<00:03, 82.09it/s, est. speed input: 105635.66 toks/s, output: 103.16 toks/s]
Processed prompts:  48%|████▊     | 244/512 [00:02<00:03, 78.28it/s, est. speed input: 102398.65 toks/s, output: 100.00 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:02<00:03, 72.58it/s, est. speed input: 98856.13 toks/s, output: 96.54 toks/s]  
Processed prompts:  51%|█████▏    | 263/512 [00:02<00:03, 72.26it/s, est. speed input: 97637.38 toks/s, output: 95.35 toks/s]
Processed prompts:  53%|█████▎    | 271/512 [00:02<00:03, 70.35it/s, est. speed input: 96154.58 toks/s, output: 93.90 toks/s]
Processed prompts:  54%|█████▍    | 279/512 [00:03<00:03, 68.67it/s, est. speed input: 94803.49 toks/s, output: 92.58 toks/s]
Processed prompts:  56%|█████▌    | 287/512 [00:03<00:03, 67.14it/s, est. speed input: 93538.22 toks/s, output: 91.35 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:03<00:03, 64.14it/s, est. speed input: 92108.55 toks/s, output: 89.95 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:03<00:03, 63.70it/s, est. speed input: 91048.63 toks/s, output: 88.91 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:03<00:03, 64.70it/s, est. speed input: 90310.42 toks/s, output: 88.19 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:03, 64.12it/s, est. speed input: 89395.60 toks/s, output: 87.30 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:03<00:02, 63.68it/s, est. speed input: 88539.07 toks/s, output: 86.46 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:03<00:02, 63.17it/s, est. speed input: 87707.05 toks/s, output: 85.65 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:04<00:02, 63.03it/s, est. speed input: 86961.97 toks/s, output: 84.92 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:04<00:02, 62.87it/s, est. speed input: 86254.22 toks/s, output: 84.23 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:04<00:02, 62.78it/s, est. speed input: 85592.05 toks/s, output: 83.59 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:04<00:02, 62.74it/s, est. speed input: 84970.94 toks/s, output: 82.98 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:04<00:02, 62.71it/s, est. speed input: 84384.83 toks/s, output: 82.41 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:04<00:02, 62.66it/s, est. speed input: 83827.13 toks/s, output: 81.86 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:04<00:01, 62.58it/s, est. speed input: 83293.77 toks/s, output: 81.34 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:04<00:01, 62.51it/s, est. speed input: 82787.36 toks/s, output: 80.85 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:05<00:01, 62.60it/s, est. speed input: 82321.22 toks/s, output: 80.39 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:05<00:01, 62.55it/s, est. speed input: 81865.82 toks/s, output: 79.95 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:05<00:01, 62.55it/s, est. speed input: 81436.47 toks/s, output: 79.53 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:05<00:01, 62.63it/s, est. speed input: 81035.30 toks/s, output: 79.14 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:05<00:01, 64.14it/s, est. speed input: 80792.66 toks/s, output: 78.90 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:05<00:01, 63.43it/s, est. speed input: 80394.54 toks/s, output: 78.51 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:05<00:00, 63.20it/s, est. speed input: 80038.03 toks/s, output: 78.16 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:05<00:00, 63.09it/s, est. speed input: 79701.87 toks/s, output: 77.83 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:06<00:00, 62.91it/s, est. speed input: 79370.19 toks/s, output: 77.51 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:06<00:00, 62.93it/s, est. speed input: 79065.00 toks/s, output: 77.21 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:06<00:00, 62.87it/s, est. speed input: 78765.79 toks/s, output: 76.92 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:06<00:00, 62.70it/s, est. speed input: 78467.49 toks/s, output: 76.63 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:06<00:00, 62.60it/s, est. speed input: 78182.56 toks/s, output: 76.35 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:06<00:00, 63.41it/s, est. speed input: 77977.73 toks/s, output: 76.15 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 63.41it/s, est. speed input: 78282.62 toks/s, output: 76.45 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 76.45it/s, est. speed input: 78282.62 toks/s, output: 76.45 toks/s]
[rank0]:[W126 18:27:40.745833649 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 18:27:42
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:27:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=912288) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=912288) WARNING 01-26 18:28:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=912288) WARNING 01-26 18:28:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 67.27 requests/s, 68950.59 total tokens/s, 67.27 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 18:27:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:27:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:27:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:27:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:27:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:27:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:27:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:27:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:27:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:27:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:27:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:27:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:27:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:27:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:28:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:28:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:28:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:28:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:28:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:28:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:28:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:28:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:28:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:28:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:28:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:28:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:28:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:28:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=912288) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=912288) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.03s/it]
(EngineCore_DP0 pid=912288) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.62it/s]
(EngineCore_DP0 pid=912288) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.25it/s]
(EngineCore_DP0 pid=912288) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.11it/s]
(EngineCore_DP0 pid=912288) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.16it/s]
(EngineCore_DP0 pid=912288) 
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=912288) [2026-01-26 18:28:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=912288) 2026-01-26 18:28:32,929 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=912288) 2026-01-26 18:28:32,969 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=912288) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.39it/s]
(EngineCore_DP0 pid=912288) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 11.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.36it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 275.67it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 317.83it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 315.14it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 320.87it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 330.42it/s]
Adding requests:  20%|█▉        | 200/1024 [00:00<00:02, 341.39it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:02, 345.82it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:02, 344.04it/s]
Adding requests:  30%|███       | 308/1024 [00:00<00:02, 350.74it/s]
Adding requests:  34%|███▎      | 345/1024 [00:01<00:01, 354.83it/s]
Adding requests:  37%|███▋      | 382/1024 [00:01<00:01, 356.77it/s]
Adding requests:  41%|████      | 421/1024 [00:01<00:01, 364.23it/s]
Adding requests:  45%|████▍     | 458/1024 [00:01<00:01, 361.92it/s]
Adding requests:  49%|████▊     | 498/1024 [00:01<00:01, 370.47it/s]
Adding requests:  52%|█████▏    | 537/1024 [00:01<00:01, 374.74it/s]
Adding requests:  56%|█████▌    | 575/1024 [00:01<00:01, 372.62it/s]
Adding requests:  60%|█████▉    | 613/1024 [00:01<00:01, 362.35it/s]
Adding requests:  63%|██████▎   | 650/1024 [00:01<00:01, 356.36it/s]
Adding requests:  67%|██████▋   | 687/1024 [00:01<00:00, 359.82it/s]
Adding requests:  71%|███████   | 724/1024 [00:02<00:00, 355.50it/s]
Adding requests:  74%|███████▍  | 760/1024 [00:02<00:00, 353.85it/s]
Adding requests:  78%|███████▊  | 796/1024 [00:02<00:00, 355.05it/s]
Adding requests:  81%|████████▏ | 833/1024 [00:02<00:00, 358.97it/s]
Adding requests:  85%|████████▍ | 870/1024 [00:02<00:00, 361.02it/s]
Adding requests:  89%|████████▊ | 907/1024 [00:02<00:00, 362.58it/s]
Adding requests:  92%|█████████▏| 944/1024 [00:02<00:00, 356.01it/s]
Adding requests:  96%|█████████▌| 980/1024 [00:02<00:00, 356.53it/s]
Adding requests:  99%|█████████▉| 1016/1024 [00:02<00:00, 352.55it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 353.17it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:00<00:00, 1526.93it/s, est. speed input: 1563676.25 toks/s, output: 1526.96 toks/s]
Processed prompts:  34%|███▍      | 347/1024 [00:02<00:05, 123.96it/s, est. speed input: 150061.94 toks/s, output: 146.54 toks/s]   
Processed prompts:  40%|████      | 414/1024 [00:03<00:05, 104.57it/s, est. speed input: 128034.87 toks/s, output: 125.03 toks/s]
Processed prompts:  44%|████▍     | 454/1024 [00:03<00:05, 95.67it/s, est. speed input: 119305.19 toks/s, output: 116.51 toks/s] 
Processed prompts:  47%|████▋     | 481/1024 [00:04<00:05, 92.29it/s, est. speed input: 115878.45 toks/s, output: 113.16 toks/s]
Processed prompts:  49%|████▉     | 501/1024 [00:04<00:06, 85.05it/s, est. speed input: 111375.79 toks/s, output: 108.77 toks/s]
Processed prompts:  50%|█████     | 516/1024 [00:04<00:06, 81.57it/s, est. speed input: 109101.11 toks/s, output: 106.54 toks/s]
Processed prompts:  52%|█████▏    | 529/1024 [00:04<00:05, 84.54it/s, est. speed input: 109201.33 toks/s, output: 106.64 toks/s]
Processed prompts:  53%|█████▎    | 541/1024 [00:05<00:06, 77.06it/s, est. speed input: 106646.56 toks/s, output: 104.15 toks/s]
Processed prompts:  54%|█████▍    | 551/1024 [00:05<00:06, 77.79it/s, est. speed input: 106132.19 toks/s, output: 103.64 toks/s]
Processed prompts:  55%|█████▍    | 561/1024 [00:05<00:05, 79.05it/s, est. speed input: 105730.40 toks/s, output: 103.25 toks/s]
Processed prompts:  56%|█████▌    | 571/1024 [00:05<00:06, 67.35it/s, est. speed input: 103151.49 toks/s, output: 100.73 toks/s]
Processed prompts:  57%|█████▋    | 579/1024 [00:05<00:06, 67.51it/s, est. speed input: 102476.89 toks/s, output: 100.08 toks/s]
Processed prompts:  57%|█████▋    | 587/1024 [00:05<00:06, 67.00it/s, est. speed input: 101733.71 toks/s, output: 99.35 toks/s] 
Processed prompts:  58%|█████▊    | 595/1024 [00:06<00:06, 66.76it/s, est. speed input: 101046.97 toks/s, output: 98.68 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:06<00:06, 64.74it/s, est. speed input: 100250.74 toks/s, output: 97.90 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:06<00:06, 65.59it/s, est. speed input: 99675.97 toks/s, output: 97.34 toks/s] 
Processed prompts:  60%|██████    | 618/1024 [00:06<00:06, 66.69it/s, est. speed input: 99168.46 toks/s, output: 96.84 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:06<00:05, 66.60it/s, est. speed input: 98590.24 toks/s, output: 96.28 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:06<00:05, 67.03it/s, est. speed input: 98078.12 toks/s, output: 95.78 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:06<00:05, 67.24it/s, est. speed input: 97575.02 toks/s, output: 95.29 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:06<00:05, 67.10it/s, est. speed input: 97064.93 toks/s, output: 94.79 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:06<00:05, 67.58it/s, est. speed input: 96620.08 toks/s, output: 94.36 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:07<00:05, 67.66it/s, est. speed input: 96168.39 toks/s, output: 93.91 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:07<00:05, 67.22it/s, est. speed input: 95693.38 toks/s, output: 93.45 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:07<00:05, 67.71it/s, est. speed input: 95294.45 toks/s, output: 93.06 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:07<00:04, 68.17it/s, est. speed input: 94916.41 toks/s, output: 92.69 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:07<00:04, 67.63it/s, est. speed input: 94488.04 toks/s, output: 92.27 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:07<00:04, 67.04it/s, est. speed input: 94056.49 toks/s, output: 91.85 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:07<00:04, 66.72it/s, est. speed input: 93645.12 toks/s, output: 91.45 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:07<00:04, 66.74it/s, est. speed input: 93263.82 toks/s, output: 91.08 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:08<00:04, 66.55it/s, est. speed input: 92879.21 toks/s, output: 90.70 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:08<00:04, 66.61it/s, est. speed input: 92519.05 toks/s, output: 90.35 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:08<00:04, 66.77it/s, est. speed input: 92177.72 toks/s, output: 90.02 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:08<00:04, 66.46it/s, est. speed input: 91817.86 toks/s, output: 89.67 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:08<00:03, 67.10it/s, est. speed input: 91523.66 toks/s, output: 89.38 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:08<00:03, 67.17it/s, est. speed input: 91213.31 toks/s, output: 89.08 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:08<00:03, 67.22it/s, est. speed input: 90911.69 toks/s, output: 88.78 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:08<00:03, 69.16it/s, est. speed input: 90729.57 toks/s, output: 88.60 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:08<00:03, 69.01it/s, est. speed input: 90464.95 toks/s, output: 88.34 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:09<00:03, 68.05it/s, est. speed input: 90158.52 toks/s, output: 88.05 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:09<00:03, 68.06it/s, est. speed input: 89898.08 toks/s, output: 87.79 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:09<00:03, 68.01it/s, est. speed input: 89641.60 toks/s, output: 87.54 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:09<00:02, 67.93it/s, est. speed input: 89388.33 toks/s, output: 87.29 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:09<00:02, 67.48it/s, est. speed input: 89120.53 toks/s, output: 87.03 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:09<00:02, 67.73it/s, est. speed input: 88889.15 toks/s, output: 86.81 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:09<00:02, 67.33it/s, est. speed input: 88633.00 toks/s, output: 86.56 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:09<00:02, 66.93it/s, est. speed input: 88376.72 toks/s, output: 86.31 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:10<00:02, 66.92it/s, est. speed input: 88140.25 toks/s, output: 86.07 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:10<00:02, 67.38it/s, est. speed input: 87933.50 toks/s, output: 85.87 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:10<00:02, 67.44it/s, est. speed input: 87718.34 toks/s, output: 85.66 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:10<00:01, 67.69it/s, est. speed input: 87518.02 toks/s, output: 85.47 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:10<00:01, 67.31it/s, est. speed input: 87295.44 toks/s, output: 85.25 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:10<00:01, 66.52it/s, est. speed input: 87051.95 toks/s, output: 85.01 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:10<00:01, 67.17it/s, est. speed input: 86871.94 toks/s, output: 84.84 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:10<00:01, 66.79it/s, est. speed input: 86655.93 toks/s, output: 84.62 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:11<00:01, 66.15it/s, est. speed input: 86427.43 toks/s, output: 84.40 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:11<00:01, 66.60it/s, est. speed input: 86245.20 toks/s, output: 84.22 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:11<00:01, 67.37it/s, est. speed input: 86087.67 toks/s, output: 84.07 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:11<00:01, 67.46it/s, est. speed input: 85913.04 toks/s, output: 83.90 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:11<00:00, 67.47it/s, est. speed input: 85739.99 toks/s, output: 83.73 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:11<00:00, 68.04it/s, est. speed input: 85594.32 toks/s, output: 83.59 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:11<00:00, 67.37it/s, est. speed input: 85405.99 toks/s, output: 83.40 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:11<00:00, 66.90it/s, est. speed input: 85221.60 toks/s, output: 83.22 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:11<00:00, 66.99it/s, est. speed input: 85058.27 toks/s, output: 83.06 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:12<00:00, 66.81it/s, est. speed input: 84888.08 toks/s, output: 82.90 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:12<00:00, 66.47it/s, est. speed input: 84712.53 toks/s, output: 82.73 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:12<00:00, 67.74it/s, est. speed input: 84601.47 toks/s, output: 82.62 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 67.74it/s, est. speed input: 85099.26 toks/s, output: 83.10 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 83.10it/s, est. speed input: 85099.26 toks/s, output: 83.10 toks/s]
[rank0]:[W126 18:28:51.414176064 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 18:28:53
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:29:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=913886) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=913886) WARNING 01-26 18:29:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=913886) WARNING 01-26 18:29:52 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 61.35 requests/s, 62881.70 total tokens/s, 61.35 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 18:29:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:29:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:29:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:29:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:29:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:29:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:29:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:29:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:29:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:29:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:29:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:29:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:29:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:29:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:29:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:29:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:29:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=913886) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=913886) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.02s/it]
(EngineCore_DP0 pid=913886) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.66it/s]
(EngineCore_DP0 pid=913886) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.31it/s]
(EngineCore_DP0 pid=913886) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.14it/s]
(EngineCore_DP0 pid=913886) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.20it/s]
(EngineCore_DP0 pid=913886) 
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=913886) [2026-01-26 18:29:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=913886) [rank0]:W0126 18:29:42.035000 913886 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=913886) [rank0]:W0126 18:29:42.104000 913886 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=913886) [rank0]:W0126 18:29:42.900000 913886 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=913886) [rank0]:W0126 18:29:43.009000 913886 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=913886) 2026-01-26 18:29:51,835 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=913886) 2026-01-26 18:29:51,890 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=913886) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 10.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 10.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 10.03it/s]
(EngineCore_DP0 pid=913886) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 10.95it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 10.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.51it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 276.66it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 316.63it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 313.84it/s]
Adding requests:   6%|▌         | 127/2048 [00:00<00:06, 306.82it/s]
Adding requests:   8%|▊         | 162/2048 [00:00<00:05, 318.95it/s]
Adding requests:  10%|▉         | 198/2048 [00:00<00:05, 331.80it/s]
Adding requests:  11%|█▏        | 234/2048 [00:00<00:05, 338.88it/s]
Adding requests:  13%|█▎        | 269/2048 [00:00<00:05, 339.62it/s]
Adding requests:  15%|█▍        | 305/2048 [00:00<00:05, 344.17it/s]
Adding requests:  17%|█▋        | 341/2048 [00:01<00:04, 347.81it/s]
Adding requests:  18%|█▊        | 377/2048 [00:01<00:04, 350.69it/s]
Adding requests:  20%|██        | 415/2048 [00:01<00:04, 357.82it/s]
Adding requests:  22%|██▏       | 451/2048 [00:01<00:04, 354.31it/s]
Adding requests:  24%|██▍       | 491/2048 [00:01<00:04, 366.35it/s]
Adding requests:  26%|██▌       | 531/2048 [00:01<00:04, 375.30it/s]
Adding requests:  28%|██▊       | 569/2048 [00:01<00:03, 372.12it/s]
Adding requests:  30%|██▉       | 607/2048 [00:01<00:04, 354.69it/s]
Adding requests:  31%|███▏      | 643/2048 [00:01<00:04, 349.09it/s]
Adding requests:  33%|███▎      | 679/2048 [00:01<00:03, 350.44it/s]
Adding requests:  35%|███▍      | 716/2048 [00:02<00:03, 354.11it/s]
Adding requests:  37%|███▋      | 752/2048 [00:02<00:03, 349.46it/s]
Adding requests:  39%|███▊      | 789/2048 [00:02<00:03, 352.11it/s]
Adding requests:  40%|████      | 825/2048 [00:02<00:03, 354.10it/s]
Adding requests:  42%|████▏     | 863/2048 [00:02<00:03, 359.31it/s]
Adding requests:  44%|████▍     | 900/2048 [00:02<00:03, 362.35it/s]
Adding requests:  46%|████▌     | 937/2048 [00:02<00:03, 356.21it/s]
Adding requests:  48%|████▊     | 974/2048 [00:02<00:02, 358.78it/s]
Adding requests:  49%|████▉     | 1010/2048 [00:02<00:02, 354.54it/s]
Adding requests:  51%|█████     | 1046/2048 [00:02<00:02, 353.01it/s]
Adding requests:  53%|█████▎    | 1082/2048 [00:03<00:02, 353.66it/s]
Adding requests:  55%|█████▍    | 1118/2048 [00:03<00:02, 351.45it/s]
Adding requests:  56%|█████▋    | 1154/2048 [00:03<00:02, 353.94it/s]
Adding requests:  58%|█████▊    | 1190/2048 [00:03<00:02, 354.41it/s]
Adding requests:  60%|█████▉    | 1228/2048 [00:03<00:02, 360.57it/s]
Adding requests:  62%|██████▏   | 1265/2048 [00:03<00:02, 358.30it/s]
Adding requests:  64%|██████▎   | 1301/2048 [00:03<00:02, 355.99it/s]
Adding requests:  65%|██████▌   | 1337/2048 [00:03<00:02, 354.36it/s]
Adding requests:  67%|██████▋   | 1374/2048 [00:03<00:01, 357.72it/s]
Adding requests:  69%|██████▉   | 1410/2048 [00:04<00:01, 347.22it/s]
Adding requests:  71%|███████   | 1446/2048 [00:04<00:01, 348.43it/s]
Adding requests:  72%|███████▏  | 1484/2048 [00:04<00:01, 354.89it/s]
Adding requests:  74%|███████▍  | 1521/2048 [00:04<00:01, 359.18it/s]
Adding requests:  76%|███████▌  | 1557/2048 [00:04<00:01, 354.53it/s]
Adding requests:  78%|███████▊  | 1593/2048 [00:04<00:01, 351.71it/s]
Adding requests:  80%|███████▉  | 1629/2048 [00:04<00:01, 345.78it/s]
Adding requests:  81%|████████▏ | 1664/2048 [00:04<00:01, 339.35it/s]
Adding requests:  83%|████████▎ | 1700/2048 [00:04<00:01, 344.44it/s]
Adding requests:  85%|████████▍ | 1737/2048 [00:04<00:00, 351.05it/s]
Adding requests:  87%|████████▋ | 1775/2048 [00:05<00:00, 358.98it/s]
Adding requests:  88%|████████▊ | 1811/2048 [00:05<00:00, 344.63it/s]
Adding requests:  90%|█████████ | 1848/2048 [00:05<00:00, 349.30it/s]
Adding requests:  92%|█████████▏| 1885/2048 [00:05<00:00, 353.49it/s]
Adding requests:  94%|█████████▍| 1922/2048 [00:05<00:00, 357.60it/s]
Adding requests:  96%|█████████▌| 1960/2048 [00:05<00:00, 363.22it/s]
Adding requests:  98%|█████████▊| 1997/2048 [00:05<00:00, 355.31it/s]
Adding requests:  99%|█████████▉| 2033/2048 [00:05<00:00, 350.24it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 350.95it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:00<00:00, 1666.87it/s, est. speed input: 1706926.71 toks/s, output: 1666.88 toks/s]
Processed prompts:  28%|██▊       | 569/2048 [00:02<00:08, 180.96it/s, est. speed input: 228470.12 toks/s, output: 223.11 toks/s]   
Processed prompts:  31%|███▏      | 642/2048 [00:03<00:10, 133.29it/s, est. speed input: 176810.40 toks/s, output: 172.67 toks/s]
Processed prompts:  33%|███▎      | 685/2048 [00:04<00:10, 125.59it/s, est. speed input: 167603.04 toks/s, output: 163.67 toks/s]
Processed prompts:  35%|███▍      | 715/2048 [00:04<00:11, 112.83it/s, est. speed input: 157490.50 toks/s, output: 153.80 toks/s]
Processed prompts:  36%|███▌      | 737/2048 [00:04<00:11, 110.26it/s, est. speed input: 154507.49 toks/s, output: 150.89 toks/s]
Processed prompts:  37%|███▋      | 755/2048 [00:05<00:14, 90.96it/s, est. speed input: 144574.99 toks/s, output: 141.19 toks/s] 
Processed prompts:  38%|███▊      | 770/2048 [00:05<00:14, 86.72it/s, est. speed input: 141331.13 toks/s, output: 138.02 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:05<00:14, 84.41it/s, est. speed input: 138867.26 toks/s, output: 135.61 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:06<00:15, 80.99it/s, est. speed input: 136205.50 toks/s, output: 133.01 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:06<00:15, 78.16it/s, est. speed input: 133769.79 toks/s, output: 130.63 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:06<00:16, 75.73it/s, est. speed input: 131483.10 toks/s, output: 128.40 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:06<00:16, 74.06it/s, est. speed input: 129407.75 toks/s, output: 126.37 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:06<00:16, 72.63it/s, est. speed input: 127441.00 toks/s, output: 124.45 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:07<00:16, 71.55it/s, est. speed input: 125597.30 toks/s, output: 122.65 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:07<00:16, 70.90it/s, est. speed input: 123896.98 toks/s, output: 120.99 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:07<00:16, 70.29it/s, est. speed input: 122271.87 toks/s, output: 119.41 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:07<00:15, 69.92it/s, est. speed input: 120756.47 toks/s, output: 117.93 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:08<00:15, 69.73it/s, est. speed input: 119338.64 toks/s, output: 116.54 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:08<00:15, 68.97it/s, est. speed input: 117899.39 toks/s, output: 115.14 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:08<00:15, 68.93it/s, est. speed input: 116616.90 toks/s, output: 113.88 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:08<00:15, 69.00it/s, est. speed input: 115415.78 toks/s, output: 112.71 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:09<00:15, 68.92it/s, est. speed input: 114257.47 toks/s, output: 111.58 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:09<00:14, 68.77it/s, est. speed input: 113144.63 toks/s, output: 110.49 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:09<00:15, 66.95it/s, est. speed input: 111915.71 toks/s, output: 109.29 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:09<00:15, 64.19it/s, est. speed input: 110409.46 toks/s, output: 107.82 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:10<00:15, 62.35it/s, est. speed input: 109033.65 toks/s, output: 106.48 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:10<00:15, 62.32it/s, est. speed input: 107945.08 toks/s, output: 105.41 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:10<00:15, 60.70it/s, est. speed input: 106613.60 toks/s, output: 104.11 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:10<00:15, 60.18it/s, est. speed input: 105460.10 toks/s, output: 102.99 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:11<00:15, 60.56it/s, est. speed input: 104470.28 toks/s, output: 102.02 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:11<00:15, 56.51it/s, est. speed input: 102920.01 toks/s, output: 100.51 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:11<00:15, 58.03it/s, est. speed input: 102050.33 toks/s, output: 99.66 toks/s] 
Processed prompts:  58%|█████▊    | 1186/2048 [00:11<00:14, 59.83it/s, est. speed input: 101302.03 toks/s, output: 98.93 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:12<00:13, 61.32it/s, est. speed input: 100608.06 toks/s, output: 98.25 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:12<00:14, 58.82it/s, est. speed input: 99525.19 toks/s, output: 97.19 toks/s] 
Processed prompts:  60%|██████    | 1234/2048 [00:12<00:14, 57.55it/s, est. speed input: 98535.35 toks/s, output: 96.22 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:13<00:15, 52.85it/s, est. speed input: 97082.76 toks/s, output: 94.81 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:13<00:14, 53.00it/s, est. speed input: 96138.02 toks/s, output: 93.88 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:13<00:13, 56.20it/s, est. speed input: 95621.23 toks/s, output: 93.38 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:13<00:12, 57.73it/s, est. speed input: 95015.78 toks/s, output: 92.79 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:14<00:12, 59.32it/s, est. speed input: 94485.13 toks/s, output: 92.27 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:14<00:12, 58.40it/s, est. speed input: 93765.92 toks/s, output: 91.57 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:14<00:12, 57.67it/s, est. speed input: 93064.43 toks/s, output: 90.88 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:15<00:11, 57.86it/s, est. speed input: 92455.39 toks/s, output: 90.29 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:15<00:11, 56.44it/s, est. speed input: 91718.93 toks/s, output: 89.57 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:15<00:11, 55.44it/s, est. speed input: 91006.50 toks/s, output: 88.87 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:15<00:11, 56.74it/s, est. speed input: 90515.03 toks/s, output: 88.39 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:16<00:11, 54.62it/s, est. speed input: 89765.09 toks/s, output: 87.65 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:16<00:11, 53.52it/s, est. speed input: 89042.13 toks/s, output: 86.96 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:16<00:11, 49.59it/s, est. speed input: 88025.25 toks/s, output: 85.96 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:17<00:12, 45.31it/s, est. speed input: 86818.40 toks/s, output: 84.78 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:17<00:11, 47.99it/s, est. speed input: 86334.42 toks/s, output: 84.31 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:17<00:10, 51.56it/s, est. speed input: 86013.10 toks/s, output: 84.00 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:18<00:10, 51.46it/s, est. speed input: 85439.37 toks/s, output: 83.44 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:18<00:09, 53.06it/s, est. speed input: 85033.18 toks/s, output: 83.04 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:18<00:08, 55.14it/s, est. speed input: 84711.43 toks/s, output: 82.73 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:19<00:08, 57.40it/s, est. speed input: 84452.01 toks/s, output: 82.47 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:19<00:07, 58.89it/s, est. speed input: 84182.40 toks/s, output: 82.21 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:19<00:07, 59.75it/s, est. speed input: 83908.36 toks/s, output: 81.94 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:19<00:07, 57.77it/s, est. speed input: 83472.31 toks/s, output: 81.52 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:20<00:07, 57.03it/s, est. speed input: 83087.92 toks/s, output: 81.14 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:20<00:06, 57.32it/s, est. speed input: 82768.57 toks/s, output: 80.83 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:20<00:06, 57.89it/s, est. speed input: 82478.53 toks/s, output: 80.55 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:20<00:06, 59.19it/s, est. speed input: 82253.80 toks/s, output: 80.32 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:21<00:05, 58.48it/s, est. speed input: 81935.35 toks/s, output: 80.01 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:21<00:05, 58.93it/s, est. speed input: 81679.73 toks/s, output: 79.77 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:21<00:05, 53.78it/s, est. speed input: 81091.47 toks/s, output: 79.19 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:22<00:05, 56.53it/s, est. speed input: 80917.34 toks/s, output: 79.02 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:22<00:05, 56.81it/s, est. speed input: 80662.89 toks/s, output: 78.75 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:22<00:04, 56.75it/s, est. speed input: 80385.53 toks/s, output: 78.48 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:22<00:04, 58.38it/s, est. speed input: 80179.18 toks/s, output: 78.30 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:23<00:04, 49.52it/s, est. speed input: 79378.54 toks/s, output: 77.52 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:23<00:04, 50.53it/s, est. speed input: 79059.46 toks/s, output: 77.21 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:23<00:03, 52.45it/s, est. speed input: 78825.53 toks/s, output: 76.98 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:24<00:03, 49.78it/s, est. speed input: 78331.56 toks/s, output: 76.50 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:24<00:03, 53.26it/s, est. speed input: 78197.59 toks/s, output: 76.36 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:24<00:03, 46.54it/s, est. speed input: 77460.05 toks/s, output: 75.64 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:25<00:02, 48.65it/s, est. speed input: 77207.91 toks/s, output: 75.40 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:25<00:02, 52.34it/s, est. speed input: 77088.84 toks/s, output: 75.28 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:25<00:01, 55.04it/s, est. speed input: 76958.81 toks/s, output: 75.16 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:26<00:01, 57.88it/s, est. speed input: 76870.08 toks/s, output: 75.07 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:26<00:01, 59.13it/s, est. speed input: 76741.05 toks/s, output: 74.94 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:26<00:01, 61.21it/s, est. speed input: 76665.51 toks/s, output: 74.87 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:27<00:00, 48.98it/s, est. speed input: 75912.67 toks/s, output: 74.13 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:27<00:00, 51.63it/s, est. speed input: 75759.03 toks/s, output: 73.98 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:27<00:00, 54.35it/s, est. speed input: 75644.29 toks/s, output: 73.87 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:27<00:00, 54.35it/s, est. speed input: 76160.31 toks/s, output: 74.38 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:27<00:00, 74.37it/s, est. speed input: 76160.31 toks/s, output: 74.38 toks/s]
[rank0]:[W126 18:30:29.937631351 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 18:30:39
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:31:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=916160) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=916160) WARNING 01-26 18:31:51 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=916160) WARNING 01-26 18:32:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 69.52 requests/s, 71258.28 total tokens/s, 69.52 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 18:31:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:31:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:31:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:31:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:31:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:31:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:31:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:31:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:31:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:31:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:31:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:31:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:31:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:31:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:31:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:31:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:31:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=916160) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=916160) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.53s/it]
(EngineCore_DP0 pid=916160) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.16it/s]
(EngineCore_DP0 pid=916160) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.08s/it]
(EngineCore_DP0 pid=916160) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.22s/it]
(EngineCore_DP0 pid=916160) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]
(EngineCore_DP0 pid=916160) 
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=916160) [2026-01-26 18:31:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=916160) [rank0]:W0126 18:32:00.488000 916160 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=916160) [rank0]:W0126 18:32:00.607000 916160 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=916160) [rank0]:W0126 18:32:01.710000 916160 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=916160) [rank0]:W0126 18:32:01.825000 916160 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=916160) 2026-01-26 18:32:13,046 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=916160) 2026-01-26 18:32:13,158 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=916160) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  6.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:01,  6.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:01,  5.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  6.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.69it/s]
(EngineCore_DP0 pid=916160) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  9.76it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  9.89it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  6.31it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  6.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  6.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  6.83it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   0%|          | 13/4096 [00:00<00:35, 113.68it/s]
Adding requests:   1%|          | 25/4096 [00:00<00:41, 97.71it/s] 
Adding requests:   1%|          | 35/4096 [00:00<00:41, 97.31it/s]
Adding requests:   1%|          | 46/4096 [00:00<00:39, 101.42it/s]
Adding requests:   2%|▏         | 73/4096 [00:00<00:25, 156.53it/s]
Adding requests:   2%|▏         | 100/4096 [00:00<00:20, 191.99it/s]
Adding requests:   3%|▎         | 127/4096 [00:00<00:18, 215.04it/s]
Adding requests:   4%|▎         | 149/4096 [00:00<00:18, 215.98it/s]
Adding requests:   4%|▍         | 177/4096 [00:00<00:16, 233.14it/s]
Adding requests:   5%|▌         | 205/4096 [00:01<00:15, 245.01it/s]
Adding requests:   6%|▌         | 234/4096 [00:01<00:15, 257.42it/s]
Adding requests:   6%|▋         | 260/4096 [00:01<00:14, 257.86it/s]
Adding requests:   7%|▋         | 289/4096 [00:01<00:14, 266.36it/s]
Adding requests:   8%|▊         | 319/4096 [00:01<00:13, 274.43it/s]
Adding requests:   8%|▊         | 347/4096 [00:01<00:13, 272.26it/s]
Adding requests:   9%|▉         | 375/4096 [00:01<00:14, 261.38it/s]
Adding requests:  10%|▉         | 403/4096 [00:01<00:13, 266.04it/s]
Adding requests:  11%|█         | 431/4096 [00:01<00:13, 269.90it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:13, 272.26it/s]
Adding requests:  12%|█▏        | 490/4096 [00:02<00:12, 282.49it/s]
Adding requests:  13%|█▎        | 520/4096 [00:02<00:12, 287.14it/s]
Adding requests:  13%|█▎        | 550/4096 [00:02<00:12, 289.00it/s]
Adding requests:  14%|█▍        | 579/4096 [00:02<00:16, 213.74it/s]
Adding requests:  15%|█▍        | 604/4096 [00:02<00:17, 194.69it/s]
Adding requests:  15%|█▌        | 633/4096 [00:02<00:16, 216.39it/s]
Adding requests:  16%|█▌        | 661/4096 [00:02<00:14, 230.55it/s]
Adding requests:  17%|█▋        | 690/4096 [00:02<00:13, 245.70it/s]
Adding requests:  18%|█▊        | 717/4096 [00:03<00:13, 248.11it/s]
Adding requests:  18%|█▊        | 743/4096 [00:03<00:13, 250.39it/s]
Adding requests:  19%|█▉        | 770/4096 [00:03<00:13, 255.35it/s]
Adding requests:  19%|█▉        | 797/4096 [00:03<00:12, 258.37it/s]
Adding requests:  20%|██        | 827/4096 [00:03<00:12, 269.81it/s]
Adding requests:  21%|██        | 857/4096 [00:03<00:11, 278.61it/s]
Adding requests:  22%|██▏       | 886/4096 [00:03<00:11, 276.48it/s]
Adding requests:  22%|██▏       | 914/4096 [00:03<00:11, 267.76it/s]
Adding requests:  23%|██▎       | 941/4096 [00:03<00:11, 264.62it/s]
Adding requests:  24%|██▎       | 968/4096 [00:04<00:11, 264.56it/s]
Adding requests:  24%|██▍       | 995/4096 [00:04<00:11, 261.69it/s]
Adding requests:  25%|██▌       | 1024/4096 [00:04<00:11, 267.35it/s]
Adding requests:  26%|██▌       | 1052/4096 [00:04<00:11, 270.87it/s]
Adding requests:  26%|██▋       | 1080/4096 [00:04<00:11, 272.35it/s]
Adding requests:  27%|██▋       | 1108/4096 [00:04<00:11, 261.65it/s]
Adding requests:  28%|██▊       | 1135/4096 [00:04<00:11, 261.69it/s]
Adding requests:  28%|██▊       | 1162/4096 [00:04<00:11, 251.49it/s]
Adding requests:  29%|██▉       | 1189/4096 [00:04<00:11, 255.04it/s]
Adding requests:  30%|██▉       | 1215/4096 [00:04<00:11, 247.43it/s]
Adding requests:  30%|███       | 1241/4096 [00:05<00:11, 250.46it/s]
Adding requests:  31%|███       | 1268/4096 [00:05<00:11, 255.75it/s]
Adding requests:  32%|███▏      | 1295/4096 [00:05<00:10, 257.44it/s]
Adding requests:  32%|███▏      | 1321/4096 [00:05<00:10, 257.21it/s]
Adding requests:  33%|███▎      | 1349/4096 [00:05<00:10, 262.63it/s]
Adding requests:  34%|███▎      | 1378/4096 [00:05<00:10, 269.92it/s]
Adding requests:  34%|███▍      | 1406/4096 [00:05<00:09, 270.96it/s]
Adding requests:  35%|███▌      | 1434/4096 [00:05<00:11, 240.87it/s]
Adding requests:  36%|███▌      | 1460/4096 [00:05<00:10, 244.10it/s]
Adding requests:  36%|███▋      | 1487/4096 [00:06<00:10, 249.50it/s]
Adding requests:  37%|███▋      | 1515/4096 [00:06<00:10, 258.08it/s]
Adding requests:  38%|███▊      | 1543/4096 [00:06<00:09, 263.87it/s]
Adding requests:  38%|███▊      | 1570/4096 [00:06<00:09, 261.35it/s]
Adding requests:  39%|███▉      | 1597/4096 [00:06<00:09, 254.64it/s]
Adding requests:  40%|███▉      | 1623/4096 [00:06<00:11, 208.51it/s]
Adding requests:  40%|████      | 1646/4096 [00:06<00:15, 163.05it/s]
Adding requests:  41%|████      | 1665/4096 [00:07<00:16, 146.13it/s]
Adding requests:  41%|████      | 1682/4096 [00:07<00:16, 146.80it/s]
Adding requests:  41%|████▏     | 1698/4096 [00:07<00:17, 139.13it/s]
Adding requests:  42%|████▏     | 1714/4096 [00:07<00:16, 143.48it/s]
Adding requests:  43%|████▎     | 1741/4096 [00:07<00:13, 174.63it/s]
Adding requests:  43%|████▎     | 1772/4096 [00:07<00:11, 208.11it/s]
Adding requests:  44%|████▍     | 1798/4096 [00:07<00:10, 220.90it/s]
Adding requests:  45%|████▍     | 1826/4096 [00:07<00:09, 235.48it/s]
Adding requests:  45%|████▌     | 1854/4096 [00:07<00:09, 246.53it/s]
Adding requests:  46%|████▌     | 1880/4096 [00:08<00:10, 211.12it/s]
Adding requests:  46%|████▋     | 1903/4096 [00:08<00:13, 166.82it/s]
Adding requests:  47%|████▋     | 1922/4096 [00:08<00:13, 160.23it/s]
Adding requests:  48%|████▊     | 1953/4096 [00:08<00:11, 192.92it/s]
Adding requests:  48%|████▊     | 1982/4096 [00:08<00:09, 216.07it/s]
Adding requests:  49%|████▉     | 2009/4096 [00:08<00:09, 229.38it/s]
Adding requests:  50%|████▉     | 2036/4096 [00:08<00:08, 237.82it/s]
Adding requests:  50%|█████     | 2064/4096 [00:08<00:08, 248.76it/s]
Adding requests:  51%|█████     | 2090/4096 [00:09<00:08, 246.23it/s]
Adding requests:  52%|█████▏    | 2121/4096 [00:09<00:07, 262.25it/s]
Adding requests:  52%|█████▏    | 2149/4096 [00:09<00:07, 265.24it/s]
Adding requests:  53%|█████▎    | 2176/4096 [00:09<00:07, 262.85it/s]
Adding requests:  54%|█████▍    | 2204/4096 [00:09<00:07, 266.84it/s]
Adding requests:  54%|█████▍    | 2231/4096 [00:09<00:06, 266.44it/s]
Adding requests:  55%|█████▌    | 2258/4096 [00:09<00:06, 266.65it/s]
Adding requests:  56%|█████▌    | 2285/4096 [00:09<00:06, 264.50it/s]
Adding requests:  56%|█████▋    | 2313/4096 [00:09<00:06, 267.22it/s]
Adding requests:  57%|█████▋    | 2340/4096 [00:09<00:06, 267.41it/s]
Adding requests:  58%|█████▊    | 2367/4096 [00:10<00:06, 263.54it/s]
Adding requests:  58%|█████▊    | 2395/4096 [00:10<00:06, 267.46it/s]
Adding requests:  59%|█████▉    | 2424/4096 [00:10<00:06, 271.56it/s]
Adding requests:  60%|█████▉    | 2452/4096 [00:10<00:06, 267.51it/s]
Adding requests:  61%|██████    | 2480/4096 [00:10<00:06, 268.21it/s]
Adding requests:  61%|██████    | 2507/4096 [00:10<00:05, 266.49it/s]
Adding requests:  62%|██████▏   | 2535/4096 [00:10<00:05, 270.10it/s]
Adding requests:  63%|██████▎   | 2563/4096 [00:10<00:05, 271.24it/s]
Adding requests:  63%|██████▎   | 2591/4096 [00:11<00:08, 184.59it/s]
Adding requests:  64%|██████▍   | 2614/4096 [00:11<00:09, 160.34it/s]
Adding requests:  64%|██████▍   | 2634/4096 [00:11<00:10, 136.28it/s]
Adding requests:  65%|██████▍   | 2651/4096 [00:11<00:10, 137.55it/s]
Adding requests:  65%|██████▌   | 2669/4096 [00:11<00:09, 146.09it/s]
Adding requests:  66%|██████▌   | 2698/4096 [00:11<00:07, 178.32it/s]
Adding requests:  67%|██████▋   | 2727/4096 [00:11<00:06, 203.69it/s]
Adding requests:  67%|██████▋   | 2750/4096 [00:12<00:06, 203.27it/s]
Adding requests:  68%|██████▊   | 2772/4096 [00:12<00:06, 190.52it/s]
Adding requests:  68%|██████▊   | 2804/4096 [00:12<00:05, 222.27it/s]
Adding requests:  69%|██████▉   | 2834/4096 [00:12<00:05, 239.55it/s]
Adding requests:  70%|██████▉   | 2864/4096 [00:12<00:04, 254.71it/s]
Adding requests:  71%|███████   | 2891/4096 [00:12<00:04, 255.96it/s]
Adding requests:  71%|███████▏  | 2919/4096 [00:12<00:04, 260.59it/s]
Adding requests:  72%|███████▏  | 2947/4096 [00:12<00:04, 265.77it/s]
Adding requests:  73%|███████▎  | 2974/4096 [00:12<00:04, 261.66it/s]
Adding requests:  73%|███████▎  | 3002/4096 [00:12<00:04, 264.93it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:13<00:03, 268.08it/s]
Adding requests:  75%|███████▍  | 3060/4096 [00:13<00:03, 277.17it/s]
Adding requests:  75%|███████▌  | 3090/4096 [00:13<00:03, 282.66it/s]
Adding requests:  76%|███████▌  | 3120/4096 [00:13<00:03, 286.60it/s]
Adding requests:  77%|███████▋  | 3150/4096 [00:13<00:03, 288.28it/s]
Adding requests:  78%|███████▊  | 3179/4096 [00:13<00:03, 275.90it/s]
Adding requests:  78%|███████▊  | 3207/4096 [00:13<00:03, 271.85it/s]
Adding requests:  79%|███████▉  | 3235/4096 [00:13<00:03, 261.31it/s]
Adding requests:  80%|███████▉  | 3263/4096 [00:13<00:03, 265.70it/s]
Adding requests:  80%|████████  | 3290/4096 [00:14<00:03, 247.61it/s]
Adding requests:  81%|████████  | 3316/4096 [00:14<00:03, 204.16it/s]
Adding requests:  82%|████████▏ | 3345/4096 [00:14<00:03, 223.96it/s]
Adding requests:  82%|████████▏ | 3369/4096 [00:14<00:03, 219.58it/s]
Adding requests:  83%|████████▎ | 3399/4096 [00:14<00:02, 238.91it/s]
Adding requests:  84%|████████▎ | 3428/4096 [00:14<00:02, 250.53it/s]
Adding requests:  84%|████████▍ | 3455/4096 [00:14<00:02, 255.74it/s]
Adding requests:  85%|████████▌ | 3483/4096 [00:14<00:02, 260.37it/s]
Adding requests:  86%|████████▌ | 3513/4096 [00:14<00:02, 271.02it/s]
Adding requests:  86%|████████▋ | 3541/4096 [00:15<00:02, 271.09it/s]
Adding requests:  87%|████████▋ | 3569/4096 [00:15<00:01, 269.14it/s]
Adding requests:  88%|████████▊ | 3597/4096 [00:15<00:01, 261.24it/s]
Adding requests:  88%|████████▊ | 3624/4096 [00:15<00:01, 253.93it/s]
Adding requests:  89%|████████▉ | 3653/4096 [00:15<00:01, 261.21it/s]
Adding requests:  90%|████████▉ | 3681/4096 [00:15<00:01, 263.82it/s]
Adding requests:  91%|█████████ | 3709/4096 [00:15<00:01, 265.83it/s]
Adding requests:  91%|█████████ | 3736/4096 [00:15<00:01, 243.35it/s]
Adding requests:  92%|█████████▏| 3761/4096 [00:15<00:01, 236.03it/s]
Adding requests:  92%|█████████▏| 3786/4096 [00:16<00:01, 238.95it/s]
Adding requests:  93%|█████████▎| 3811/4096 [00:16<00:01, 236.61it/s]
Adding requests:  94%|█████████▍| 3840/4096 [00:16<00:01, 250.12it/s]
Adding requests:  94%|█████████▍| 3867/4096 [00:16<00:00, 253.82it/s]
Adding requests:  95%|█████████▌| 3893/4096 [00:16<00:00, 232.19it/s]
Adding requests:  96%|█████████▌| 3917/4096 [00:16<00:00, 195.26it/s]
Adding requests:  96%|█████████▋| 3943/4096 [00:16<00:00, 210.81it/s]
Adding requests:  97%|█████████▋| 3970/4096 [00:16<00:00, 224.24it/s]
Adding requests:  98%|█████████▊| 3997/4096 [00:16<00:00, 236.29it/s]
Adding requests:  98%|█████████▊| 4027/4096 [00:17<00:00, 251.15it/s]
Adding requests:  99%|█████████▉| 4053/4096 [00:17<00:00, 223.82it/s]
Adding requests: 100%|█████████▉| 4077/4096 [00:17<00:00, 169.11it/s]
Adding requests: 100%|██████████| 4096/4096 [00:17<00:00, 232.56it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▉       | 1194/4096 [00:00<00:01, 2736.97it/s, est. speed input: 2802824.67 toks/s, output: 2737.03 toks/s]
Processed prompts:  36%|███▌      | 1468/4096 [00:04<00:09, 280.43it/s, est. speed input: 367691.75 toks/s, output: 359.07 toks/s]   
Processed prompts:  39%|███▊      | 1586/4096 [00:05<00:12, 195.20it/s, est. speed input: 275148.74 toks/s, output: 268.70 toks/s]
Processed prompts:  40%|████      | 1653/4096 [00:06<00:14, 168.54it/s, est. speed input: 248645.54 toks/s, output: 242.82 toks/s]
Processed prompts:  41%|████▏     | 1697/4096 [00:07<00:15, 158.12it/s, est. speed input: 238844.13 toks/s, output: 233.25 toks/s]
Processed prompts:  42%|████▏     | 1729/4096 [00:07<00:16, 143.87it/s, est. speed input: 229035.67 toks/s, output: 223.67 toks/s]
Processed prompts:  43%|████▎     | 1752/4096 [00:08<00:18, 127.34it/s, est. speed input: 219724.18 toks/s, output: 214.57 toks/s]
Processed prompts:  43%|████▎     | 1770/4096 [00:08<00:21, 108.55it/s, est. speed input: 210242.09 toks/s, output: 205.31 toks/s]
Processed prompts:  44%|████▍     | 1802/4096 [00:09<00:23, 99.56it/s, est. speed input: 203289.26 toks/s, output: 198.52 toks/s] 
Processed prompts:  45%|████▍     | 1834/4096 [00:09<00:24, 92.25it/s, est. speed input: 197022.19 toks/s, output: 192.40 toks/s]
Processed prompts:  46%|████▌     | 1866/4096 [00:09<00:25, 86.77it/s, est. speed input: 191438.09 toks/s, output: 186.95 toks/s]
Processed prompts:  46%|████▋     | 1898/4096 [00:10<00:26, 81.96it/s, est. speed input: 186121.31 toks/s, output: 181.76 toks/s]
Processed prompts:  47%|████▋     | 1930/4096 [00:10<00:27, 78.82it/s, est. speed input: 181401.03 toks/s, output: 177.15 toks/s]
Processed prompts:  48%|████▊     | 1962/4096 [00:11<00:27, 76.75it/s, est. speed input: 177132.92 toks/s, output: 172.98 toks/s]
Processed prompts:  49%|████▊     | 1994/4096 [00:11<00:28, 74.79it/s, est. speed input: 173050.10 toks/s, output: 168.99 toks/s]
Processed prompts:  49%|████▉     | 2026/4096 [00:12<00:28, 73.38it/s, est. speed input: 169269.77 toks/s, output: 165.30 toks/s]
Processed prompts:  50%|█████     | 2058/4096 [00:12<00:27, 72.89it/s, est. speed input: 165903.36 toks/s, output: 162.01 toks/s]
Processed prompts:  51%|█████     | 2090/4096 [00:13<00:27, 71.69it/s, est. speed input: 162541.19 toks/s, output: 158.73 toks/s]
Processed prompts:  52%|█████▏    | 2122/4096 [00:13<00:27, 71.24it/s, est. speed input: 159505.38 toks/s, output: 155.77 toks/s]
Processed prompts:  53%|█████▎    | 2154/4096 [00:14<00:27, 70.92it/s, est. speed input: 156665.87 toks/s, output: 152.99 toks/s]
Processed prompts:  53%|█████▎    | 2186/4096 [00:14<00:26, 71.17it/s, est. speed input: 154112.60 toks/s, output: 150.50 toks/s]
Processed prompts:  54%|█████▍    | 2218/4096 [00:15<00:27, 69.02it/s, est. speed input: 151198.18 toks/s, output: 147.65 toks/s]
Processed prompts:  55%|█████▍    | 2250/4096 [00:15<00:26, 70.99it/s, est. speed input: 149203.68 toks/s, output: 145.71 toks/s]
Processed prompts:  56%|█████▌    | 2282/4096 [00:15<00:25, 70.74it/s, est. speed input: 146983.47 toks/s, output: 143.54 toks/s]
Processed prompts:  56%|█████▋    | 2314/4096 [00:16<00:25, 70.61it/s, est. speed input: 144897.01 toks/s, output: 141.50 toks/s]
Processed prompts:  57%|█████▋    | 2346/4096 [00:16<00:24, 70.11it/s, est. speed input: 142847.49 toks/s, output: 139.50 toks/s]
Processed prompts:  58%|█████▊    | 2378/4096 [00:17<00:24, 69.53it/s, est. speed input: 140866.74 toks/s, output: 137.57 toks/s]
Processed prompts:  59%|█████▉    | 2410/4096 [00:17<00:24, 69.73it/s, est. speed input: 139094.14 toks/s, output: 135.83 toks/s]
Processed prompts:  60%|█████▉    | 2442/4096 [00:18<00:23, 69.63it/s, est. speed input: 137371.53 toks/s, output: 134.15 toks/s]
Processed prompts:  60%|██████    | 2474/4096 [00:18<00:23, 69.66it/s, est. speed input: 135748.29 toks/s, output: 132.57 toks/s]
Processed prompts:  61%|██████    | 2506/4096 [00:19<00:22, 69.92it/s, est. speed input: 134241.85 toks/s, output: 131.10 toks/s]
Processed prompts:  62%|██████▏   | 2538/4096 [00:19<00:22, 70.37it/s, est. speed input: 132842.78 toks/s, output: 129.73 toks/s]
Processed prompts:  63%|██████▎   | 2570/4096 [00:20<00:21, 70.35it/s, est. speed input: 131458.80 toks/s, output: 128.38 toks/s]
Processed prompts:  64%|██████▎   | 2602/4096 [00:20<00:21, 70.81it/s, est. speed input: 130201.05 toks/s, output: 127.15 toks/s]
Processed prompts:  64%|██████▍   | 2634/4096 [00:20<00:20, 70.49it/s, est. speed input: 128912.55 toks/s, output: 125.89 toks/s]
Processed prompts:  65%|██████▌   | 2666/4096 [00:21<00:20, 70.52it/s, est. speed input: 127711.75 toks/s, output: 124.72 toks/s]
Processed prompts:  66%|██████▌   | 2698/4096 [00:21<00:19, 70.33it/s, est. speed input: 126535.06 toks/s, output: 123.57 toks/s]
Processed prompts:  67%|██████▋   | 2730/4096 [00:22<00:19, 70.48it/s, est. speed input: 125439.65 toks/s, output: 122.50 toks/s]
Processed prompts:  67%|██████▋   | 2762/4096 [00:22<00:18, 70.32it/s, est. speed input: 124357.15 toks/s, output: 121.44 toks/s]
Processed prompts:  68%|██████▊   | 2794/4096 [00:23<00:18, 70.20it/s, est. speed input: 123316.51 toks/s, output: 120.43 toks/s]
Processed prompts:  69%|██████▉   | 2826/4096 [00:23<00:18, 69.91it/s, est. speed input: 122292.41 toks/s, output: 119.43 toks/s]
Processed prompts:  70%|██████▉   | 2858/4096 [00:24<00:17, 69.89it/s, est. speed input: 121328.59 toks/s, output: 118.48 toks/s]
Processed prompts:  71%|███████   | 2890/4096 [00:24<00:17, 70.64it/s, est. speed input: 120480.49 toks/s, output: 117.66 toks/s]
Processed prompts:  71%|███████▏  | 2922/4096 [00:25<00:16, 71.00it/s, est. speed input: 119645.70 toks/s, output: 116.84 toks/s]
Processed prompts:  72%|███████▏  | 2954/4096 [00:25<00:16, 70.77it/s, est. speed input: 118791.91 toks/s, output: 116.01 toks/s]
Processed prompts:  73%|███████▎  | 2986/4096 [00:25<00:15, 70.52it/s, est. speed input: 117959.78 toks/s, output: 115.19 toks/s]
Processed prompts:  74%|███████▎  | 3018/4096 [00:26<00:15, 70.43it/s, est. speed input: 117163.98 toks/s, output: 114.42 toks/s]
Processed prompts:  74%|███████▍  | 3050/4096 [00:26<00:14, 70.44it/s, est. speed input: 116402.28 toks/s, output: 113.67 toks/s]
Processed prompts:  75%|███████▌  | 3082/4096 [00:27<00:14, 70.39it/s, est. speed input: 115660.32 toks/s, output: 112.95 toks/s]
Processed prompts:  76%|███████▌  | 3114/4096 [00:27<00:14, 70.12it/s, est. speed input: 114922.44 toks/s, output: 112.23 toks/s]
Processed prompts:  77%|███████▋  | 3146/4096 [00:28<00:13, 70.04it/s, est. speed input: 114217.05 toks/s, output: 111.54 toks/s]
Processed prompts:  78%|███████▊  | 3178/4096 [00:28<00:13, 69.99it/s, est. speed input: 113535.78 toks/s, output: 110.87 toks/s]
Processed prompts:  78%|███████▊  | 3210/4096 [00:29<00:12, 69.77it/s, est. speed input: 112859.94 toks/s, output: 110.21 toks/s]
Processed prompts:  79%|███████▉  | 3242/4096 [00:29<00:12, 70.06it/s, est. speed input: 112241.88 toks/s, output: 109.61 toks/s]
Processed prompts:  80%|███████▉  | 3274/4096 [00:30<00:11, 70.03it/s, est. speed input: 111623.14 toks/s, output: 109.01 toks/s]
Processed prompts:  81%|████████  | 3306/4096 [00:30<00:11, 69.91it/s, est. speed input: 111015.88 toks/s, output: 108.41 toks/s]
Processed prompts:  81%|████████▏ | 3338/4096 [00:30<00:10, 70.06it/s, est. speed input: 110444.04 toks/s, output: 107.86 toks/s]
Processed prompts:  82%|████████▏ | 3370/4096 [00:31<00:10, 69.99it/s, est. speed input: 109876.30 toks/s, output: 107.30 toks/s]
Processed prompts:  83%|████████▎ | 3402/4096 [00:31<00:09, 69.96it/s, est. speed input: 109325.58 toks/s, output: 106.76 toks/s]
Processed prompts:  84%|████████▍ | 3434/4096 [00:32<00:09, 70.00it/s, est. speed input: 108795.04 toks/s, output: 106.25 toks/s]
Processed prompts:  85%|████████▍ | 3466/4096 [00:32<00:09, 69.89it/s, est. speed input: 108269.64 toks/s, output: 105.73 toks/s]
Processed prompts:  85%|████████▌ | 3498/4096 [00:33<00:08, 70.03it/s, est. speed input: 107774.14 toks/s, output: 105.25 toks/s]
Processed prompts:  86%|████████▌ | 3530/4096 [00:33<00:08, 70.14it/s, est. speed input: 107292.60 toks/s, output: 104.78 toks/s]
Processed prompts:  87%|████████▋ | 3562/4096 [00:34<00:07, 70.70it/s, est. speed input: 106856.04 toks/s, output: 104.35 toks/s]
Processed prompts:  88%|████████▊ | 3594/4096 [00:34<00:07, 70.32it/s, est. speed input: 106380.39 toks/s, output: 103.89 toks/s]
Processed prompts:  89%|████████▊ | 3626/4096 [00:35<00:06, 70.29it/s, est. speed input: 105932.33 toks/s, output: 103.45 toks/s]
Processed prompts:  89%|████████▉ | 3658/4096 [00:35<00:06, 69.03it/s, est. speed input: 105414.26 toks/s, output: 102.94 toks/s]
Processed prompts:  90%|█████████ | 3690/4096 [00:35<00:05, 71.08it/s, est. speed input: 105097.13 toks/s, output: 102.63 toks/s]
Processed prompts:  91%|█████████ | 3722/4096 [00:36<00:05, 70.70it/s, est. speed input: 104674.29 toks/s, output: 102.22 toks/s]
Processed prompts:  92%|█████████▏| 3754/4096 [00:36<00:04, 70.37it/s, est. speed input: 104258.25 toks/s, output: 101.81 toks/s]
Processed prompts:  92%|█████████▏| 3786/4096 [00:37<00:04, 70.26it/s, est. speed input: 103859.40 toks/s, output: 101.43 toks/s]
Processed prompts:  93%|█████████▎| 3818/4096 [00:37<00:03, 70.14it/s, est. speed input: 103467.69 toks/s, output: 101.04 toks/s]
Processed prompts:  94%|█████████▍| 3850/4096 [00:38<00:03, 70.05it/s, est. speed input: 103084.80 toks/s, output: 100.67 toks/s]
Processed prompts:  95%|█████████▍| 3882/4096 [00:38<00:03, 68.73it/s, est. speed input: 102637.02 toks/s, output: 100.23 toks/s]
Processed prompts:  96%|█████████▌| 3914/4096 [00:39<00:02, 70.98it/s, est. speed input: 102382.48 toks/s, output: 99.98 toks/s] 
Processed prompts:  96%|█████████▋| 3946/4096 [00:39<00:02, 70.73it/s, est. speed input: 102030.36 toks/s, output: 99.64 toks/s]
Processed prompts:  97%|█████████▋| 3978/4096 [00:40<00:01, 70.47it/s, est. speed input: 101682.18 toks/s, output: 99.30 toks/s]
Processed prompts:  98%|█████████▊| 4010/4096 [00:40<00:01, 71.03it/s, est. speed input: 101381.01 toks/s, output: 99.00 toks/s]
Processed prompts:  99%|█████████▊| 4042/4096 [00:40<00:00, 70.80it/s, est. speed input: 101054.04 toks/s, output: 98.69 toks/s]
Processed prompts:  99%|█████████▉| 4074/4096 [00:41<00:00, 76.51it/s, est. speed input: 101016.47 toks/s, output: 98.65 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:41<00:00, 76.51it/s, est. speed input: 101560.86 toks/s, output: 99.18 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:41<00:00, 99.18it/s, est. speed input: 101560.86 toks/s, output: 99.18 toks/s]
[rank0]:[W126 18:33:17.041322047 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 18:33:21
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:34:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=919146) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=919146) WARNING 01-26 18:34:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=919146) WARNING 01-26 18:35:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 71.49 requests/s, 73274.34 total tokens/s, 71.49 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 18:34:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:34:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:34:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:34:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:34:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:34:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:34:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:34:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:34:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:34:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:34:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:34:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:34:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:34:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:34:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:34:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:34:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=919146) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=919146) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.25s/it]
(EngineCore_DP0 pid=919146) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.37it/s]
(EngineCore_DP0 pid=919146) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.07it/s]
(EngineCore_DP0 pid=919146) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
(EngineCore_DP0 pid=919146) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.02s/it]
(EngineCore_DP0 pid=919146) 
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=919146) [2026-01-26 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=919146) [rank0]:W0126 18:35:01.309000 919146 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=919146) [rank0]:W0126 18:35:01.379000 919146 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=919146) [rank0]:W0126 18:35:02.190000 919146 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=919146) [rank0]:W0126 18:35:02.300000 919146 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=919146) 2026-01-26 18:35:12,679 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=919146) 2026-01-26 18:35:12,885 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=919146) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  4.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  4.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  5.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  7.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  8.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00,  9.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  9.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.29it/s]
(EngineCore_DP0 pid=919146) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 10.68it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 10.74it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 10.22it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 10.48it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 10.65it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.62it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 26/8192 [00:00<00:31, 256.73it/s]
Adding requests:   1%|          | 59/8192 [00:00<00:27, 299.49it/s]
Adding requests:   1%|          | 91/8192 [00:00<00:26, 304.86it/s]
Adding requests:   2%|▏         | 124/8192 [00:00<00:25, 310.97it/s]
Adding requests:   2%|▏         | 157/8192 [00:00<00:25, 315.75it/s]
Adding requests:   2%|▏         | 193/8192 [00:00<00:24, 328.03it/s]
Adding requests:   3%|▎         | 227/8192 [00:00<00:24, 329.06it/s]
Adding requests:   3%|▎         | 260/8192 [00:00<00:24, 326.47it/s]
Adding requests:   4%|▎         | 294/8192 [00:00<00:23, 329.83it/s]
Adding requests:   4%|▍         | 328/8192 [00:01<00:23, 332.18it/s]
Adding requests:   4%|▍         | 364/8192 [00:01<00:23, 338.50it/s]
Adding requests:   5%|▍         | 399/8192 [00:01<00:22, 340.21it/s]
Adding requests:   5%|▌         | 434/8192 [00:01<00:22, 342.56it/s]
Adding requests:   6%|▌         | 469/8192 [00:01<00:22, 342.46it/s]
Adding requests:   6%|▌         | 504/8192 [00:01<00:22, 342.29it/s]
Adding requests:   7%|▋         | 541/8192 [00:01<00:21, 349.86it/s]
Adding requests:   7%|▋         | 577/8192 [00:01<00:21, 350.03it/s]
Adding requests:   7%|▋         | 613/8192 [00:01<00:22, 340.78it/s]
Adding requests:   8%|▊         | 648/8192 [00:01<00:22, 336.03it/s]
Adding requests:   8%|▊         | 683/8192 [00:02<00:22, 337.47it/s]
Adding requests:   9%|▉         | 717/8192 [00:02<00:22, 327.90it/s]
Adding requests:   9%|▉         | 750/8192 [00:02<00:23, 323.01it/s]
Adding requests:  10%|▉         | 785/8192 [00:02<00:22, 327.72it/s]
Adding requests:  10%|▉         | 818/8192 [00:02<00:22, 328.06it/s]
Adding requests:  10%|█         | 855/8192 [00:02<00:21, 338.47it/s]
Adding requests:  11%|█         | 890/8192 [00:02<00:21, 340.65it/s]
Adding requests:  11%|█▏        | 925/8192 [00:02<00:21, 333.60it/s]
Adding requests:  12%|█▏        | 960/8192 [00:02<00:21, 336.94it/s]
Adding requests:  12%|█▏        | 994/8192 [00:02<00:21, 329.88it/s]
Adding requests:  13%|█▎        | 1028/8192 [00:03<00:21, 331.41it/s]
Adding requests:  13%|█▎        | 1062/8192 [00:03<00:21, 330.50it/s]
Adding requests:  13%|█▎        | 1096/8192 [00:03<00:21, 328.56it/s]
Adding requests:  14%|█▍        | 1133/8192 [00:03<00:20, 337.43it/s]
Adding requests:  14%|█▍        | 1167/8192 [00:03<00:21, 333.86it/s]
Adding requests:  15%|█▍        | 1201/8192 [00:03<00:20, 334.00it/s]
Adding requests:  15%|█▌        | 1237/8192 [00:03<00:20, 340.98it/s]
Adding requests:  16%|█▌        | 1272/8192 [00:03<00:20, 336.48it/s]
Adding requests:  16%|█▌        | 1306/8192 [00:03<00:20, 336.86it/s]
Adding requests:  16%|█▋        | 1340/8192 [00:04<00:20, 337.65it/s]
Adding requests:  17%|█▋        | 1376/8192 [00:04<00:19, 342.18it/s]
Adding requests:  17%|█▋        | 1411/8192 [00:04<00:20, 338.79it/s]
Adding requests:  18%|█▊        | 1445/8192 [00:04<00:19, 338.49it/s]
Adding requests:  18%|█▊        | 1479/8192 [00:04<00:20, 335.36it/s]
Adding requests:  18%|█▊        | 1515/8192 [00:04<00:19, 340.93it/s]
Adding requests:  19%|█▉        | 1550/8192 [00:04<00:19, 338.83it/s]
Adding requests:  19%|█▉        | 1584/8192 [00:04<00:19, 332.43it/s]
Adding requests:  20%|█▉        | 1618/8192 [00:04<00:20, 327.60it/s]
Adding requests:  20%|██        | 1651/8192 [00:04<00:20, 314.55it/s]
Adding requests:  21%|██        | 1683/8192 [00:05<00:20, 314.77it/s]
Adding requests:  21%|██        | 1718/8192 [00:05<00:20, 322.28it/s]
Adding requests:  21%|██▏       | 1751/8192 [00:05<00:19, 322.23it/s]
Adding requests:  22%|██▏       | 1784/8192 [00:05<00:19, 322.78it/s]
Adding requests:  22%|██▏       | 1817/8192 [00:05<00:19, 322.76it/s]
Adding requests:  23%|██▎       | 1851/8192 [00:05<00:19, 327.51it/s]
Adding requests:  23%|██▎       | 1886/8192 [00:05<00:18, 332.71it/s]
Adding requests:  23%|██▎       | 1922/8192 [00:05<00:18, 338.38it/s]
Adding requests:  24%|██▍       | 1957/8192 [00:05<00:18, 341.70it/s]
Adding requests:  24%|██▍       | 1992/8192 [00:05<00:18, 337.57it/s]
Adding requests:  25%|██▍       | 2026/8192 [00:06<00:18, 330.22it/s]
Adding requests:  25%|██▌       | 2060/8192 [00:06<00:18, 328.11it/s]
Adding requests:  26%|██▌       | 2093/8192 [00:06<00:18, 326.03it/s]
Adding requests:  26%|██▌       | 2128/8192 [00:06<00:18, 331.29it/s]
Adding requests:  26%|██▋       | 2162/8192 [00:06<00:18, 328.45it/s]
Adding requests:  27%|██▋       | 2195/8192 [00:06<00:18, 325.15it/s]
Adding requests:  27%|██▋       | 2229/8192 [00:06<00:18, 327.47it/s]
Adding requests:  28%|██▊       | 2264/8192 [00:06<00:17, 332.40it/s]
Adding requests:  28%|██▊       | 2301/8192 [00:06<00:17, 341.70it/s]
Adding requests:  29%|██▊       | 2336/8192 [00:07<00:17, 344.11it/s]
Adding requests:  29%|██▉       | 2371/8192 [00:07<00:17, 341.20it/s]
Adding requests:  29%|██▉       | 2407/8192 [00:07<00:16, 346.17it/s]
Adding requests:  30%|██▉       | 2443/8192 [00:07<00:16, 346.93it/s]
Adding requests:  30%|███       | 2478/8192 [00:07<00:16, 345.81it/s]
Adding requests:  31%|███       | 2514/8192 [00:07<00:16, 349.71it/s]
Adding requests:  31%|███       | 2551/8192 [00:07<00:15, 353.94it/s]
Adding requests:  32%|███▏      | 2589/8192 [00:07<00:15, 358.38it/s]
Adding requests:  32%|███▏      | 2625/8192 [00:07<00:15, 350.43it/s]
Adding requests:  32%|███▏      | 2661/8192 [00:07<00:16, 343.94it/s]
Adding requests:  33%|███▎      | 2696/8192 [00:08<00:16, 339.60it/s]
Adding requests:  33%|███▎      | 2730/8192 [00:08<00:16, 339.33it/s]
Adding requests:  34%|███▍      | 2767/8192 [00:08<00:15, 346.39it/s]
Adding requests:  34%|███▍      | 2804/8192 [00:08<00:15, 350.65it/s]
Adding requests:  35%|███▍      | 2840/8192 [00:08<00:15, 347.60it/s]
Adding requests:  35%|███▌      | 2875/8192 [00:08<00:15, 346.46it/s]
Adding requests:  36%|███▌      | 2910/8192 [00:08<00:15, 346.10it/s]
Adding requests:  36%|███▌      | 2945/8192 [00:08<00:15, 335.92it/s]
Adding requests:  36%|███▋      | 2979/8192 [00:08<00:15, 336.45it/s]
Adding requests:  37%|███▋      | 3016/8192 [00:08<00:15, 344.33it/s]
Adding requests:  37%|███▋      | 3051/8192 [00:09<00:14, 345.17it/s]
Adding requests:  38%|███▊      | 3086/8192 [00:09<00:14, 345.37it/s]
Adding requests:  38%|███▊      | 3122/8192 [00:09<00:14, 347.30it/s]
Adding requests:  39%|███▊      | 3157/8192 [00:09<00:14, 338.35it/s]
Adding requests:  39%|███▉      | 3191/8192 [00:09<00:15, 332.32it/s]
Adding requests:  39%|███▉      | 3225/8192 [00:09<00:15, 331.08it/s]
Adding requests:  40%|███▉      | 3259/8192 [00:09<00:14, 330.52it/s]
Adding requests:  40%|████      | 3293/8192 [00:09<00:15, 317.87it/s]
Adding requests:  41%|████      | 3325/8192 [00:09<00:15, 316.64it/s]
Adding requests:  41%|████      | 3358/8192 [00:10<00:15, 319.70it/s]
Adding requests:  41%|████▏     | 3392/8192 [00:10<00:14, 322.40it/s]
Adding requests:  42%|████▏     | 3426/8192 [00:10<00:14, 325.32it/s]
Adding requests:  42%|████▏     | 3461/8192 [00:10<00:14, 331.28it/s]
Adding requests:  43%|████▎     | 3495/8192 [00:10<00:14, 330.18it/s]
Adding requests:  43%|████▎     | 3533/8192 [00:10<00:13, 342.13it/s]
Adding requests:  44%|████▎     | 3570/8192 [00:10<00:13, 347.34it/s]
Adding requests:  44%|████▍     | 3605/8192 [00:10<00:13, 344.61it/s]
Adding requests:  44%|████▍     | 3642/8192 [00:10<00:13, 348.59it/s]
Adding requests:  45%|████▍     | 3677/8192 [00:10<00:13, 344.30it/s]
Adding requests:  45%|████▌     | 3712/8192 [00:11<00:13, 341.36it/s]
Adding requests:  46%|████▌     | 3747/8192 [00:11<00:13, 340.11it/s]
Adding requests:  46%|████▌     | 3782/8192 [00:11<00:13, 331.63it/s]
Adding requests:  47%|████▋     | 3816/8192 [00:11<00:13, 323.28it/s]
Adding requests:  47%|████▋     | 3851/8192 [00:11<00:13, 328.83it/s]
Adding requests:  47%|████▋     | 3886/8192 [00:11<00:12, 332.42it/s]
Adding requests:  48%|████▊     | 3920/8192 [00:11<00:13, 328.18it/s]
Adding requests:  48%|████▊     | 3954/8192 [00:11<00:12, 330.83it/s]
Adding requests:  49%|████▊     | 3988/8192 [00:11<00:12, 331.37it/s]
Adding requests:  49%|████▉     | 4022/8192 [00:12<00:12, 332.86it/s]
Adding requests:  50%|████▉     | 4056/8192 [00:12<00:12, 331.04it/s]
Adding requests:  50%|████▉     | 4090/8192 [00:12<00:12, 330.71it/s]
Adding requests:  50%|█████     | 4126/8192 [00:12<00:12, 337.53it/s]
Adding requests:  51%|█████     | 4160/8192 [00:12<00:12, 334.92it/s]
Adding requests:  51%|█████     | 4195/8192 [00:12<00:11, 338.00it/s]
Adding requests:  52%|█████▏    | 4229/8192 [00:12<00:11, 335.96it/s]
Adding requests:  52%|█████▏    | 4263/8192 [00:12<00:12, 327.18it/s]
Adding requests:  52%|█████▏    | 4297/8192 [00:12<00:11, 330.07it/s]
Adding requests:  53%|█████▎    | 4331/8192 [00:12<00:11, 331.81it/s]
Adding requests:  53%|█████▎    | 4367/8192 [00:13<00:11, 337.62it/s]
Adding requests:  54%|█████▎    | 4401/8192 [00:13<00:11, 335.93it/s]
Adding requests:  54%|█████▍    | 4436/8192 [00:13<00:11, 339.01it/s]
Adding requests:  55%|█████▍    | 4470/8192 [00:13<00:10, 338.78it/s]
Adding requests:  55%|█████▍    | 4505/8192 [00:13<00:10, 341.29it/s]
Adding requests:  55%|█████▌    | 4541/8192 [00:13<00:10, 344.12it/s]
Adding requests:  56%|█████▌    | 4576/8192 [00:13<00:10, 341.05it/s]
Adding requests:  56%|█████▋    | 4611/8192 [00:13<00:10, 338.79it/s]
Adding requests:  57%|█████▋    | 4645/8192 [00:13<00:10, 337.15it/s]
Adding requests:  57%|█████▋    | 4679/8192 [00:13<00:10, 332.23it/s]
Adding requests:  58%|█████▊    | 4713/8192 [00:14<00:10, 332.58it/s]
Adding requests:  58%|█████▊    | 4750/8192 [00:14<00:10, 340.58it/s]
Adding requests:  58%|█████▊    | 4785/8192 [00:14<00:10, 338.47it/s]
Adding requests:  59%|█████▉    | 4820/8192 [00:14<00:09, 339.42it/s]
Adding requests:  59%|█████▉    | 4854/8192 [00:14<00:09, 335.80it/s]
Adding requests:  60%|█████▉    | 4889/8192 [00:14<00:09, 338.75it/s]
Adding requests:  60%|██████    | 4923/8192 [00:14<00:09, 337.12it/s]
Adding requests:  61%|██████    | 4958/8192 [00:14<00:09, 340.39it/s]
Adding requests:  61%|██████    | 4994/8192 [00:14<00:09, 341.75it/s]
Adding requests:  61%|██████▏   | 5031/8192 [00:14<00:09, 347.83it/s]
Adding requests:  62%|██████▏   | 5066/8192 [00:15<00:09, 344.42it/s]
Adding requests:  62%|██████▏   | 5101/8192 [00:15<00:08, 345.38it/s]
Adding requests:  63%|██████▎   | 5136/8192 [00:15<00:08, 344.65it/s]
Adding requests:  63%|██████▎   | 5171/8192 [00:15<00:08, 342.04it/s]
Adding requests:  64%|██████▎   | 5206/8192 [00:15<00:08, 340.47it/s]
Adding requests:  64%|██████▍   | 5241/8192 [00:15<00:08, 336.76it/s]
Adding requests:  64%|██████▍   | 5276/8192 [00:15<00:08, 337.68it/s]
Adding requests:  65%|██████▍   | 5311/8192 [00:15<00:08, 339.39it/s]
Adding requests:  65%|██████▌   | 5346/8192 [00:15<00:08, 340.82it/s]
Adding requests:  66%|██████▌   | 5381/8192 [00:16<00:08, 339.85it/s]
Adding requests:  66%|██████▌   | 5415/8192 [00:16<00:08, 330.33it/s]
Adding requests:  67%|██████▋   | 5451/8192 [00:16<00:08, 337.10it/s]
Adding requests:  67%|██████▋   | 5487/8192 [00:16<00:07, 340.97it/s]
Adding requests:  67%|██████▋   | 5523/8192 [00:16<00:07, 344.28it/s]
Adding requests:  68%|██████▊   | 5558/8192 [00:16<00:07, 342.74it/s]
Adding requests:  68%|██████▊   | 5593/8192 [00:16<00:07, 325.19it/s]
Adding requests:  69%|██████▊   | 5629/8192 [00:16<00:07, 334.48it/s]
Adding requests:  69%|██████▉   | 5664/8192 [00:16<00:07, 336.75it/s]
Adding requests:  70%|██████▉   | 5698/8192 [00:16<00:07, 336.41it/s]
Adding requests:  70%|██████▉   | 5732/8192 [00:17<00:07, 336.42it/s]
Adding requests:  70%|███████   | 5768/8192 [00:17<00:07, 341.19it/s]
Adding requests:  71%|███████   | 5804/8192 [00:17<00:06, 345.88it/s]
Adding requests:  71%|███████▏  | 5839/8192 [00:17<00:06, 344.75it/s]
Adding requests:  72%|███████▏  | 5874/8192 [00:17<00:06, 346.25it/s]
Adding requests:  72%|███████▏  | 5910/8192 [00:17<00:06, 347.27it/s]
Adding requests:  73%|███████▎  | 5946/8192 [00:17<00:06, 349.15it/s]
Adding requests:  73%|███████▎  | 5982/8192 [00:17<00:06, 349.83it/s]
Adding requests:  73%|███████▎  | 6018/8192 [00:17<00:06, 350.13it/s]
Adding requests:  74%|███████▍  | 6054/8192 [00:17<00:06, 346.98it/s]
Adding requests:  74%|███████▍  | 6089/8192 [00:18<00:06, 345.13it/s]
Adding requests:  75%|███████▍  | 6125/8192 [00:18<00:05, 346.49it/s]
Adding requests:  75%|███████▌  | 6161/8192 [00:18<00:05, 349.36it/s]
Adding requests:  76%|███████▌  | 6196/8192 [00:18<00:05, 343.18it/s]
Adding requests:  76%|███████▌  | 6231/8192 [00:18<00:05, 341.89it/s]
Adding requests:  76%|███████▋  | 6266/8192 [00:18<00:05, 343.34it/s]
Adding requests:  77%|███████▋  | 6301/8192 [00:18<00:05, 343.38it/s]
Adding requests:  77%|███████▋  | 6339/8192 [00:18<00:05, 351.99it/s]
Adding requests:  78%|███████▊  | 6375/8192 [00:18<00:05, 352.75it/s]
Adding requests:  78%|███████▊  | 6411/8192 [00:19<00:05, 343.94it/s]
Adding requests:  79%|███████▊  | 6446/8192 [00:19<00:05, 338.65it/s]
Adding requests:  79%|███████▉  | 6480/8192 [00:19<00:05, 334.98it/s]
Adding requests:  80%|███████▉  | 6516/8192 [00:19<00:04, 340.06it/s]
Adding requests:  80%|███████▉  | 6551/8192 [00:19<00:04, 339.32it/s]
Adding requests:  80%|████████  | 6585/8192 [00:19<00:04, 337.79it/s]
Adding requests:  81%|████████  | 6620/8192 [00:19<00:04, 339.33it/s]
Adding requests:  81%|████████▏ | 6656/8192 [00:19<00:04, 343.24it/s]
Adding requests:  82%|████████▏ | 6691/8192 [00:19<00:04, 342.91it/s]
Adding requests:  82%|████████▏ | 6726/8192 [00:19<00:04, 342.37it/s]
Adding requests:  83%|████████▎ | 6762/8192 [00:20<00:04, 345.58it/s]
Adding requests:  83%|████████▎ | 6797/8192 [00:20<00:04, 345.27it/s]
Adding requests:  83%|████████▎ | 6832/8192 [00:20<00:04, 338.77it/s]
Adding requests:  84%|████████▍ | 6867/8192 [00:20<00:03, 341.96it/s]
Adding requests:  84%|████████▍ | 6902/8192 [00:20<00:03, 341.35it/s]
Adding requests:  85%|████████▍ | 6937/8192 [00:20<00:03, 333.50it/s]
Adding requests:  85%|████████▌ | 6971/8192 [00:20<00:03, 328.94it/s]
Adding requests:  86%|████████▌ | 7008/8192 [00:20<00:03, 339.41it/s]
Adding requests:  86%|████████▌ | 7042/8192 [00:20<00:03, 338.73it/s]
Adding requests:  86%|████████▋ | 7076/8192 [00:20<00:03, 338.95it/s]
Adding requests:  87%|████████▋ | 7110/8192 [00:21<00:03, 336.55it/s]
Adding requests:  87%|████████▋ | 7146/8192 [00:21<00:03, 341.91it/s]
Adding requests:  88%|████████▊ | 7181/8192 [00:21<00:02, 342.82it/s]
Adding requests:  88%|████████▊ | 7219/8192 [00:21<00:02, 350.98it/s]
Adding requests:  89%|████████▊ | 7255/8192 [00:21<00:02, 349.83it/s]
Adding requests:  89%|████████▉ | 7291/8192 [00:21<00:02, 352.11it/s]
Adding requests:  89%|████████▉ | 7327/8192 [00:21<00:02, 336.18it/s]
Adding requests:  90%|████████▉ | 7362/8192 [00:21<00:02, 338.75it/s]
Adding requests:  90%|█████████ | 7396/8192 [00:21<00:02, 334.43it/s]
Adding requests:  91%|█████████ | 7431/8192 [00:22<00:02, 337.35it/s]
Adding requests:  91%|█████████ | 7465/8192 [00:22<00:02, 336.59it/s]
Adding requests:  92%|█████████▏| 7500/8192 [00:22<00:02, 339.69it/s]
Adding requests:  92%|█████████▏| 7535/8192 [00:22<00:01, 339.53it/s]
Adding requests:  92%|█████████▏| 7570/8192 [00:22<00:01, 341.91it/s]
Adding requests:  93%|█████████▎| 7605/8192 [00:22<00:01, 339.03it/s]
Adding requests:  93%|█████████▎| 7639/8192 [00:22<00:01, 339.00it/s]
Adding requests:  94%|█████████▎| 7677/8192 [00:22<00:01, 348.44it/s]
Adding requests:  94%|█████████▍| 7712/8192 [00:22<00:01, 348.03it/s]
Adding requests:  95%|█████████▍| 7747/8192 [00:22<00:01, 345.79it/s]
Adding requests:  95%|█████████▍| 7782/8192 [00:23<00:01, 340.63it/s]
Adding requests:  95%|█████████▌| 7817/8192 [00:23<00:01, 341.50it/s]
Adding requests:  96%|█████████▌| 7852/8192 [00:23<00:01, 336.36it/s]
Adding requests:  96%|█████████▋| 7886/8192 [00:23<00:00, 335.83it/s]
Adding requests:  97%|█████████▋| 7923/8192 [00:23<00:00, 344.54it/s]
Adding requests:  97%|█████████▋| 7959/8192 [00:23<00:00, 348.75it/s]
Adding requests:  98%|█████████▊| 7996/8192 [00:23<00:00, 354.62it/s]
Adding requests:  98%|█████████▊| 8032/8192 [00:23<00:00, 344.53it/s]
Adding requests:  98%|█████████▊| 8069/8192 [00:23<00:00, 349.88it/s]
Adding requests:  99%|█████████▉| 8105/8192 [00:23<00:00, 343.58it/s]
Adding requests:  99%|█████████▉| 8140/8192 [00:24<00:00, 344.00it/s]
Adding requests: 100%|█████████▉| 8175/8192 [00:24<00:00, 336.12it/s]
Adding requests: 100%|██████████| 8192/8192 [00:24<00:00, 337.73it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 1682/8192 [00:00<00:00, 10312.56it/s, est. speed input: 10560674.49 toks/s, output: 10312.74 toks/s]
Processed prompts:  33%|███▎      | 2714/8192 [00:14<00:35, 153.69it/s, est. speed input: 192667.85 toks/s, output: 188.15 toks/s]      
Processed prompts:  34%|███▍      | 2770/8192 [00:15<00:37, 146.31it/s, est. speed input: 185207.16 toks/s, output: 180.87 toks/s]
Processed prompts:  39%|███▉      | 3199/8192 [00:20<00:41, 118.98it/s, est. speed input: 158390.65 toks/s, output: 154.68 toks/s]
Processed prompts:  42%|████▏     | 3436/8192 [00:24<00:45, 104.06it/s, est. speed input: 144987.28 toks/s, output: 141.59 toks/s]
Processed prompts:  44%|████▎     | 3582/8192 [00:26<00:45, 100.29it/s, est. speed input: 140764.72 toks/s, output: 137.47 toks/s]
Processed prompts:  45%|████▍     | 3679/8192 [00:27<00:49, 91.46it/s, est. speed input: 135315.95 toks/s, output: 132.14 toks/s] 
Processed prompts:  46%|████▌     | 3745/8192 [00:28<00:49, 89.29it/s, est. speed input: 133445.25 toks/s, output: 130.32 toks/s]
Processed prompts:  46%|████▋     | 3794/8192 [00:29<00:52, 84.20it/s, est. speed input: 131111.56 toks/s, output: 128.04 toks/s]
Processed prompts:  47%|████▋     | 3858/8192 [00:30<00:52, 81.92it/s, est. speed input: 129395.60 toks/s, output: 126.36 toks/s]
Processed prompts:  48%|████▊     | 3922/8192 [00:31<00:53, 80.00it/s, est. speed input: 127830.45 toks/s, output: 124.83 toks/s]
Processed prompts:  49%|████▊     | 3986/8192 [00:32<00:53, 78.21it/s, est. speed input: 126337.19 toks/s, output: 123.38 toks/s]
Processed prompts:  49%|████▉     | 4050/8192 [00:33<00:54, 76.62it/s, est. speed input: 124909.22 toks/s, output: 121.98 toks/s]
Processed prompts:  50%|█████     | 4114/8192 [00:34<00:54, 75.37it/s, est. speed input: 123563.58 toks/s, output: 120.67 toks/s]
Processed prompts:  51%|█████     | 4178/8192 [00:34<00:54, 74.27it/s, est. speed input: 122266.03 toks/s, output: 119.40 toks/s]
Processed prompts:  52%|█████▏    | 4242/8192 [00:35<00:53, 73.49it/s, est. speed input: 121040.53 toks/s, output: 118.20 toks/s]
Processed prompts:  53%|█████▎    | 4306/8192 [00:36<00:53, 72.68it/s, est. speed input: 119841.82 toks/s, output: 117.03 toks/s]
Processed prompts:  53%|█████▎    | 4370/8192 [00:37<00:52, 72.26it/s, est. speed input: 118723.49 toks/s, output: 115.94 toks/s]
Processed prompts:  54%|█████▍    | 4434/8192 [00:38<00:52, 72.21it/s, est. speed input: 117690.09 toks/s, output: 114.93 toks/s]
Processed prompts:  55%|█████▍    | 4498/8192 [00:39<00:51, 71.85it/s, est. speed input: 116662.08 toks/s, output: 113.93 toks/s]
Processed prompts:  56%|█████▌    | 4562/8192 [00:40<00:50, 72.13it/s, est. speed input: 115745.83 toks/s, output: 113.03 toks/s]
Processed prompts:  56%|█████▋    | 4626/8192 [00:41<00:49, 71.93it/s, est. speed input: 114820.81 toks/s, output: 112.13 toks/s]
Processed prompts:  57%|█████▋    | 4690/8192 [00:42<00:48, 71.78it/s, est. speed input: 113935.15 toks/s, output: 111.26 toks/s]
Processed prompts:  58%|█████▊    | 4754/8192 [00:43<00:47, 71.65it/s, est. speed input: 113083.70 toks/s, output: 110.43 toks/s]
Processed prompts:  59%|█████▉    | 4818/8192 [00:43<00:47, 71.48it/s, est. speed input: 112258.39 toks/s, output: 109.63 toks/s]
Processed prompts:  60%|█████▉    | 4882/8192 [00:44<00:46, 71.63it/s, est. speed input: 111493.55 toks/s, output: 108.88 toks/s]
Processed prompts:  60%|██████    | 4946/8192 [00:45<00:45, 71.63it/s, est. speed input: 110748.42 toks/s, output: 108.15 toks/s]
Processed prompts:  61%|██████    | 5010/8192 [00:46<00:44, 71.48it/s, est. speed input: 110016.88 toks/s, output: 107.44 toks/s]
Processed prompts:  62%|██████▏   | 5074/8192 [00:47<00:43, 71.33it/s, est. speed input: 109309.08 toks/s, output: 106.75 toks/s]
Processed prompts:  63%|██████▎   | 5138/8192 [00:48<00:42, 71.25it/s, est. speed input: 108629.87 toks/s, output: 106.08 toks/s]
Processed prompts:  64%|██████▎   | 5202/8192 [00:49<00:41, 71.41it/s, est. speed input: 107995.21 toks/s, output: 105.46 toks/s]
Processed prompts:  64%|██████▍   | 5266/8192 [00:50<00:40, 71.47it/s, est. speed input: 107377.66 toks/s, output: 104.86 toks/s]
Processed prompts:  65%|██████▌   | 5330/8192 [00:51<00:40, 71.37it/s, est. speed input: 106770.39 toks/s, output: 104.27 toks/s]
Processed prompts:  66%|██████▌   | 5394/8192 [00:52<00:39, 71.38it/s, est. speed input: 106190.42 toks/s, output: 103.70 toks/s]
Processed prompts:  67%|██████▋   | 5458/8192 [00:52<00:38, 71.36it/s, est. speed input: 105627.71 toks/s, output: 103.15 toks/s]
Processed prompts:  67%|██████▋   | 5522/8192 [00:53<00:37, 71.50it/s, est. speed input: 105096.48 toks/s, output: 102.63 toks/s]
Processed prompts:  68%|██████▊   | 5586/8192 [00:54<00:36, 71.33it/s, est. speed input: 104561.21 toks/s, output: 102.11 toks/s]
Processed prompts:  69%|██████▉   | 5650/8192 [00:55<00:35, 71.21it/s, est. speed input: 104043.06 toks/s, output: 101.60 toks/s]
Processed prompts:  70%|██████▉   | 5714/8192 [00:56<00:34, 71.10it/s, est. speed input: 103539.61 toks/s, output: 101.11 toks/s]
Processed prompts:  71%|███████   | 5778/8192 [00:57<00:33, 71.07it/s, est. speed input: 103055.43 toks/s, output: 100.64 toks/s]
Processed prompts:  71%|███████▏  | 5842/8192 [00:58<00:33, 71.10it/s, est. speed input: 102589.93 toks/s, output: 100.19 toks/s]
Processed prompts:  72%|███████▏  | 5906/8192 [00:59<00:32, 71.37it/s, est. speed input: 102156.96 toks/s, output: 99.76 toks/s] 
Processed prompts:  73%|███████▎  | 5970/8192 [01:00<00:31, 71.58it/s, est. speed input: 101738.07 toks/s, output: 99.35 toks/s]
Processed prompts:  74%|███████▎  | 6034/8192 [01:00<00:30, 71.53it/s, est. speed input: 101317.82 toks/s, output: 98.94 toks/s]
Processed prompts:  74%|███████▍  | 6098/8192 [01:01<00:29, 71.46it/s, est. speed input: 100907.14 toks/s, output: 98.54 toks/s]
Processed prompts:  75%|███████▌  | 6162/8192 [01:02<00:28, 71.32it/s, est. speed input: 100502.33 toks/s, output: 98.15 toks/s]
Processed prompts:  76%|███████▌  | 6226/8192 [01:03<00:27, 71.55it/s, est. speed input: 100130.13 toks/s, output: 97.78 toks/s]
Processed prompts:  77%|███████▋  | 6290/8192 [01:04<00:26, 71.48it/s, est. speed input: 99753.64 toks/s, output: 97.42 toks/s] 
Processed prompts:  78%|███████▊  | 6354/8192 [01:05<00:25, 71.34it/s, est. speed input: 99381.29 toks/s, output: 97.05 toks/s]
Processed prompts:  78%|███████▊  | 6418/8192 [01:06<00:24, 71.26it/s, est. speed input: 99020.49 toks/s, output: 96.70 toks/s]
Processed prompts:  79%|███████▉  | 6482/8192 [01:07<00:24, 71.24it/s, est. speed input: 98671.69 toks/s, output: 96.36 toks/s]
Processed prompts:  80%|███████▉  | 6546/8192 [01:08<00:23, 71.19it/s, est. speed input: 98329.35 toks/s, output: 96.02 toks/s]
Processed prompts:  81%|████████  | 6610/8192 [01:09<00:22, 71.18it/s, est. speed input: 97998.13 toks/s, output: 95.70 toks/s]
Processed prompts:  81%|████████▏ | 6674/8192 [01:09<00:21, 71.15it/s, est. speed input: 97673.31 toks/s, output: 95.38 toks/s]
Processed prompts:  82%|████████▏ | 6738/8192 [01:10<00:20, 71.16it/s, est. speed input: 97358.88 toks/s, output: 95.08 toks/s]
Processed prompts:  83%|████████▎ | 6802/8192 [01:11<00:19, 71.23it/s, est. speed input: 97056.33 toks/s, output: 94.78 toks/s]
Processed prompts:  84%|████████▍ | 6866/8192 [01:12<00:18, 71.29it/s, est. speed input: 96761.40 toks/s, output: 94.49 toks/s]
Processed prompts:  85%|████████▍ | 6930/8192 [01:13<00:17, 71.34it/s, est. speed input: 96474.15 toks/s, output: 94.21 toks/s]
Processed prompts:  85%|████████▌ | 6994/8192 [01:14<00:16, 71.24it/s, est. speed input: 96186.33 toks/s, output: 93.93 toks/s]
Processed prompts:  86%|████████▌ | 7058/8192 [01:15<00:15, 71.51it/s, est. speed input: 95923.55 toks/s, output: 93.68 toks/s]
Processed prompts:  87%|████████▋ | 7122/8192 [01:16<00:14, 71.72it/s, est. speed input: 95668.04 toks/s, output: 93.43 toks/s]
Processed prompts:  88%|████████▊ | 7186/8192 [01:17<00:14, 71.51it/s, est. speed input: 95399.97 toks/s, output: 93.16 toks/s]
Processed prompts:  89%|████████▊ | 7250/8192 [01:18<00:13, 71.58it/s, est. speed input: 95149.32 toks/s, output: 92.92 toks/s]
Processed prompts:  89%|████████▉ | 7314/8192 [01:18<00:12, 71.44it/s, est. speed input: 94894.87 toks/s, output: 92.67 toks/s]
Processed prompts:  90%|█████████ | 7378/8192 [01:19<00:11, 71.29it/s, est. speed input: 94643.58 toks/s, output: 92.43 toks/s]
Processed prompts:  91%|█████████ | 7442/8192 [01:20<00:10, 71.41it/s, est. speed input: 94408.49 toks/s, output: 92.20 toks/s]
Processed prompts:  92%|█████████▏| 7506/8192 [01:21<00:09, 71.43it/s, est. speed input: 94175.65 toks/s, output: 91.97 toks/s]
Processed prompts:  92%|█████████▏| 7570/8192 [01:22<00:08, 71.40it/s, est. speed input: 93945.90 toks/s, output: 91.74 toks/s]
Processed prompts:  93%|█████████▎| 7634/8192 [01:23<00:07, 71.58it/s, est. speed input: 93730.63 toks/s, output: 91.53 toks/s]
Processed prompts:  94%|█████████▍| 7698/8192 [01:24<00:06, 71.72it/s, est. speed input: 93520.05 toks/s, output: 91.33 toks/s]
Processed prompts:  95%|█████████▍| 7762/8192 [01:25<00:06, 71.58it/s, est. speed input: 93303.68 toks/s, output: 91.12 toks/s]
Processed prompts:  96%|█████████▌| 7826/8192 [01:26<00:05, 71.53it/s, est. speed input: 93093.41 toks/s, output: 90.91 toks/s]
Processed prompts:  96%|█████████▋| 7890/8192 [01:26<00:04, 71.42it/s, est. speed input: 92884.60 toks/s, output: 90.71 toks/s]
Processed prompts:  97%|█████████▋| 7954/8192 [01:27<00:03, 71.53it/s, est. speed input: 92688.05 toks/s, output: 90.52 toks/s]
Processed prompts:  98%|█████████▊| 8018/8192 [01:28<00:02, 71.50it/s, est. speed input: 92490.62 toks/s, output: 90.32 toks/s]
Processed prompts:  99%|█████████▊| 8082/8192 [01:29<00:01, 71.32it/s, est. speed input: 92290.28 toks/s, output: 90.13 toks/s]
Processed prompts:  99%|█████████▉| 8146/8192 [01:30<00:00, 77.55it/s, est. speed input: 92344.73 toks/s, output: 90.18 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:30<00:00, 77.55it/s, est. speed input: 92865.56 toks/s, output: 90.69 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:30<00:00, 90.69it/s, est. speed input: 92865.56 toks/s, output: 90.69 toks/s]
[rank0]:[W126 18:37:13.146039866 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

